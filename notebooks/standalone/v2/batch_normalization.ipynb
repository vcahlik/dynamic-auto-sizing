{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23284cdb-b6aa-41da-a0d5-c140938079c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Oct 26 15:44:57 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.103.01   Driver Version: 470.103.01   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM...  On   | 00000000:01:00.0 Off |                    0 |\n",
      "| N/A   53C    P0   160W / 275W |  23162MiB / 40536MiB |     99%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM...  On   | 00000000:47:00.0 Off |                    0 |\n",
      "| N/A   54C    P0   269W / 275W |  37398MiB / 40536MiB |    100%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-SXM...  On   | 00000000:81:00.0 Off |                    0 |\n",
      "| N/A   45C    P0    57W / 275W |   9718MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA DGX Display  On   | 00000000:C1:00.0 Off |                  N/A |\n",
      "| 33%   36C    P8    N/A /  50W |     14MiB /  3911MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA A100-SXM...  On   | 00000000:C2:00.0 Off |                    0 |\n",
      "| N/A   49C    P0    96W / 275W |  35877MiB / 40536MiB |     19%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   3123534      C   Unknown Error                    1953MiB |\n",
      "|    0   N/A  N/A   3160224      C   Unknown Error                   20021MiB |\n",
      "|    0   N/A  N/A   3749212      C   Unknown Error                    1183MiB |\n",
      "|    1   N/A  N/A   3689572      C   Unknown Error                   37323MiB |\n",
      "|    2   N/A  N/A   3591610      C   ...ic-auto-sizing/bin/python     9715MiB |\n",
      "|    3   N/A  N/A      5111      G   Unknown Error                       9MiB |\n",
      "|    3   N/A  N/A      5392      G   Unknown Error                       2MiB |\n",
      "|    4   N/A  N/A   2708819      C   Unknown Error                   33701MiB |\n",
      "|    4   N/A  N/A   3749212      C   Unknown Error                    2171MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f992d9a1-faa6-45b6-acf8-facb1a4a606e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-26 15:44:58.453648: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-26 15:44:58.613384: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-26 15:44:59.233193: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-26 15:44:59.233257: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-26 15:44:59.233262: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from enum import Enum\n",
    "import imageio\n",
    "import os\n",
    "import hashlib\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "dtype = 'float32'\n",
    "tf.keras.backend.set_floatx(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d775242d-75c7-4266-9de3-0d351ca3ad8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### datasets.py\n",
    "\n",
    "def get_dataset_sample(X, y, fraction, seed=None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)  # Set random seed\n",
    "    selection = np.random.choice([True, False], len(X), p=[fraction, 1 - fraction])\n",
    "    if seed is not None:\n",
    "        np.random.seed()  # Unset random seed\n",
    "    X_sampled = X[selection]\n",
    "    y_sampled = y[selection]\n",
    "    return X_sampled, y_sampled\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, X_train, y_train, X_test, y_test, shape, shape_flattened, fraction, vision=True,\n",
    "                 standardize=True):\n",
    "        if fraction is not None:\n",
    "            X_train, y_train = get_dataset_sample(X_train, y_train, fraction, seed=42)\n",
    "            X_test, y_test = get_dataset_sample(X_test, y_test, fraction, seed=42)\n",
    "\n",
    "        X_train = X_train.astype(dtype)\n",
    "        y_train = y_train.astype(dtype)\n",
    "        X_test = X_test.astype(dtype)\n",
    "        y_test = y_test.astype(dtype)\n",
    "\n",
    "        if vision:\n",
    "            X_train = X_train / 255.0\n",
    "            X_test = X_test / 255.0\n",
    "\n",
    "        X_train = np.reshape(X_train, shape_flattened)\n",
    "        X_test = np.reshape(X_test, shape_flattened)\n",
    "\n",
    "        X = np.concatenate((X_train, X_test), axis=0)\n",
    "        y = np.concatenate((y_train, y_test), axis=0)\n",
    "\n",
    "        if standardize:\n",
    "            from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "            scaler = StandardScaler()\n",
    "            scaler.fit(X_train)  # Scaling each feature independently\n",
    "\n",
    "            X_norm = scaler.transform(X)\n",
    "            del X\n",
    "            X_train_norm = scaler.transform(X_train)\n",
    "            del X_train\n",
    "            X_test_norm = scaler.transform(X_test)\n",
    "            del X_test\n",
    "        else:\n",
    "            X_norm = X\n",
    "            X_train_norm = X_train\n",
    "            X_test_norm = X_test\n",
    "\n",
    "        X_norm = np.reshape(X_norm, shape)\n",
    "        X_train_norm = np.reshape(X_train_norm, shape)\n",
    "        X_test_norm = np.reshape(X_test_norm, shape)\n",
    "\n",
    "        # Shuffle X_norm and y\n",
    "        assert len(X_norm) == len(y)\n",
    "        p = np.random.permutation(len(X_norm))\n",
    "        X_norm, y = X_norm[p], y[p]\n",
    "\n",
    "        self.X_norm = X_norm\n",
    "        self.y = y\n",
    "        self.X_train_norm = X_train_norm\n",
    "        self.y_train = y_train\n",
    "        self.X_test_norm = X_test_norm\n",
    "        self.y_test = y_test\n",
    "\n",
    "\n",
    "def get_cifar_10_dataset(fraction=None):\n",
    "    cifar10 = tf.keras.datasets.cifar10\n",
    "    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "    shape = (-1, 32, 32, 3)\n",
    "    shape_flattened = (-1, 3072)  # Scaling each feature independently\n",
    "    return Dataset(X_train, y_train, X_test, y_test, shape=shape, shape_flattened=shape_flattened, fraction=fraction)\n",
    "\n",
    "\n",
    "def get_cifar_100_dataset(fraction=None):\n",
    "    cifar100 = tf.keras.datasets.cifar100\n",
    "    (X_train, y_train), (X_test, y_test) = cifar100.load_data()\n",
    "    shape = (-1, 32, 32, 3)\n",
    "    shape_flattened = (-1, 3072)  # Scaling each feature independently\n",
    "    return Dataset(X_train, y_train, X_test, y_test, shape=shape, shape_flattened=shape_flattened, fraction=fraction)\n",
    "\n",
    "\n",
    "def get_svhn_dataset(fraction=None):\n",
    "    from urllib.request import urlretrieve\n",
    "    from scipy import io\n",
    "\n",
    "    train_filename, _ = urlretrieve('http://ufldl.stanford.edu/housenumbers/train_32x32.mat')\n",
    "    test_filename, _ = urlretrieve('http://ufldl.stanford.edu/housenumbers/test_32x32.mat')\n",
    "\n",
    "    X_train = io.loadmat(train_filename, variable_names='X').get('X')\n",
    "    y_train = io.loadmat(train_filename, variable_names='y').get('y')\n",
    "    X_test = io.loadmat(test_filename, variable_names='X').get('X')\n",
    "    y_test = io.loadmat(test_filename, variable_names='y').get('y')\n",
    "\n",
    "    X_train = np.moveaxis(X_train, -1, 0)\n",
    "    y_train -= 1\n",
    "    X_test = np.moveaxis(X_test, -1, 0)\n",
    "    y_test -= 1\n",
    "\n",
    "    shape = (-1, 32, 32, 3)\n",
    "    shape_flattened = (-1, 3072)  # Scaling each feature independently\n",
    "    return Dataset(X_train, y_train, X_test, y_test, shape=shape, shape_flattened=shape_flattened, fraction=fraction)\n",
    "\n",
    "\n",
    "def get_tiny_imagenet_dataset(fraction=None):\n",
    "    \"\"\"\n",
    "    Original source: https://github.com/sonugiri1043/Train_ResNet_On_Tiny_ImageNet/blob/master/Train_ResNet_On_Tiny_ImageNet.ipynb\n",
    "    Original author: sonugiri1043@gmail.com\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.isdir('IMagenet'):\n",
    "        os.system('git clone https://github.com/seshuad/IMagenet')\n",
    "\n",
    "    print(\"Processing the downloaded dataset...\")\n",
    "\n",
    "    path = 'IMagenet/tiny-imagenet-200/'\n",
    "\n",
    "    id_dict = {}\n",
    "    for i, line in enumerate(open(path + 'wnids.txt', 'r')):\n",
    "        id_dict[line.replace('\\n', '')] = i\n",
    "\n",
    "    train_data = list()\n",
    "    test_data = list()\n",
    "    train_labels = list()\n",
    "    test_labels = list()\n",
    "\n",
    "    for key, value in id_dict.items():\n",
    "        train_data += [imageio.imread(path + 'train/{}/images/{}_{}.JPEG'.format(key, key, str(i)), pilmode='RGB') for i\n",
    "                       in range(500)]\n",
    "        train_labels_ = np.array([[0] * 200] * 500)\n",
    "        train_labels_[:, value] = 1\n",
    "        train_labels += train_labels_.tolist()\n",
    "\n",
    "    X_train = np.array(train_data)\n",
    "    X_test = np.array(test_data)\n",
    "    del train_data, train_labels_\n",
    "\n",
    "    for line in open(path + 'val/val_annotations.txt'):\n",
    "        img_name, class_id = line.split('\\t')[:2]\n",
    "        test_data.append(imageio.imread(path + 'val/images/{}'.format(img_name), pilmode='RGB'))\n",
    "        test_labels_ = np.array([[0] * 200])\n",
    "        test_labels_[0, id_dict[class_id]] = 1\n",
    "        test_labels += test_labels_.tolist()\n",
    "\n",
    "    y_train = np.argmax(np.array(train_labels), axis=1)\n",
    "    y_test = np.argmax(np.array(test_labels), axis=1)\n",
    "    del train_labels\n",
    "    del test_data, test_labels_, test_labels\n",
    "\n",
    "    shape = (-1, 64, 64, 3)\n",
    "    shape_flattened = (-1, 12288)  # Scaling each feature independently\n",
    "    print(\"Calling Dataset()\")\n",
    "    return Dataset(X_train, y_train, X_test, y_test, shape=shape, shape_flattened=shape_flattened, fraction=fraction)\n",
    "\n",
    "\n",
    "def get_mnist_dataset(fraction=None):\n",
    "    mnist = tf.keras.datasets.mnist\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "    shape = (-1, 28, 28, 1)\n",
    "    shape_flattened = (-1, 1)  # Scaling all features together\n",
    "    return Dataset(X_train, y_train, X_test, y_test, shape=shape, shape_flattened=shape_flattened, fraction=fraction)\n",
    "\n",
    "\n",
    "def get_fashion_mnist_dataset(fraction=None):\n",
    "    fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "    (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "    shape = (-1, 28, 28, 1)\n",
    "    shape_flattened = (-1, 1)  # Scaling all features together\n",
    "    return Dataset(X_train, y_train, X_test, y_test, shape=shape, shape_flattened=shape_flattened, fraction=fraction)\n",
    "\n",
    "\n",
    "def get_fifteen_puzzle_dataset(path=None, fraction=None):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    if path is None:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/gdrive')\n",
    "        path = 'gdrive/MyDrive/15-costs-v3.csv'\n",
    "    costs = pd.read_csv(path)\n",
    "    costs = costs.sample(frac=fraction, random_state=42)\n",
    "\n",
    "    X_raw = costs.iloc[:, :-1].values\n",
    "    y = costs['cost'].values\n",
    "    X = np.apply_along_axis(lambda x: np.eye(16)[x].ravel(), 1, X_raw)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    del X, X_raw, y\n",
    "\n",
    "    shape = (-1, 256)\n",
    "    shape_flattened = (-1, 256)  # Scaling all features together\n",
    "    return Dataset(X_train, y_train, X_test, y_test, shape=shape, shape_flattened=shape_flattened, vision=False,\n",
    "                   fraction=None)\n",
    "\n",
    "##### models.py\n",
    "\n",
    "dtype = 'float32'\n",
    "tf.keras.backend.set_floatx(dtype)\n",
    "\n",
    "\n",
    "class Regularizer(tf.keras.regularizers.Regularizer):\n",
    "    def __init__(self):\n",
    "        self.n_new_neurons = 0\n",
    "        self.scaling_tensor = None\n",
    "        self.set_regularization_penalty(0.)\n",
    "        self.set_regularization_method(None)\n",
    "\n",
    "    def copy(self):\n",
    "        regularizer_copy = Regularizer.__new__(Regularizer)\n",
    "        regularizer_copy.n_new_neurons = self.n_new_neurons\n",
    "        regularizer_copy.scaling_tensor = self.scaling_tensor\n",
    "        regularizer_copy.set_regularization_penalty(self.regularization_penalty)\n",
    "        regularizer_copy.set_regularization_method(self.regularization_method)\n",
    "        return regularizer_copy\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if self.regularization_method is None or self.regularization_penalty == 0:\n",
    "            return 0\n",
    "        elif self.regularization_method == 'weighted_l1':\n",
    "            return self.weighted_l1(x)\n",
    "        elif self.regularization_method == 'weighted_l1_reordered':\n",
    "            return self.weighted_l1_reordered(x)\n",
    "        elif self.regularization_method == 'group_sparsity':\n",
    "            return self.group_sparsity(x)\n",
    "        elif self.regularization_method == 'l1':\n",
    "            return self.l1(x)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Unknown regularization method {self.regularization_method}\")\n",
    "\n",
    "    def weighted_l1(self, x):\n",
    "        # I.e. for a parameter matrix of 4 input and 10 output neurons:\n",
    "        #\n",
    "        # [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
    "        #  [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
    "        #  [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
    "        #  [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]\n",
    "        #\n",
    "        # the scaling tensor, as well as the resulting weighted values, could be:\n",
    "        #\n",
    "        # [[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n",
    "        #  [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n",
    "        #  [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n",
    "        #  [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]]\n",
    "        #\n",
    "        # Therefore every additional output neuron is regularized more.\n",
    "\n",
    "        scaling_tensor = tf.cumsum(tf.constant(self.regularization_penalty, shape=x.shape, dtype=dtype), axis=-1)\n",
    "        weighted_values = scaling_tensor * tf.abs(x)\n",
    "        return tf.reduce_sum(weighted_values)\n",
    "\n",
    "    def weighted_l1_reordered(self, x):\n",
    "        if self.update_scaling_tensor:\n",
    "            scaling_tensor_raw = tf.cumsum(tf.constant(self.regularization_penalty, shape=x.shape, dtype=dtype),\n",
    "                                           axis=-1)\n",
    "\n",
    "            scaling_tensor_old_neurons = scaling_tensor_raw[:, :-self.n_new_neurons]\n",
    "            scaling_tensor_new_neurons = scaling_tensor_raw[:, -self.n_new_neurons:]\n",
    "            scaling_tensor_old_neurons_shuffled = tf.transpose(\n",
    "                tf.random.shuffle(tf.transpose(scaling_tensor_old_neurons)))\n",
    "            self.scaling_tensor = tf.concat([scaling_tensor_old_neurons_shuffled, scaling_tensor_new_neurons], axis=-1)\n",
    "            self.update_scaling_tensor = False\n",
    "\n",
    "        weighted_values = self.scaling_tensor * tf.abs(x)\n",
    "        return tf.reduce_sum(weighted_values)\n",
    "\n",
    "    def group_sparsity(self, x):\n",
    "        # I.e. for a parameter matrix of 3 input and 5 output neurons:\n",
    "        #\n",
    "        # [[1., 1., 1., 1., 1.],\n",
    "        #  [1., 2., 2., 1., 2.],\n",
    "        #  [2., 2., 3., 1., 3.]]\n",
    "        #\n",
    "        # The resulting vector of group norms is [2., 2., 3., 1., 3.], therefore for\n",
    "        # every output neuron, its incoming connections form a group.\n",
    "\n",
    "        group_norms = tf.norm(x, ord=2, axis=0)\n",
    "        # assert group_norms.shape[0] == x.shape[1]\n",
    "        return self.regularization_penalty * tf.reduce_sum(group_norms)\n",
    "\n",
    "    def l1(self, x):\n",
    "        weighted_values = self.regularization_penalty * tf.abs(x)\n",
    "        return tf.reduce_sum(weighted_values)\n",
    "\n",
    "    def prune(self):\n",
    "        self.n_new_neurons = 0\n",
    "        if self.regularization_method == 'weighted_l1_reordered':\n",
    "            self.update_scaling_tensor = True\n",
    "\n",
    "    def grow(self, n_new_neurons):\n",
    "        self.n_new_neurons = n_new_neurons\n",
    "        if self.regularization_method == 'weighted_l1_reordered':\n",
    "            self.update_scaling_tensor = True\n",
    "\n",
    "    def set_regularization_penalty(self, regularization_penalty):\n",
    "        self.regularization_penalty = regularization_penalty\n",
    "\n",
    "    def set_regularization_method(self, regularization_method):\n",
    "        self.regularization_method = regularization_method\n",
    "        if self.regularization_method == 'weighted_l1_reordered':\n",
    "            self.update_scaling_tensor = True\n",
    "        else:\n",
    "            self.update_scaling_tensor = None\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'regularization_penalty': float(self.regularization_penalty)}\n",
    "\n",
    "\n",
    "class DASLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_shape, fixed_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self._input_shape = input_shape\n",
    "        self.fixed_size = fixed_size\n",
    "        self._built = False\n",
    "\n",
    "\n",
    "class Dense(DASLayer):\n",
    "    def __init__(self, units, activation, kernel_initializer='glorot_uniform',\n",
    "                 bias_initializer='zeros', input_shape=None, fixed_size=False,\n",
    "                 regularizer=None):\n",
    "        super().__init__(input_shape, fixed_size)\n",
    "\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.bias_initializer = bias_initializer\n",
    "\n",
    "        self.A = tf.keras.activations.get(activation)\n",
    "        self.W_init = tf.keras.initializers.get(kernel_initializer)\n",
    "        self.b_init = tf.keras.initializers.get(bias_initializer)\n",
    "        if regularizer is not None:\n",
    "            self.regularizer = regularizer\n",
    "        else:\n",
    "            self.regularizer = Regularizer()\n",
    "\n",
    "    def copy(self):\n",
    "        layer_copy = Dense.__new__(Dense)\n",
    "        super(Dense, layer_copy).__init__(self._input_shape)\n",
    "\n",
    "        layer_copy.units = self.units\n",
    "        layer_copy.activation = self.activation\n",
    "        layer_copy.kernel_initializer = self.kernel_initializer\n",
    "        layer_copy.bias_initializer = self.bias_initializer\n",
    "        layer_copy.fixed_size = self.fixed_size\n",
    "\n",
    "        layer_copy.A = self.A\n",
    "        layer_copy.W_init = self.W_init\n",
    "        layer_copy.b_init = self.b_init\n",
    "        layer_copy.regularizer = self.regularizer.copy()\n",
    "\n",
    "        layer_copy.W = tf.Variable(\n",
    "            name='W',\n",
    "            initial_value=self.W,\n",
    "            trainable=True)\n",
    "\n",
    "        layer_copy.b = tf.Variable(\n",
    "            name='b',\n",
    "            initial_value=self.b,\n",
    "            trainable=True)\n",
    "\n",
    "        layer_copy.add_regularizer_loss()\n",
    "\n",
    "        layer_copy._built = True\n",
    "        return layer_copy\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if self._built:\n",
    "            return\n",
    "\n",
    "        input_units = input_shape[-1]\n",
    "\n",
    "        self.W = tf.Variable(\n",
    "            name='W',\n",
    "            initial_value=self.W_init(shape=(input_units, self.units), dtype=dtype),\n",
    "            trainable=True)\n",
    "\n",
    "        self.b = tf.Variable(\n",
    "            name='b',\n",
    "            initial_value=self.b_init(shape=(self.units,), dtype=dtype),\n",
    "            trainable=True)\n",
    "\n",
    "        self.add_regularizer_loss()\n",
    "\n",
    "        self._built = True\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        return self.A(tf.matmul(inputs, self.W) + self.b)\n",
    "\n",
    "    def add_regularizer_loss(self):\n",
    "        self.add_loss(lambda: self.regularizer(tf.concat([self.W, tf.reshape(self.b, (1, -1))], axis=0)))\n",
    "\n",
    "    def get_size(self):\n",
    "        return self.W.shape[0], self.W.shape[1]\n",
    "\n",
    "    def prune(self, threshold, active_input_units_indices):\n",
    "        # Remove connections from pruned units in previous layer\n",
    "        new_W = tf.gather(self.W.value(), active_input_units_indices, axis=0)\n",
    "\n",
    "        if self.fixed_size:\n",
    "            active_output_neurons_indices = list(range(new_W.shape[1]))\n",
    "        else:\n",
    "            # Prune units in this layer\n",
    "            weights_with_biases = tf.concat([new_W, tf.reshape(self.b.value(), (1, -1))], axis=0)\n",
    "            neurons_are_active = tf.math.reduce_max(tf.abs(weights_with_biases), axis=0) >= threshold\n",
    "            active_output_neurons_indices = tf.reshape(tf.where(neurons_are_active), (-1,))\n",
    "\n",
    "            new_W = tf.gather(new_W, active_output_neurons_indices, axis=1)\n",
    "            new_b = tf.gather(self.b.value(), active_output_neurons_indices, axis=0)\n",
    "\n",
    "            self.b = tf.Variable(name='b', initial_value=new_b, trainable=True)\n",
    "\n",
    "        self.W = tf.Variable(name='W', initial_value=new_W, trainable=True)\n",
    "\n",
    "        self.regularizer.prune()\n",
    "        return active_output_neurons_indices\n",
    "\n",
    "    def grow(self, n_new_input_units, percentage, min_new_units, scaling_factor):\n",
    "        if n_new_input_units > 0:\n",
    "            # Add connections to grown units in previous layer\n",
    "            W_growth = self.W_init(shape=(self.W.shape[0] + n_new_input_units, self.W.shape[1]), dtype=dtype)[\n",
    "                       -n_new_input_units:,\n",
    "                       :] * scaling_factor  # TODO is it better to be multiplying here by scaling_factor? It does help with not increasing the max weights of existing neurons when new neurons are added.\n",
    "            new_W = tf.concat([self.W.value(), W_growth], axis=0)\n",
    "        else:\n",
    "            new_W = self.W.value()\n",
    "\n",
    "        if self.fixed_size:\n",
    "            n_new_output_units = 0\n",
    "        else:\n",
    "            # Grow new units in this layer\n",
    "            n_new_output_units = max(min_new_units, int(new_W.shape[1] * percentage))\n",
    "            if n_new_output_units > 0:\n",
    "                W_growth = self.W_init(shape=(new_W.shape[0], new_W.shape[1] + n_new_output_units), dtype=dtype)[:,\n",
    "                           -n_new_output_units:] * scaling_factor\n",
    "                b_growth = self.b_init(shape=(n_new_output_units,),\n",
    "                                       dtype=dtype)  # TODO for all possible bias initializers to work properly, the whole bias vector should be initialized at once\n",
    "                new_W = tf.concat([new_W, W_growth], axis=1)\n",
    "                new_b = tf.concat([self.b.value(), b_growth], axis=0)\n",
    "\n",
    "                self.b = tf.Variable(name='b', initial_value=new_b, trainable=True)\n",
    "\n",
    "        self.W = tf.Variable(name='W', initial_value=new_W, trainable=True)\n",
    "\n",
    "        self.regularizer.grow(n_new_output_units)\n",
    "        return n_new_output_units\n",
    "\n",
    "    def mutate(self, mutation_strength):\n",
    "        self.W.assign_add(tf.random.normal(self.W.shape, mean=0.0, stddev=mutation_strength))\n",
    "        self.b.assign_add(tf.random.normal(self.b.shape, mean=0.0, stddev=mutation_strength))\n",
    "\n",
    "    def set_regularization_penalty(self, regularization_penalty):\n",
    "        if not self.fixed_size:\n",
    "            self.regularizer.set_regularization_penalty(regularization_penalty)\n",
    "\n",
    "    def set_regularization_method(self, regularization_method):\n",
    "        if not self.fixed_size:\n",
    "            self.regularizer.set_regularization_method(regularization_method)\n",
    "\n",
    "    def get_param_string(self):\n",
    "        param_string = \"\"\n",
    "        weights_with_bias = tf.concat([self.W, tf.reshape(self.b, (1, -1))], axis=0)\n",
    "        max_parameters = tf.math.reduce_max(tf.abs(weights_with_bias), axis=0).numpy()\n",
    "        magnitudes = np.floor(np.log10(max_parameters))\n",
    "        for m in magnitudes:\n",
    "            if m > 0:\n",
    "                m = 0\n",
    "            param_string += str(int(-m))\n",
    "        return param_string\n",
    "\n",
    "\n",
    "class Conv2D(DASLayer):\n",
    "    def __init__(self, filters, filter_size, activation, strides=(1, 1),\n",
    "                 padding='SAME', kernel_initializer='glorot_uniform',\n",
    "                 bias_initializer='zeros', input_shape=None, fixed_size=False,\n",
    "                 regularizer=None):\n",
    "        super().__init__(input_shape, fixed_size)\n",
    "\n",
    "        self.filters = filters\n",
    "        self.filter_size = filter_size\n",
    "        self.activation = activation\n",
    "        self.strides = strides\n",
    "        self.padding = padding\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.bias_initializer = bias_initializer\n",
    "\n",
    "        self.A = tf.keras.activations.get(activation)\n",
    "        self.F_init = tf.keras.initializers.get(kernel_initializer)\n",
    "        self.b_init = tf.keras.initializers.get(bias_initializer)\n",
    "        if regularizer is not None:\n",
    "            self.regularizer = regularizer\n",
    "        else:\n",
    "            self.regularizer = Regularizer()\n",
    "\n",
    "    def copy(self):\n",
    "        layer_copy = Conv2D.__new__(Conv2D)\n",
    "        super(Conv2D, layer_copy).__init__(self._input_shape)\n",
    "\n",
    "        layer_copy.filters = self.filters\n",
    "        layer_copy.filter_size = self.filter_size\n",
    "        layer_copy.activation = self.activation\n",
    "        layer_copy.strides = self.strides\n",
    "        layer_copy.padding = self.padding\n",
    "        layer_copy.kernel_initializer = self.kernel_initializer\n",
    "        layer_copy.bias_initializer = self.bias_initializer\n",
    "        layer_copy.fixed_size = self.fixed_size\n",
    "\n",
    "        layer_copy.A = self.A\n",
    "        layer_copy.F_init = self.F_init\n",
    "        layer_copy.b_init = self.b_init\n",
    "        layer_copy.regularizer = self.regularizer.copy()\n",
    "\n",
    "        layer_copy.F = tf.Variable(\n",
    "            name='F',\n",
    "            initial_value=self.F,\n",
    "            trainable=True)\n",
    "\n",
    "        layer_copy.b = tf.Variable(\n",
    "            name='b',\n",
    "            initial_value=self.b,\n",
    "            trainable=True)\n",
    "\n",
    "        layer_copy.add_regularizer_loss()\n",
    "\n",
    "        layer_copy._built = True\n",
    "        return layer_copy\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if self._built:\n",
    "            return\n",
    "\n",
    "        input_filters = input_shape[-1]\n",
    "\n",
    "        self.F = tf.Variable(\n",
    "            name='F',\n",
    "            initial_value=self.F_init(\n",
    "                shape=(self.filter_size[0], self.filter_size[1], input_filters, self.filters), dtype=dtype\n",
    "            ),\n",
    "            trainable=True)\n",
    "\n",
    "        self.b = tf.Variable(\n",
    "            name='b',\n",
    "            initial_value=self.b_init(shape=(self.filters,), dtype=dtype),\n",
    "            trainable=True)\n",
    "\n",
    "        self.add_regularizer_loss()\n",
    "\n",
    "        self._built = True\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        y = tf.nn.conv2d(inputs, self.F, strides=self.strides, padding=self.padding)\n",
    "        y = tf.nn.bias_add(y, self.b)\n",
    "        y = self.A(y)\n",
    "        return y\n",
    "\n",
    "    def add_regularizer_loss(self):\n",
    "        self.add_loss(lambda: self.regularizer(\n",
    "            tf.concat([tf.reshape(self.F, (-1, self.F.shape[-1])), tf.reshape(self.b, (1, -1))], axis=0)))\n",
    "\n",
    "    def get_size(self):\n",
    "        return self.F.shape[-2], self.F.shape[-1]\n",
    "\n",
    "    def prune(self, threshold, active_input_units_indices):\n",
    "        # Remove connections from pruned units in previous layer\n",
    "        new_F = tf.gather(self.F.value(), active_input_units_indices, axis=-2)\n",
    "\n",
    "        if self.fixed_size:\n",
    "            active_output_filters_indices = list(range(new_F.shape[-1]))\n",
    "        else:\n",
    "            # Prune units in this layer\n",
    "            F_reduced_max = tf.reshape(tf.math.reduce_max(tf.abs(new_F), axis=(0, 1, 2)), (1, -1))\n",
    "            F_reduced_max_with_biases = tf.concat([F_reduced_max, tf.reshape(self.b.value(), (1, -1))], axis=0)\n",
    "            filters_are_active = tf.math.reduce_max(tf.abs(F_reduced_max_with_biases), axis=0) >= threshold\n",
    "            active_output_filters_indices = tf.reshape(tf.where(filters_are_active), (-1,))\n",
    "\n",
    "            new_F = tf.gather(new_F, active_output_filters_indices, axis=-1)\n",
    "            new_b = tf.gather(self.b.value(), active_output_filters_indices, axis=0)\n",
    "\n",
    "            self.b = tf.Variable(name='b', initial_value=new_b, trainable=True)\n",
    "\n",
    "        self.F = tf.Variable(name='F', initial_value=new_F, trainable=True)\n",
    "\n",
    "        self.regularizer.prune()\n",
    "        return active_output_filters_indices\n",
    "\n",
    "    def grow(self, n_new_input_units, percentage, min_new_units, scaling_factor):\n",
    "        if n_new_input_units > 0:\n",
    "            # Add connections to grown units in previous layer\n",
    "            F_growth = self.F_init(\n",
    "                shape=(self.F.shape[0], self.F.shape[1], self.F.shape[2] + n_new_input_units, self.F.shape[3]),\n",
    "                dtype=dtype)[:, :, -n_new_input_units:,\n",
    "                       :] * scaling_factor  # TODO is it better to be multiplying here by scaling_factor? It does help with not increasing the max weights of existing neurons when new neurons are added.\n",
    "            new_F = tf.concat([self.F.value(), F_growth], axis=-2)\n",
    "        else:\n",
    "            new_F = self.F.value()\n",
    "\n",
    "        if self.fixed_size:\n",
    "            n_new_output_units = 0\n",
    "        else:\n",
    "            # Grow new units in this layer\n",
    "            n_new_output_units = max(min_new_units, int(new_F.shape[-1] * percentage))\n",
    "            if n_new_output_units > 0:\n",
    "                F_growth = self.F_init(\n",
    "                    shape=(new_F.shape[0], new_F.shape[1], new_F.shape[2], new_F.shape[3] + n_new_output_units),\n",
    "                    dtype=dtype)[:, :, :, -n_new_output_units:] * scaling_factor\n",
    "                b_growth = self.b_init(shape=(n_new_output_units,),\n",
    "                                       dtype=dtype)  # TODO for all possible bias initializers to work properly, the whole bias vector should be initialized at once\n",
    "                new_F = tf.concat([new_F, F_growth], axis=-1)\n",
    "                new_b = tf.concat([self.b.value(), b_growth], axis=0)\n",
    "\n",
    "                self.b = tf.Variable(name='b', initial_value=new_b, trainable=True)\n",
    "\n",
    "        self.F = tf.Variable(name='F', initial_value=new_F, trainable=True)\n",
    "\n",
    "        self.regularizer.grow(n_new_output_units)\n",
    "        return n_new_output_units\n",
    "\n",
    "    def mutate(self, mutation_strength):\n",
    "        self.F.assign_add(tf.random.normal(self.F.shape, mean=0.0, stddev=mutation_strength))\n",
    "        self.b.assign_add(tf.random.normal(self.b.shape, mean=0.0, stddev=mutation_strength))\n",
    "\n",
    "    def set_regularization_penalty(self, regularization_penalty):\n",
    "        if not self.fixed_size:\n",
    "            self.regularizer.set_regularization_penalty(regularization_penalty)\n",
    "\n",
    "    def set_regularization_method(self, regularization_method):\n",
    "        if not self.fixed_size:\n",
    "            self.regularizer.set_regularization_method(regularization_method)\n",
    "\n",
    "    def get_param_string(self):\n",
    "        param_string = \"\"\n",
    "        # TODO\n",
    "        return param_string\n",
    "\n",
    "\n",
    "class Flatten(tf.keras.layers.Layer):\n",
    "    def call(self, inputs, training=None):\n",
    "        return tf.reshape(tf.transpose(inputs, perm=[0, 3, 1, 2]), (inputs.shape[0], -1))\n",
    "\n",
    "\n",
    "class Sequential(tf.keras.Model):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lrs = layers\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        x = inputs\n",
    "        for layer in self.lrs:\n",
    "            x = layer(x, training=training)\n",
    "        return x\n",
    "\n",
    "    def copy(self):\n",
    "        copied_layers = list()\n",
    "        for layer in self.lrs:\n",
    "            if isinstance(layer, DASLayer):\n",
    "                layer_copy = layer.copy()\n",
    "            else:\n",
    "                layer_copy = copy.deepcopy(layer)\n",
    "            copied_layers.append(layer_copy)\n",
    "\n",
    "        model_copy = Sequential(copied_layers)\n",
    "        return model_copy\n",
    "\n",
    "    def get_layer_input_shape(self, target_layer):\n",
    "        if target_layer._input_shape is not None:\n",
    "            return target_layer._input_shape\n",
    "\n",
    "        input = np.random.normal(size=(1,) + self.lrs[0]._input_shape)\n",
    "        for layer in self.lrs:\n",
    "            if layer is target_layer:\n",
    "                return tuple(input.shape[1:])\n",
    "            input = layer(input)\n",
    "        raise Exception(\"Layer not found in the model.\")\n",
    "\n",
    "    def get_layer_output_shape(self, target_layer):\n",
    "        input = np.random.normal(size=(1,) + self.lrs[0]._input_shape)\n",
    "        for layer in self.lrs:\n",
    "            output = layer(input)\n",
    "            if layer is target_layer:\n",
    "                return tuple(output.shape[1:])\n",
    "            input = output\n",
    "        raise Exception(\"Layer not found in the model.\")\n",
    "\n",
    "    def get_layer_sizes(self):\n",
    "        \"\"\"\n",
    "        Returns the sizes of all layers in the model, including the input and output layer.\n",
    "        \"\"\"\n",
    "        layer_sizes = list()\n",
    "        first_layer = True\n",
    "        for l in range(len(self.lrs)):\n",
    "            layer = self.lrs[l]\n",
    "            if isinstance(layer, DASLayer) and not layer.fixed_size:\n",
    "                layer_size = layer.get_size()\n",
    "                if first_layer:\n",
    "                    layer_sizes.append(layer_size[0])\n",
    "                    first_layer = False\n",
    "                layer_sizes.append(layer_size[1])\n",
    "        return layer_sizes\n",
    "\n",
    "    def get_hidden_layer_sizes(self):\n",
    "        return self.get_layer_sizes()[1:-1]\n",
    "\n",
    "    def get_regularization_penalty(self):\n",
    "        # TODO improve\n",
    "        dense_layers = [l for l in self.lrs if isinstance(l, Dense)]\n",
    "        return dense_layers[-2].regularizer.regularization_penalty\n",
    "\n",
    "    def set_regularization_penalty(self, regularization_penalty):\n",
    "        for layer in self.lrs:\n",
    "            if isinstance(layer, DASLayer) and not layer.fixed_size:\n",
    "                layer.set_regularization_penalty(regularization_penalty)\n",
    "\n",
    "    def set_regularization_method(self, regularization_method):\n",
    "        for layer in self.lrs:\n",
    "            if isinstance(layer, DASLayer) and not layer.fixed_size:\n",
    "                layer.set_regularization_method(regularization_method)\n",
    "\n",
    "    def prune(self, params):\n",
    "        input_shape = self.get_layer_input_shape(self.lrs[0])\n",
    "        n_input_units = input_shape[-1]\n",
    "        active_units_indices = list(range(n_input_units))\n",
    "\n",
    "        last_custom_layer = None\n",
    "        for layer in self.lrs:\n",
    "            if isinstance(layer, DASLayer):\n",
    "                if last_custom_layer is not None and type(last_custom_layer) != type(layer):\n",
    "                    if type(last_custom_layer) == Conv2D and type(layer) == Dense:\n",
    "                        convolutional_shape = self.get_layer_output_shape(last_custom_layer)\n",
    "                        active_units_indices = self.convert_channel_indices_to_flattened_indices(active_units_indices,\n",
    "                                                                                                 convolutional_shape)\n",
    "                    else:\n",
    "                        raise Exception(\"Incorrect order of custom layer types.\")\n",
    "                active_units_indices = layer.prune(params.pruning_threshold, active_units_indices)\n",
    "                last_custom_layer = layer\n",
    "\n",
    "    def grow(self, params):\n",
    "        n_new_units = 0\n",
    "\n",
    "        last_custom_layer = None\n",
    "        for layer in self.lrs:\n",
    "            if isinstance(layer, DASLayer):\n",
    "                if last_custom_layer is not None and type(last_custom_layer) != type(layer):\n",
    "                    if type(last_custom_layer) == Conv2D and type(layer) == Dense:\n",
    "                        convolutional_shape = self.get_layer_output_shape(last_custom_layer)\n",
    "                        n_new_units = n_new_units * convolutional_shape[0] * convolutional_shape[1]\n",
    "                    else:\n",
    "                        raise Exception(\"Incorrect order of custom layer types.\")\n",
    "                n_new_units = layer.grow(n_new_units, params.growth_percentage, min_new_units=params.min_new_neurons,\n",
    "                                         scaling_factor=params.pruning_threshold)\n",
    "                last_custom_layer = layer\n",
    "\n",
    "    def mutate(self, mutation_strength):\n",
    "        for layer in self.lrs:\n",
    "            if isinstance(layer, DASLayer):\n",
    "                layer.mutate(mutation_strength)\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_channel_indices_to_flattened_indices(channel_indices, convolutional_shape):\n",
    "        dense_indices = list()\n",
    "        units_per_channel = convolutional_shape[0] * convolutional_shape[1]\n",
    "        for channel_index in channel_indices:\n",
    "            for iter in range(units_per_channel):\n",
    "                dense_indices.append(channel_index * units_per_channel + iter)\n",
    "        return dense_indices\n",
    "\n",
    "    def print_neurons(self):\n",
    "        for layer in self.lrs[:-1]:\n",
    "            print(layer.get_param_string())\n",
    "\n",
    "    def evaluate(self, params, summed_training_loss, summed_training_metric):\n",
    "        # Calculate training loss and metric\n",
    "        if summed_training_loss is not None:\n",
    "            loss = summed_training_loss / params.x.shape[0]\n",
    "        else:\n",
    "            loss = None\n",
    "\n",
    "        if summed_training_metric is not None:\n",
    "            metric = summed_training_metric / params.x.shape[0]\n",
    "        else:\n",
    "            metric = None\n",
    "\n",
    "        # Calculate val loss and metric\n",
    "        summed_val_loss = 0\n",
    "        summed_val_metric = 0\n",
    "        n_val_instances = 0\n",
    "\n",
    "        for step, (x_batch, y_batch) in enumerate(params.val_dataset):\n",
    "            # y_pred = tf.reshape(self(x_batch, training=False), y_batch.shape)\n",
    "            y_pred = self(x_batch, training=False)\n",
    "            summed_val_loss += tf.reduce_sum(params.loss_fn(y_batch, y_pred))\n",
    "            summed_val_metric += float(tf.reduce_sum(params.metric_fn(y_batch, y_pred)))\n",
    "            n_val_instances += x_batch.shape[0]\n",
    "\n",
    "        val_loss = summed_val_loss / n_val_instances\n",
    "        val_metric = summed_val_metric / n_val_instances\n",
    "\n",
    "        return loss, metric, val_loss, val_metric\n",
    "\n",
    "    def list_params(self):\n",
    "        trainable_count = np.sum([K.count_params(w) for w in self.trainable_weights])\n",
    "        non_trainable_count = np.sum([K.count_params(w) for w in self.non_trainable_weights])\n",
    "        total_count = trainable_count + non_trainable_count\n",
    "\n",
    "        print('Total params: {:,}'.format(total_count))\n",
    "        print('Trainable params: {:,}'.format(trainable_count))\n",
    "        print('Non-trainable params: {:,}'.format(non_trainable_count))\n",
    "\n",
    "        return total_count, trainable_count, non_trainable_count\n",
    "\n",
    "    def print_epoch_statistics(self, params, summed_training_loss, summed_training_metric, message=None,\n",
    "                               require_result=False):\n",
    "        if not params.verbose:\n",
    "            if require_result:\n",
    "                return self.evaluate(params, summed_training_loss, summed_training_metric)\n",
    "            else:\n",
    "                return\n",
    "\n",
    "        loss, metric, val_loss, val_metric = self.evaluate(params, summed_training_loss, summed_training_metric)\n",
    "\n",
    "        if message is not None:\n",
    "            print(message)\n",
    "\n",
    "        print(\n",
    "            f\"loss: {loss} - metric: {metric} - val_loss: {val_loss} - val_metric: {val_metric} - penalty: {self.get_regularization_penalty()}\")\n",
    "        hidden_layer_sizes = self.get_hidden_layer_sizes()\n",
    "        print(f\"hidden layer sizes: {hidden_layer_sizes}, total units: {sum(hidden_layer_sizes)}\")\n",
    "        if params.print_neurons:\n",
    "            self.print_neurons()\n",
    "\n",
    "        if require_result:\n",
    "            return loss, metric, val_loss, val_metric\n",
    "\n",
    "    def update_history(self, params, loss, metric, val_loss, val_metric):\n",
    "        params.history['loss'].append(float(loss))\n",
    "        params.history['metric'].append(float(metric))\n",
    "        params.history['val_loss'].append(float(val_loss))\n",
    "        params.history['val_metric'].append(float(val_metric))\n",
    "        params.history['hidden_layer_sizes'].append(self.get_hidden_layer_sizes())\n",
    "\n",
    "    @staticmethod\n",
    "    def prepare_datasets(x, y, batch_size, validation_data):\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "        train_dataset = train_dataset.shuffle(buffer_size=20000).batch(batch_size)\n",
    "        val_dataset = tf.data.Dataset.from_tensor_slices(validation_data).batch(batch_size)\n",
    "        return train_dataset.prefetch(tf.data.AUTOTUNE), val_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    def manage_dynamic_regularization(self, params, val_loss):\n",
    "        if val_loss >= params.best_conditional_val_loss * params.stall_coefficient:\n",
    "            # Training is currently in stall\n",
    "            if not params.training_stalled:\n",
    "                penalty = self.get_regularization_penalty() * params.regularization_penalty_multiplier\n",
    "                print(\"Changing penalty...\")\n",
    "                # TODO this must be modified, penalty can differ for each layer\n",
    "                self.set_regularization_penalty(penalty)\n",
    "                params.training_stalled = True\n",
    "        else:\n",
    "            params.best_conditional_val_loss = val_loss\n",
    "            params.training_stalled = False\n",
    "\n",
    "    def grow_wrapper(self, params):\n",
    "        dynamic_reqularization_active = params.regularization_penalty_multiplier != 1.\n",
    "        if dynamic_reqularization_active:\n",
    "            loss, metric, val_loss, val_metric = self.print_epoch_statistics(params, None, None, \"Before growing:\",\n",
    "                                                                             require_result=True)\n",
    "            self.manage_dynamic_regularization(params, val_loss)\n",
    "        else:\n",
    "            self.print_epoch_statistics(params, None, None, \"Before growing:\")\n",
    "\n",
    "        self.grow(params)\n",
    "        self.print_epoch_statistics(params, None, None, \"After growing:\")\n",
    "\n",
    "    def prune_wrapper(self, params, summed_loss, summed_metric):\n",
    "        loss, metric, _, _ = self.print_epoch_statistics(params, summed_loss, summed_metric, \"Before pruning:\",\n",
    "                                                         require_result=True)\n",
    "        self.prune(params)\n",
    "        _, _, val_loss, val_metric = self.print_epoch_statistics(params, None, None, \"After pruning:\",\n",
    "                                                                 require_result=True)\n",
    "        self.update_history(params, loss, metric, val_loss, val_metric)\n",
    "\n",
    "    class ParameterContainer:\n",
    "        def __init__(self, x, y, optimizer, batch_size, min_new_neurons, validation_data, pruning_threshold,\n",
    "                     regularization_penalty_multiplier,\n",
    "                     stall_coefficient, growth_percentage, mini_epochs_per_epoch, verbose, print_neurons,\n",
    "                     use_static_graph, loss_fn, metric_fn):\n",
    "            self.x = x\n",
    "            self.y = y\n",
    "            self.optimizer = optimizer\n",
    "            self.batch_size = batch_size\n",
    "            self.min_new_neurons = min_new_neurons\n",
    "            self.validation_data = validation_data\n",
    "            self.pruning_threshold = pruning_threshold\n",
    "            self.regularization_penalty_multiplier = regularization_penalty_multiplier\n",
    "            self.stall_coefficient = stall_coefficient\n",
    "            self.growth_percentage = growth_percentage\n",
    "            self.mini_epochs_per_epoch = mini_epochs_per_epoch\n",
    "            self.verbose = verbose\n",
    "            self.print_neurons = print_neurons\n",
    "            self.use_static_graph = use_static_graph\n",
    "            self.loss_fn = loss_fn\n",
    "            self.metric_fn = metric_fn\n",
    "\n",
    "            self.train_dataset, self.val_dataset = Sequential.prepare_datasets(x, y, batch_size, validation_data)\n",
    "            self.history = self.prepare_history()\n",
    "\n",
    "            self.best_conditional_val_loss = np.inf\n",
    "            self.training_stalled = False\n",
    "\n",
    "        @staticmethod\n",
    "        def prepare_history():\n",
    "            history = {\n",
    "                'loss': list(),\n",
    "                'metric': list(),\n",
    "                'val_loss': list(),\n",
    "                'val_metric': list(),\n",
    "                'hidden_layer_sizes': list(),\n",
    "            }\n",
    "            return history\n",
    "\n",
    "    def fit_single_step(self, x_batch, y_batch, optimizer, loss_fn, metric_fn):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # y_pred = tf.reshape(self(x_batch, training=True), y_batch.shape)\n",
    "            y_pred = self(x_batch, training=True)\n",
    "            raw_loss = loss_fn(y_batch, y_pred)\n",
    "            loss_value = tf.reduce_mean(raw_loss)\n",
    "            loss_value += sum(self.losses)  # Add losses registered by model.add_loss\n",
    "\n",
    "            loss = tf.reduce_sum(raw_loss)\n",
    "            metric = float(tf.reduce_sum(metric_fn(y_batch, y_pred)))\n",
    "\n",
    "        grads = tape.gradient(loss_value, self.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "\n",
    "        return loss, metric\n",
    "\n",
    "    def fit_single_epoch(self, params):\n",
    "        summed_loss = 0\n",
    "        summed_metric = 0\n",
    "\n",
    "        for mini_epoch in range(params.mini_epochs_per_epoch):\n",
    "            summed_loss = 0\n",
    "            summed_metric = 0\n",
    "\n",
    "            if params.use_static_graph:\n",
    "                fit_single_step_function = tf.function(self.fit_single_step)\n",
    "            else:\n",
    "                fit_single_step_function = self.fit_single_step\n",
    "            for step, (x_batch, y_batch) in enumerate(params.train_dataset):\n",
    "                loss, metric = fit_single_step_function(x_batch, y_batch, params.optimizer, params.loss_fn,\n",
    "                                                        params.metric_fn)\n",
    "                summed_loss += loss\n",
    "                summed_metric += metric\n",
    "\n",
    "        return summed_loss, summed_metric\n",
    "\n",
    "    def fit(self, x, y, optimizer, schedule, batch_size, min_new_neurons, validation_data, pruning_threshold=0.001,\n",
    "            regularization_penalty_multiplier=1.,\n",
    "            stall_coefficient=1, growth_percentage=0.2, mini_epochs_per_epoch=1, verbose=True, print_neurons=False,\n",
    "            use_static_graph=True,\n",
    "            loss_fn=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "            metric_fn=tf.keras.metrics.sparse_categorical_accuracy):\n",
    "        params = self.ParameterContainer(x=x, y=y, optimizer=optimizer, batch_size=batch_size,\n",
    "                                         min_new_neurons=min_new_neurons, validation_data=validation_data,\n",
    "                                         pruning_threshold=pruning_threshold,\n",
    "                                         regularization_penalty_multiplier=regularization_penalty_multiplier,\n",
    "                                         stall_coefficient=stall_coefficient,\n",
    "                                         growth_percentage=growth_percentage,\n",
    "                                         mini_epochs_per_epoch=mini_epochs_per_epoch, verbose=verbose,\n",
    "                                         print_neurons=print_neurons,\n",
    "                                         use_static_graph=use_static_graph, loss_fn=loss_fn, metric_fn=metric_fn)\n",
    "        self.build(x.shape)  # Necessary when verbose == False\n",
    "\n",
    "        for epoch_no, epoch in enumerate(schedule):\n",
    "            if verbose:\n",
    "                print(\"##########################################################\")\n",
    "                print(f\"Epoch {epoch_no + 1}/{len(schedule)}\")\n",
    "\n",
    "            self.set_regularization_penalty(epoch.regularization_penalty)\n",
    "            self.set_regularization_method(epoch.regularization_method)\n",
    "\n",
    "            if epoch.grow:\n",
    "                self.grow_wrapper(params)\n",
    "\n",
    "            summed_loss, summed_metric = self.fit_single_epoch(params)\n",
    "\n",
    "            if epoch.prune:\n",
    "                self.prune_wrapper(params, summed_loss, summed_metric)\n",
    "            else:\n",
    "                loss, metric, val_loss, val_metric = self.print_epoch_statistics(params, summed_loss, summed_metric,\n",
    "                                                                                 require_result=True)\n",
    "                self.update_history(params, loss, metric, val_loss, val_metric)\n",
    "\n",
    "        return params.history\n",
    "\n",
    "##### schedule.py\n",
    "\n",
    "class Epoch:\n",
    "    def __init__(self, grow, prune, regularization_penalty, regularization_method):\n",
    "        self.grow = grow\n",
    "        self.prune = prune\n",
    "        self.regularization_penalty = regularization_penalty\n",
    "        self.regularization_method = regularization_method\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'{int(self.grow)}{int(self.prune)}{self.regularization_penalty}{self.regularization_method}'\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "\n",
    "class DynamicEpoch(Epoch):\n",
    "    def __init__(self, regularization_penalty, regularization_method):\n",
    "        super().__init__(True, True, regularization_penalty, regularization_method)\n",
    "\n",
    "\n",
    "class StaticEpoch(Epoch):\n",
    "    def __init__(self, regularization_penalty, regularization_method):\n",
    "        super().__init__(False, False, regularization_penalty, regularization_method)\n",
    "\n",
    "\n",
    "class StaticEpochNoRegularization(StaticEpoch):\n",
    "    def __init__(self):\n",
    "        super().__init__(0., None)\n",
    "\n",
    "\n",
    "class Schedule:\n",
    "    def __init__(self, epochs):\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self.epochs.__iter__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.epochs)\n",
    "\n",
    "    def __str__(self):\n",
    "        text = ''.join([str(epoch) for epoch in self.epochs])\n",
    "        _hash = hashlib.sha1(text.encode('utf-8')).hexdigest()[:10]\n",
    "        return f'{_hash}({self.epochs[0].regularization_penalty})'\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "##### helpers.py\n",
    "\n",
    "def get_statistics_from_history(history):\n",
    "    best_epoch_number = np.argmax(history['val_metric'])\n",
    "    best_loss = history['loss'][best_epoch_number]\n",
    "    best_metric = history['metric'][best_epoch_number]\n",
    "    best_val_loss = history['val_loss'][best_epoch_number]\n",
    "    best_val_metric = history['val_metric'][best_epoch_number]\n",
    "    best_hidden_layer_sizes = history['hidden_layer_sizes'][best_epoch_number]\n",
    "    return best_loss, best_metric, best_val_loss, best_val_metric, best_hidden_layer_sizes\n",
    "\n",
    "\n",
    "def get_statistics_from_histories(histories):\n",
    "    best_val_losses = list()\n",
    "    best_val_metrics = list()\n",
    "    all_best_hidden_layer_sizes = list()\n",
    "\n",
    "    for history in histories:\n",
    "        _, _, best_val_loss, best_val_metric, best_hidden_layer_sizes = get_statistics_from_history(history)\n",
    "        best_val_losses.append(best_val_loss)\n",
    "        best_val_metrics.append(best_val_metric)\n",
    "        all_best_hidden_layer_sizes.append(best_hidden_layer_sizes)\n",
    "\n",
    "    mean_best_val_loss = np.mean(best_val_losses)\n",
    "    mean_best_val_metric = np.mean(best_val_metrics)\n",
    "    mean_best_hidden_layer_sizes = [np.mean(layer) for layer in list(zip(*all_best_hidden_layer_sizes))]\n",
    "\n",
    "    return mean_best_val_loss, mean_best_val_metric, mean_best_hidden_layer_sizes\n",
    "\n",
    "\n",
    "def cross_validate(train_fn, x, y, n_splits, random_state=42, *args, **kwargs):\n",
    "    from sklearn.model_selection import KFold\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "    histories = list()\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(x)):\n",
    "        xtrain, xtest = x[train_index], x[test_index]\n",
    "        ytrain, ytest = y[train_index], y[test_index]\n",
    "\n",
    "        history = train_fn(xtrain, ytrain, validation_data=(xtest, ytest), *args, **kwargs)\n",
    "        histories.append(history)\n",
    "\n",
    "        _, _, best_val_loss, best_val_metric, best_hidden_layer_sizes = get_statistics_from_history(history)\n",
    "        print(\n",
    "            f\"Run {i} completed, best_val_loss: {best_val_loss}, best_val_metric: {best_val_metric}, best_hidden_layer_sizes: {best_hidden_layer_sizes}\")\n",
    "\n",
    "    mean_best_val_loss, mean_best_val_metric, mean_best_hidden_layer_sizes = get_statistics_from_histories(histories)\n",
    "    print(f'mean_best_val_loss: {mean_best_val_loss}')\n",
    "    print(f'mean_best_val_metric: {mean_best_val_metric}')\n",
    "    print(f'mean_best_hidden_layer_sizes: {mean_best_hidden_layer_sizes}')\n",
    "\n",
    "    return histories, mean_best_hidden_layer_sizes\n",
    "\n",
    "\n",
    "def hyperparameter_search(train_fn, x, y, validation_data, *args, **kwargs):\n",
    "    from itertools import product\n",
    "\n",
    "    all_params = [*args] + list(kwargs.values())\n",
    "    histories = list()\n",
    "\n",
    "    best_overall_val_loss = np.inf\n",
    "    best_overall_val_metric = None\n",
    "    best_overall_combination = None\n",
    "\n",
    "    for combination in product(*all_params):\n",
    "        combination_args = combination[:len(args)]\n",
    "\n",
    "        combination_kwargs_values = combination[len(args):]\n",
    "        combination_kwargs = dict(zip(kwargs.keys(), combination_kwargs_values))\n",
    "\n",
    "        history = train_fn(x, y, validation_data, *combination_args, **combination_kwargs)\n",
    "        history['parameters'] = combination\n",
    "        histories.append(history)\n",
    "\n",
    "        _, _, best_val_loss, best_val_metric, best_hidden_layer_sizes = get_statistics_from_history(history)\n",
    "        print(\n",
    "            f\"Run with parameters {combination} completed, best_val_loss: {best_val_loss}, best_val_metric: {best_val_metric}, best_hidden_layer_sizes: {best_hidden_layer_sizes}\")\n",
    "\n",
    "        if best_val_loss < best_overall_val_loss:\n",
    "            best_overall_val_loss = best_val_loss\n",
    "            best_overall_val_metric = best_val_metric\n",
    "            best_overall_combination = combination\n",
    "\n",
    "    print(f'Best overall combination: {best_overall_combination}, val_metric: {best_overall_val_metric}')\n",
    "\n",
    "    return histories, best_overall_combination\n",
    "\n",
    "\n",
    "\n",
    "def merge_histories(history1, history2):\n",
    "    merged_history = dict()\n",
    "    for key in history1.keys():\n",
    "        merged_history[key] = history1[key] + history2[key]\n",
    "    return merged_history\n",
    "\n",
    "\n",
    "def get_convolutional_model(x, layer_sizes, output_neurons=10):\n",
    "    dropout_rate = 0.3\n",
    "    model = Sequential([\n",
    "        Conv2D(layer_sizes[0], filter_size=(3, 3), activation='selu', padding='SAME', kernel_initializer='lecun_normal', input_shape=x[0, :, :, :].shape),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "        tf.keras.layers.Dropout(dropout_rate),\n",
    "        Conv2D(layer_sizes[1], filter_size=(3, 3), activation='selu', padding='SAME', kernel_initializer='lecun_normal'),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "        tf.keras.layers.Dropout(dropout_rate),\n",
    "        Conv2D(layer_sizes[2], filter_size=(3, 3), activation='selu', padding='SAME', kernel_initializer='lecun_normal'),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(dropout_rate),\n",
    "        Conv2D(layer_sizes[3], filter_size=(3, 3), activation='selu', padding='SAME', kernel_initializer='lecun_normal'),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(dropout_rate),\n",
    "        Conv2D(layer_sizes[4], filter_size=(3, 3), activation='selu', padding='SAME', kernel_initializer='lecun_normal'),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "        tf.keras.layers.Dropout(dropout_rate),\n",
    "        Flatten(),\n",
    "        Dense(layer_sizes[5], activation='selu', kernel_initializer='lecun_normal'),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(dropout_rate),\n",
    "        Dense(layer_sizes[6], activation='selu', kernel_initializer='lecun_normal'),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(dropout_rate),\n",
    "        Dense(output_neurons, activation='softmax', fixed_size=True),\n",
    "        \n",
    "        # Conv2D(layer_sizes[1], filter_size=(3, 3), activation='selu', strides=(2, 2), padding='SAME',\n",
    "        #        kernel_initializer='lecun_normal'),\n",
    "        # tf.keras.layers.Dropout(0.2),\n",
    "        # Conv2D(layer_sizes[2], filter_size=(3, 3), activation='selu', strides=(1, 1), padding='SAME',\n",
    "        #        kernel_initializer='lecun_normal'),\n",
    "        # Conv2D(layer_sizes[3], filter_size=(3, 3), activation='selu', strides=(2, 2), padding='SAME',\n",
    "        #        kernel_initializer='lecun_normal'),\n",
    "        # tf.keras.layers.Dropout(0.5),\n",
    "        # Flatten(),\n",
    "        # Dense(layer_sizes[4], activation='selu', kernel_initializer='lecun_normal'),\n",
    "        # Dense(output_neurons, activation='softmax', fixed_size=True),\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_dense_model(x, layer_sizes):\n",
    "    layers = list()\n",
    "\n",
    "    layers.append(\n",
    "        Dense(layer_sizes[0], activation='selu', kernel_initializer='lecun_normal', input_shape=x[0, :].shape))\n",
    "    for layer_size in layer_sizes[1:]:\n",
    "        layers.append(Dense(layer_size, activation='selu', kernel_initializer='lecun_normal'))\n",
    "    layers.append(Dense(1, activation=None, kernel_initializer='lecun_normal', fixed_size=True))\n",
    "\n",
    "    model = Sequential(layers)\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_fn_conv(x, y, validation_data, learning_rate, schedule, layer_sizes, output_neurons=10, min_new_neurons=20,\n",
    "                  growth_percentage=0.2, verbose=False, use_static_graph=True, batch_size=128):\n",
    "    model = get_convolutional_model(x, layer_sizes, output_neurons)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    history = model.fit(x=x, y=y, optimizer=optimizer, schedule=schedule, batch_size=batch_size,\n",
    "                        min_new_neurons=min_new_neurons,\n",
    "                        validation_data=validation_data, growth_percentage=growth_percentage, verbose=verbose,\n",
    "                        use_static_graph=use_static_graph)\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "def squared_error(y_true, y_pred):\n",
    "    return (y_true - y_pred) ** 2\n",
    "\n",
    "\n",
    "def negative_squared_error(y_true, y_pred):\n",
    "    return - ((y_true - y_pred) ** 2)\n",
    "\n",
    "\n",
    "def train_fn_dense(x, y, validation_data, learning_rate, schedule, layer_sizes, min_new_neurons=20,\n",
    "                   growth_percentage=0.2, verbose=False, use_static_graph=True, batch_size=128):\n",
    "    model = get_dense_model(x, layer_sizes)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    history = model.fit(x=x, y=y, optimizer=optimizer, schedule=schedule, batch_size=batch_size,\n",
    "                        min_new_neurons=min_new_neurons,\n",
    "                        validation_data=validation_data, growth_percentage=growth_percentage, verbose=verbose,\n",
    "                        use_static_graph=use_static_graph,\n",
    "                        loss_fn=squared_error, metric_fn=negative_squared_error)\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "def early_stopping_conv(x, y, validation_data, learning_rate, schedule, layer_sizes, output_neurons=10,\n",
    "                        min_new_neurons=20,\n",
    "                        growth_percentage=0.2, verbose=False, use_static_graph=True, batch_size=128, max_setbacks=2):\n",
    "    assert len(schedule) == 1\n",
    "\n",
    "    model = get_convolutional_model(x, layer_sizes, output_neurons)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    history = Sequential.ParameterContainer.prepare_history()\n",
    "\n",
    "    best_val_loss = np.inf\n",
    "    n_setbacks = 0\n",
    "    while True:\n",
    "        epoch_history = model.fit(x=x, y=y, optimizer=optimizer, schedule=schedule, batch_size=batch_size,\n",
    "                                  min_new_neurons=min_new_neurons,\n",
    "                                  validation_data=validation_data, growth_percentage=growth_percentage, verbose=verbose,\n",
    "                                  use_static_graph=use_static_graph)\n",
    "        history = merge_histories(history, epoch_history)\n",
    "        val_loss = epoch_history['val_loss'][-1]\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            n_setbacks = 0\n",
    "        else:\n",
    "            n_setbacks += 1\n",
    "            if n_setbacks > max_setbacks:\n",
    "                break\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "def early_stopping_dense(x, y, validation_data, learning_rate, schedule, layer_sizes, output_neurons=1,\n",
    "                         min_new_neurons=20,\n",
    "                         growth_percentage=0.2, verbose=False, use_static_graph=True, batch_size=128, max_setbacks=2):\n",
    "    assert len(schedule) == 1\n",
    "    assert output_neurons == 1\n",
    "\n",
    "    model = get_dense_model(x, layer_sizes)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    history = Sequential.ParameterContainer.prepare_history()\n",
    "\n",
    "    best_val_loss = np.inf\n",
    "    n_setbacks = 0\n",
    "    while True:\n",
    "        epoch_history = model.fit(x=x, y=y, optimizer=optimizer, schedule=schedule, batch_size=batch_size,\n",
    "                                  min_new_neurons=min_new_neurons,\n",
    "                                  validation_data=validation_data, growth_percentage=growth_percentage, verbose=verbose,\n",
    "                                  use_static_graph=use_static_graph,\n",
    "                                  loss_fn=squared_error, metric_fn=negative_squared_error)\n",
    "        history = merge_histories(history, epoch_history)\n",
    "        val_loss = epoch_history['val_loss'][-1]\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            n_setbacks = 0\n",
    "        else:\n",
    "            n_setbacks += 1\n",
    "            if n_setbacks > max_setbacks:\n",
    "                break\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "def layer_sizes_join_postprocess(args, kwargs):\n",
    "    kwargs['layer_sizes'] = kwargs['layer_1_size'], kwargs['layer_2_size'], kwargs['layer_3_size'], kwargs[\n",
    "        'layer_4_size'], kwargs['layer_5_size']\n",
    "    del kwargs['layer_1_size'], kwargs['layer_2_size'], kwargs['layer_3_size'], kwargs['layer_4_size'], kwargs[\n",
    "        'layer_5_size']\n",
    "    return args, kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc6ce8e5-8fe3-4e17-9ab4-7daabca0903b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar100 = get_cifar_100_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42b3916e-9885-4e33-9f48-5d834d6cb795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[32, 85, 128, 128, 85, 1365, 1365]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alexnet_layer_sizes = [96, 256, 384, 384, 256, 4096, 4096]\n",
    "layer_sizes = [size // 3 for size in alexnet_layer_sizes]\n",
    "layer_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9574a342-41be-47c6-b129-7a48d1c30bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormalization(DASLayer):\n",
    "    def __init__(self, momentum=0.99, epsilon=0.001, input_shape=None):\n",
    "        super().__init__(input_shape, fixed_size=True)\n",
    "\n",
    "        self.momentum = momentum\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def copy(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        if self._built:\n",
    "            return\n",
    "        \n",
    "        self.offset = tf.Variable(\n",
    "            name='offset',\n",
    "            initial_value=tf.zeros(input_shape[1:]),\n",
    "            trainable=True)\n",
    "        self.scale = tf.Variable(\n",
    "            name='scale',\n",
    "            initial_value=tf.ones(input_shape[1:]),\n",
    "            trainable=True)\n",
    "        self.moving_mean = tf.Variable(\n",
    "            name='moving_mean',\n",
    "            initial_value=tf.zeros(input_shape[1:]),\n",
    "            trainable=False)\n",
    "        self.moving_variance = tf.Variable(\n",
    "            name='moving_variance',\n",
    "            initial_value=tf.ones(input_shape[1:]),\n",
    "            trainable=False)\n",
    "\n",
    "#         input_filters = input_shape[-1]\n",
    "\n",
    "#         self.F = tf.Variable(\n",
    "#             name='F',\n",
    "#             initial_value=self.F_init(\n",
    "#                 shape=(self.filter_size[0], self.filter_size[1], input_filters, self.filters), dtype=dtype\n",
    "#             ),\n",
    "#             trainable=True)\n",
    "\n",
    "#         self.b = tf.Variable(\n",
    "#             name='b',\n",
    "#             initial_value=self.b_init(shape=(self.filters,), dtype=dtype),\n",
    "#             trainable=True)\n",
    "\n",
    "#         self.add_regularizer_loss()\n",
    "\n",
    "        self._built = True\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        if training:\n",
    "            mean, variance = tf.nn.moments(inputs, axes=[0])\n",
    "            self.moving_mean.assign(self.moving_mean * self.momentum + mean * (1 - self.momentum))\n",
    "            self.moving_variance.assign(self.moving_variance * self.momentum + variance * (1 - self.momentum))\n",
    "        else:\n",
    "            mean = self.moving_mean\n",
    "            variance = self.moving_variance\n",
    "        return tf.nn.batch_normalization(inputs, mean, variance, self.offset, self.scale, self.epsilon)\n",
    "\n",
    "    # def get_size(self):\n",
    "    #     return self.F.shape[-2], self.F.shape[-1]\n",
    "\n",
    "#     def prune(self, threshold, active_input_units_indices):\n",
    "#         # Remove connections from pruned units in previous layer\n",
    "#         new_F = tf.gather(self.F.value(), active_input_units_indices, axis=-2)\n",
    "\n",
    "#         if self.fixed_size:\n",
    "#             active_output_filters_indices = list(range(new_F.shape[-1]))\n",
    "#         else:\n",
    "#             # Prune units in this layer\n",
    "#             F_reduced_max = tf.reshape(tf.math.reduce_max(tf.abs(new_F), axis=(0, 1, 2)), (1, -1))\n",
    "#             F_reduced_max_with_biases = tf.concat([F_reduced_max, tf.reshape(self.b.value(), (1, -1))], axis=0)\n",
    "#             filters_are_active = tf.math.reduce_max(tf.abs(F_reduced_max_with_biases), axis=0) >= threshold\n",
    "#             active_output_filters_indices = tf.reshape(tf.where(filters_are_active), (-1,))\n",
    "\n",
    "#             new_F = tf.gather(new_F, active_output_filters_indices, axis=-1)\n",
    "#             new_b = tf.gather(self.b.value(), active_output_filters_indices, axis=0)\n",
    "\n",
    "#             self.b = tf.Variable(name='b', initial_value=new_b, trainable=True)\n",
    "\n",
    "#         self.F = tf.Variable(name='F', initial_value=new_F, trainable=True)\n",
    "\n",
    "#         self.regularizer.prune()\n",
    "#         return active_output_filters_indices\n",
    "\n",
    "#     def grow(self, n_new_input_units, percentage, min_new_units, scaling_factor):\n",
    "#         if n_new_input_units > 0:\n",
    "#             # Add connections to grown units in previous layer\n",
    "#             F_growth = self.F_init(\n",
    "#                 shape=(self.F.shape[0], self.F.shape[1], self.F.shape[2] + n_new_input_units, self.F.shape[3]),\n",
    "#                 dtype=dtype)[:, :, -n_new_input_units:,\n",
    "#                        :] * scaling_factor  # TODO is it better to be multiplying here by scaling_factor? It does help with not increasing the max weights of existing neurons when new neurons are added.\n",
    "#             new_F = tf.concat([self.F.value(), F_growth], axis=-2)\n",
    "#         else:\n",
    "#             new_F = self.F.value()\n",
    "\n",
    "#         if self.fixed_size:\n",
    "#             n_new_output_units = 0\n",
    "#         else:\n",
    "#             # Grow new units in this layer\n",
    "#             n_new_output_units = max(min_new_units, int(new_F.shape[-1] * percentage))\n",
    "#             if n_new_output_units > 0:\n",
    "#                 F_growth = self.F_init(\n",
    "#                     shape=(new_F.shape[0], new_F.shape[1], new_F.shape[2], new_F.shape[3] + n_new_output_units),\n",
    "#                     dtype=dtype)[:, :, :, -n_new_output_units:] * scaling_factor\n",
    "#                 b_growth = self.b_init(shape=(n_new_output_units,),\n",
    "#                                        dtype=dtype)  # TODO for all possible bias initializers to work properly, the whole bias vector should be initialized at once\n",
    "#                 new_F = tf.concat([new_F, F_growth], axis=-1)\n",
    "#                 new_b = tf.concat([self.b.value(), b_growth], axis=0)\n",
    "\n",
    "#                 self.b = tf.Variable(name='b', initial_value=new_b, trainable=True)\n",
    "\n",
    "#         self.F = tf.Variable(name='F', initial_value=new_F, trainable=True)\n",
    "\n",
    "#         self.regularizer.grow(n_new_output_units)\n",
    "#         return n_new_output_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "63757bb4-eb0a-4d1c-9ae3-1f204cd89146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################################################\n",
      "Epoch 1/40\n",
      "loss: 4.146322727203369 - metric: 0.11819999665021896 - val_loss: 4.363019943237305 - val_metric: 0.1479 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365], total units: 1823\n",
      "##########################################################\n",
      "Epoch 2/40\n",
      "loss: 3.3023056983947754 - metric: 0.21884000301361084 - val_loss: 3.285404682159424 - val_metric: 0.2614 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365], total units: 1823\n",
      "##########################################################\n",
      "Epoch 3/40\n",
      "loss: 2.9311931133270264 - metric: 0.2805199921131134 - val_loss: 3.438206672668457 - val_metric: 0.2585 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365], total units: 1823\n",
      "##########################################################\n",
      "Epoch 4/40\n",
      "loss: 2.699477434158325 - metric: 0.3206000030040741 - val_loss: 2.7169628143310547 - val_metric: 0.3392 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365], total units: 1823\n",
      "##########################################################\n",
      "Epoch 5/40\n",
      "loss: 2.5319430828094482 - metric: 0.3518800139427185 - val_loss: 2.7569358348846436 - val_metric: 0.3553 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365], total units: 1823\n",
      "##########################################################\n",
      "Epoch 6/40\n",
      "loss: 2.401651620864868 - metric: 0.3762800097465515 - val_loss: 2.5003840923309326 - val_metric: 0.4025 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365], total units: 1823\n",
      "##########################################################\n",
      "Epoch 7/40\n",
      "loss: 2.2984726428985596 - metric: 0.3975600004196167 - val_loss: 2.362548351287842 - val_metric: 0.4141 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365], total units: 1823\n",
      "##########################################################\n",
      "Epoch 8/40\n",
      "loss: 2.1929128170013428 - metric: 0.42013999819755554 - val_loss: 2.2060601711273193 - val_metric: 0.433 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365], total units: 1823\n",
      "##########################################################\n",
      "Epoch 9/40\n",
      "loss: 2.1016652584075928 - metric: 0.4402199983596802 - val_loss: 2.1189348697662354 - val_metric: 0.4555 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365], total units: 1823\n",
      "##########################################################\n",
      "Epoch 10/40\n",
      "loss: 2.0345873832702637 - metric: 0.4526999890804291 - val_loss: 2.1341845989227295 - val_metric: 0.4603 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365], total units: 1823\n",
      "##########################################################\n",
      "Epoch 11/40\n",
      "loss: 1.9640848636627197 - metric: 0.4674600064754486 - val_loss: 2.3143856525421143 - val_metric: 0.4309 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365], total units: 1823\n",
      "##########################################################\n",
      "Epoch 12/40\n",
      "loss: 1.8940409421920776 - metric: 0.4819999933242798 - val_loss: 2.0681142807006836 - val_metric: 0.4773 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365], total units: 1823\n",
      "##########################################################\n",
      "Epoch 13/40\n",
      "loss: 1.8306972980499268 - metric: 0.49842000007629395 - val_loss: 2.0364325046539307 - val_metric: 0.4825 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365], total units: 1823\n",
      "##########################################################\n",
      "Epoch 14/40\n",
      "loss: 1.7782930135726929 - metric: 0.5071200132369995 - val_loss: 1.9921547174453735 - val_metric: 0.4954 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365], total units: 1823\n",
      "##########################################################\n",
      "Epoch 15/40\n",
      "loss: 1.7250207662582397 - metric: 0.5210000276565552 - val_loss: 1.9976344108581543 - val_metric: 0.4967 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365], total units: 1823\n",
      "##########################################################\n",
      "Epoch 16/40\n",
      "loss: 1.6620696783065796 - metric: 0.5317999720573425 - val_loss: 2.0994513034820557 - val_metric: 0.4862 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365], total units: 1823\n",
      "##########################################################\n",
      "Epoch 17/40\n",
      "loss: 1.62185800075531 - metric: 0.5414199829101562 - val_loss: 1.990612506866455 - val_metric: 0.5014 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365], total units: 1823\n",
      "##########################################################\n",
      "Epoch 18/40\n",
      "loss: 1.5670740604400635 - metric: 0.5543199777603149 - val_loss: 1.9070950746536255 - val_metric: 0.5225 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365], total units: 1823\n",
      "##########################################################\n",
      "Epoch 19/40\n",
      "loss: 1.5340765714645386 - metric: 0.561519980430603 - val_loss: 1.837328314781189 - val_metric: 0.5298 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365], total units: 1823\n",
      "##########################################################\n",
      "Epoch 20/40\n",
      "loss: 1.5057016611099243 - metric: 0.5679399967193604 - val_loss: 1.9042580127716064 - val_metric: 0.5327 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365], total units: 1823\n",
      "##########################################################\n",
      "Epoch 21/40\n",
      "loss: 1.45797598361969 - metric: 0.580780029296875 - val_loss: 1.9251824617385864 - val_metric: 0.5223 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365], total units: 1823\n",
      "##########################################################\n",
      "Epoch 22/40\n",
      "loss: 1.414971113204956 - metric: 0.5931000113487244 - val_loss: 1.8315805196762085 - val_metric: 0.5296 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365], total units: 1823\n",
      "##########################################################\n",
      "Epoch 23/40\n",
      "loss: 1.3846651315689087 - metric: 0.598039984703064 - val_loss: 1.9451990127563477 - val_metric: 0.5295 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365], total units: 1823\n",
      "##########################################################\n",
      "Epoch 24/40\n",
      "loss: 1.352605938911438 - metric: 0.6080999970436096 - val_loss: 1.9719635248184204 - val_metric: 0.5337 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365], total units: 1823\n",
      "##########################################################\n",
      "Epoch 25/40\n",
      "loss: 1.3171238899230957 - metric: 0.6145200133323669 - val_loss: 1.881309151649475 - val_metric: 0.5317 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365], total units: 1823\n",
      "##########################################################\n",
      "Epoch 26/40\n",
      "loss: 1.2873550653457642 - metric: 0.6194000244140625 - val_loss: 1.9319027662277222 - val_metric: 0.53 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365], total units: 1823\n",
      "##########################################################\n",
      "Epoch 27/40\n",
      "loss: 1.263723373413086 - metric: 0.6285799741744995 - val_loss: 1.8675023317337036 - val_metric: 0.5432 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365], total units: 1823\n",
      "##########################################################\n",
      "Epoch 28/40\n",
      "loss: 1.2335950136184692 - metric: 0.6367200016975403 - val_loss: 1.9029099941253662 - val_metric: 0.5452 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365], total units: 1823\n",
      "##########################################################\n",
      "Epoch 29/40\n",
      "loss: 1.197895884513855 - metric: 0.6416400074958801 - val_loss: 1.8336591720581055 - val_metric: 0.5453 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365], total units: 1823\n",
      "##########################################################\n",
      "Epoch 30/40\n",
      "loss: 1.1858625411987305 - metric: 0.6455399990081787 - val_loss: 1.8016176223754883 - val_metric: 0.553 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365], total units: 1823\n",
      "##########################################################\n",
      "Epoch 31/40\n",
      "loss: 1.1534004211425781 - metric: 0.6544600129127502 - val_loss: 1.8212826251983643 - val_metric: 0.5561 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365], total units: 1823\n",
      "##########################################################\n",
      "Epoch 32/40\n",
      "loss: 1.1233491897583008 - metric: 0.6644799709320068 - val_loss: 1.8157249689102173 - val_metric: 0.5557 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365], total units: 1823\n",
      "##########################################################\n",
      "Epoch 33/40\n",
      "loss: 1.1079055070877075 - metric: 0.6672199964523315 - val_loss: 1.8415335416793823 - val_metric: 0.5525 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365], total units: 1823\n",
      "##########################################################\n",
      "Epoch 34/40\n",
      "loss: 1.0919687747955322 - metric: 0.6713399887084961 - val_loss: 1.8622171878814697 - val_metric: 0.5451 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365], total units: 1823\n",
      "##########################################################\n",
      "Epoch 35/40\n",
      "loss: 1.064728021621704 - metric: 0.6773399710655212 - val_loss: 1.799882411956787 - val_metric: 0.565 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365], total units: 1823\n",
      "##########################################################\n",
      "Epoch 36/40\n",
      "loss: 1.0491877794265747 - metric: 0.6823599934577942 - val_loss: 1.8265093564987183 - val_metric: 0.5601 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365], total units: 1823\n",
      "##########################################################\n",
      "Epoch 37/40\n",
      "loss: 1.0311603546142578 - metric: 0.6861600279808044 - val_loss: 1.7906944751739502 - val_metric: 0.5681 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365], total units: 1823\n",
      "##########################################################\n",
      "Epoch 38/40\n",
      "loss: 1.0074830055236816 - metric: 0.6934999823570251 - val_loss: 1.8563413619995117 - val_metric: 0.5573 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365], total units: 1823\n",
      "##########################################################\n",
      "Epoch 39/40\n",
      "loss: 0.9884796142578125 - metric: 0.6974400281906128 - val_loss: 1.8019973039627075 - val_metric: 0.5651 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365], total units: 1823\n",
      "##########################################################\n",
      "Epoch 40/40\n",
      "loss: 0.9770123362541199 - metric: 0.6998199820518494 - val_loss: 1.8686860799789429 - val_metric: 0.5534 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365], total units: 1823\n",
      "CPU times: user 3min 12s, sys: 6.07 s, total: 3min 18s\n",
      "Wall time: 2min 57s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': [4.146322727203369,\n",
       "  3.3023056983947754,\n",
       "  2.9311931133270264,\n",
       "  2.699477434158325,\n",
       "  2.5319430828094482,\n",
       "  2.401651620864868,\n",
       "  2.2984726428985596,\n",
       "  2.1929128170013428,\n",
       "  2.1016652584075928,\n",
       "  2.0345873832702637,\n",
       "  1.9640848636627197,\n",
       "  1.8940409421920776,\n",
       "  1.8306972980499268,\n",
       "  1.7782930135726929,\n",
       "  1.7250207662582397,\n",
       "  1.6620696783065796,\n",
       "  1.62185800075531,\n",
       "  1.5670740604400635,\n",
       "  1.5340765714645386,\n",
       "  1.5057016611099243,\n",
       "  1.45797598361969,\n",
       "  1.414971113204956,\n",
       "  1.3846651315689087,\n",
       "  1.352605938911438,\n",
       "  1.3171238899230957,\n",
       "  1.2873550653457642,\n",
       "  1.263723373413086,\n",
       "  1.2335950136184692,\n",
       "  1.197895884513855,\n",
       "  1.1858625411987305,\n",
       "  1.1534004211425781,\n",
       "  1.1233491897583008,\n",
       "  1.1079055070877075,\n",
       "  1.0919687747955322,\n",
       "  1.064728021621704,\n",
       "  1.0491877794265747,\n",
       "  1.0311603546142578,\n",
       "  1.0074830055236816,\n",
       "  0.9884796142578125,\n",
       "  0.9770123362541199],\n",
       " 'metric': [0.11819999665021896,\n",
       "  0.21884000301361084,\n",
       "  0.2805199921131134,\n",
       "  0.3206000030040741,\n",
       "  0.3518800139427185,\n",
       "  0.3762800097465515,\n",
       "  0.3975600004196167,\n",
       "  0.42013999819755554,\n",
       "  0.4402199983596802,\n",
       "  0.4526999890804291,\n",
       "  0.4674600064754486,\n",
       "  0.4819999933242798,\n",
       "  0.49842000007629395,\n",
       "  0.5071200132369995,\n",
       "  0.5210000276565552,\n",
       "  0.5317999720573425,\n",
       "  0.5414199829101562,\n",
       "  0.5543199777603149,\n",
       "  0.561519980430603,\n",
       "  0.5679399967193604,\n",
       "  0.580780029296875,\n",
       "  0.5931000113487244,\n",
       "  0.598039984703064,\n",
       "  0.6080999970436096,\n",
       "  0.6145200133323669,\n",
       "  0.6194000244140625,\n",
       "  0.6285799741744995,\n",
       "  0.6367200016975403,\n",
       "  0.6416400074958801,\n",
       "  0.6455399990081787,\n",
       "  0.6544600129127502,\n",
       "  0.6644799709320068,\n",
       "  0.6672199964523315,\n",
       "  0.6713399887084961,\n",
       "  0.6773399710655212,\n",
       "  0.6823599934577942,\n",
       "  0.6861600279808044,\n",
       "  0.6934999823570251,\n",
       "  0.6974400281906128,\n",
       "  0.6998199820518494],\n",
       " 'val_loss': [4.363019943237305,\n",
       "  3.285404682159424,\n",
       "  3.438206672668457,\n",
       "  2.7169628143310547,\n",
       "  2.7569358348846436,\n",
       "  2.5003840923309326,\n",
       "  2.362548351287842,\n",
       "  2.2060601711273193,\n",
       "  2.1189348697662354,\n",
       "  2.1341845989227295,\n",
       "  2.3143856525421143,\n",
       "  2.0681142807006836,\n",
       "  2.0364325046539307,\n",
       "  1.9921547174453735,\n",
       "  1.9976344108581543,\n",
       "  2.0994513034820557,\n",
       "  1.990612506866455,\n",
       "  1.9070950746536255,\n",
       "  1.837328314781189,\n",
       "  1.9042580127716064,\n",
       "  1.9251824617385864,\n",
       "  1.8315805196762085,\n",
       "  1.9451990127563477,\n",
       "  1.9719635248184204,\n",
       "  1.881309151649475,\n",
       "  1.9319027662277222,\n",
       "  1.8675023317337036,\n",
       "  1.9029099941253662,\n",
       "  1.8336591720581055,\n",
       "  1.8016176223754883,\n",
       "  1.8212826251983643,\n",
       "  1.8157249689102173,\n",
       "  1.8415335416793823,\n",
       "  1.8622171878814697,\n",
       "  1.799882411956787,\n",
       "  1.8265093564987183,\n",
       "  1.7906944751739502,\n",
       "  1.8563413619995117,\n",
       "  1.8019973039627075,\n",
       "  1.8686860799789429],\n",
       " 'val_metric': [0.1479,\n",
       "  0.2614,\n",
       "  0.2585,\n",
       "  0.3392,\n",
       "  0.3553,\n",
       "  0.4025,\n",
       "  0.4141,\n",
       "  0.433,\n",
       "  0.4555,\n",
       "  0.4603,\n",
       "  0.4309,\n",
       "  0.4773,\n",
       "  0.4825,\n",
       "  0.4954,\n",
       "  0.4967,\n",
       "  0.4862,\n",
       "  0.5014,\n",
       "  0.5225,\n",
       "  0.5298,\n",
       "  0.5327,\n",
       "  0.5223,\n",
       "  0.5296,\n",
       "  0.5295,\n",
       "  0.5337,\n",
       "  0.5317,\n",
       "  0.53,\n",
       "  0.5432,\n",
       "  0.5452,\n",
       "  0.5453,\n",
       "  0.553,\n",
       "  0.5561,\n",
       "  0.5557,\n",
       "  0.5525,\n",
       "  0.5451,\n",
       "  0.565,\n",
       "  0.5601,\n",
       "  0.5681,\n",
       "  0.5573,\n",
       "  0.5651,\n",
       "  0.5534],\n",
       " 'hidden_layer_sizes': [[32, 85, 128, 128, 85, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365]]}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "schedule = Schedule([StaticEpochNoRegularization()] * 40)\n",
    "train_fn_conv(x=cifar100.X_train_norm, y=cifar100.y_train, \n",
    "              validation_data=(cifar100.X_test_norm, cifar100.y_test), learning_rate=0.001, \n",
    "              schedule=schedule, layer_sizes=layer_sizes, output_neurons=100, verbose=True, use_static_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce684671-bc1a-4b63-9bbb-5e3e247d3e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################################################\n",
      "Epoch 1/40\n",
      "loss: 4.164563179016113 - metric: 0.1171799972653389 - val_loss: 4.554275989532471 - val_metric: 0.1434 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365, 1365], total units: 3188\n",
      "##########################################################\n",
      "Epoch 2/40\n",
      "loss: 3.3377609252929688 - metric: 0.21318000555038452 - val_loss: 3.5045278072357178 - val_metric: 0.2326 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365, 1365], total units: 3188\n",
      "##########################################################\n",
      "Epoch 3/40\n",
      "loss: 2.9550507068634033 - metric: 0.2761400043964386 - val_loss: 3.0465550422668457 - val_metric: 0.3036 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365, 1365], total units: 3188\n",
      "##########################################################\n",
      "Epoch 4/40\n",
      "loss: 2.701124429702759 - metric: 0.3158800005912781 - val_loss: 2.7370235919952393 - val_metric: 0.3477 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365, 1365], total units: 3188\n",
      "##########################################################\n",
      "Epoch 5/40\n",
      "loss: 2.4953885078430176 - metric: 0.3582000136375427 - val_loss: 2.6952080726623535 - val_metric: 0.3539 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365, 1365], total units: 3188\n",
      "##########################################################\n",
      "Epoch 6/40\n",
      "loss: 2.3433902263641357 - metric: 0.38741999864578247 - val_loss: 2.462343692779541 - val_metric: 0.3981 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365, 1365], total units: 3188\n",
      "##########################################################\n",
      "Epoch 7/40\n",
      "loss: 2.222804069519043 - metric: 0.41449999809265137 - val_loss: 2.2611353397369385 - val_metric: 0.4195 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365, 1365], total units: 3188\n",
      "##########################################################\n",
      "Epoch 8/40\n",
      "loss: 2.1114468574523926 - metric: 0.43397998809814453 - val_loss: 2.329519271850586 - val_metric: 0.4221 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365, 1365], total units: 3188\n",
      "##########################################################\n",
      "Epoch 9/40\n",
      "loss: 2.018197536468506 - metric: 0.45399999618530273 - val_loss: 2.129441499710083 - val_metric: 0.4502 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365, 1365], total units: 3188\n",
      "##########################################################\n",
      "Epoch 10/40\n",
      "loss: 1.9336079359054565 - metric: 0.4733000099658966 - val_loss: 2.0682883262634277 - val_metric: 0.4616 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365, 1365], total units: 3188\n",
      "##########################################################\n",
      "Epoch 11/40\n",
      "loss: 1.860167384147644 - metric: 0.4905799925327301 - val_loss: 1.979614019393921 - val_metric: 0.4791 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365, 1365], total units: 3188\n",
      "##########################################################\n",
      "Epoch 12/40\n",
      "loss: 1.7841598987579346 - metric: 0.5075399875640869 - val_loss: 1.975751519203186 - val_metric: 0.4847 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365, 1365], total units: 3188\n",
      "##########################################################\n",
      "Epoch 13/40\n",
      "loss: 1.7304868698120117 - metric: 0.5205199718475342 - val_loss: 1.9861687421798706 - val_metric: 0.4878 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365, 1365], total units: 3188\n",
      "##########################################################\n",
      "Epoch 14/40\n",
      "loss: 1.6743803024291992 - metric: 0.530780017375946 - val_loss: 1.924804925918579 - val_metric: 0.5016 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365, 1365], total units: 3188\n",
      "##########################################################\n",
      "Epoch 15/40\n",
      "loss: 1.6206616163253784 - metric: 0.5448200106620789 - val_loss: 1.873439073562622 - val_metric: 0.5141 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365, 1365], total units: 3188\n",
      "##########################################################\n",
      "Epoch 16/40\n",
      "loss: 1.5723124742507935 - metric: 0.5571399927139282 - val_loss: 1.8757586479187012 - val_metric: 0.5164 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365, 1365], total units: 3188\n",
      "##########################################################\n",
      "Epoch 17/40\n",
      "loss: 1.5143556594848633 - metric: 0.5683599710464478 - val_loss: 1.8468409776687622 - val_metric: 0.5233 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365, 1365], total units: 3188\n",
      "##########################################################\n",
      "Epoch 18/40\n",
      "loss: 1.4662446975708008 - metric: 0.5814200043678284 - val_loss: 1.8844707012176514 - val_metric: 0.514 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365, 1365], total units: 3188\n",
      "##########################################################\n",
      "Epoch 19/40\n",
      "loss: 1.4362691640853882 - metric: 0.5896400213241577 - val_loss: 1.7855619192123413 - val_metric: 0.5399 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365, 1365], total units: 3188\n",
      "##########################################################\n",
      "Epoch 20/40\n",
      "loss: 1.3897252082824707 - metric: 0.5994399785995483 - val_loss: 1.8389984369277954 - val_metric: 0.5267 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365, 1365], total units: 3188\n",
      "##########################################################\n",
      "Epoch 21/40\n",
      "loss: 1.3558814525604248 - metric: 0.6070600152015686 - val_loss: 1.792283535003662 - val_metric: 0.5402 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365, 1365], total units: 3188\n",
      "##########################################################\n",
      "Epoch 22/40\n",
      "loss: 1.3159631490707397 - metric: 0.6165000200271606 - val_loss: 1.758955717086792 - val_metric: 0.5458 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365, 1365], total units: 3188\n",
      "##########################################################\n",
      "Epoch 23/40\n",
      "loss: 1.282547116279602 - metric: 0.6223400235176086 - val_loss: 1.8370418548583984 - val_metric: 0.538 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365, 1365], total units: 3188\n",
      "##########################################################\n",
      "Epoch 24/40\n",
      "loss: 1.2527399063110352 - metric: 0.6282399892807007 - val_loss: 1.787279486656189 - val_metric: 0.545 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365, 1365], total units: 3188\n",
      "##########################################################\n",
      "Epoch 25/40\n",
      "loss: 1.2210615873336792 - metric: 0.63919997215271 - val_loss: 1.7691081762313843 - val_metric: 0.5465 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365, 1365], total units: 3188\n",
      "##########################################################\n",
      "Epoch 26/40\n",
      "loss: 1.2001926898956299 - metric: 0.6452800035476685 - val_loss: 1.781398057937622 - val_metric: 0.5502 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365, 1365], total units: 3188\n",
      "##########################################################\n",
      "Epoch 27/40\n",
      "loss: 1.1615076065063477 - metric: 0.656059980392456 - val_loss: 1.834904670715332 - val_metric: 0.5446 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365, 1365], total units: 3188\n",
      "##########################################################\n",
      "Epoch 28/40\n",
      "loss: 1.130737066268921 - metric: 0.6616799831390381 - val_loss: 1.816506028175354 - val_metric: 0.548 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365, 1365], total units: 3188\n",
      "##########################################################\n",
      "Epoch 29/40\n",
      "loss: 1.1033198833465576 - metric: 0.6673799753189087 - val_loss: 1.762529730796814 - val_metric: 0.5596 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365, 1365], total units: 3188\n",
      "##########################################################\n",
      "Epoch 30/40\n",
      "loss: 1.0799574851989746 - metric: 0.6754999756813049 - val_loss: 1.788341760635376 - val_metric: 0.5569 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365, 1365], total units: 3188\n",
      "##########################################################\n",
      "Epoch 31/40\n",
      "loss: 1.0673094987869263 - metric: 0.6765400171279907 - val_loss: 1.7922840118408203 - val_metric: 0.558 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365, 1365], total units: 3188\n",
      "##########################################################\n",
      "Epoch 32/40\n",
      "loss: 1.0290958881378174 - metric: 0.6879799962043762 - val_loss: 1.814652681350708 - val_metric: 0.5569 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365, 1365], total units: 3188\n",
      "##########################################################\n",
      "Epoch 33/40\n",
      "loss: 1.008555293083191 - metric: 0.6929200291633606 - val_loss: 1.779449462890625 - val_metric: 0.5625 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365, 1365], total units: 3188\n",
      "##########################################################\n",
      "Epoch 34/40\n",
      "loss: 0.9994961619377136 - metric: 0.6942200064659119 - val_loss: 1.7505155801773071 - val_metric: 0.5666 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365, 1365], total units: 3188\n",
      "##########################################################\n",
      "Epoch 35/40\n",
      "loss: 0.9761124849319458 - metric: 0.7009000182151794 - val_loss: 1.8000221252441406 - val_metric: 0.5595 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365, 1365], total units: 3188\n",
      "##########################################################\n",
      "Epoch 36/40\n",
      "loss: 0.9613146185874939 - metric: 0.7058600187301636 - val_loss: 1.7974814176559448 - val_metric: 0.5656 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365, 1365], total units: 3188\n",
      "##########################################################\n",
      "Epoch 37/40\n",
      "loss: 0.9395744800567627 - metric: 0.7095999717712402 - val_loss: 1.8462581634521484 - val_metric: 0.5596 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365, 1365], total units: 3188\n",
      "##########################################################\n",
      "Epoch 38/40\n",
      "loss: 0.9266930222511292 - metric: 0.7168400287628174 - val_loss: 1.8270609378814697 - val_metric: 0.5638 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365, 1365], total units: 3188\n",
      "##########################################################\n",
      "Epoch 39/40\n",
      "loss: 0.9204065799713135 - metric: 0.7160599827766418 - val_loss: 1.7986501455307007 - val_metric: 0.5677 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365, 1365], total units: 3188\n",
      "##########################################################\n",
      "Epoch 40/40\n",
      "loss: 0.8923746943473816 - metric: 0.724399983882904 - val_loss: 1.8381491899490356 - val_metric: 0.5631 - penalty: 0.0\n",
      "hidden layer sizes: [32, 85, 128, 128, 85, 1365, 1365], total units: 3188\n",
      "CPU times: user 2min 46s, sys: 5.68 s, total: 2min 52s\n",
      "Wall time: 3min 31s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': [4.164563179016113,\n",
       "  3.3377609252929688,\n",
       "  2.9550507068634033,\n",
       "  2.701124429702759,\n",
       "  2.4953885078430176,\n",
       "  2.3433902263641357,\n",
       "  2.222804069519043,\n",
       "  2.1114468574523926,\n",
       "  2.018197536468506,\n",
       "  1.9336079359054565,\n",
       "  1.860167384147644,\n",
       "  1.7841598987579346,\n",
       "  1.7304868698120117,\n",
       "  1.6743803024291992,\n",
       "  1.6206616163253784,\n",
       "  1.5723124742507935,\n",
       "  1.5143556594848633,\n",
       "  1.4662446975708008,\n",
       "  1.4362691640853882,\n",
       "  1.3897252082824707,\n",
       "  1.3558814525604248,\n",
       "  1.3159631490707397,\n",
       "  1.282547116279602,\n",
       "  1.2527399063110352,\n",
       "  1.2210615873336792,\n",
       "  1.2001926898956299,\n",
       "  1.1615076065063477,\n",
       "  1.130737066268921,\n",
       "  1.1033198833465576,\n",
       "  1.0799574851989746,\n",
       "  1.0673094987869263,\n",
       "  1.0290958881378174,\n",
       "  1.008555293083191,\n",
       "  0.9994961619377136,\n",
       "  0.9761124849319458,\n",
       "  0.9613146185874939,\n",
       "  0.9395744800567627,\n",
       "  0.9266930222511292,\n",
       "  0.9204065799713135,\n",
       "  0.8923746943473816],\n",
       " 'metric': [0.1171799972653389,\n",
       "  0.21318000555038452,\n",
       "  0.2761400043964386,\n",
       "  0.3158800005912781,\n",
       "  0.3582000136375427,\n",
       "  0.38741999864578247,\n",
       "  0.41449999809265137,\n",
       "  0.43397998809814453,\n",
       "  0.45399999618530273,\n",
       "  0.4733000099658966,\n",
       "  0.4905799925327301,\n",
       "  0.5075399875640869,\n",
       "  0.5205199718475342,\n",
       "  0.530780017375946,\n",
       "  0.5448200106620789,\n",
       "  0.5571399927139282,\n",
       "  0.5683599710464478,\n",
       "  0.5814200043678284,\n",
       "  0.5896400213241577,\n",
       "  0.5994399785995483,\n",
       "  0.6070600152015686,\n",
       "  0.6165000200271606,\n",
       "  0.6223400235176086,\n",
       "  0.6282399892807007,\n",
       "  0.63919997215271,\n",
       "  0.6452800035476685,\n",
       "  0.656059980392456,\n",
       "  0.6616799831390381,\n",
       "  0.6673799753189087,\n",
       "  0.6754999756813049,\n",
       "  0.6765400171279907,\n",
       "  0.6879799962043762,\n",
       "  0.6929200291633606,\n",
       "  0.6942200064659119,\n",
       "  0.7009000182151794,\n",
       "  0.7058600187301636,\n",
       "  0.7095999717712402,\n",
       "  0.7168400287628174,\n",
       "  0.7160599827766418,\n",
       "  0.724399983882904],\n",
       " 'val_loss': [4.554275989532471,\n",
       "  3.5045278072357178,\n",
       "  3.0465550422668457,\n",
       "  2.7370235919952393,\n",
       "  2.6952080726623535,\n",
       "  2.462343692779541,\n",
       "  2.2611353397369385,\n",
       "  2.329519271850586,\n",
       "  2.129441499710083,\n",
       "  2.0682883262634277,\n",
       "  1.979614019393921,\n",
       "  1.975751519203186,\n",
       "  1.9861687421798706,\n",
       "  1.924804925918579,\n",
       "  1.873439073562622,\n",
       "  1.8757586479187012,\n",
       "  1.8468409776687622,\n",
       "  1.8844707012176514,\n",
       "  1.7855619192123413,\n",
       "  1.8389984369277954,\n",
       "  1.792283535003662,\n",
       "  1.758955717086792,\n",
       "  1.8370418548583984,\n",
       "  1.787279486656189,\n",
       "  1.7691081762313843,\n",
       "  1.781398057937622,\n",
       "  1.834904670715332,\n",
       "  1.816506028175354,\n",
       "  1.762529730796814,\n",
       "  1.788341760635376,\n",
       "  1.7922840118408203,\n",
       "  1.814652681350708,\n",
       "  1.779449462890625,\n",
       "  1.7505155801773071,\n",
       "  1.8000221252441406,\n",
       "  1.7974814176559448,\n",
       "  1.8462581634521484,\n",
       "  1.8270609378814697,\n",
       "  1.7986501455307007,\n",
       "  1.8381491899490356],\n",
       " 'val_metric': [0.1434,\n",
       "  0.2326,\n",
       "  0.3036,\n",
       "  0.3477,\n",
       "  0.3539,\n",
       "  0.3981,\n",
       "  0.4195,\n",
       "  0.4221,\n",
       "  0.4502,\n",
       "  0.4616,\n",
       "  0.4791,\n",
       "  0.4847,\n",
       "  0.4878,\n",
       "  0.5016,\n",
       "  0.5141,\n",
       "  0.5164,\n",
       "  0.5233,\n",
       "  0.514,\n",
       "  0.5399,\n",
       "  0.5267,\n",
       "  0.5402,\n",
       "  0.5458,\n",
       "  0.538,\n",
       "  0.545,\n",
       "  0.5465,\n",
       "  0.5502,\n",
       "  0.5446,\n",
       "  0.548,\n",
       "  0.5596,\n",
       "  0.5569,\n",
       "  0.558,\n",
       "  0.5569,\n",
       "  0.5625,\n",
       "  0.5666,\n",
       "  0.5595,\n",
       "  0.5656,\n",
       "  0.5596,\n",
       "  0.5638,\n",
       "  0.5677,\n",
       "  0.5631],\n",
       " 'hidden_layer_sizes': [[32, 85, 128, 128, 85, 1365, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365, 1365],\n",
       "  [32, 85, 128, 128, 85, 1365, 1365]]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "schedule = Schedule([StaticEpochNoRegularization()] * 40)\n",
    "train_fn_conv(x=cifar100.X_train_norm, y=cifar100.y_train, \n",
    "              validation_data=(cifar100.X_test_norm, cifar100.y_test), learning_rate=0.001, \n",
    "              schedule=schedule, layer_sizes=layer_sizes, output_neurons=100, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe5bb57-fc12-4669-a46f-b3dd7f896b7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
