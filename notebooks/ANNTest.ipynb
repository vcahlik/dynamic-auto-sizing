{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Implements the sigmoid activation in numpy\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of sigmoid(z), same shape as Z\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implement the RELU function.\n",
    "\n",
    "    Arguments:\n",
    "    Z -- Output of the linear layer, of any shape\n",
    "\n",
    "    Returns:\n",
    "    A -- Post-activation parameter, of the same shape as Z\n",
    "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    A = np.maximum(0,Z)\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    cache = Z \n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n",
    "    mini_batch_size -- size of the mini-batches, integer\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(seed)            # To make your \"random\" minibatches the same as ours\n",
    "    m = X.shape[1]                  # number of training examples\n",
    "    mini_batches = []\n",
    "        \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((1,m))\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k * mini_batch_size : (k+1) * mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : (k+1) * mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : ]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : ]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = W.dot(A) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)    \n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], \"relu\")\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y, debug=False):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    if debug:\n",
    "        print(m)\n",
    "    # Compute loss from aL and y.\n",
    "    cost = -1/m * np.sum((Y * np.log(AL)) + ((1-Y) * np.log(1-AL)))\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = 1/m * (dZ.dot(A_prev.T))\n",
    "    db = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = W.T.dot(dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, \"sigmoid\")\n",
    "    \n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, \"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l + 1)]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_epochs = 3000, print_cost=False):#lr was 0.009\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization. (≈ 1 line of code)\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for epoch_no in range(0, num_epochs):\n",
    "        mini_batches = random_mini_batches(X, Y, 64)\n",
    "        \n",
    "        for X_batch, y_batch in mini_batches:\n",
    "            # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "            AL, caches = L_model_forward(X_batch, parameters)\n",
    "\n",
    "            # Compute cost.\n",
    "            cost = compute_cost(AL, y_batch)\n",
    "\n",
    "            # Backward propagation.\n",
    "            grads = L_model_backward(AL, y_batch, caches)\n",
    "\n",
    "            # Update parameters.\n",
    "            parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "#         # Print the cost every 100 training example\n",
    "#         if print_cost and epoch_no % 50 == 0:\n",
    "#             print (\"Cost after epoch %i: %f\" %(epoch_no, cost))\n",
    "#         if print_cost and epoch_no % 50 == 0:\n",
    "#             costs.append(cost)\n",
    "        if print_cost:\n",
    "            print (\"Cost after epoch %i: %f\" %(epoch_no, cost))\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR_PATH = '../mnist/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(DATA_DIR_PATH + 'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pixels = data.iloc[3, 1:].to_numpy().reshape((28, 28))\n",
    "plt.imshow(pixels, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, 1:]\n",
    "y = (data['label'] == 3).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = X_train.to_numpy().T / 255\n",
    "y_train = y_train.to_numpy().reshape((1, -1))\n",
    "X_test = X_test.to_numpy().T / 255\n",
    "y_test = y_test.to_numpy().reshape((1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((784, 33600), (1, 33600))"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((784, 8400), (1, 8400))"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dims = [784, 20, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 0.307430\n",
      "Cost after epoch 1: 0.224911\n",
      "Cost after epoch 2: 0.192119\n",
      "Cost after epoch 3: 0.152696\n",
      "Cost after epoch 4: 0.116729\n",
      "Cost after epoch 5: 0.102807\n",
      "Cost after epoch 6: 0.095372\n",
      "Cost after epoch 7: 0.090521\n",
      "Cost after epoch 8: 0.087073\n",
      "Cost after epoch 9: 0.084371\n",
      "Cost after epoch 10: 0.082173\n",
      "Cost after epoch 11: 0.080344\n",
      "Cost after epoch 12: 0.078794\n",
      "Cost after epoch 13: 0.077448\n",
      "Cost after epoch 14: 0.076255\n",
      "Cost after epoch 15: 0.075200\n",
      "Cost after epoch 16: 0.074227\n",
      "Cost after epoch 17: 0.073354\n",
      "Cost after epoch 18: 0.072551\n",
      "Cost after epoch 19: 0.071802\n",
      "Cost after epoch 20: 0.071095\n",
      "Cost after epoch 21: 0.070352\n",
      "Cost after epoch 22: 0.069572\n",
      "Cost after epoch 23: 0.068800\n",
      "Cost after epoch 24: 0.068200\n",
      "Cost after epoch 25: 0.067607\n",
      "Cost after epoch 26: 0.067025\n",
      "Cost after epoch 27: 0.066461\n",
      "Cost after epoch 28: 0.065930\n",
      "Cost after epoch 29: 0.065378\n",
      "Cost after epoch 30: 0.064838\n",
      "Cost after epoch 31: 0.064301\n",
      "Cost after epoch 32: 0.063765\n",
      "Cost after epoch 33: 0.063196\n",
      "Cost after epoch 34: 0.062612\n",
      "Cost after epoch 35: 0.062028\n",
      "Cost after epoch 36: 0.061449\n",
      "Cost after epoch 37: 0.060841\n",
      "Cost after epoch 38: 0.060259\n",
      "Cost after epoch 39: 0.059635\n",
      "Cost after epoch 40: 0.058985\n",
      "Cost after epoch 41: 0.058326\n",
      "Cost after epoch 42: 0.057653\n",
      "Cost after epoch 43: 0.056946\n",
      "Cost after epoch 44: 0.056229\n",
      "Cost after epoch 45: 0.055507\n",
      "Cost after epoch 46: 0.054784\n",
      "Cost after epoch 47: 0.054012\n",
      "Cost after epoch 48: 0.053265\n",
      "Cost after epoch 49: 0.052525\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = L_layer_model(X_train, y_train, layers_dims, learning_rate = 0.01, num_epochs=50, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.11839740685625613\n",
      "0.1783026316238803\n",
      "-0.2230970016221596\n",
      "0.6351707345547597\n",
      "-1.2587204884254504\n",
      "0.9536863713369919\n",
      "-0.56101259366799\n",
      "-0.56101259366799\n"
     ]
    }
   ],
   "source": [
    "for key, matrix in parameters.items():\n",
    "    print(matrix.min())\n",
    "    print(matrix.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, _ = L_model_forward(X_train, parameters)\n",
    "y_train_pred = (p > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9802083333333333"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_train.reshape((-1),), y_train_pred.reshape((-1),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9758333333333333"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p, _ = L_model_forward(X_test, parameters)\n",
    "y_test_pred = (p > 0.5).astype(int)\n",
    "accuracy_score(y_test.reshape((-1),), y_test_pred.reshape((-1),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
