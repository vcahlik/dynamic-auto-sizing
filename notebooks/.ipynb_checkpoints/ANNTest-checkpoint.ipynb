{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Implements the sigmoid activation in numpy\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of sigmoid(z), same shape as Z\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implement the RELU function.\n",
    "\n",
    "    Arguments:\n",
    "    Z -- Output of the linear layer, of any shape\n",
    "\n",
    "    Returns:\n",
    "    A -- Post-activation parameter, of the same shape as Z\n",
    "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    A = np.maximum(0,Z)\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    cache = Z \n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = W.dot(A) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)    \n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], \"relu\")\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    cost = -1/m * np.sum((Y * np.log(AL)) + ((1-Y) * np.log(1-AL)))\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = 1/m * (dZ.dot(A_prev.T))\n",
    "    db = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = W.T.dot(dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, \"sigmoid\")\n",
    "    \n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, \"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l + 1)]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):#lr was 0.009\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization. (≈ 1 line of code)\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        \n",
    "        # Compute cost.\n",
    "        cost = compute_cost(AL, Y)\n",
    "    \n",
    "        # Backward propagation.\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    " \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 50 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 50 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR_PATH = '../mnist/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(DATA_DIR_PATH + 'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pixels = data.iloc[3, 1:].to_numpy().reshape((28, 28))\n",
    "plt.imshow(pixels, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, 1:]\n",
    "y = (data['label'] == 3).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = X_train.to_numpy().T\n",
    "y_train = y_train.to_numpy().reshape((1, -1))\n",
    "X_test = X_test.to_numpy().T\n",
    "y_test = y_test.to_numpy().reshape((1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((784, 33600), (1, 33600))"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((784, 8400), (1, 8400))"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dims = [784, 20, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.820615\n",
      "Cost after iteration 50: 0.122285\n",
      "Cost after iteration 100: 0.109544\n",
      "Cost after iteration 150: 0.102582\n",
      "Cost after iteration 200: 0.097950\n",
      "Cost after iteration 250: 0.094464\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = L_layer_model(X_train, y_train, layers_dims, num_iterations = 300, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, _ = L_model_forward(X_train, parameters)\n",
    "y_train_pred = (p > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9812202380952381"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_train.reshape((-1),), y_train_pred.reshape((-1),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9748809523809524"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p, _ = L_model_forward(X_test, parameters)\n",
    "y_test_pred = (p > 0.5).astype(int)\n",
    "accuracy_score(y_test.reshape((-1),), y_test_pred.reshape((-1),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
