{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f_deAUKlniFk",
    "outputId": "434f0bdd-1358-46dc-adc2-aee14230789c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: nvidia-smi\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
    "  print('and then re-execute this cell.')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "yKwUwV_NneIo"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-06 09:47:32.409151: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-06 09:47:32.409180: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-04-06 09:47:34.690443: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-04-06 09:47:34.690472: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-04-06 09:47:34.690492: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (batbook): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from enum import Enum\n",
    "import imageio\n",
    "import hashlib\n",
    "import copy\n",
    "import time\n",
    "import abc\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "dtype = 'float32'\n",
    "tf.keras.backend.set_floatx(dtype)\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "UTZq4KMpneIv"
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# DATASETS\n",
    "################################################################################\n",
    "\n",
    "\n",
    "def get_dataset_sample(X, y, fraction, seed=None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)  # Set random seed\n",
    "    selection = np.random.choice([True, False], len(X), p=[fraction, 1 - fraction])\n",
    "    if seed is not None:\n",
    "        np.random.seed()  # Unset random seed\n",
    "    X_sampled = X[selection]\n",
    "    y_sampled = y[selection]\n",
    "    return X_sampled, y_sampled\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, X_train, y_train, X_test, y_test, shape, shape_flattened, fraction, vision=True, standardize=True):\n",
    "        if fraction is not None:\n",
    "            X_train, y_train = get_dataset_sample(X_train, y_train, fraction, seed=42)\n",
    "            X_test, y_test = get_dataset_sample(X_test, y_test, fraction, seed=42)\n",
    "        \n",
    "        X_train = X_train.astype(dtype)\n",
    "        y_train = y_train.astype(dtype)\n",
    "        X_test = X_test.astype(dtype)\n",
    "        y_test = y_test.astype(dtype)\n",
    "\n",
    "        if vision:\n",
    "            X_train = X_train / 255.0\n",
    "            X_test = X_test / 255.0\n",
    "\n",
    "        X_train = np.reshape(X_train, shape_flattened)\n",
    "        X_test = np.reshape(X_test, shape_flattened)\n",
    "\n",
    "        X = np.concatenate((X_train, X_test), axis=0)\n",
    "        y = np.concatenate((y_train, y_test), axis=0)\n",
    "\n",
    "        if standardize:\n",
    "            from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "            scaler = StandardScaler()\n",
    "            scaler.fit(X_train)  # Scaling each feature independently\n",
    "\n",
    "            X_norm = scaler.transform(X)\n",
    "            X_train_norm = scaler.transform(X_train)\n",
    "            X_test_norm = scaler.transform(X_test)\n",
    "        else:\n",
    "            X_norm = X.copy()\n",
    "            X_train_norm = X_train.copy()\n",
    "            X_test_norm = X_test.copy()\n",
    "\n",
    "        X_norm = np.reshape(X_norm, shape)\n",
    "        X_train_norm = np.reshape(X_train_norm, shape)\n",
    "        X_test_norm = np.reshape(X_test_norm, shape)\n",
    "\n",
    "        del X, X_train, X_test\n",
    "\n",
    "        self.X_norm = X_norm\n",
    "        self.y = y\n",
    "        self.X_train_norm = X_train_norm\n",
    "        self.y_train = y_train\n",
    "        self.X_test_norm = X_test_norm\n",
    "        self.y_test = y_test\n",
    "\n",
    "\n",
    "def get_cifar_10_dataset(fraction=None):\n",
    "    cifar10 = tf.keras.datasets.cifar10\n",
    "    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "    shape = (-1, 32, 32, 3)\n",
    "    shape_flattened = (-1, 3072)  # Scaling each feature independently\n",
    "    return Dataset(X_train, y_train, X_test, y_test, shape=shape, shape_flattened=shape_flattened, fraction=fraction)\n",
    "\n",
    "\n",
    "def get_cifar_100_dataset(fraction=None):\n",
    "    cifar100 = tf.keras.datasets.cifar100\n",
    "    (X_train, y_train), (X_test, y_test) = cifar100.load_data()\n",
    "    shape = (-1, 32, 32, 3)\n",
    "    shape_flattened = (-1, 3072)  # Scaling each feature independently\n",
    "    return Dataset(X_train, y_train, X_test, y_test, shape=shape, shape_flattened=shape_flattened, fraction=fraction)\n",
    "\n",
    "\n",
    "def get_svhn_dataset(fraction=None):\n",
    "    from urllib.request import urlretrieve\n",
    "    from scipy import io\n",
    "\n",
    "    train_filename, _ = urlretrieve('http://ufldl.stanford.edu/housenumbers/train_32x32.mat')\n",
    "    test_filename, _ = urlretrieve('http://ufldl.stanford.edu/housenumbers/test_32x32.mat')\n",
    "\n",
    "    X_train = io.loadmat(train_filename, variable_names='X').get('X')\n",
    "    y_train = io.loadmat(train_filename, variable_names='y').get('y')\n",
    "    X_test = io.loadmat(test_filename, variable_names='X').get('X')\n",
    "    y_test = io.loadmat(test_filename, variable_names='y').get('y')\n",
    "\n",
    "    X_train = np.moveaxis(X_train, -1, 0)\n",
    "    y_train -= 1\n",
    "    X_test = np.moveaxis(X_test, -1, 0)\n",
    "    y_test -= 1\n",
    "\n",
    "    shape = (-1, 32, 32, 3)\n",
    "    shape_flattened = (-1, 3072)  # Scaling each feature independently\n",
    "    return Dataset(X_train, y_train, X_test, y_test, shape=shape, shape_flattened=shape_flattened, fraction=fraction)\n",
    "\n",
    "\n",
    "def get_tiny_imagenet_dataset(fraction=None):\n",
    "    \"\"\"\n",
    "    Original source: https://github.com/sonugiri1043/Train_ResNet_On_Tiny_ImageNet/blob/master/Train_ResNet_On_Tiny_ImageNet.ipynb\n",
    "    Original author: sonugiri1043@gmail.com\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.isdir('IMagenet'):\n",
    "        ! git clone https://github.com/seshuad/IMagenet\n",
    "\n",
    "    print(\"Processing the downloaded dataset...\")\n",
    "\n",
    "    path = 'IMagenet/tiny-imagenet-200/'\n",
    "\n",
    "    id_dict = {}\n",
    "    for i, line in enumerate(open(path + 'wnids.txt', 'r')):\n",
    "        id_dict[line.replace('\\n', '')] = i\n",
    "\n",
    "    train_data = list()\n",
    "    test_data = list()\n",
    "    train_labels = list()\n",
    "    test_labels = list()\n",
    "\n",
    "    for key, value in id_dict.items():\n",
    "        train_data += [imageio.imread(path + 'train/{}/images/{}_{}.JPEG'.format(key, key, str(i)), pilmode='RGB') for i in range(500)]\n",
    "        train_labels_ = np.array([[0]*200]*500)\n",
    "        train_labels_[:, value] = 1\n",
    "        train_labels += train_labels_.tolist()\n",
    "\n",
    "    for line in open(path + 'val/val_annotations.txt'):\n",
    "        img_name, class_id = line.split('\\t')[:2]\n",
    "        test_data.append(imageio.imread(path + 'val/images/{}'.format(img_name), pilmode='RGB'))\n",
    "        test_labels_ = np.array([[0]*200])\n",
    "        test_labels_[0, id_dict[class_id]] = 1\n",
    "        test_labels += test_labels_.tolist()\n",
    "\n",
    "    X_train = np.array(train_data)\n",
    "    y_train = np.argmax(np.array(train_labels), axis=1)\n",
    "    X_test = np.array(test_data)\n",
    "    y_test = np.argmax(np.array(test_labels), axis=1)\n",
    "\n",
    "    shape = (-1, 64, 64, 3)\n",
    "    shape_flattened = (-1, 12288)  # Scaling each feature independently\n",
    "    return Dataset(X_train, y_train, X_test, y_test, shape=shape, shape_flattened=shape_flattened, fraction=fraction)\n",
    "\n",
    "\n",
    "def get_mnist_dataset(fraction=None):\n",
    "    mnist = tf.keras.datasets.mnist\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "    shape = (-1, 28, 28, 1)\n",
    "    shape_flattened = (-1, 1)  # Scaling all features together\n",
    "    return Dataset(X_train, y_train, X_test, y_test, shape=shape, shape_flattened=shape_flattened, fraction=fraction)\n",
    "\n",
    "\n",
    "def get_fashion_mnist_dataset(fraction=None):\n",
    "    fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "    (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "    shape = (-1, 28, 28, 1)\n",
    "    shape_flattened = (-1, 1)  # Scaling all features together\n",
    "    return Dataset(X_train, y_train, X_test, y_test, shape=shape, shape_flattened=shape_flattened, fraction=fraction)\n",
    "\n",
    "\n",
    "def get_fifteen_puzzle_dataset(path=None, fraction=None):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    if path is None:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/gdrive')\n",
    "        path = 'gdrive/MyDrive/15-costs-v3.csv'\n",
    "    costs = pd.read_csv(path)\n",
    "\n",
    "    X_raw = costs.iloc[:,:-1].values\n",
    "    y = costs['cost'].values\n",
    "    X = np.apply_along_axis(lambda x: np.eye(16)[x].ravel(), 1, X_raw)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    del X, X_raw, y\n",
    "\n",
    "    shape = (-1, 256)\n",
    "    shape_flattened = (-1, 256)  # Scaling all features together\n",
    "    return Dataset(X_train, y_train, X_test, y_test, shape=shape, shape_flattened=shape_flattened, vision=False, fraction=fraction)\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# REGULARIZERS\n",
    "################################################################################\n",
    "\n",
    "\n",
    "class Regularizer(tf.keras.regularizers.Regularizer):\n",
    "    def __init__(self):\n",
    "        self.n_new_neurons = 0\n",
    "        self.scaling_tensor = None\n",
    "        self.set_regularization_penalty(0.)\n",
    "        self.set_regularization_method(None)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if self.regularization_method is None or self.regularization_penalty == 0:\n",
    "            return 0\n",
    "        elif self.regularization_method == 'weighted_l1':\n",
    "            return self.weighted_l1(x)\n",
    "        elif self.regularization_method == 'weighted_l1_reordered':\n",
    "            return self.weighted_l1_reordered(x)\n",
    "        elif self.regularization_method == 'group_sparsity':\n",
    "            return self.group_sparsity(x)\n",
    "        elif self.regularization_method == 'l1':\n",
    "            return self.l1(x)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Unknown regularization method {self.regularization_method}\")\n",
    "    \n",
    "    def weighted_l1(self, x):\n",
    "        # I.e. for a parameter matrix of 4 input and 10 output neurons:\n",
    "        #\n",
    "        # [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
    "        #  [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
    "        #  [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
    "        #  [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]\n",
    "        #\n",
    "        # the scaling tensor, as well as the resulting weighted values, could be:\n",
    "        #\n",
    "        # [[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n",
    "        #  [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n",
    "        #  [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n",
    "        #  [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]]\n",
    "        #\n",
    "        # Therefore every additional output neuron is regularized more.\n",
    "\n",
    "        scaling_tensor = tf.cumsum(tf.constant(self.regularization_penalty, shape=x.shape, dtype=dtype), axis=-1)\n",
    "        weighted_values = scaling_tensor * tf.abs(x)\n",
    "        return tf.reduce_sum(weighted_values)\n",
    "    \n",
    "    def weighted_l1_reordered(self, x):\n",
    "        # I.e. for a parameter matrix of 4 input and 10 output neurons:\n",
    "        #\n",
    "        # [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
    "        #  [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
    "        #  [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
    "        #  [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]\n",
    "        #\n",
    "        # the scaling tensor, as well as the resulting weighted values, could be:\n",
    "        #\n",
    "        # [[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n",
    "        #  [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n",
    "        #  [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n",
    "        #  [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]]\n",
    "        #\n",
    "        # Therefore every additional output neuron is regularized more.\n",
    "\n",
    "        if self.update_scaling_tensor:\n",
    "            scaling_tensor_raw = tf.cumsum(tf.constant(self.regularization_penalty, shape=x.shape, dtype=dtype), axis=-1)\n",
    "\n",
    "            scaling_tensor_old_neurons = scaling_tensor_raw[:, :-self.n_new_neurons]\n",
    "            scaling_tensor_new_neurons = scaling_tensor_raw[:, -self.n_new_neurons:]\n",
    "            scaling_tensor_old_neurons_shuffled = tf.transpose(tf.random.shuffle(tf.transpose(scaling_tensor_old_neurons)))\n",
    "            self.scaling_tensor = tf.concat([scaling_tensor_old_neurons_shuffled, scaling_tensor_new_neurons], axis=-1)\n",
    "            self.update_scaling_tensor = False\n",
    "\n",
    "        weighted_values = self.scaling_tensor * tf.abs(x)\n",
    "        return tf.reduce_sum(weighted_values)\n",
    "    \n",
    "    def group_sparsity(self, x):\n",
    "        # I.e. for a parameter matrix of 3 input and 5 output neurons:\n",
    "        #\n",
    "        # [[1., 1., 1., 1., 1.],\n",
    "        #  [1., 2., 2., 1., 2.],\n",
    "        #  [2., 2., 3., 1., 3.]]\n",
    "        #\n",
    "        # The resulting vector of group norms is [2., 2., 3., 1., 3.], therefore for\n",
    "        # every output neuron, its incoming connections form a group.\n",
    "\n",
    "        group_norms = tf.norm(x, ord=2, axis=0)\n",
    "        # assert group_norms.shape[0] == x.shape[1]\n",
    "        return self.regularization_penalty * tf.reduce_sum(group_norms)\n",
    "    \n",
    "    def l1(self, x):\n",
    "        weighted_values = self.regularization_penalty * tf.abs(x)\n",
    "        return tf.reduce_sum(weighted_values)\n",
    "    \n",
    "    def prune(self):\n",
    "        self.n_new_neurons = 0\n",
    "        if self.regularization_method == 'weighted_l1_reordered':\n",
    "            self.update_scaling_tensor = True\n",
    "    \n",
    "    def grow(self, n_new_neurons):\n",
    "        self.n_new_neurons = n_new_neurons\n",
    "        if self.regularization_method == 'weighted_l1_reordered':\n",
    "            self.update_scaling_tensor = True\n",
    "    \n",
    "    def set_regularization_penalty(self, regularization_penalty):\n",
    "        self.regularization_penalty = regularization_penalty\n",
    "    \n",
    "    def set_regularization_method(self, regularization_method):\n",
    "        self.regularization_method = regularization_method\n",
    "        if self.regularization_method == 'weighted_l1_reordered':\n",
    "            self.update_scaling_tensor = True\n",
    "        else:\n",
    "            self.update_scaling_tensor = None\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'regularization_penalty': float(self.regularization_penalty)}\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# LAYERS\n",
    "################################################################################\n",
    "\n",
    "\n",
    "class DASLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_shape):\n",
    "        super().__init__()\n",
    "\n",
    "        self._input_shape = input_shape\n",
    "\n",
    "\n",
    "class Dense(DASLayer):\n",
    "    def __init__(self, units, activation, kernel_initializer='glorot_uniform', \n",
    "                 bias_initializer='zeros', input_shape=None, fixed_size=False,\n",
    "                 regularizer=None):\n",
    "        super().__init__(input_shape)\n",
    "\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.bias_initializer = bias_initializer\n",
    "        self.fixed_size = fixed_size\n",
    "        \n",
    "        self.A = tf.keras.activations.get(activation)\n",
    "        self.W_init = tf.keras.initializers.get(kernel_initializer)\n",
    "        self.b_init = tf.keras.initializers.get(bias_initializer)\n",
    "        if regularizer is not None:\n",
    "            self.regularizer = regularizer\n",
    "        else:\n",
    "            self.regularizer = Regularizer()\n",
    "    \n",
    "    # def copy(self):\n",
    "    #     input_shape = self._input_shape\n",
    "    #     regularizer_copy = self.regularizer.copy()\n",
    "    #     layer_copy = Dense(self.units, self.activation, self.kernel_initializer, \n",
    "    #                        self.bias_initializer, input_shape, self.fixed_size, \n",
    "    #                        regularizer_copy)\n",
    "    #     return layer_copy\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        input_units = input_shape[-1]\n",
    "\n",
    "        self.W = tf.Variable(\n",
    "            name='W',\n",
    "            initial_value=self.W_init(shape=(input_units, self.units), dtype=dtype),\n",
    "            trainable=True)\n",
    "        \n",
    "        self.b = tf.Variable(\n",
    "            name='b',\n",
    "            initial_value=self.b_init(shape=(self.units,), dtype=dtype),\n",
    "            trainable=True)\n",
    "        \n",
    "        self.add_regularizer_loss()\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        return self.A(tf.matmul(inputs, self.W) + self.b)\n",
    "    \n",
    "    def add_regularizer_loss(self):\n",
    "        self.add_loss(lambda: self.regularizer(tf.concat([self.W, tf.reshape(self.b, (1, -1))], axis=0)))\n",
    "\n",
    "    def get_size(self):\n",
    "        return self.W.shape[0], self.W.shape[1]\n",
    "    \n",
    "    def prune(self, threshold, active_input_units_indices):\n",
    "        # Remove connections from pruned units in previous layer\n",
    "        new_W = tf.gather(self.W.value(), active_input_units_indices, axis=0)\n",
    "\n",
    "        if self.fixed_size:\n",
    "            active_output_neurons_indices = list(range(new_W.shape[1]))\n",
    "        else:\n",
    "            # Prune units in this layer\n",
    "            weights_with_biases = tf.concat([new_W, tf.reshape(self.b.value(), (1, -1))], axis=0)\n",
    "            neurons_are_active = tf.math.reduce_max(tf.abs(weights_with_biases), axis=0) >= threshold\n",
    "            active_output_neurons_indices = tf.reshape(tf.where(neurons_are_active), (-1,))\n",
    "            \n",
    "            new_W = tf.gather(new_W, active_output_neurons_indices, axis=1)\n",
    "            new_b = tf.gather(self.b.value(), active_output_neurons_indices, axis=0)\n",
    "\n",
    "            self.b = tf.Variable(name='b', initial_value=new_b, trainable=True)\n",
    "\n",
    "        self.W = tf.Variable(name='W', initial_value=new_W, trainable=True)\n",
    "\n",
    "        self.regularizer.prune()\n",
    "        return active_output_neurons_indices\n",
    "    \n",
    "    def grow(self, n_new_input_units, percentage, min_new_units, scaling_factor):\n",
    "        if n_new_input_units > 0:\n",
    "            # Add connections to grown units in previous layer\n",
    "            W_growth = self.W_init(shape=(self.W.shape[0] + n_new_input_units, self.W.shape[1]), dtype=dtype)[-n_new_input_units:, :] * scaling_factor  # TODO is it better to be multiplying here by scaling_factor? It does help with not increasing the max weights of existing neurons when new neurons are added.\n",
    "            new_W = tf.concat([self.W.value(), W_growth], axis=0)\n",
    "        else:\n",
    "            new_W = self.W.value()\n",
    "\n",
    "        if self.fixed_size:\n",
    "            n_new_output_units = 0\n",
    "        else:\n",
    "            # Grow new units in this layer\n",
    "            n_new_output_units = max(min_new_units, int(new_W.shape[1] * percentage))\n",
    "            if n_new_output_units > 0:\n",
    "                W_growth = self.W_init(shape=(new_W.shape[0], new_W.shape[1] + n_new_output_units), dtype=dtype)[:, -n_new_output_units:] * scaling_factor\n",
    "                b_growth = self.b_init(shape=(n_new_output_units,), dtype=dtype)  # TODO for all possible bias initializers to work properly, the whole bias vector should be initialized at once\n",
    "                new_W = tf.concat([new_W, W_growth], axis=1)\n",
    "                new_b = tf.concat([self.b.value(), b_growth], axis=0)\n",
    "\n",
    "                self.b = tf.Variable(name='b', initial_value=new_b, trainable=True)\n",
    "\n",
    "        self.W = tf.Variable(name='W', initial_value=new_W, trainable=True)\n",
    "\n",
    "        self.regularizer.grow(n_new_output_units)\n",
    "        return n_new_output_units\n",
    "    \n",
    "    def mutate(self, mutation_strength):\n",
    "        self.W.assign_add(tf.random.normal(self.W.shape, mean=0.0, stddev=mutation_strength))\n",
    "        self.b.assign_add(tf.random.normal(self.b.shape, mean=0.0, stddev=mutation_strength))\n",
    "    \n",
    "    def set_regularization_penalty(self, regularization_penalty):\n",
    "        if not self.fixed_size:\n",
    "            self.regularizer.set_regularization_penalty(regularization_penalty)\n",
    "    \n",
    "    def set_regularization_method(self, regularization_method):\n",
    "        if not self.fixed_size:\n",
    "            self.regularizer.set_regularization_method(regularization_method)\n",
    "    \n",
    "    def get_param_string():\n",
    "        param_string = \"\"\n",
    "        weights_with_bias = tf.concat([self.W, tf.reshape(self.b, (1, -1))], axis=0)\n",
    "        max_parameters = tf.math.reduce_max(tf.abs(weights_with_bias), axis=0).numpy()\n",
    "        magnitudes = np.floor(np.log10(max_parameters))\n",
    "        for m in magnitudes:\n",
    "            if m > 0:\n",
    "                m = 0\n",
    "            param_string += str(int(-m))\n",
    "        return param_string\n",
    "\n",
    "\n",
    "class Conv2D(DASLayer):\n",
    "    def __init__(self, filters, filter_size, activation, strides=(1, 1), \n",
    "                 padding='SAME', kernel_initializer='glorot_uniform',\n",
    "                 bias_initializer='zeros', input_shape=None, fixed_size=False,\n",
    "                 regularizer=None):\n",
    "        super().__init__(input_shape)\n",
    "    \n",
    "        self.filters = filters\n",
    "        self.filter_size = filter_size\n",
    "        self.activation = activation\n",
    "        self.strides = strides\n",
    "        self.padding = padding\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.bias_initializer = bias_initializer\n",
    "        self.fixed_size = fixed_size\n",
    "        \n",
    "        self.A = tf.keras.activations.get(activation)\n",
    "        self.F_init = tf.keras.initializers.get(kernel_initializer)\n",
    "        self.b_init = tf.keras.initializers.get(bias_initializer)\n",
    "        if regularizer is not None:\n",
    "            self.regularizer = regularizer\n",
    "        else:\n",
    "            self.regularizer = Regularizer()            \n",
    "        \n",
    "    # def copy(self):\n",
    "    #     input_shape = self._input_shape\n",
    "    #     regularizer_copy = self.regularizer.copy()\n",
    "    #     layer_copy = Conv2D(self.filters, self.filter_size, self.activation, self.strides, \n",
    "    #                         self.padding, self.kernel_initializer, self.bias_initializer, \n",
    "    #                         input_shape, self.fixed_size, regularizer_copy)\n",
    "    #     return layer_copy\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        input_filters = input_shape[-1]\n",
    "\n",
    "        self.F = tf.Variable(\n",
    "            name='F',\n",
    "            initial_value=self.F_init(\n",
    "                shape=(self.filter_size[0], self.filter_size[1], input_filters, self.filters), dtype=dtype\n",
    "            ),\n",
    "            trainable=True)\n",
    "        \n",
    "        self.b = tf.Variable(\n",
    "            name='b',\n",
    "            initial_value=self.b_init(shape=(self.filters,), dtype=dtype),\n",
    "            trainable=True)\n",
    "\n",
    "        self.add_regularizer_loss()\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        y = tf.nn.conv2d(inputs, self.F, strides=self.strides, padding=self.padding)\n",
    "        y = tf.nn.bias_add(y, self.b)\n",
    "        y = self.A(y)\n",
    "        return y\n",
    "    \n",
    "    def add_regularizer_loss(self):\n",
    "        self.add_loss(lambda: self.regularizer(tf.concat([tf.reshape(self.F, (-1, self.F.shape[-1])), tf.reshape(self.b, (1, -1))], axis=0)))\n",
    "    \n",
    "    def get_size(self):\n",
    "        return self.F.shape[-2], self.F.shape[-1]\n",
    "    \n",
    "    def prune(self, threshold, active_input_units_indices):\n",
    "        # Remove connections from pruned units in previous layer\n",
    "        new_F = tf.gather(self.F.value(), active_input_units_indices, axis=-2)\n",
    "\n",
    "        if self.fixed_size:\n",
    "            active_output_filters_indices = list(range(new_F.shape[-1]))\n",
    "        else:\n",
    "            # Prune units in this layer\n",
    "            F_reduced_max = tf.reshape(tf.math.reduce_max(tf.abs(new_F), axis=(0, 1, 2)), (1, -1))\n",
    "            F_reduced_max_with_biases = tf.concat([F_reduced_max, tf.reshape(self.b.value(), (1, -1))], axis=0)\n",
    "            filters_are_active = tf.math.reduce_max(tf.abs(F_reduced_max_with_biases), axis=0) >= threshold\n",
    "            active_output_filters_indices = tf.reshape(tf.where(filters_are_active), (-1,))\n",
    "            \n",
    "            new_F = tf.gather(new_F, active_output_filters_indices, axis=-1)\n",
    "            new_b = tf.gather(self.b.value(), active_output_filters_indices, axis=0)\n",
    "\n",
    "            self.b = tf.Variable(name='b', initial_value=new_b, trainable=True)\n",
    "\n",
    "        self.F = tf.Variable(name='F', initial_value=new_F, trainable=True)\n",
    "\n",
    "        self.regularizer.prune()\n",
    "        return active_output_filters_indices\n",
    "\n",
    "    def grow(self, n_new_input_units, percentage, min_new_units, scaling_factor):\n",
    "        if n_new_input_units > 0:\n",
    "            # Add connections to grown units in previous layer\n",
    "            F_growth = self.F_init(shape=(self.F.shape[0], self.F.shape[1], self.F.shape[2] + n_new_input_units, self.F.shape[3]), dtype=dtype)[:, :, -n_new_input_units:, :] * scaling_factor  # TODO is it better to be multiplying here by scaling_factor? It does help with not increasing the max weights of existing neurons when new neurons are added.\n",
    "            new_F = tf.concat([self.F.value(), F_growth], axis=-2)\n",
    "        else:\n",
    "            new_F = self.F.value()\n",
    "\n",
    "        if self.fixed_size:\n",
    "            n_new_output_units = 0\n",
    "        else:\n",
    "            # Grow new units in this layer\n",
    "            n_new_output_units = max(min_new_units, int(new_F.shape[-1] * percentage))\n",
    "            if n_new_output_units > 0:\n",
    "                F_growth = self.F_init(shape=(new_F.shape[0], new_F.shape[1], new_F.shape[2], new_F.shape[3] + n_new_output_units), dtype=dtype)[:, :, :, -n_new_output_units:] * scaling_factor\n",
    "                b_growth = self.b_init(shape=(n_new_output_units,), dtype=dtype)  # TODO for all possible bias initializers to work properly, the whole bias vector should be initialized at once\n",
    "                new_F = tf.concat([new_F, F_growth], axis=-1)\n",
    "                new_b = tf.concat([self.b.value(), b_growth], axis=0)\n",
    "\n",
    "                self.b = tf.Variable(name='b', initial_value=new_b, trainable=True)\n",
    "\n",
    "        self.F = tf.Variable(name='F', initial_value=new_F, trainable=True)\n",
    "\n",
    "        self.regularizer.grow(n_new_output_units)\n",
    "        return n_new_output_units\n",
    "    \n",
    "    def mutate(self, mutation_strength):\n",
    "        self.F.assign_add(tf.random.normal(self.F.shape, mean=0.0, stddev=mutation_strength))\n",
    "        self.b.assign_add(tf.random.normal(self.b.shape, mean=0.0, stddev=mutation_strength))\n",
    "    \n",
    "    def set_regularization_penalty(self, regularization_penalty):\n",
    "        if not self.fixed_size:\n",
    "            self.regularizer.set_regularization_penalty(regularization_penalty)\n",
    "    \n",
    "    def set_regularization_method(self, regularization_method):\n",
    "        if not self.fixed_size:\n",
    "            self.regularizer.set_regularization_method(regularization_method)\n",
    "\n",
    "    def get_param_string():\n",
    "        param_string = \"\"\n",
    "        # TODO\n",
    "        return param_string\n",
    "\n",
    "\n",
    "class Flatten(tf.keras.layers.Layer):\n",
    "    def call(self, inputs, training=None):\n",
    "        return tf.reshape(tf.transpose(inputs, perm=[0, 3, 1, 2]), (inputs.shape[0], -1))\n",
    "    \n",
    "    # def copy(self):\n",
    "    #     return Flatten()\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# MODELS\n",
    "################################################################################\n",
    "\n",
    "\n",
    "class Epoch:\n",
    "    def __init__(self, grow, prune, regularization_penalty, regularization_method):\n",
    "        self.grow = grow\n",
    "        self.prune = prune\n",
    "        self.regularization_penalty = regularization_penalty\n",
    "        self.regularization_method = regularization_method\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f'{int(self.grow)}{int(self.prune)}{self.regularization_penalty}{self.regularization_method}'\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "\n",
    "class DynamicEpoch(Epoch):\n",
    "    def __init__(self, regularization_penalty, regularization_method):\n",
    "        super().__init__(True, True, regularization_penalty, regularization_method)\n",
    "\n",
    "\n",
    "class StaticEpoch(Epoch):\n",
    "    def __init__(self, regularization_penalty, regularization_method):\n",
    "        super().__init__(False, False, regularization_penalty, regularization_method)\n",
    "\n",
    "\n",
    "class StaticEpochNoRegularization(StaticEpoch):\n",
    "    def __init__(self):\n",
    "        super().__init__(0., None)\n",
    "\n",
    "\n",
    "class Schedule:\n",
    "    def __init__(self, epochs):\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self.epochs.__iter__()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.epochs)\n",
    "    \n",
    "    def __str__(self):\n",
    "        text = ''.join([str(epoch) for epoch in self.epochs])\n",
    "        return hashlib.sha1(text.encode('utf-8')).hexdigest()[:10]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "\n",
    "class Sequential(tf.keras.Model):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lrs = layers\n",
    "        \n",
    "    def call(self, inputs, training=None):\n",
    "        x = inputs\n",
    "        for layer in self.lrs:\n",
    "            x = layer(x, training=training)\n",
    "        return x\n",
    "    \n",
    "    def copy(self):\n",
    "        copied_layers = list()\n",
    "        for layer in self.lrs:\n",
    "            if isinstance(layer, DASLayer):\n",
    "                # layer_copy = layer.copy()\n",
    "                layer_copy = copy.deepcopy(layer)\n",
    "                layer_copy.add_regularizer_loss()\n",
    "            else:\n",
    "                layer_copy = copy.deepcopy(layer)\n",
    "            copied_layers.append(layer_copy)\n",
    "        \n",
    "        model_copy = Sequential(copied_layers)\n",
    "        return model_copy\n",
    "    \n",
    "    def get_layer_input_shape(self, target_layer):\n",
    "        if target_layer._input_shape is not None:\n",
    "            return target_layer._input_shape\n",
    "\n",
    "        input = np.random.normal(size=(1,) + self.lrs[0]._input_shape)\n",
    "        for layer in self.lrs:\n",
    "            if layer is target_layer:\n",
    "                return tuple(input.shape[1:])\n",
    "            input = layer(input)\n",
    "        raise Exception(\"Layer not found in the model.\")\n",
    "\n",
    "    def get_layer_output_shape(self, target_layer):\n",
    "        input = np.random.normal(size=(1,) + self.lrs[0]._input_shape)\n",
    "        for layer in self.lrs:\n",
    "            output = layer(input)\n",
    "            if layer is target_layer:\n",
    "                return tuple(output.shape[1:])\n",
    "            input = output\n",
    "        raise Exception(\"Layer not found in the model.\")\n",
    "    \n",
    "    def get_layer_sizes(self):\n",
    "        \"\"\"\n",
    "        Returns the sizes of all layers in the model, including the input and output layer.\n",
    "        \"\"\"\n",
    "        layer_sizes = list()\n",
    "        first_layer = True\n",
    "        for l in range(len(self.lrs)):\n",
    "            layer = self.lrs[l]\n",
    "            if isinstance(layer, DASLayer):\n",
    "                layer_size = layer.get_size()\n",
    "                if first_layer:\n",
    "                    layer_sizes.append(layer_size[0])\n",
    "                    first_layer = False\n",
    "                layer_sizes.append(layer_size[1])\n",
    "        return layer_sizes\n",
    "    \n",
    "    def get_hidden_layer_sizes(self):\n",
    "        return self.get_layer_sizes()[1:-1]\n",
    "    \n",
    "    def get_regularization_penalty(self):\n",
    "        #TODO improve\n",
    "        return self.lrs[-2].regularizer.regularization_penalty\n",
    "    \n",
    "    def set_regularization_penalty(self, regularization_penalty):\n",
    "        for layer in self.lrs:\n",
    "            if isinstance(layer, DASLayer) and not layer.fixed_size:\n",
    "                layer.set_regularization_penalty(regularization_penalty)\n",
    "    \n",
    "    def set_regularization_method(self, regularization_method):\n",
    "        for layer in self.lrs:\n",
    "            if isinstance(layer, DASLayer) and not layer.fixed_size:\n",
    "                layer.set_regularization_method(regularization_method)\n",
    "\n",
    "    def prune(self, params):\n",
    "        input_shape = self.get_layer_input_shape(self.lrs[0])\n",
    "        n_input_units = input_shape[-1]\n",
    "        active_units_indices = list(range(n_input_units))\n",
    "\n",
    "        last_custom_layer = None\n",
    "        for layer in self.lrs:\n",
    "            if isinstance(layer, DASLayer):\n",
    "                if last_custom_layer is not None and type(last_custom_layer) != type(layer):\n",
    "                    if type(last_custom_layer) == Conv2D and type(layer) == Dense:\n",
    "                        convolutional_shape = self.get_layer_output_shape(last_custom_layer)\n",
    "                        active_units_indices = self.convert_channel_indices_to_flattened_indices(active_units_indices, convolutional_shape)\n",
    "                    else:\n",
    "                        raise Exception(\"Incorrect order of custom layer types.\")\n",
    "                active_units_indices = layer.prune(params.pruning_threshold, active_units_indices)\n",
    "                last_custom_layer = layer\n",
    "    \n",
    "    def grow(self, params):   \n",
    "        n_new_units = 0\n",
    "\n",
    "        last_custom_layer = None\n",
    "        for layer in self.lrs:\n",
    "            if isinstance(layer, DASLayer):\n",
    "                if last_custom_layer is not None and type(last_custom_layer) != type(layer):\n",
    "                    if type(last_custom_layer) == Conv2D and type(layer) == Dense:\n",
    "                        convolutional_shape = self.get_layer_output_shape(last_custom_layer)\n",
    "                        n_new_units = n_new_units * convolutional_shape[0] * convolutional_shape[1]\n",
    "                    else:\n",
    "                        raise Exception(\"Incorrect order of custom layer types.\")\n",
    "                n_new_units = layer.grow(n_new_units, params.growth_percentage, min_new_units=params.min_new_neurons, scaling_factor=params.pruning_threshold)\n",
    "                last_custom_layer = layer\n",
    "    \n",
    "    def mutate(self, mutation_strength):\n",
    "        for layer in self.lrs:\n",
    "            if isinstance(layer, DASLayer):\n",
    "                layer.mutate(mutation_strength)\n",
    "    \n",
    "    @staticmethod\n",
    "    def convert_channel_indices_to_flattened_indices(channel_indices, convolutional_shape):\n",
    "        dense_indices = list()\n",
    "        units_per_channel = convolutional_shape[0] * convolutional_shape[1]\n",
    "        for channel_index in channel_indices:\n",
    "            for iter in range(units_per_channel):\n",
    "                dense_indices.append(channel_index * units_per_channel + iter)\n",
    "        return dense_indices\n",
    "    \n",
    "    def print_neurons(self):\n",
    "        for layer in self.lrs[:-1]:\n",
    "            print(layer.get_param_string())\n",
    "    \n",
    "    def evaluate(self, params, summed_training_loss, summed_training_metric):\n",
    "        # Calculate training loss and metric\n",
    "        if summed_training_loss is not None:\n",
    "            loss = summed_training_loss / params.x.shape[0]\n",
    "        else:\n",
    "            loss = None\n",
    "        \n",
    "        if summed_training_metric is not None:\n",
    "            metric = summed_training_metric / params.x.shape[0]\n",
    "        else:\n",
    "            metric = None\n",
    "        \n",
    "        # Calculate val loss and metric\n",
    "        summed_val_loss = 0\n",
    "        summed_val_metric = 0\n",
    "        n_val_instances = 0\n",
    "        \n",
    "        for step, (x_batch, y_batch) in enumerate(params.val_dataset):\n",
    "            # y_pred = tf.reshape(self(x_batch, training=False), y_batch.shape)\n",
    "            y_pred = self(x_batch, training=False)\n",
    "            summed_val_loss += tf.reduce_sum(params.loss_fn(y_batch, y_pred))\n",
    "            summed_val_metric += float(tf.reduce_sum(params.metric_fn(y_batch, y_pred)))\n",
    "            n_val_instances += x_batch.shape[0]\n",
    "        \n",
    "        val_loss = summed_val_loss / n_val_instances\n",
    "        val_metric = summed_val_metric / n_val_instances\n",
    "\n",
    "        return loss, metric, val_loss, val_metric\n",
    "\n",
    "    def list_params(self):\n",
    "        trainable_count = np.sum([K.count_params(w) for w in self.trainable_weights])\n",
    "        non_trainable_count = np.sum([K.count_params(w) for w in self.non_trainable_weights])\n",
    "        total_count = trainable_count + non_trainable_count\n",
    "\n",
    "        print('Total params: {:,}'.format(total_count))\n",
    "        print('Trainable params: {:,}'.format(trainable_count))\n",
    "        print('Non-trainable params: {:,}'.format(non_trainable_count))\n",
    "\n",
    "        return total_count, trainable_count, non_trainable_count\n",
    "    \n",
    "    def print_epoch_statistics(self, params, summed_training_loss, summed_training_metric, message=None, require_result=False):\n",
    "        if not params.verbose:\n",
    "            if require_result:\n",
    "                return self.evaluate(params, summed_training_loss, summed_training_metric)\n",
    "            else:\n",
    "                return\n",
    "        \n",
    "        loss, metric, val_loss, val_metric = self.evaluate(params, summed_training_loss, summed_training_metric)  \n",
    "\n",
    "        if message is not None:\n",
    "            print(message)\n",
    "        \n",
    "        print(f\"loss: {loss} - metric: {metric} - val_loss: {val_loss} - val_metric: {val_metric} - penalty: {self.get_regularization_penalty()}\")\n",
    "        hidden_layer_sizes = self.get_hidden_layer_sizes()\n",
    "        print(f\"hidden layer sizes: {hidden_layer_sizes}, total units: {sum(hidden_layer_sizes)}\")\n",
    "        if params.print_neurons:\n",
    "            self.print_neurons()\n",
    "        \n",
    "        if require_result:\n",
    "            return loss, metric, val_loss, val_metric\n",
    "    \n",
    "    def update_history(self, params, loss, metric, val_loss, val_metric):\n",
    "        params.history['loss'].append(float(loss))\n",
    "        params.history['metric'].append(float(metric))\n",
    "        params.history['val_loss'].append(float(val_loss))\n",
    "        params.history['val_metric'].append(float(val_metric))\n",
    "        params.history['hidden_layer_sizes'].append(self.get_hidden_layer_sizes())\n",
    "    \n",
    "    @staticmethod\n",
    "    def prepare_datasets(x, y, batch_size, validation_data):\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "        train_dataset = train_dataset.shuffle(buffer_size=20000).batch(batch_size)\n",
    "        val_dataset = tf.data.Dataset.from_tensor_slices(validation_data).batch(batch_size)\n",
    "        return train_dataset.prefetch(tf.data.AUTOTUNE), val_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    def manage_dynamic_regularization(self, params, val_loss):\n",
    "        if val_loss >= params.best_conditional_val_loss * params.stall_coefficient:\n",
    "            # Training is currently in stall\n",
    "            if not params.training_stalled:\n",
    "                penalty = self.get_regularization_penalty() * params.regularization_penalty_multiplier\n",
    "                print(\"Changing penalty...\")\n",
    "                # TODO this must be modified, penalty can differ for each layer\n",
    "                self.set_regularization_penalty(penalty)\n",
    "                params.training_stalled = True\n",
    "        else:\n",
    "            params.best_conditional_val_loss = val_loss\n",
    "            params.training_stalled = False\n",
    "    \n",
    "    def grow_wrapper(self, params):\n",
    "        dynamic_reqularization_active = params.regularization_penalty_multiplier != 1.\n",
    "        if dynamic_reqularization_active:\n",
    "            loss, metric, val_loss, val_metric = self.print_epoch_statistics(params, None, None, \"Before growing:\", require_result=True)\n",
    "            self.manage_dynamic_regularization(params, val_loss)\n",
    "        else:\n",
    "            self.print_epoch_statistics(params, None, None, \"Before growing:\")\n",
    "\n",
    "        self.grow(params)\n",
    "        self.print_epoch_statistics(params, None, None, \"After growing:\")\n",
    "    \n",
    "    def prune_wrapper(self, params, summed_loss, summed_metric):\n",
    "        loss, metric, _, _ = self.print_epoch_statistics(params, summed_loss, summed_metric, \"Before pruning:\", require_result=True)\n",
    "        self.prune(params)\n",
    "        _, _, val_loss, val_metric = self.print_epoch_statistics(params, None, None, \"After pruning:\", require_result=True)\n",
    "        self.update_history(params, loss, metric, val_loss, val_metric)\n",
    "    \n",
    "    class ParameterContainer:\n",
    "        def __init__(self, x, y, optimizer, batch_size, min_new_neurons, validation_data, pruning_threshold, regularization_penalty_multiplier, \n",
    "                     stall_coefficient, growth_percentage, mini_epochs_per_epoch, verbose, print_neurons, use_static_graph, loss_fn, metric_fn):\n",
    "            self.x = x\n",
    "            self.y = y\n",
    "            self.optimizer = optimizer\n",
    "            self.batch_size = batch_size\n",
    "            self.min_new_neurons = min_new_neurons\n",
    "            self.validation_data = validation_data\n",
    "            self.pruning_threshold = pruning_threshold\n",
    "            self.regularization_penalty_multiplier = regularization_penalty_multiplier\n",
    "            self.stall_coefficient = stall_coefficient\n",
    "            self.growth_percentage = growth_percentage\n",
    "            self.mini_epochs_per_epoch = mini_epochs_per_epoch\n",
    "            self.verbose = verbose\n",
    "            self.print_neurons = print_neurons\n",
    "            self.use_static_graph = use_static_graph\n",
    "            self.loss_fn = loss_fn\n",
    "            self.metric_fn = metric_fn\n",
    "\n",
    "            self.train_dataset, self.val_dataset = Sequential.prepare_datasets(x, y, batch_size, validation_data)\n",
    "            self.history = self.prepare_history()\n",
    "\n",
    "            self.best_conditional_val_loss = np.inf\n",
    "            self.training_stalled = False\n",
    "        \n",
    "        @staticmethod\n",
    "        def prepare_history():\n",
    "            history = {\n",
    "                'loss': list(),\n",
    "                'metric': list(),\n",
    "                'val_loss': list(),\n",
    "                'val_metric': list(),\n",
    "                'hidden_layer_sizes': list(),\n",
    "            }\n",
    "            return history\n",
    "    \n",
    "    def fit_single_step(self, x_batch, y_batch, optimizer, loss_fn, metric_fn):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # y_pred = tf.reshape(self(x_batch, training=True), y_batch.shape)\n",
    "            y_pred = self(x_batch, training=True)\n",
    "            raw_loss = loss_fn(y_batch, y_pred)\n",
    "            loss_value = tf.reduce_mean(raw_loss)\n",
    "            loss_value += sum(self.losses)  # Add losses registered by model.add_loss\n",
    "\n",
    "            loss = tf.reduce_sum(raw_loss)\n",
    "            metric = float(tf.reduce_sum(metric_fn(y_batch, y_pred)))\n",
    "\n",
    "        grads = tape.gradient(loss_value, self.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "\n",
    "        return loss, metric\n",
    "    \n",
    "    def fit_single_epoch(self, params):\n",
    "        summed_loss = 0\n",
    "        summed_metric = 0\n",
    "        \n",
    "        for mini_epoch in range(params.mini_epochs_per_epoch):\n",
    "            summed_loss = 0\n",
    "            summed_metric = 0\n",
    "\n",
    "            if params.use_static_graph:\n",
    "                fit_single_step_function = tf.function(self.fit_single_step)\n",
    "            else:\n",
    "                fit_single_step_function = self.fit_single_step\n",
    "            for step, (x_batch, y_batch) in enumerate(params.train_dataset):\n",
    "                loss, metric = fit_single_step_function(x_batch, y_batch, params.optimizer, params.loss_fn, params.metric_fn)\n",
    "                summed_loss += loss\n",
    "                summed_metric += metric\n",
    "        \n",
    "        return summed_loss, summed_metric\n",
    "\n",
    "    def fit(self, x, y, optimizer, schedule, batch_size, min_new_neurons, validation_data, pruning_threshold=0.001, regularization_penalty_multiplier=1., \n",
    "            stall_coefficient=1, growth_percentage=0.2, mini_epochs_per_epoch=1, verbose=True, print_neurons=False, use_static_graph=True, \n",
    "            loss_fn=tf.keras.losses.sparse_categorical_crossentropy, metric_fn=tf.keras.metrics.sparse_categorical_accuracy):\n",
    "        params = self.ParameterContainer(x=x, y=y, optimizer=optimizer, batch_size=batch_size, min_new_neurons=min_new_neurons, validation_data=validation_data, \n",
    "                                         pruning_threshold=pruning_threshold, regularization_penalty_multiplier=regularization_penalty_multiplier, stall_coefficient=stall_coefficient, \n",
    "                                         growth_percentage=growth_percentage, mini_epochs_per_epoch=mini_epochs_per_epoch, verbose=verbose, print_neurons=print_neurons, \n",
    "                                         use_static_graph=use_static_graph, loss_fn=loss_fn, metric_fn=metric_fn)\n",
    "        self.build(x.shape)  # Necessary when verbose == False\n",
    "\n",
    "        for epoch_no, epoch in enumerate(schedule):\n",
    "            if verbose:\n",
    "                print(\"##########################################################\")\n",
    "                print(f\"Epoch {epoch_no + 1}/{len(schedule)}\")\n",
    "            \n",
    "            self.set_regularization_penalty(epoch.regularization_penalty)\n",
    "            self.set_regularization_method(epoch.regularization_method)\n",
    "\n",
    "            if epoch.grow:\n",
    "                self.grow_wrapper(params)\n",
    "\n",
    "            summed_loss, summed_metric = self.fit_single_epoch(params)\n",
    "            \n",
    "            if epoch.prune:\n",
    "                self.prune_wrapper(params, summed_loss, summed_metric)\n",
    "            else:\n",
    "                loss, metric, val_loss, val_metric = self.print_epoch_statistics(params, summed_loss, summed_metric, require_result=True)\n",
    "                self.update_history(params, loss, metric, val_loss, val_metric)\n",
    "        \n",
    "        return params.history\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# HELPER FUNCTIONS\n",
    "################################################################################\n",
    "\n",
    "\n",
    "def get_statistics_from_history(history):\n",
    "    best_epoch_number = np.argmin(history['val_loss'])\n",
    "    best_val_loss = history['val_loss'][best_epoch_number]\n",
    "    best_val_metric = history['val_metric'][best_epoch_number]\n",
    "    best_hidden_layer_sizes = history['hidden_layer_sizes'][best_epoch_number]\n",
    "    return best_val_loss, best_val_metric, best_hidden_layer_sizes\n",
    "\n",
    "\n",
    "def get_statistics_from_histories(histories):\n",
    "    best_val_losses = list()\n",
    "    best_val_metrics = list()\n",
    "    all_best_hidden_layer_sizes = list()\n",
    "\n",
    "    for history in histories:\n",
    "        best_val_loss, best_val_metric, best_hidden_layer_sizes = get_statistics_from_history(history)\n",
    "        best_val_losses.append(best_val_loss)\n",
    "        best_val_metrics.append(best_val_metric)\n",
    "        all_best_hidden_layer_sizes.append(best_hidden_layer_sizes)\n",
    "    \n",
    "    mean_best_val_loss = np.mean(best_val_losses)\n",
    "    mean_best_val_metric = np.mean(best_val_metrics)\n",
    "    mean_best_hidden_layer_sizes = [np.mean(layer) for layer in list(zip(*all_best_hidden_layer_sizes))]\n",
    "    \n",
    "    return mean_best_val_loss, mean_best_val_metric, mean_best_hidden_layer_sizes\n",
    "\n",
    "\n",
    "def cross_validate(train_fn, x, y, n_splits, random_state=42, *args, **kwargs):\n",
    "    from sklearn.model_selection import KFold\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "    histories = list()\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(x)):\n",
    "        xtrain, xtest = x[train_index], x[test_index]\n",
    "        ytrain, ytest = y[train_index], y[test_index]\n",
    "\n",
    "        history = train_fn(xtrain, ytrain, validation_data=(xtest, ytest), *args, **kwargs)\n",
    "        histories.append(history)\n",
    "\n",
    "        best_val_loss, best_val_metric, best_hidden_layer_sizes = get_statistics_from_history(history)\n",
    "        print(f\"Run {i} completed, best_val_loss: {best_val_loss}, best_val_metric: {best_val_metric}, best_hidden_layer_sizes: {best_hidden_layer_sizes}\")\n",
    "\n",
    "    mean_best_val_loss, mean_best_val_metric, mean_best_hidden_layer_sizes = get_statistics_from_histories(histories)\n",
    "    print(f'mean_best_val_loss: {mean_best_val_loss}')\n",
    "    print(f'mean_best_val_metric: {mean_best_val_metric}')\n",
    "    print(f'mean_best_hidden_layer_sizes: {mean_best_hidden_layer_sizes}')\n",
    "\n",
    "    return histories, mean_best_hidden_layer_sizes\n",
    "\n",
    "\n",
    "def hyperparameter_search(train_fn, x, y, validation_data, *args, **kwargs):\n",
    "    from itertools import product\n",
    "\n",
    "    all_params = [*args] + list(kwargs.values())\n",
    "    histories = list()\n",
    "\n",
    "    best_overall_val_loss = np.inf\n",
    "    best_overall_val_metric = None\n",
    "    best_overall_combination = None\n",
    "\n",
    "    for combination in product(*all_params):\n",
    "        combination_args = combination[:len(args)]\n",
    "\n",
    "        combination_kwargs_values = combination[len(args):]\n",
    "        combination_kwargs = dict(zip(kwargs.keys(), combination_kwargs_values))\n",
    "\n",
    "        history = train_fn(x, y, validation_data, *combination_args, **combination_kwargs)\n",
    "        history['parameters'] = combination\n",
    "        histories.append(history)\n",
    "\n",
    "        best_val_loss, best_val_metric, best_hidden_layer_sizes = get_statistics_from_history(history)\n",
    "        print(f\"Run with parameters {combination} completed, best_val_loss: {best_val_loss}, best_val_metric: {best_val_metric}, best_hidden_layer_sizes: {best_hidden_layer_sizes}\")\n",
    "\n",
    "        if best_val_loss < best_overall_val_loss:\n",
    "            best_overall_val_loss = best_val_loss\n",
    "            best_overall_val_metric = best_val_metric\n",
    "            best_overall_combination = combination\n",
    "    \n",
    "    print(f'Best overall combination: {best_overall_combination}, val_metric: {best_overall_val_metric}')\n",
    "\n",
    "    return histories, best_overall_combination\n",
    "\n",
    "\n",
    "def interruptible(f):\n",
    "    def function(*args, **kwargs):\n",
    "        try:\n",
    "            return f(*args, **kwargs)\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Interrupted by user.\")\n",
    "    return function\n",
    "\n",
    "\n",
    "class Range:\n",
    "    def __init__(self, min_value, max_value, transformation=None):\n",
    "        self.min_value = min_value\n",
    "        self.max_value = max_value\n",
    "        self.transformation = transformation\n",
    "        \n",
    "    @abc.abstractmethod\n",
    "    def sample():\n",
    "        pass\n",
    "\n",
    "\n",
    "class UniformRange(Range):\n",
    "    def sample(self):\n",
    "        x = np.random.uniform(self.min_value, self.max_value)\n",
    "        if self.transformation is not None:\n",
    "            result = self.transformation(x)\n",
    "            print(f\"Transformed value {x} to {result}.\")\n",
    "            return result\n",
    "        return x\n",
    "\n",
    "\n",
    "class PowerRange(Range):\n",
    "    def sample(self):\n",
    "        exponent = np.random.uniform(self.min_value, self.max_value)\n",
    "        x = 10. ** exponent\n",
    "        if self.transformation is not None:\n",
    "            result = self.transformation(x)\n",
    "            print(f\"Transformed value {x} to {result}.\")\n",
    "            return result\n",
    "        return x\n",
    "\n",
    "\n",
    "@interruptible\n",
    "def random_search(train_fn, x, y, validation_data, *args, **kwargs):\n",
    "    def sample_combination(values):\n",
    "        combination = list()\n",
    "        for value in values:\n",
    "            if isinstance(value, Range):\n",
    "                combination.append(value.sample())\n",
    "            else:\n",
    "                combination.append(value)\n",
    "        return tuple(combination)\n",
    "    \n",
    "    histories = list()\n",
    "\n",
    "    best_overall_val_loss = np.inf\n",
    "    best_overall_val_metric = None\n",
    "    best_overall_combination = None\n",
    "\n",
    "    while True:\n",
    "        combination_args = sample_combination([*args])\n",
    "        \n",
    "        combination_kwargs_values = sample_combination(list(kwargs.values()))\n",
    "        combination_kwargs = dict(zip(kwargs.keys(), combination_kwargs_values))\n",
    "        \n",
    "        combination = combination_args + combination_kwargs_values\n",
    "        \n",
    "        print(f\"Run with parameters {combination} started...\")\n",
    "        \n",
    "        history = train_fn(x, y, validation_data, *combination_args, **combination_kwargs)\n",
    "        history['parameters'] = combination\n",
    "        histories.append(history)\n",
    "\n",
    "        best_val_loss, best_val_metric, best_hidden_layer_sizes = get_statistics_from_history(history)\n",
    "        print(f\"Run with parameters {combination} completed, best_val_loss: {best_val_loss}, best_val_metric: {best_val_metric}, best_hidden_layer_sizes: {best_hidden_layer_sizes}\")\n",
    "\n",
    "        if best_val_loss < best_overall_val_loss:\n",
    "            best_overall_val_loss = best_val_loss\n",
    "            best_overall_val_metric = best_val_metric\n",
    "            best_overall_combination = combination\n",
    "    \n",
    "    print(f'Best overall combination: {best_overall_combination}, val_metric: {best_overall_val_metric}')\n",
    "\n",
    "    return histories, best_overall_combination\n",
    "\n",
    "\n",
    "def get_convolutional_model(x, layer_sizes, output_neurons=10):\n",
    "    model = Sequential([\n",
    "        Conv2D(layer_sizes[0], filter_size=(3, 3), activation='selu', strides=(1, 1), padding='SAME', kernel_initializer='lecun_normal', input_shape=x[0,:,:,:].shape),\n",
    "        Conv2D(layer_sizes[1], filter_size=(3, 3), activation='selu', strides=(2, 2), padding='SAME', kernel_initializer='lecun_normal'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        Conv2D(layer_sizes[2], filter_size=(3, 3), activation='selu', strides=(1, 1), padding='SAME', kernel_initializer='lecun_normal'),\n",
    "        Conv2D(layer_sizes[3], filter_size=(3, 3), activation='selu', strides=(2, 2), padding='SAME', kernel_initializer='lecun_normal'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        Flatten(),\n",
    "        Dense(layer_sizes[4], activation='selu', kernel_initializer='lecun_normal'),\n",
    "        Dense(output_neurons, activation='softmax', fixed_size=True),\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_dense_model(x, layer_sizes):\n",
    "    layers = list()\n",
    "    \n",
    "    layers.append(Dense(layer_sizes[0], activation='selu', kernel_initializer='lecun_normal', input_shape=x[0, :].shape))\n",
    "    for layer_size in layer_sizes[1:]:\n",
    "        layers.append(Dense(layer_size, activation='selu', kernel_initializer='lecun_normal'))\n",
    "    layers.append(Dense(1, activation=None, kernel_initializer='lecun_normal', fixed_size=True))\n",
    "    \n",
    "    model = Sequential(layers)\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_fn_conv(x, y, validation_data, learning_rate, schedule, layer_sizes, output_neurons=10, min_new_neurons=20, \n",
    "             growth_percentage=0.2, verbose=False, use_static_graph=True):\n",
    "    batch_size = 128\n",
    "\n",
    "    model = get_convolutional_model(x, layer_sizes, output_neurons)\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    history = model.fit(x=x, y=y, optimizer=optimizer, schedule=schedule, batch_size=batch_size, min_new_neurons=min_new_neurons, \n",
    "                        validation_data=validation_data, growth_percentage=growth_percentage, verbose=verbose, use_static_graph=use_static_graph)\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "def squared_error(y_true, y_pred):\n",
    "    return (y_true - y_pred) ** 2\n",
    "\n",
    "\n",
    "def train_fn_dense(x, y, validation_data, learning_rate, schedule, layer_sizes, min_new_neurons=20, \n",
    "             growth_percentage=0.2, verbose=False, use_static_graph=True):\n",
    "    batch_size = 128\n",
    "\n",
    "    model = get_dense_model(x, layer_sizes)\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    history = model.fit(x=x, y=y, optimizer=optimizer, schedule=schedule, batch_size=batch_size, min_new_neurons=min_new_neurons, \n",
    "                        validation_data=validation_data, growth_percentage=growth_percentage, verbose=verbose, use_static_graph=use_static_graph,\n",
    "                        loss_fn=squared_error, metric_fn=squared_error)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Individual:\n",
    "    def __init__(self, genome, model, optimizer, history=None, age=0):\n",
    "        self.genome = genome\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        if history is not None:\n",
    "            self.history = history\n",
    "        else:\n",
    "            self.history = Sequential.ParameterContainer.prepare_history()\n",
    "        self.age = age\n",
    "    \n",
    "    def copy(self):\n",
    "        genome_copy = self.genome.copy()\n",
    "        model_copy = self.model.copy()\n",
    "        optimizer_copy = copy.deepcopy(self.optimizer)\n",
    "        history_copy = copy.deepcopy(self.history)\n",
    "        individual_copy = Individual(genome_copy, model_copy, optimizer_copy, history_copy, self.age)\n",
    "        return individual_copy\n",
    "    \n",
    "    def mutate(self, mutation_strength):\n",
    "        self.model.mutate(mutation_strength)\n",
    "        self.age = 0\n",
    "    \n",
    "    def correct(self):\n",
    "        self.genome[0] = max(self.genome[0], 2.5)  # Regularization penalty\n",
    "        self.genome[1] = np.clip(self.genome[1], 0.1, 1)  # Dataset sample size\n",
    "    \n",
    "    def get_val_metric(self):\n",
    "        return self.history['val_metric'][-1]\n",
    "    \n",
    "    def get_age(self):\n",
    "        return len(self.history['val_metric'])\n",
    "    \n",
    "    def get_age_penalty_coefficient(self, age_penalty_period):\n",
    "        age = self.get_age()\n",
    "        return 1 / (2 ** max(0, (age - age_penalty_period) / age_penalty_period))\n",
    "    \n",
    "    def get_fitness(self, age_penalty_period):\n",
    "        if age_penalty_period is None:\n",
    "            return self.get_val_metric()\n",
    "        return self.get_val_metric() * self.get_age_penalty_coefficient(age_penalty_period)\n",
    "    \n",
    "    def get_regularization_penalty(self):\n",
    "        return 10. ** -self.genome[0]\n",
    "    \n",
    "    def get_dataset_sample_size(self):\n",
    "        return self.genome[1]\n",
    "    \n",
    "    def get_hidden_layer_sizes(self):\n",
    "        return self.history['hidden_layer_sizes'][-1]\n",
    "\n",
    "\n",
    "def create_new_individual(x, layer_sizes, output_neurons, learning_rate):\n",
    "    genome = np.array([3, 0.1])\n",
    "    model = get_convolutional_model(x, layer_sizes, output_neurons=output_neurons)\n",
    "    model.build(x.shape)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    individual = Individual(genome, model, optimizer)\n",
    "    return individual\n",
    "\n",
    "\n",
    "def initialize_population(population_size, x, layer_sizes, output_neurons, learning_rate):\n",
    "    population = [create_new_individual(x, layer_sizes, output_neurons, learning_rate) for _ in range(population_size)]\n",
    "    return population\n",
    "\n",
    "\n",
    "def introduce_new_individuals(population, n_introduced, x, layer_sizes, output_neurons, learning_rate):\n",
    "    introduced_individuals = [create_new_individual(x, layer_sizes, output_neurons, learning_rate) for _ in range(n_introduced)]\n",
    "    return population + introduced_individuals\n",
    "\n",
    "\n",
    "def get_best_individual(population, age_penalty_period):\n",
    "    best_individual = None\n",
    "    best_fitness = - np.inf\n",
    "    for individual in population:\n",
    "        fitness = individual.get_fitness(age_penalty_period)\n",
    "        if fitness > best_fitness:\n",
    "            best_individual = individual\n",
    "            best_fitness = fitness\n",
    "    return best_individual\n",
    "\n",
    "\n",
    "def crossover(population, n_parents, strategy):\n",
    "    novel_population = list()\n",
    "    for individual in population:\n",
    "        parents_selection = np.random.choice(list(range(len(population))), size=n_parents, replace=False)\n",
    "        parent_genomes = [population[index].genome for index in parents_selection]\n",
    "        offspring_genome = np.mean(np.vstack(parent_genomes), axis=0)\n",
    "        offspring = individual.copy()\n",
    "        offspring.genome = offspring_genome\n",
    "        offspring.genome += np.random.normal(0, 1, offspring.genome.shape) * strategy\n",
    "        novel_population.append(offspring)\n",
    "    return population + novel_population\n",
    "\n",
    "\n",
    "def correct(population):\n",
    "    for individual in population:\n",
    "        individual.correct()\n",
    "    return population\n",
    "\n",
    "\n",
    "# def mutation(population, mutation_strength):\n",
    "#     new_population = list()\n",
    "#     for individual in population:\n",
    "#         individual_copy = individual.copy()\n",
    "#         individual_copy.mutate(mutation_strength)\n",
    "#         new_population.extend([individual, individual_copy])\n",
    "#     return new_population\n",
    "\n",
    "\n",
    "def extend_history(old_history, new_history):\n",
    "    for key in old_history.keys():\n",
    "        old_history[key].extend(new_history[key])\n",
    "\n",
    "\n",
    "def training(population, x, y, validation_data, batch_size, min_new_neurons, growth_percentage, verbose, use_static_graph):\n",
    "    for individual in population:\n",
    "        model = individual.model\n",
    "        optimizer = individual.optimizer\n",
    "        # schedule = Schedule([StaticEpochNoRegularization()])\n",
    "        schedule = Schedule([DynamicEpoch(individual.get_regularization_penalty(), 'weighted_l1')])\n",
    "        # x_train_sample, y_train_sample = get_dataset_sample(x, y, individual.get_dataset_sample_size())\n",
    "#         x_train_sample, y_train_sample = get_dataset_sample(x, y, 0.1)\n",
    "        # x_test_sample, y_test_sample = get_dataset_sample(validation_data[0], validation_data[1], individual.get_dataset_sample_size())\n",
    "#         x_test_sample, y_test_sample = get_dataset_sample(validation_data[0], validation_data[1], 0.1, seed=42)\n",
    "        x_train_sample, y_train_sample = x, y\n",
    "        x_test_sample, y_test_sample = validation_data[0], validation_data[1]\n",
    "        history = model.fit(x=x_train_sample, y=y_train_sample, optimizer=optimizer, schedule=schedule, batch_size=batch_size, \n",
    "                            min_new_neurons=min_new_neurons, validation_data=(x_test_sample, y_test_sample), growth_percentage=growth_percentage, \n",
    "                            verbose=verbose, use_static_graph=use_static_graph)\n",
    "        extend_history(individual.history, history)\n",
    "        individual.age += 1\n",
    "    return population\n",
    "\n",
    "\n",
    "def tournament_selection(population, population_size, tournament_size, age_penalty_period):\n",
    "    new_population = list()\n",
    "\n",
    "    while len(new_population) < population_size:\n",
    "        selection = np.random.choice(list(range(len(population))), size=tournament_size, replace=False)\n",
    "        best_individual = None\n",
    "        best_fitness = - np.inf\n",
    "        for individual_index in selection:\n",
    "            individual = population[individual_index]\n",
    "            fitness = individual.get_fitness(age_penalty_period)\n",
    "            if fitness > best_fitness:\n",
    "                best_individual = individual\n",
    "                best_fitness = fitness\n",
    "        new_population.append(best_individual.copy())\n",
    "\n",
    "    return new_population\n",
    "\n",
    "\n",
    "def measure_fitnesses(population, age_penalty_period):\n",
    "    fitnesses = list()\n",
    "    for individual in population:\n",
    "        fitnesses.append(individual.get_fitness(age_penalty_period))\n",
    "    return fitnesses\n",
    "\n",
    "\n",
    "def print_generation_statistics(generation, population, duration, age_penalty_period):\n",
    "    sorted_population = sorted(population, key=lambda x: x.get_fitness(age_penalty_period), reverse=True)\n",
    "    individuals = [(individual.get_age(), round(individual.get_val_metric(), 4), round(individual.get_fitness(age_penalty_period), 4), round(individual.genome[0], 1), round(individual.genome[1], 2), individual.get_hidden_layer_sizes()) for individual in sorted_population]\n",
    "    print(f\"Generation {generation}: {round(duration, 1)} s, {individuals}\")\n",
    "\n",
    "\n",
    "@interruptible\n",
    "def evolution(x, y, validation_data, batch_size, layer_sizes, output_neurons, learning_rate, n_parents, strategy, population_size=10, n_generations=10, \n",
    "              tournament_size=3, elitism=True, n_introduced=0, age_penalty_period=None, min_new_neurons=20, growth_percentage=0.2, use_static_graph=True):\n",
    "    population = initialize_population(population_size, x, layer_sizes, output_neurons, learning_rate)\n",
    "    best_individual = None\n",
    "    fitnesses_history = list()\n",
    "    for generation in range(n_generations):\n",
    "        start_time = time.time()\n",
    "        population = crossover(population, n_parents, strategy)\n",
    "        # population = mutation(population, mutation_strength)\n",
    "        population = introduce_new_individuals(population, n_introduced, x, layer_sizes, output_neurons, learning_rate)\n",
    "        population = training(population, x, y, validation_data, batch_size, min_new_neurons, \n",
    "                              growth_percentage, verbose=False, use_static_graph=use_static_graph)\n",
    "        population = tournament_selection(population, population_size, tournament_size, age_penalty_period)\n",
    "        if elitism:\n",
    "            if best_individual is not None:\n",
    "                population.append(best_individual)\n",
    "            best_individual = get_best_individual(population, age_penalty_period).copy()\n",
    "        fitnesses = measure_fitnesses(population, age_penalty_period)\n",
    "        duration = time.time() - start_time\n",
    "        print_generation_statistics(generation, population, duration, age_penalty_period)\n",
    "        # fitnesses_history.append(fitnesses)\n",
    "\n",
    "    # best_individual = get_best_individual(population)\n",
    "    # fitness = best_individual.get_fitness()\n",
    "\n",
    "    # return fitness, fitnesses_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground for standalone models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = get_fashion_mnist_dataset(fraction=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stats time: 0.04924893379211426\n",
      "pruning time: 0.09215831756591797\n",
      "stats time: 0.028137922286987305\n",
      "update history time: 0.0002963542938232422\n",
      "total pruning time: 0.17015743255615234\n",
      "2.089315176010132\n",
      "[20, 20, 20, 20, 20]\n",
      "Interrupted by user.\n"
     ]
    }
   ],
   "source": [
    "evolution(x=fashion_mnist.X_train_norm, y=fashion_mnist.y_train, validation_data=(fashion_mnist.X_test_norm, fashion_mnist.y_test),\n",
    "          batch_size=32, layer_sizes=[20, 20, 20, 20, 20], output_neurons=10, learning_rate=0.0004, n_parents=5, strategy=[0.5, 0.05], \n",
    "          population_size=10, n_generations=50, tournament_size=3, n_introduced=2, age_penalty_period=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 0: 22.9 s, [(1, 0.5263, 0.5263, 3.2, 0.17, [20, 20, 20, 20, 20]), (1, 0.4737, 0.4737, 3.0, 0.1, [20, 20, 20, 20, 20]), (1, 0.4737, 0.4737, 3.4, 0.11, [20, 20, 20, 20, 20]), (1, 0.4737, 0.4737, 3.4, 0.11, [20, 20, 20, 20, 20]), (1, 0.4737, 0.4737, 3.0, 0.1, [20, 20, 20, 20, 20]), (1, 0.4211, 0.4211, 3.0, 0.1, [20, 20, 20, 20, 20]), (1, 0.3947, 0.3947, 3.0, 0.1, [20, 20, 20, 20, 20]), (1, 0.3947, 0.3947, 3.0, 0.1, [20, 20, 20, 20, 20]), (1, 0.3947, 0.3947, 3.0, 0.1, [20, 20, 20, 20, 20]), (1, 0.3684, 0.3684, 2.8, 0.11, [20, 20, 20, 20, 20])]\n",
      "Generation 1: 27.5 s, [(2, 0.6053, 0.6053, 2.9, 0.1, [20, 20, 20, 20, 20]), (2, 0.6053, 0.6053, 2.9, 0.1, [20, 20, 20, 20, 20]), (2, 0.6053, 0.6053, 2.9, 0.1, [20, 20, 20, 20, 20]), (2, 0.5526, 0.5526, 3.6, 0.16, [20, 20, 20, 20, 20]), (2, 0.5526, 0.5526, 3.0, 0.1, [20, 20, 20, 20, 20]), (2, 0.5526, 0.5526, 3.0, 0.1, [20, 20, 20, 20, 20]), (2, 0.5526, 0.5526, 3.0, 0.1, [20, 20, 20, 20, 20]), (2, 0.5526, 0.5526, 3.5, -0.02, [20, 20, 20, 20, 20]), (2, 0.5263, 0.5263, 3.1, 0.04, [20, 20, 20, 20, 20]), (1, 0.5263, 0.5263, 3.2, 0.17, [20, 20, 20, 20, 20]), (2, 0.4737, 0.4737, 3.4, 0.11, [20, 20, 20, 20, 20])]\n",
      "Generation 2: 30.9 s, [(3, 0.7105, 0.7105, 2.9, 0.1, [20, 20, 20, 20, 20]), (3, 0.7105, 0.7105, 2.9, 0.1, [20, 20, 20, 20, 20]), (3, 0.7105, 0.7105, 3.0, 0.1, [20, 20, 20, 20, 20]), (3, 0.6842, 0.6842, 3.0, 0.0, [20, 20, 20, 20, 20]), (3, 0.6579, 0.6579, 2.9, 0.1, [20, 20, 20, 20, 20]), (3, 0.6579, 0.6579, 3.6, 0.06, [20, 20, 20, 20, 20]), (3, 0.6579, 0.6579, 3.6, 0.06, [20, 20, 20, 20, 20]), (3, 0.6579, 0.6579, 3.5, 0.11, [20, 20, 20, 20, 20]), (3, 0.6316, 0.6316, 2.8, 0.11, [20, 20, 20, 20, 20]), (3, 0.6053, 0.6053, 3.4, 0.11, [20, 20, 20, 20, 20]), (2, 0.6053, 0.6053, 2.9, 0.1, [20, 20, 20, 20, 20])]\n",
      "Generation 3: 33.6 s, [(4, 0.7105, 0.7105, 2.9, 0.1, [20, 20, 20, 20, 20]), (4, 0.7105, 0.7105, 4.0, 0.17, [20, 20, 20, 20, 20]), (3, 0.7105, 0.7105, 2.9, 0.1, [20, 20, 20, 20, 20]), (4, 0.6842, 0.6842, 3.6, 0.06, [20, 20, 20, 20, 20]), (4, 0.6842, 0.6842, 3.4, 0.01, [20, 20, 20, 20, 20]), (4, 0.6842, 0.6842, 3.6, 0.06, [20, 20, 20, 20, 20]), (4, 0.6842, 0.6842, 2.9, 0.1, [20, 20, 20, 20, 20]), (4, 0.6842, 0.6842, 2.8, -0.01, [20, 20, 20, 20, 20]), (4, 0.6842, 0.6842, 3.0, 0.1, [20, 20, 20, 20, 20]), (4, 0.6842, 0.6842, 3.5, 0.11, [20, 20, 20, 20, 20]), (4, 0.6579, 0.6579, 3.4, 0.11, [20, 20, 20, 20, 20])]\n",
      "Generation 4: 36.4 s, [(5, 0.7632, 0.7632, 3.1, 0.12, [20, 20, 20, 20, 20]), (5, 0.7632, 0.7632, 3.1, 0.12, [20, 20, 20, 20, 20]), (5, 0.7632, 0.7632, 3.9, 0.09, [20, 20, 20, 20, 20]), (5, 0.7632, 0.7632, 3.4, 0.11, [20, 20, 20, 20, 20]), (5, 0.7368, 0.7368, 2.9, 0.15, [20, 20, 20, 20, 20]), (5, 0.7105, 0.7105, 2.8, 0.03, [20, 20, 20, 20, 20]), (4, 0.7105, 0.7105, 2.9, 0.1, [20, 20, 20, 20, 20]), (5, 0.6842, 0.6842, 2.8, -0.01, [20, 20, 20, 20, 20]), (4, 0.6842, 0.6842, 2.9, 0.1, [20, 20, 20, 20, 20]), (5, 0.6842, 0.6842, 2.8, -0.01, [20, 20, 20, 20, 20]), (4, 0.6842, 0.6842, 2.9, 0.1, [20, 20, 20, 20, 20])]\n",
      "Generation 5: 38.1 s, [(5, 0.8158, 0.8158, 2.8, 0.08, [20, 20, 20, 20, 20]), (5, 0.7632, 0.7632, 3.1, 0.12, [20, 20, 20, 20, 20]), (5, 0.7368, 0.7368, 3.3, 0.13, [20, 20, 20, 20, 20]), (6, 0.7368, 0.7368, 2.8, -0.01, [20, 20, 20, 20, 20]), (6, 0.7105, 0.7105, 2.9, 0.15, [20, 20, 20, 20, 20]), (6, 0.7105, 0.7105, 3.1, 0.11, [20, 20, 20, 20, 20]), (6, 0.7105, 0.7105, 3.4, 0.11, [20, 20, 20, 20, 20]), (6, 0.7105, 0.7105, 2.9, 0.15, [20, 20, 20, 20, 20]), (6, 0.7105, 0.7105, 3.4, 0.11, [20, 20, 20, 20, 20]), (6, 0.7105, 0.7105, 3.1, 0.12, [20, 20, 20, 20, 20]), (6, 0.7105, 0.7105, 3.1, 0.11, [20, 20, 20, 20, 20])]\n",
      "Generation 6: 41.5 s, [(7, 0.8158, 0.8158, 2.9, 0.15, [20, 20, 20, 20, 20]), (7, 0.8158, 0.8158, 2.9, 0.15, [20, 20, 20, 20, 20]), (5, 0.8158, 0.8158, 2.8, 0.08, [20, 20, 20, 20, 20]), (7, 0.7632, 0.7632, 2.8, -0.01, [20, 20, 20, 20, 20]), (7, 0.7632, 0.7632, 2.8, -0.01, [20, 20, 20, 20, 20]), (6, 0.7632, 0.7632, 3.1, 0.12, [20, 20, 20, 20, 20]), (7, 0.7632, 0.7632, 2.9, 0.15, [20, 20, 20, 20, 20]), (7, 0.7632, 0.7632, 2.8, -0.01, [20, 20, 20, 20, 20]), (7, 0.7368, 0.7368, 3.1, 0.11, [20, 20, 20, 20, 20]), (7, 0.7368, 0.7368, 3.4, 0.15, [20, 20, 20, 20, 20]), (7, 0.7368, 0.7368, 3.1, 0.14, [20, 20, 20, 20, 20])]\n",
      "Generation 7: 43.7 s, [(7, 0.8158, 0.8158, 2.9, 0.15, [20, 20, 20, 20, 20]), (8, 0.7895, 0.7895, 2.8, -0.01, [20, 20, 20, 20, 20]), (8, 0.7632, 0.7632, 2.9, 0.15, [20, 20, 20, 20, 20]), (8, 0.7632, 0.7632, 3.4, 0.15, [20, 20, 20, 20, 20]), (7, 0.7632, 0.7632, 3.1, 0.12, [20, 20, 20, 20, 20]), (8, 0.7632, 0.7632, 3.1, 0.14, [20, 20, 20, 20, 20]), (8, 0.7632, 0.7632, 2.8, 0.03, [20, 20, 20, 20, 20]), (8, 0.7632, 0.7632, 2.5, 0.18, [20, 20, 20, 20, 20]), (8, 0.7632, 0.7632, 3.1, 0.14, [20, 20, 20, 20, 20]), (7, 0.7632, 0.7632, 3.1, 0.12, [20, 20, 20, 20, 20]), (6, 0.7368, 0.7368, 2.8, 0.08, [20, 20, 20, 20, 20])]\n",
      "Generation 8: 45.7 s, [(8, 0.8158, 0.8158, 3.1, 0.03, [20, 20, 20, 20, 20]), (9, 0.8158, 0.8158, 2.8, -0.01, [20, 20, 20, 20, 20]), (8, 0.8158, 0.8158, 3.1, 0.03, [20, 20, 20, 20, 20]), (9, 0.8158, 0.8158, 2.8, -0.01, [20, 20, 20, 20, 20]), (9, 0.8158, 0.8158, 2.8, 0.07, [20, 20, 20, 20, 20]), (7, 0.8158, 0.8158, 2.9, 0.15, [20, 20, 20, 20, 20]), (9, 0.7895, 0.7895, 2.5, 0.18, [20, 20, 20, 20, 20]), (8, 0.7632, 0.7632, 2.8, 0.1, [20, 20, 20, 20, 20]), (8, 0.7632, 0.7632, 2.8, 0.1, [20, 20, 20, 20, 20]), (7, 0.7632, 0.7632, 2.8, 0.08, [20, 20, 20, 20, 20]), (9, 0.7632, 0.7632, 2.9, 0.13, [20, 20, 20, 20, 20])]\n",
      "Generation 9: 51.4 s, [(10, 0.8421, 0.8421, 2.5, 0.18, [20, 20, 20, 20, 20]), (9, 0.8158, 0.8158, 2.8, 0.1, [20, 20, 20, 20, 20]), (10, 0.8158, 0.8158, 2.9, 0.11, [20, 20, 20, 20, 20]), (10, 0.8158, 0.8158, 2.9, 0.11, [20, 20, 20, 20, 20]), (9, 0.8158, 0.8158, 2.8, 0.1, [20, 20, 20, 20, 20]), (9, 0.8158, 0.8158, 2.8, 0.1, [20, 20, 20, 20, 20]), (10, 0.8158, 0.8158, 2.9, 0.11, [20, 20, 20, 20, 20]), (8, 0.8158, 0.8158, 3.1, 0.03, [20, 20, 20, 20, 20]), (10, 0.7895, 0.7895, 3.5, 0.15, [20, 20, 20, 20, 20]), (10, 0.7895, 0.7895, 3.5, 0.15, [20, 20, 20, 20, 20]), (10, 0.7632, 0.7632, 3.2, 0.06, [20, 20, 20, 20, 20])]\n",
      "Interrupted by user.\n"
     ]
    }
   ],
   "source": [
    "evolution(x=fashion_mnist.X_train_norm, y=fashion_mnist.y_train, validation_data=(fashion_mnist.X_test_norm, fashion_mnist.y_test),\n",
    "          batch_size=32, layer_sizes=[20, 20, 20, 20, 20], output_neurons=10, learning_rate=0.0004, n_parents=5, strategy=[0.5, 0.05], \n",
    "          population_size=10, n_generations=50, tournament_size=3, n_introduced=2, age_penalty_period=10, use_static_graph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 0: 23.8 s, [(1, 0.4737, 0.4737, 3.1, 0.09, [20, 20, 20, 20, 20]), (1, 0.4474, 0.4474, 2.8, 0.07, [20, 20, 20, 20, 20]), (1, 0.4474, 0.4474, 3.0, 0.1, [20, 20, 20, 20, 20]), (1, 0.4474, 0.4474, 3.0, 0.1, [20, 20, 20, 20, 20]), (1, 0.4474, 0.4474, 2.8, 0.07, [20, 20, 20, 20, 20]), (1, 0.4211, 0.4211, 3.0, 0.1, [20, 20, 20, 20, 20]), (1, 0.3947, 0.3947, 2.6, -0.1, [20, 20, 20, 20, 20]), (1, 0.3947, 0.3947, 3.0, 0.1, [20, 20, 20, 20, 20]), (1, 0.2895, 0.2895, 3.0, 0.1, [20, 20, 20, 20, 20]), (1, 0.2895, 0.2895, 3.0, 0.1, [20, 20, 20, 20, 20])]\n",
      "Generation 1: 26.1 s, [(2, 0.5526, 0.5526, 3.0, 0.1, [20, 20, 20, 20, 20]), (1, 0.5526, 0.5526, 3.0, 0.1, [20, 20, 20, 20, 20]), (1, 0.5526, 0.5526, 3.0, 0.1, [20, 20, 20, 20, 20]), (2, 0.5263, 0.5263, 3.1, 0.09, [20, 20, 20, 20, 20]), (2, 0.5, 0.5, 2.8, 0.07, [20, 20, 20, 20, 20]), (2, 0.5, 0.5, 3.0, 0.1, [20, 20, 20, 20, 20]), (2, 0.5, 0.5, 3.0, 0.1, [20, 20, 20, 20, 20]), (2, 0.5, 0.5, 2.8, 0.05, [20, 20, 20, 20, 20]), (2, 0.4737, 0.4737, 3.0, 0.15, [20, 20, 20, 20, 20]), (2, 0.4737, 0.4737, 3.0, 0.1, [20, 20, 20, 20, 20]), (1, 0.4737, 0.4737, 3.1, 0.09, [20, 20, 20, 20, 20])]\n",
      "Generation 2: 29.9 s, [(3, 0.6053, 0.6053, 3.0, 0.17, [20, 20, 20, 20, 20]), (3, 0.6053, 0.6053, 3.0, 0.17, [20, 20, 20, 20, 20]), (3, 0.6053, 0.6053, 2.8, 0.05, [20, 20, 20, 20, 20]), (3, 0.6053, 0.6053, 3.0, 0.17, [20, 20, 20, 20, 20]), (3, 0.5789, 0.5789, 2.9, 0.18, [20, 20, 20, 20, 20]), (3, 0.5789, 0.5789, 2.9, 0.18, [20, 20, 20, 20, 20]), (3, 0.5789, 0.5789, 2.9, 0.18, [20, 20, 20, 20, 20]), (3, 0.5789, 0.5789, 3.3, 0.26, [20, 20, 20, 20, 20]), (3, 0.5789, 0.5789, 3.3, 0.26, [20, 20, 20, 20, 20]), (2, 0.5526, 0.5526, 3.0, 0.1, [20, 20, 20, 20, 20]), (3, 0.5, 0.5, 3.0, 0.1, [20, 20, 20, 20, 20])]\n",
      "Generation 3: 37.0 s, [(4, 0.6316, 0.6316, 2.7, 0.11, [20, 20, 20, 20, 20]), (4, 0.6316, 0.6316, 3.6, 0.26, [20, 20, 20, 20, 20]), (4, 0.6316, 0.6316, 2.7, 0.11, [20, 20, 20, 20, 20]), (4, 0.6316, 0.6316, 2.9, 0.18, [20, 20, 20, 20, 20]), (4, 0.6316, 0.6316, 2.9, 0.18, [20, 20, 20, 20, 20]), (4, 0.6316, 0.6316, 3.6, 0.26, [20, 20, 20, 20, 20]), (4, 0.6053, 0.6053, 3.0, 0.17, [20, 20, 20, 20, 20]), (4, 0.6053, 0.6053, 2.9, 0.18, [20, 20, 20, 20, 20]), (4, 0.6053, 0.6053, 3.9, 0.07, [20, 20, 20, 20, 20]), (3, 0.6053, 0.6053, 3.0, 0.17, [20, 20, 20, 20, 20]), (3, 0.5526, 0.5526, 3.0, 0.1, [20, 20, 20, 20, 20])]\n",
      "Generation 4: 40.9 s, [(5, 0.6842, 0.6842, 3.0, 0.17, [20, 20, 20, 20, 20]), (5, 0.6842, 0.6842, 2.7, 0.11, [20, 20, 20, 20, 20]), (5, 0.6842, 0.6842, 3.0, 0.17, [20, 20, 20, 20, 20]), (5, 0.6579, 0.6579, 2.9, 0.15, [20, 20, 20, 20, 20]), (5, 0.6579, 0.6579, 2.4, 0.13, [20, 20, 20, 20, 20]), (5, 0.6316, 0.6316, 4.1, 0.09, [20, 20, 20, 20, 20]), (5, 0.6316, 0.6316, 3.4, 0.14, [20, 20, 20, 20, 20]), (5, 0.6316, 0.6316, 2.0, 0.19, [20, 20, 20, 20, 20]), (5, 0.6316, 0.6316, 2.9, 0.18, [20, 20, 20, 20, 20]), (4, 0.6316, 0.6316, 2.7, 0.11, [20, 20, 20, 20, 20]), (5, 0.6053, 0.6053, 2.8, 0.24, [20, 20, 20, 20, 20])]\n",
      "Generation 5: 42.6 s, [(6, 0.7105, 0.7105, 3.4, 0.14, [20, 20, 20, 20, 20]), (6, 0.6842, 0.6842, 2.8, 0.24, [20, 20, 20, 20, 20]), (6, 0.6842, 0.6842, 2.8, 0.24, [20, 20, 20, 20, 20]), (6, 0.6842, 0.6842, 3.1, 0.12, [20, 20, 20, 20, 20]), (6, 0.6842, 0.6842, 3.4, 0.15, [20, 20, 20, 20, 20]), (5, 0.6842, 0.6842, 3.0, 0.17, [20, 20, 20, 20, 20]), (6, 0.6579, 0.6579, 3.2, 0.24, [20, 20, 20, 20, 20]), (6, 0.6579, 0.6579, 2.4, 0.13, [20, 20, 20, 20, 20]), (6, 0.6579, 0.6579, 2.5, 0.15, [20, 20, 20, 20, 20]), (6, 0.6316, 0.6316, 3.0, 0.17, [20, 20, 20, 20, 20]), (6, 0.6316, 0.6316, 2.8, 0.2, [20, 20, 20, 20, 20])]\n",
      "Generation 6: 45.2 s, [(7, 0.7105, 0.7105, 2.8, 0.2, [20, 20, 20, 20, 20]), (7, 0.7105, 0.7105, 3.4, 0.15, [20, 20, 20, 20, 20]), (6, 0.7105, 0.7105, 3.4, 0.14, [20, 20, 20, 20, 20]), (7, 0.6842, 0.6842, 3.5, 0.18, [20, 20, 20, 20, 20]), (6, 0.6842, 0.6842, 3.0, 0.17, [20, 20, 20, 20, 20]), (7, 0.6842, 0.6842, 2.8, 0.24, [20, 20, 20, 20, 20]), (7, 0.6842, 0.6842, 2.5, 0.15, [20, 20, 20, 20, 20]), (7, 0.6842, 0.6842, 3.4, 0.19, [20, 20, 20, 20, 20]), (7, 0.6842, 0.6842, 3.5, 0.18, [20, 20, 20, 20, 20]), (7, 0.6579, 0.6579, 3.4, 0.14, [20, 20, 20, 20, 20]), (7, 0.6579, 0.6579, 3.6, 0.2, [20, 20, 20, 20, 20])]\n",
      "Generation 7: 49.5 s, [(8, 0.7368, 0.7368, 3.0, 0.17, [20, 20, 20, 20, 20]), (8, 0.7368, 0.7368, 3.0, 0.17, [20, 20, 20, 20, 20]), (8, 0.7368, 0.7368, 3.4, 0.19, [20, 20, 20, 20, 20]), (8, 0.7368, 0.7368, 3.8, 0.17, [20, 20, 20, 20, 20]), (8, 0.7368, 0.7368, 3.0, 0.17, [20, 20, 20, 20, 20]), (7, 0.7105, 0.7105, 3.9, 0.18, [20, 20, 20, 20, 20]), (8, 0.7105, 0.7105, 3.4, 0.14, [20, 20, 20, 20, 20]), (8, 0.7105, 0.7105, 3.4, 0.14, [20, 20, 20, 20, 20]), (8, 0.7105, 0.7105, 3.4, 0.15, [20, 20, 20, 20, 20]), (7, 0.7105, 0.7105, 2.8, 0.2, [20, 20, 20, 20, 20]), (8, 0.6842, 0.6842, 3.5, 0.18, [20, 20, 20, 20, 20])]\n",
      "Generation 8: 50.6 s, [(9, 0.7632, 0.7632, 3.2, 0.23, [20, 20, 20, 20, 20]), (9, 0.7632, 0.7632, 3.5, 0.18, [20, 20, 20, 20, 20]), (9, 0.7368, 0.7368, 3.0, 0.17, [20, 20, 20, 20, 20]), (8, 0.7368, 0.7368, 3.0, 0.17, [20, 20, 20, 20, 20]), (9, 0.7105, 0.7105, 3.4, 0.17, [20, 20, 20, 20, 20]), (9, 0.7105, 0.7105, 3.0, 0.17, [20, 20, 20, 20, 20]), (9, 0.7105, 0.7105, 3.4, 0.17, [20, 20, 20, 20, 20]), (8, 0.6842, 0.6842, 3.4, 0.12, [20, 20, 20, 20, 20]), (8, 0.6842, 0.6842, 2.8, 0.2, [20, 20, 20, 20, 20]), (9, 0.6842, 0.6842, 4.2, 0.14, [20, 20, 20, 25, 20]), (8, 0.6579, 0.6579, 3.7, 0.19, [20, 20, 20, 20, 20])]\n",
      "Generation 9: 53.9 s, [(9, 0.7895, 0.7895, 3.4, 0.12, [20, 20, 20, 20, 20]), (10, 0.7632, 0.7632, 3.0, 0.17, [20, 20, 20, 20, 20]), (9, 0.7632, 0.7632, 3.2, 0.23, [20, 20, 20, 20, 20]), (9, 0.7368, 0.7368, 3.6, 0.04, [20, 20, 20, 20, 20]), (9, 0.7368, 0.7368, 3.6, 0.04, [20, 20, 20, 20, 20]), (10, 0.7368, 0.7368, 2.4, 0.22, [20, 20, 20, 20, 20]), (9, 0.7368, 0.7368, 3.0, 0.17, [20, 20, 20, 20, 20]), (9, 0.7368, 0.7368, 3.0, 0.17, [20, 20, 20, 20, 20]), (9, 0.7368, 0.7368, 3.0, 0.17, [20, 20, 20, 20, 20]), (10, 0.7368, 0.7368, 2.4, 0.22, [20, 20, 20, 20, 20]), (10, 0.7105, 0.7105, 3.2, 0.23, [20, 20, 20, 20, 20])]\n",
      "Generation 10: 55.9 s, [(9, 0.7895, 0.7895, 3.4, 0.12, [20, 20, 20, 20, 20]), (10, 0.7632, 0.7632, 3.0, 0.17, [20, 20, 20, 20, 20]), (10, 0.7632, 0.7632, 3.6, 0.04, [20, 20, 20, 20, 20]), (10, 0.7632, 0.7632, 3.0, 0.17, [20, 20, 20, 20, 20]), (10, 0.7632, 0.7632, 3.0, 0.17, [20, 20, 20, 20, 20]), (10, 0.7368, 0.7368, 3.7, 0.24, [20, 20, 20, 20, 20]), (10, 0.7368, 0.7368, 3.7, 0.24, [20, 20, 20, 20, 20]), (11, 0.7632, 0.7121, 3.0, 0.19, [20, 20, 20, 20, 20]), (10, 0.7105, 0.7105, 3.1, 0.22, [20, 20, 20, 20, 20]), (10, 0.7105, 0.7105, 3.3, 0.15, [20, 20, 20, 20, 20]), (10, 0.7105, 0.7105, 3.1, 0.22, [20, 20, 20, 20, 20])]\n",
      "Generation 11: 59.3 s, [(9, 0.7895, 0.7895, 3.4, 0.12, [20, 20, 20, 20, 20]), (10, 0.7368, 0.7368, 2.9, 0.13, [20, 20, 20, 20, 20]), (10, 0.7368, 0.7368, 2.9, 0.13, [20, 20, 20, 20, 20]), (11, 0.7632, 0.7121, 3.2, 0.24, [20, 20, 20, 20, 20]), (11, 0.7632, 0.7121, 3.0, 0.17, [20, 20, 20, 20, 20]), (11, 0.7632, 0.7121, 3.2, 0.24, [20, 20, 20, 20, 20]), (11, 0.7632, 0.7121, 3.2, 0.24, [20, 20, 20, 20, 20]), (11, 0.7632, 0.7121, 3.0, 0.21, [20, 20, 20, 20, 20]), (11, 0.7632, 0.7121, 3.2, 0.24, [20, 20, 20, 20, 20]), (11, 0.7368, 0.6875, 2.6, 0.18, [20, 20, 20, 20, 20]), (11, 0.7105, 0.6629, 3.1, 0.22, [20, 20, 20, 20, 20])]\n",
      "Generation 12: 62.8 s, [(9, 0.7895, 0.7895, 3.4, 0.12, [20, 20, 20, 20, 20]), (10, 0.7632, 0.7632, 3.2, 0.17, [20, 20, 20, 20, 20]), (11, 0.7632, 0.7121, 2.6, 0.22, [20, 20, 20, 20, 20]), (11, 0.7632, 0.7121, 2.6, 0.22, [20, 20, 20, 20, 20]), (11, 0.7632, 0.7121, 2.6, 0.22, [20, 20, 20, 20, 20]), (12, 0.7895, 0.6873, 3.2, 0.24, [20, 20, 20, 20, 20]), (12, 0.7895, 0.6873, 3.7, 0.18, [20, 20, 20, 20, 20]), (12, 0.7895, 0.6873, 3.7, 0.18, [20, 20, 20, 20, 20]), (11, 0.7105, 0.6629, 2.9, 0.13, [20, 20, 20, 20, 20]), (11, 0.7105, 0.6629, 2.9, 0.13, [20, 20, 20, 20, 20]), (11, 0.7105, 0.6629, 2.9, 0.13, [20, 20, 20, 20, 20])]\n",
      "Generation 13: 66.7 s, [(9, 0.7895, 0.7895, 3.4, 0.12, [20, 20, 20, 20, 20]), (10, 0.7368, 0.7368, 2.3, 0.22, [20, 20, 20, 20, 20]), (10, 0.7368, 0.7368, 2.3, 0.22, [20, 20, 20, 20, 20]), (11, 0.7895, 0.7366, 3.2, 0.17, [20, 20, 20, 20, 20]), (11, 0.7895, 0.7366, 3.2, 0.17, [20, 20, 20, 20, 20]), (10, 0.7105, 0.7105, 3.4, 0.12, [20, 20, 20, 20, 20]), (11, 0.7368, 0.6875, 3.6, 0.19, [20, 20, 20, 20, 20]), (12, 0.7368, 0.6415, 3.6, 0.18, [20, 20, 20, 20, 20]), (13, 0.7895, 0.6413, 2.8, 0.07, [20, 20, 20, 20, 20]), (12, 0.7105, 0.6185, 3.7, 0.17, [20, 20, 20, 20, 20]), (13, 0.7368, 0.5985, 3.7, 0.18, [20, 20, 20, 20, 20])]\n",
      "Interrupted by user.\n"
     ]
    }
   ],
   "source": [
    "evolution(x=fashion_mnist.X_train_norm, y=fashion_mnist.y_train, validation_data=(fashion_mnist.X_test_norm, fashion_mnist.y_test),\n",
    "          batch_size=32, layer_sizes=[20, 20, 20, 20, 20], output_neurons=10, learning_rate=0.0004, n_parents=5, strategy=[0.5, 0.05], \n",
    "          population_size=10, n_generations=50, tournament_size=3, n_introduced=2, age_penalty_period=10, use_static_graph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_convolutional_model(fashion_mnist.X_train_norm, [20, 20, 20, 20, 20], output_neurons=10)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for _ in range(3000):\n",
    "    start_time = time.time()\n",
    "    schedule = Schedule([DynamicEpoch(0.01, 'weighted_l1')] * 1)\n",
    "    model.fit(x=fashion_mnist.X_train_norm, y=fashion_mnist.y_train, optimizer=optimizer, schedule=schedule, batch_size=32, min_new_neurons=20, \n",
    "                        validation_data=(fashion_mnist.X_test_norm, fashion_mnist.y_test), growth_percentage=0.2, verbose=True, \n",
    "                        use_static_graph=True)\n",
    "    print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed value 0.00041534645368884667 to 6ac1742dea.\n",
      "Run with parameters (0.0004, 6ac1742dea, [20, 20, 20, 20, 20], 10, 20, 0.2) started...\n",
      "Run with parameters (0.0004, 6ac1742dea, [20, 20, 20, 20, 20], 10, 20, 0.2) completed, best_val_loss: 1.0397995710372925, best_val_metric: 0.6842105263157895, best_hidden_layer_sizes: [20, 20, 20, 20, 20]\n",
      "Transformed value 0.0013678179300379248 to cdc55f9239.\n",
      "Run with parameters (0.0004, cdc55f9239, [20, 20, 20, 20, 20], 10, 20, 0.2) started...\n",
      "Run with parameters (0.0004, cdc55f9239, [20, 20, 20, 20, 20], 10, 20, 0.2) completed, best_val_loss: 0.8996500968933105, best_val_metric: 0.7105263157894737, best_hidden_layer_sizes: [20, 20, 20, 20, 20]\n",
      "Transformed value 0.000487782514601775 to 2eddbf0c7c.\n",
      "Run with parameters (0.0004, 2eddbf0c7c, [20, 20, 20, 20, 20], 10, 20, 0.2) started...\n",
      "Run with parameters (0.0004, 2eddbf0c7c, [20, 20, 20, 20, 20], 10, 20, 0.2) completed, best_val_loss: 0.8693010210990906, best_val_metric: 0.631578947368421, best_hidden_layer_sizes: [20, 20, 20, 20, 20]\n",
      "Transformed value 0.00011441483511794696 to f2955a0fe5.\n",
      "Run with parameters (0.0004, f2955a0fe5, [20, 20, 20, 20, 20], 10, 20, 0.2) started...\n",
      "Run with parameters (0.0004, f2955a0fe5, [20, 20, 20, 20, 20], 10, 20, 0.2) completed, best_val_loss: 0.8831897377967834, best_val_metric: 0.7105263157894737, best_hidden_layer_sizes: [20, 20, 20, 20, 20]\n",
      "Transformed value 0.0004694789752938634 to d9724a9a99.\n",
      "Run with parameters (0.0004, d9724a9a99, [20, 20, 20, 20, 20], 10, 20, 0.2) started...\n",
      "Run with parameters (0.0004, d9724a9a99, [20, 20, 20, 20, 20], 10, 20, 0.2) completed, best_val_loss: 0.8598389625549316, best_val_metric: 0.7631578947368421, best_hidden_layer_sizes: [20, 20, 20, 20, 20]\n",
      "Transformed value 0.0010269961867697178 to 6b38632ca6.\n",
      "Run with parameters (0.0004, 6b38632ca6, [20, 20, 20, 20, 20], 10, 20, 0.2) started...\n",
      "Run with parameters (0.0004, 6b38632ca6, [20, 20, 20, 20, 20], 10, 20, 0.2) completed, best_val_loss: 0.8992174863815308, best_val_metric: 0.6842105263157895, best_hidden_layer_sizes: [20, 20, 20, 20, 20]\n",
      "Transformed value 0.0009707670038514038 to 1210dc71b3.\n",
      "Run with parameters (0.0004, 1210dc71b3, [20, 20, 20, 20, 20], 10, 20, 0.2) started...\n",
      "Run with parameters (0.0004, 1210dc71b3, [20, 20, 20, 20, 20], 10, 20, 0.2) completed, best_val_loss: 0.9041606783866882, best_val_metric: 0.6578947368421053, best_hidden_layer_sizes: [20, 20, 20, 20, 20]\n",
      "Transformed value 0.0006781580596374959 to cae00569c5.\n",
      "Run with parameters (0.0004, cae00569c5, [20, 20, 20, 20, 20], 10, 20, 0.2) started...\n",
      "Interrupted by user.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "histories, best_overall_combination = random_search(\n",
    "    train_fn_conv, x=fashion_mnist.X_train_norm, y=fashion_mnist.y_train, validation_data=(fashion_mnist.X_test_norm, fashion_mnist.y_test), \n",
    "    learning_rate=0.0004, schedule=PowerRange(-4, -2.5, lambda x: Schedule([DynamicEpoch(x, 'weighted_l1')] * 20)), layer_sizes=[20, 20, 20, 20, 20], \n",
    "    output_neurons=10, min_new_neurons=20, growth_percentage=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed value 0.001439351576826643 to 30be2b3482.\n",
      "Run with parameters (0.0004, 30be2b3482, [20, 20, 20, 20, 20], 10, 20, 0.2) started...\n",
      "Run with parameters (0.0004, 30be2b3482, [20, 20, 20, 20, 20], 10, 20, 0.2) completed, best_val_loss: 0.95672208070755, best_val_metric: 0.6842105263157895, best_hidden_layer_sizes: [20, 20, 20, 20, 20]\n",
      "Transformed value 0.00025685508461890423 to bdcb7b6396.\n",
      "Run with parameters (0.0004, bdcb7b6396, [20, 20, 20, 20, 20], 10, 20, 0.2) started...\n",
      "Run with parameters (0.0004, bdcb7b6396, [20, 20, 20, 20, 20], 10, 20, 0.2) completed, best_val_loss: 0.7996328473091125, best_val_metric: 0.6578947368421053, best_hidden_layer_sizes: [20, 20, 20, 20, 20]\n",
      "Transformed value 0.0031164959282910234 to cd16e3065c.\n",
      "Run with parameters (0.0004, cd16e3065c, [20, 20, 20, 20, 20], 10, 20, 0.2) started...\n",
      "Run with parameters (0.0004, cd16e3065c, [20, 20, 20, 20, 20], 10, 20, 0.2) completed, best_val_loss: 1.1780805587768555, best_val_metric: 0.5789473684210527, best_hidden_layer_sizes: [20, 20, 20, 20, 20]\n",
      "Transformed value 0.00039122798800858045 to 9c60aa4e6a.\n",
      "Run with parameters (0.0004, 9c60aa4e6a, [20, 20, 20, 20, 20], 10, 20, 0.2) started...\n",
      "Run with parameters (0.0004, 9c60aa4e6a, [20, 20, 20, 20, 20], 10, 20, 0.2) completed, best_val_loss: 0.910222589969635, best_val_metric: 0.7368421052631579, best_hidden_layer_sizes: [20, 20, 20, 20, 20]\n",
      "Transformed value 0.00016212355399062043 to dab48fe66b.\n",
      "Run with parameters (0.0004, dab48fe66b, [20, 20, 20, 20, 20], 10, 20, 0.2) started...\n",
      "Run with parameters (0.0004, dab48fe66b, [20, 20, 20, 20, 20], 10, 20, 0.2) completed, best_val_loss: 0.9001054167747498, best_val_metric: 0.7105263157894737, best_hidden_layer_sizes: [20, 20, 20, 20, 20]\n",
      "Transformed value 0.0023621582857491306 to eed005961e.\n",
      "Run with parameters (0.0004, eed005961e, [20, 20, 20, 20, 20], 10, 20, 0.2) started...\n",
      "Interrupted by user.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "histories, best_overall_combination = random_search(\n",
    "    train_fn_conv, x=fashion_mnist.X_train_norm, y=fashion_mnist.y_train, validation_data=(fashion_mnist.X_test_norm, fashion_mnist.y_test), \n",
    "    learning_rate=0.0004, schedule=PowerRange(-4, -2.5, lambda x: Schedule([DynamicEpoch(x, 'weighted_l1')] * 20)), layer_sizes=[20, 20, 20, 20, 20], \n",
    "    output_neurons=10, min_new_neurons=20, growth_percentage=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "machine_shape": "hm",
   "name": "tf_multi_layer_ssnet_inverse.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
