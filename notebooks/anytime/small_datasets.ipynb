{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f_deAUKlniFk",
    "outputId": "434f0bdd-1358-46dc-adc2-aee14230789c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: nvidia-smi\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
    "  print('and then re-execute this cell.')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "yKwUwV_NneIo"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from enum import Enum\n",
    "import imageio\n",
    "import hashlib\n",
    "import copy\n",
    "import time\n",
    "import abc\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "dtype = 'float32'\n",
    "tf.keras.backend.set_floatx(dtype)\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "UTZq4KMpneIv"
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# DATASETS\n",
    "################################################################################\n",
    "\n",
    "\n",
    "def get_dataset_sample(X, y, fraction, seed=None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)  # Set random seed\n",
    "    selection = np.random.choice([True, False], len(X), p=[fraction, 1 - fraction])\n",
    "    if seed is not None:\n",
    "        np.random.seed()  # Unset random seed\n",
    "    X_sampled = X[selection]\n",
    "    y_sampled = y[selection]\n",
    "    return X_sampled, y_sampled\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, X_train, y_train, X_test, y_test, shape, shape_flattened, fraction, vision=True, standardize=True):\n",
    "        if fraction is not None:\n",
    "            X_train, y_train = get_dataset_sample(X_train, y_train, fraction, seed=42)\n",
    "            X_test, y_test = get_dataset_sample(X_test, y_test, fraction, seed=42)\n",
    "        \n",
    "        X_train = X_train.astype(dtype)\n",
    "        y_train = y_train.astype(dtype)\n",
    "        X_test = X_test.astype(dtype)\n",
    "        y_test = y_test.astype(dtype)\n",
    "\n",
    "        if vision:\n",
    "            X_train = X_train / 255.0\n",
    "            X_test = X_test / 255.0\n",
    "\n",
    "        X_train = np.reshape(X_train, shape_flattened)\n",
    "        X_test = np.reshape(X_test, shape_flattened)\n",
    "\n",
    "        X = np.concatenate((X_train, X_test), axis=0)\n",
    "        y = np.concatenate((y_train, y_test), axis=0)\n",
    "\n",
    "        if standardize:\n",
    "            from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "            scaler = StandardScaler()\n",
    "            scaler.fit(X_train)  # Scaling each feature independently\n",
    "\n",
    "            X_norm = scaler.transform(X)\n",
    "            X_train_norm = scaler.transform(X_train)\n",
    "            X_test_norm = scaler.transform(X_test)\n",
    "        else:\n",
    "            X_norm = X.copy()\n",
    "            X_train_norm = X_train.copy()\n",
    "            X_test_norm = X_test.copy()\n",
    "\n",
    "        X_norm = np.reshape(X_norm, shape)\n",
    "        X_train_norm = np.reshape(X_train_norm, shape)\n",
    "        X_test_norm = np.reshape(X_test_norm, shape)\n",
    "\n",
    "        del X, X_train, X_test\n",
    "\n",
    "        self.X_norm = X_norm\n",
    "        self.y = y\n",
    "        self.X_train_norm = X_train_norm\n",
    "        self.y_train = y_train\n",
    "        self.X_test_norm = X_test_norm\n",
    "        self.y_test = y_test\n",
    "\n",
    "\n",
    "def get_cifar_10_dataset(fraction=None):\n",
    "    cifar10 = tf.keras.datasets.cifar10\n",
    "    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "    shape = (-1, 32, 32, 3)\n",
    "    shape_flattened = (-1, 3072)  # Scaling each feature independently\n",
    "    return Dataset(X_train, y_train, X_test, y_test, shape=shape, shape_flattened=shape_flattened, fraction=fraction)\n",
    "\n",
    "\n",
    "def get_cifar_100_dataset(fraction=None):\n",
    "    cifar100 = tf.keras.datasets.cifar100\n",
    "    (X_train, y_train), (X_test, y_test) = cifar100.load_data()\n",
    "    shape = (-1, 32, 32, 3)\n",
    "    shape_flattened = (-1, 3072)  # Scaling each feature independently\n",
    "    return Dataset(X_train, y_train, X_test, y_test, shape=shape, shape_flattened=shape_flattened, fraction=fraction)\n",
    "\n",
    "\n",
    "def get_svhn_dataset(fraction=None):\n",
    "    from urllib.request import urlretrieve\n",
    "    from scipy import io\n",
    "\n",
    "    train_filename, _ = urlretrieve('http://ufldl.stanford.edu/housenumbers/train_32x32.mat')\n",
    "    test_filename, _ = urlretrieve('http://ufldl.stanford.edu/housenumbers/test_32x32.mat')\n",
    "\n",
    "    X_train = io.loadmat(train_filename, variable_names='X').get('X')\n",
    "    y_train = io.loadmat(train_filename, variable_names='y').get('y')\n",
    "    X_test = io.loadmat(test_filename, variable_names='X').get('X')\n",
    "    y_test = io.loadmat(test_filename, variable_names='y').get('y')\n",
    "\n",
    "    X_train = np.moveaxis(X_train, -1, 0)\n",
    "    y_train -= 1\n",
    "    X_test = np.moveaxis(X_test, -1, 0)\n",
    "    y_test -= 1\n",
    "\n",
    "    shape = (-1, 32, 32, 3)\n",
    "    shape_flattened = (-1, 3072)  # Scaling each feature independently\n",
    "    return Dataset(X_train, y_train, X_test, y_test, shape=shape, shape_flattened=shape_flattened, fraction=fraction)\n",
    "\n",
    "\n",
    "def get_tiny_imagenet_dataset(fraction=None):\n",
    "    \"\"\"\n",
    "    Original source: https://github.com/sonugiri1043/Train_ResNet_On_Tiny_ImageNet/blob/master/Train_ResNet_On_Tiny_ImageNet.ipynb\n",
    "    Original author: sonugiri1043@gmail.com\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.isdir('IMagenet'):\n",
    "        ! git clone https://github.com/seshuad/IMagenet\n",
    "\n",
    "    print(\"Processing the downloaded dataset...\")\n",
    "\n",
    "    path = 'IMagenet/tiny-imagenet-200/'\n",
    "\n",
    "    id_dict = {}\n",
    "    for i, line in enumerate(open(path + 'wnids.txt', 'r')):\n",
    "        id_dict[line.replace('\\n', '')] = i\n",
    "\n",
    "    train_data = list()\n",
    "    test_data = list()\n",
    "    train_labels = list()\n",
    "    test_labels = list()\n",
    "\n",
    "    for key, value in id_dict.items():\n",
    "        train_data += [imageio.imread(path + 'train/{}/images/{}_{}.JPEG'.format(key, key, str(i)), pilmode='RGB') for i in range(500)]\n",
    "        train_labels_ = np.array([[0]*200]*500)\n",
    "        train_labels_[:, value] = 1\n",
    "        train_labels += train_labels_.tolist()\n",
    "\n",
    "    for line in open(path + 'val/val_annotations.txt'):\n",
    "        img_name, class_id = line.split('\\t')[:2]\n",
    "        test_data.append(imageio.imread(path + 'val/images/{}'.format(img_name), pilmode='RGB'))\n",
    "        test_labels_ = np.array([[0]*200])\n",
    "        test_labels_[0, id_dict[class_id]] = 1\n",
    "        test_labels += test_labels_.tolist()\n",
    "\n",
    "    X_train = np.array(train_data)\n",
    "    y_train = np.argmax(np.array(train_labels), axis=1)\n",
    "    X_test = np.array(test_data)\n",
    "    y_test = np.argmax(np.array(test_labels), axis=1)\n",
    "\n",
    "    shape = (-1, 64, 64, 3)\n",
    "    shape_flattened = (-1, 12288)  # Scaling each feature independently\n",
    "    return Dataset(X_train, y_train, X_test, y_test, shape=shape, shape_flattened=shape_flattened, fraction=fraction)\n",
    "\n",
    "\n",
    "def get_mnist_dataset(fraction=None):\n",
    "    mnist = tf.keras.datasets.mnist\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "    shape = (-1, 28, 28, 1)\n",
    "    shape_flattened = (-1, 1)  # Scaling all features together\n",
    "    return Dataset(X_train, y_train, X_test, y_test, shape=shape, shape_flattened=shape_flattened, fraction=fraction)\n",
    "\n",
    "\n",
    "def get_fashion_mnist_dataset(fraction=None):\n",
    "    fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "    (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "    shape = (-1, 28, 28, 1)\n",
    "    shape_flattened = (-1, 1)  # Scaling all features together\n",
    "    return Dataset(X_train, y_train, X_test, y_test, shape=shape, shape_flattened=shape_flattened, fraction=fraction)\n",
    "\n",
    "\n",
    "def get_fifteen_puzzle_dataset(path=None, fraction=None):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    if path is None:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/gdrive')\n",
    "        path = 'gdrive/MyDrive/15-costs-v3.csv'\n",
    "    costs = pd.read_csv(path)\n",
    "\n",
    "    X_raw = costs.iloc[:,:-1].values\n",
    "    y = costs['cost'].values\n",
    "    X = np.apply_along_axis(lambda x: np.eye(16)[x].ravel(), 1, X_raw)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    del X, X_raw, y\n",
    "\n",
    "    shape = (-1, 256)\n",
    "    shape_flattened = (-1, 256)  # Scaling all features together\n",
    "    return Dataset(X_train, y_train, X_test, y_test, shape=shape, shape_flattened=shape_flattened, vision=False, fraction=fraction)\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# REGULARIZERS\n",
    "################################################################################\n",
    "\n",
    "\n",
    "class Regularizer(tf.keras.regularizers.Regularizer):\n",
    "    def __init__(self):\n",
    "        self.n_new_neurons = 0\n",
    "        self.scaling_tensor = None\n",
    "        self.set_regularization_penalty(0.)\n",
    "        self.set_regularization_method(None)\n",
    "    \n",
    "    def copy(self):\n",
    "        regularizer_copy = Regularizer.__new__(Regularizer)\n",
    "        regularizer_copy.n_new_neurons = self.n_new_neurons\n",
    "        regularizer_copy.scaling_tensor = self.scaling_tensor\n",
    "        regularizer_copy.set_regularization_penalty(self.regularization_penalty)\n",
    "        regularizer_copy.set_regularization_method(self.regularization_method)\n",
    "        return regularizer_copy\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if self.regularization_method is None or self.regularization_penalty == 0:\n",
    "            return 0\n",
    "        elif self.regularization_method == 'weighted_l1':\n",
    "            return self.weighted_l1(x)\n",
    "        elif self.regularization_method == 'weighted_l1_reordered':\n",
    "            return self.weighted_l1_reordered(x)\n",
    "        elif self.regularization_method == 'group_sparsity':\n",
    "            return self.group_sparsity(x)\n",
    "        elif self.regularization_method == 'l1':\n",
    "            return self.l1(x)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Unknown regularization method {self.regularization_method}\")\n",
    "    \n",
    "    def weighted_l1(self, x):\n",
    "        # I.e. for a parameter matrix of 4 input and 10 output neurons:\n",
    "        #\n",
    "        # [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
    "        #  [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
    "        #  [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
    "        #  [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]\n",
    "        #\n",
    "        # the scaling tensor, as well as the resulting weighted values, could be:\n",
    "        #\n",
    "        # [[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n",
    "        #  [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n",
    "        #  [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n",
    "        #  [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]]\n",
    "        #\n",
    "        # Therefore every additional output neuron is regularized more.\n",
    "\n",
    "        scaling_tensor = tf.cumsum(tf.constant(self.regularization_penalty, shape=x.shape, dtype=dtype), axis=-1)\n",
    "        weighted_values = scaling_tensor * tf.abs(x)\n",
    "        return tf.reduce_sum(weighted_values)\n",
    "    \n",
    "    def weighted_l1_reordered(self, x):\n",
    "        # I.e. for a parameter matrix of 4 input and 10 output neurons:\n",
    "        #\n",
    "        # [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
    "        #  [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
    "        #  [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
    "        #  [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]\n",
    "        #\n",
    "        # the scaling tensor, as well as the resulting weighted values, could be:\n",
    "        #\n",
    "        # [[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n",
    "        #  [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n",
    "        #  [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n",
    "        #  [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]]\n",
    "        #\n",
    "        # Therefore every additional output neuron is regularized more.\n",
    "\n",
    "        if self.update_scaling_tensor:\n",
    "            scaling_tensor_raw = tf.cumsum(tf.constant(self.regularization_penalty, shape=x.shape, dtype=dtype), axis=-1)\n",
    "\n",
    "            scaling_tensor_old_neurons = scaling_tensor_raw[:, :-self.n_new_neurons]\n",
    "            scaling_tensor_new_neurons = scaling_tensor_raw[:, -self.n_new_neurons:]\n",
    "            scaling_tensor_old_neurons_shuffled = tf.transpose(tf.random.shuffle(tf.transpose(scaling_tensor_old_neurons)))\n",
    "            self.scaling_tensor = tf.concat([scaling_tensor_old_neurons_shuffled, scaling_tensor_new_neurons], axis=-1)\n",
    "            self.update_scaling_tensor = False\n",
    "\n",
    "        weighted_values = self.scaling_tensor * tf.abs(x)\n",
    "        return tf.reduce_sum(weighted_values)\n",
    "    \n",
    "    def group_sparsity(self, x):\n",
    "        # I.e. for a parameter matrix of 3 input and 5 output neurons:\n",
    "        #\n",
    "        # [[1., 1., 1., 1., 1.],\n",
    "        #  [1., 2., 2., 1., 2.],\n",
    "        #  [2., 2., 3., 1., 3.]]\n",
    "        #\n",
    "        # The resulting vector of group norms is [2., 2., 3., 1., 3.], therefore for\n",
    "        # every output neuron, its incoming connections form a group.\n",
    "\n",
    "        group_norms = tf.norm(x, ord=2, axis=0)\n",
    "        # assert group_norms.shape[0] == x.shape[1]\n",
    "        return self.regularization_penalty * tf.reduce_sum(group_norms)\n",
    "    \n",
    "    def l1(self, x):\n",
    "        weighted_values = self.regularization_penalty * tf.abs(x)\n",
    "        return tf.reduce_sum(weighted_values)\n",
    "    \n",
    "    def prune(self):\n",
    "        self.n_new_neurons = 0\n",
    "        if self.regularization_method == 'weighted_l1_reordered':\n",
    "            self.update_scaling_tensor = True\n",
    "    \n",
    "    def grow(self, n_new_neurons):\n",
    "        self.n_new_neurons = n_new_neurons\n",
    "        if self.regularization_method == 'weighted_l1_reordered':\n",
    "            self.update_scaling_tensor = True\n",
    "    \n",
    "    def set_regularization_penalty(self, regularization_penalty):\n",
    "        self.regularization_penalty = regularization_penalty\n",
    "    \n",
    "    def set_regularization_method(self, regularization_method):\n",
    "        self.regularization_method = regularization_method\n",
    "        if self.regularization_method == 'weighted_l1_reordered':\n",
    "            self.update_scaling_tensor = True\n",
    "        else:\n",
    "            self.update_scaling_tensor = None\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'regularization_penalty': float(self.regularization_penalty)}\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# LAYERS\n",
    "################################################################################\n",
    "\n",
    "\n",
    "class DASLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_shape):\n",
    "        super().__init__()\n",
    "\n",
    "        self._input_shape = input_shape\n",
    "        self._built = False\n",
    "\n",
    "\n",
    "class Dense(DASLayer):\n",
    "    def __init__(self, units, activation, kernel_initializer='glorot_uniform', \n",
    "                 bias_initializer='zeros', input_shape=None, fixed_size=False,\n",
    "                 regularizer=None):\n",
    "        super().__init__(input_shape)\n",
    "\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.bias_initializer = bias_initializer\n",
    "        self.fixed_size = fixed_size\n",
    "        \n",
    "        self.A = tf.keras.activations.get(activation)\n",
    "        self.W_init = tf.keras.initializers.get(kernel_initializer)\n",
    "        self.b_init = tf.keras.initializers.get(bias_initializer)\n",
    "        if regularizer is not None:\n",
    "            self.regularizer = regularizer\n",
    "        else:\n",
    "            self.regularizer = Regularizer()\n",
    "    \n",
    "    def copy(self):\n",
    "        layer_copy = Dense.__new__(Dense)\n",
    "        super(Dense, layer_copy).__init__(self._input_shape)\n",
    "        \n",
    "        layer_copy.units = self.units\n",
    "        layer_copy.activation = self.activation\n",
    "        layer_copy.kernel_initializer = self.kernel_initializer\n",
    "        layer_copy.bias_initializer = self.bias_initializer\n",
    "        layer_copy.fixed_size = self.fixed_size\n",
    "        \n",
    "        layer_copy.A = self.A\n",
    "        layer_copy.W_init = self.W_init\n",
    "        layer_copy.b_init = self.b_init\n",
    "        layer_copy.regularizer = self.regularizer.copy()\n",
    "        \n",
    "        layer_copy.W = tf.Variable(\n",
    "            name='W',\n",
    "            initial_value=self.W,\n",
    "            trainable=True)\n",
    "        \n",
    "        layer_copy.b = tf.Variable(\n",
    "            name='b',\n",
    "            initial_value=self.b,\n",
    "            trainable=True)\n",
    "        \n",
    "        layer_copy.add_regularizer_loss()\n",
    "        \n",
    "        layer_copy._built = True\n",
    "        return layer_copy\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        if self._built:\n",
    "            return\n",
    "            \n",
    "        input_units = input_shape[-1]\n",
    "\n",
    "        self.W = tf.Variable(\n",
    "            name='W',\n",
    "            initial_value=self.W_init(shape=(input_units, self.units), dtype=dtype),\n",
    "            trainable=True)\n",
    "        \n",
    "        self.b = tf.Variable(\n",
    "            name='b',\n",
    "            initial_value=self.b_init(shape=(self.units,), dtype=dtype),\n",
    "            trainable=True)\n",
    "        \n",
    "        self.add_regularizer_loss()\n",
    "        \n",
    "        self._built = True\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        return self.A(tf.matmul(inputs, self.W) + self.b)\n",
    "    \n",
    "    def add_regularizer_loss(self):\n",
    "        self.add_loss(lambda: self.regularizer(tf.concat([self.W, tf.reshape(self.b, (1, -1))], axis=0)))\n",
    "\n",
    "    def get_size(self):\n",
    "        return self.W.shape[0], self.W.shape[1]\n",
    "    \n",
    "    def prune(self, threshold, active_input_units_indices):\n",
    "        # Remove connections from pruned units in previous layer\n",
    "        new_W = tf.gather(self.W.value(), active_input_units_indices, axis=0)\n",
    "\n",
    "        if self.fixed_size:\n",
    "            active_output_neurons_indices = list(range(new_W.shape[1]))\n",
    "        else:\n",
    "            # Prune units in this layer\n",
    "            weights_with_biases = tf.concat([new_W, tf.reshape(self.b.value(), (1, -1))], axis=0)\n",
    "            neurons_are_active = tf.math.reduce_max(tf.abs(weights_with_biases), axis=0) >= threshold\n",
    "            active_output_neurons_indices = tf.reshape(tf.where(neurons_are_active), (-1,))\n",
    "            \n",
    "            new_W = tf.gather(new_W, active_output_neurons_indices, axis=1)\n",
    "            new_b = tf.gather(self.b.value(), active_output_neurons_indices, axis=0)\n",
    "\n",
    "            self.b = tf.Variable(name='b', initial_value=new_b, trainable=True)\n",
    "\n",
    "        self.W = tf.Variable(name='W', initial_value=new_W, trainable=True)\n",
    "\n",
    "        self.regularizer.prune()\n",
    "        return active_output_neurons_indices\n",
    "    \n",
    "    def grow(self, n_new_input_units, percentage, min_new_units, scaling_factor):\n",
    "        if n_new_input_units > 0:\n",
    "            # Add connections to grown units in previous layer\n",
    "            W_growth = self.W_init(shape=(self.W.shape[0] + n_new_input_units, self.W.shape[1]), dtype=dtype)[-n_new_input_units:, :] * scaling_factor  # TODO is it better to be multiplying here by scaling_factor? It does help with not increasing the max weights of existing neurons when new neurons are added.\n",
    "            new_W = tf.concat([self.W.value(), W_growth], axis=0)\n",
    "        else:\n",
    "            new_W = self.W.value()\n",
    "\n",
    "        if self.fixed_size:\n",
    "            n_new_output_units = 0\n",
    "        else:\n",
    "            # Grow new units in this layer\n",
    "            n_new_output_units = max(min_new_units, int(new_W.shape[1] * percentage))\n",
    "            if n_new_output_units > 0:\n",
    "                W_growth = self.W_init(shape=(new_W.shape[0], new_W.shape[1] + n_new_output_units), dtype=dtype)[:, -n_new_output_units:] * scaling_factor\n",
    "                b_growth = self.b_init(shape=(n_new_output_units,), dtype=dtype)  # TODO for all possible bias initializers to work properly, the whole bias vector should be initialized at once\n",
    "                new_W = tf.concat([new_W, W_growth], axis=1)\n",
    "                new_b = tf.concat([self.b.value(), b_growth], axis=0)\n",
    "\n",
    "                self.b = tf.Variable(name='b', initial_value=new_b, trainable=True)\n",
    "\n",
    "        self.W = tf.Variable(name='W', initial_value=new_W, trainable=True)\n",
    "\n",
    "        self.regularizer.grow(n_new_output_units)\n",
    "        return n_new_output_units\n",
    "    \n",
    "    def mutate(self, mutation_strength):\n",
    "        self.W.assign_add(tf.random.normal(self.W.shape, mean=0.0, stddev=mutation_strength))\n",
    "        self.b.assign_add(tf.random.normal(self.b.shape, mean=0.0, stddev=mutation_strength))\n",
    "    \n",
    "    def set_regularization_penalty(self, regularization_penalty):\n",
    "        if not self.fixed_size:\n",
    "            self.regularizer.set_regularization_penalty(regularization_penalty)\n",
    "    \n",
    "    def set_regularization_method(self, regularization_method):\n",
    "        if not self.fixed_size:\n",
    "            self.regularizer.set_regularization_method(regularization_method)\n",
    "    \n",
    "    def get_param_string():\n",
    "        param_string = \"\"\n",
    "        weights_with_bias = tf.concat([self.W, tf.reshape(self.b, (1, -1))], axis=0)\n",
    "        max_parameters = tf.math.reduce_max(tf.abs(weights_with_bias), axis=0).numpy()\n",
    "        magnitudes = np.floor(np.log10(max_parameters))\n",
    "        for m in magnitudes:\n",
    "            if m > 0:\n",
    "                m = 0\n",
    "            param_string += str(int(-m))\n",
    "        return param_string\n",
    "\n",
    "\n",
    "class Conv2D(DASLayer):\n",
    "    def __init__(self, filters, filter_size, activation, strides=(1, 1), \n",
    "                 padding='SAME', kernel_initializer='glorot_uniform',\n",
    "                 bias_initializer='zeros', input_shape=None, fixed_size=False,\n",
    "                 regularizer=None):\n",
    "        super().__init__(input_shape)\n",
    "    \n",
    "        self.filters = filters\n",
    "        self.filter_size = filter_size\n",
    "        self.activation = activation\n",
    "        self.strides = strides\n",
    "        self.padding = padding\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.bias_initializer = bias_initializer\n",
    "        self.fixed_size = fixed_size\n",
    "        \n",
    "        self.A = tf.keras.activations.get(activation)\n",
    "        self.F_init = tf.keras.initializers.get(kernel_initializer)\n",
    "        self.b_init = tf.keras.initializers.get(bias_initializer)\n",
    "        if regularizer is not None:\n",
    "            self.regularizer = regularizer\n",
    "        else:\n",
    "            self.regularizer = Regularizer()\n",
    "    \n",
    "    def copy(self):\n",
    "        layer_copy = Conv2D.__new__(Conv2D)\n",
    "        super(Conv2D, layer_copy).__init__(self._input_shape)\n",
    "        \n",
    "        layer_copy.filters = self.filters\n",
    "        layer_copy.filter_size = self.filter_size\n",
    "        layer_copy.activation = self.activation\n",
    "        layer_copy.strides = self.strides\n",
    "        layer_copy.padding = self.padding\n",
    "        layer_copy.kernel_initializer = self.kernel_initializer\n",
    "        layer_copy.bias_initializer = self.bias_initializer\n",
    "        layer_copy.fixed_size = self.fixed_size\n",
    "        \n",
    "        layer_copy.A = self.A\n",
    "        layer_copy.F_init = self.F_init\n",
    "        layer_copy.b_init = self.b_init\n",
    "        layer_copy.regularizer = self.regularizer.copy()\n",
    "        \n",
    "        layer_copy.F = tf.Variable(\n",
    "            name='F',\n",
    "            initial_value=self.F,\n",
    "            trainable=True)\n",
    "        \n",
    "        layer_copy.b = tf.Variable(\n",
    "            name='b',\n",
    "            initial_value=self.b,\n",
    "            trainable=True)\n",
    "        \n",
    "        layer_copy.add_regularizer_loss()\n",
    "        \n",
    "        layer_copy._built = True\n",
    "        return layer_copy\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        if self._built:\n",
    "            return\n",
    "\n",
    "        input_filters = input_shape[-1]\n",
    "\n",
    "        self.F = tf.Variable(\n",
    "            name='F',\n",
    "            initial_value=self.F_init(\n",
    "                shape=(self.filter_size[0], self.filter_size[1], input_filters, self.filters), dtype=dtype\n",
    "            ),\n",
    "            trainable=True)\n",
    "        \n",
    "        self.b = tf.Variable(\n",
    "            name='b',\n",
    "            initial_value=self.b_init(shape=(self.filters,), dtype=dtype),\n",
    "            trainable=True)\n",
    "\n",
    "        self.add_regularizer_loss()\n",
    "        \n",
    "        self._built = True\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        y = tf.nn.conv2d(inputs, self.F, strides=self.strides, padding=self.padding)\n",
    "        y = tf.nn.bias_add(y, self.b)\n",
    "        y = self.A(y)\n",
    "        return y\n",
    "    \n",
    "    def add_regularizer_loss(self):\n",
    "        self.add_loss(lambda: self.regularizer(tf.concat([tf.reshape(self.F, (-1, self.F.shape[-1])), tf.reshape(self.b, (1, -1))], axis=0)))\n",
    "    \n",
    "    def get_size(self):\n",
    "        return self.F.shape[-2], self.F.shape[-1]\n",
    "    \n",
    "    def prune(self, threshold, active_input_units_indices):\n",
    "        # Remove connections from pruned units in previous layer\n",
    "        new_F = tf.gather(self.F.value(), active_input_units_indices, axis=-2)\n",
    "\n",
    "        if self.fixed_size:\n",
    "            active_output_filters_indices = list(range(new_F.shape[-1]))\n",
    "        else:\n",
    "            # Prune units in this layer\n",
    "            F_reduced_max = tf.reshape(tf.math.reduce_max(tf.abs(new_F), axis=(0, 1, 2)), (1, -1))\n",
    "            F_reduced_max_with_biases = tf.concat([F_reduced_max, tf.reshape(self.b.value(), (1, -1))], axis=0)\n",
    "            filters_are_active = tf.math.reduce_max(tf.abs(F_reduced_max_with_biases), axis=0) >= threshold\n",
    "            active_output_filters_indices = tf.reshape(tf.where(filters_are_active), (-1,))\n",
    "            \n",
    "            new_F = tf.gather(new_F, active_output_filters_indices, axis=-1)\n",
    "            new_b = tf.gather(self.b.value(), active_output_filters_indices, axis=0)\n",
    "\n",
    "            self.b = tf.Variable(name='b', initial_value=new_b, trainable=True)\n",
    "\n",
    "        self.F = tf.Variable(name='F', initial_value=new_F, trainable=True)\n",
    "\n",
    "        self.regularizer.prune()\n",
    "        return active_output_filters_indices\n",
    "\n",
    "    def grow(self, n_new_input_units, percentage, min_new_units, scaling_factor):\n",
    "        if n_new_input_units > 0:\n",
    "            # Add connections to grown units in previous layer\n",
    "            F_growth = self.F_init(shape=(self.F.shape[0], self.F.shape[1], self.F.shape[2] + n_new_input_units, self.F.shape[3]), dtype=dtype)[:, :, -n_new_input_units:, :] * scaling_factor  # TODO is it better to be multiplying here by scaling_factor? It does help with not increasing the max weights of existing neurons when new neurons are added.\n",
    "            new_F = tf.concat([self.F.value(), F_growth], axis=-2)\n",
    "        else:\n",
    "            new_F = self.F.value()\n",
    "\n",
    "        if self.fixed_size:\n",
    "            n_new_output_units = 0\n",
    "        else:\n",
    "            # Grow new units in this layer\n",
    "            n_new_output_units = max(min_new_units, int(new_F.shape[-1] * percentage))\n",
    "            if n_new_output_units > 0:\n",
    "                F_growth = self.F_init(shape=(new_F.shape[0], new_F.shape[1], new_F.shape[2], new_F.shape[3] + n_new_output_units), dtype=dtype)[:, :, :, -n_new_output_units:] * scaling_factor\n",
    "                b_growth = self.b_init(shape=(n_new_output_units,), dtype=dtype)  # TODO for all possible bias initializers to work properly, the whole bias vector should be initialized at once\n",
    "                new_F = tf.concat([new_F, F_growth], axis=-1)\n",
    "                new_b = tf.concat([self.b.value(), b_growth], axis=0)\n",
    "\n",
    "                self.b = tf.Variable(name='b', initial_value=new_b, trainable=True)\n",
    "\n",
    "        self.F = tf.Variable(name='F', initial_value=new_F, trainable=True)\n",
    "\n",
    "        self.regularizer.grow(n_new_output_units)\n",
    "        return n_new_output_units\n",
    "    \n",
    "    def mutate(self, mutation_strength):\n",
    "        self.F.assign_add(tf.random.normal(self.F.shape, mean=0.0, stddev=mutation_strength))\n",
    "        self.b.assign_add(tf.random.normal(self.b.shape, mean=0.0, stddev=mutation_strength))\n",
    "    \n",
    "    def set_regularization_penalty(self, regularization_penalty):\n",
    "        if not self.fixed_size:\n",
    "            self.regularizer.set_regularization_penalty(regularization_penalty)\n",
    "    \n",
    "    def set_regularization_method(self, regularization_method):\n",
    "        if not self.fixed_size:\n",
    "            self.regularizer.set_regularization_method(regularization_method)\n",
    "\n",
    "    def get_param_string():\n",
    "        param_string = \"\"\n",
    "        # TODO\n",
    "        return param_string\n",
    "\n",
    "\n",
    "class Flatten(tf.keras.layers.Layer):\n",
    "    def call(self, inputs, training=None):\n",
    "        return tf.reshape(tf.transpose(inputs, perm=[0, 3, 1, 2]), (inputs.shape[0], -1))\n",
    "    \n",
    "    # def copy(self):\n",
    "    #     return Flatten()\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# MODELS\n",
    "################################################################################\n",
    "\n",
    "\n",
    "class Epoch:\n",
    "    def __init__(self, grow, prune, regularization_penalty, regularization_method):\n",
    "        self.grow = grow\n",
    "        self.prune = prune\n",
    "        self.regularization_penalty = regularization_penalty\n",
    "        self.regularization_method = regularization_method\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f'{int(self.grow)}{int(self.prune)}{self.regularization_penalty}{self.regularization_method}'\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "\n",
    "class DynamicEpoch(Epoch):\n",
    "    def __init__(self, regularization_penalty, regularization_method):\n",
    "        super().__init__(True, True, regularization_penalty, regularization_method)\n",
    "\n",
    "\n",
    "class StaticEpoch(Epoch):\n",
    "    def __init__(self, regularization_penalty, regularization_method):\n",
    "        super().__init__(False, False, regularization_penalty, regularization_method)\n",
    "\n",
    "\n",
    "class StaticEpochNoRegularization(StaticEpoch):\n",
    "    def __init__(self):\n",
    "        super().__init__(0., None)\n",
    "\n",
    "\n",
    "class Schedule:\n",
    "    def __init__(self, epochs):\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self.epochs.__iter__()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.epochs)\n",
    "    \n",
    "    def __str__(self):\n",
    "        text = ''.join([str(epoch) for epoch in self.epochs])\n",
    "        return hashlib.sha1(text.encode('utf-8')).hexdigest()[:10]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "\n",
    "class Sequential(tf.keras.Model):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lrs = layers\n",
    "        \n",
    "    def call(self, inputs, training=None):\n",
    "        x = inputs\n",
    "        for layer in self.lrs:\n",
    "            x = layer(x, training=training)\n",
    "        return x\n",
    "    \n",
    "    def copy(self):\n",
    "        copied_layers = list()\n",
    "        for layer in self.lrs:\n",
    "            if isinstance(layer, DASLayer):\n",
    "                layer_copy = layer.copy()\n",
    "#                 layer_copy = copy.deepcopy(layer)\n",
    "#                 layer_copy.add_regularizer_loss()\n",
    "            else:\n",
    "                layer_copy = copy.deepcopy(layer)\n",
    "            copied_layers.append(layer_copy)\n",
    "        \n",
    "        model_copy = Sequential(copied_layers)\n",
    "        return model_copy\n",
    "    \n",
    "    def get_layer_input_shape(self, target_layer):\n",
    "        if target_layer._input_shape is not None:\n",
    "            return target_layer._input_shape\n",
    "\n",
    "        input = np.random.normal(size=(1,) + self.lrs[0]._input_shape)\n",
    "        for layer in self.lrs:\n",
    "            if layer is target_layer:\n",
    "                return tuple(input.shape[1:])\n",
    "            input = layer(input)\n",
    "        raise Exception(\"Layer not found in the model.\")\n",
    "\n",
    "    def get_layer_output_shape(self, target_layer):\n",
    "        input = np.random.normal(size=(1,) + self.lrs[0]._input_shape)\n",
    "        for layer in self.lrs:\n",
    "            output = layer(input)\n",
    "            if layer is target_layer:\n",
    "                return tuple(output.shape[1:])\n",
    "            input = output\n",
    "        raise Exception(\"Layer not found in the model.\")\n",
    "    \n",
    "    def get_layer_sizes(self):\n",
    "        \"\"\"\n",
    "        Returns the sizes of all layers in the model, including the input and output layer.\n",
    "        \"\"\"\n",
    "        layer_sizes = list()\n",
    "        first_layer = True\n",
    "        for l in range(len(self.lrs)):\n",
    "            layer = self.lrs[l]\n",
    "            if isinstance(layer, DASLayer):\n",
    "                layer_size = layer.get_size()\n",
    "                if first_layer:\n",
    "                    layer_sizes.append(layer_size[0])\n",
    "                    first_layer = False\n",
    "                layer_sizes.append(layer_size[1])\n",
    "        return layer_sizes\n",
    "    \n",
    "    def get_hidden_layer_sizes(self):\n",
    "        return self.get_layer_sizes()[1:-1]\n",
    "    \n",
    "    def get_regularization_penalty(self):\n",
    "        #TODO improve\n",
    "        return self.lrs[-2].regularizer.regularization_penalty\n",
    "    \n",
    "    def set_regularization_penalty(self, regularization_penalty):\n",
    "        for layer in self.lrs:\n",
    "            if isinstance(layer, DASLayer) and not layer.fixed_size:\n",
    "                layer.set_regularization_penalty(regularization_penalty)\n",
    "    \n",
    "    def set_regularization_method(self, regularization_method):\n",
    "        for layer in self.lrs:\n",
    "            if isinstance(layer, DASLayer) and not layer.fixed_size:\n",
    "                layer.set_regularization_method(regularization_method)\n",
    "\n",
    "    def prune(self, params):\n",
    "        input_shape = self.get_layer_input_shape(self.lrs[0])\n",
    "        n_input_units = input_shape[-1]\n",
    "        active_units_indices = list(range(n_input_units))\n",
    "\n",
    "        last_custom_layer = None\n",
    "        for layer in self.lrs:\n",
    "            if isinstance(layer, DASLayer):\n",
    "                if last_custom_layer is not None and type(last_custom_layer) != type(layer):\n",
    "                    if type(last_custom_layer) == Conv2D and type(layer) == Dense:\n",
    "                        convolutional_shape = self.get_layer_output_shape(last_custom_layer)\n",
    "                        active_units_indices = self.convert_channel_indices_to_flattened_indices(active_units_indices, convolutional_shape)\n",
    "                    else:\n",
    "                        raise Exception(\"Incorrect order of custom layer types.\")\n",
    "                active_units_indices = layer.prune(params.pruning_threshold, active_units_indices)\n",
    "                last_custom_layer = layer\n",
    "    \n",
    "    def grow(self, params):   \n",
    "        n_new_units = 0\n",
    "\n",
    "        last_custom_layer = None\n",
    "        for layer in self.lrs:\n",
    "            if isinstance(layer, DASLayer):\n",
    "                if last_custom_layer is not None and type(last_custom_layer) != type(layer):\n",
    "                    if type(last_custom_layer) == Conv2D and type(layer) == Dense:\n",
    "                        convolutional_shape = self.get_layer_output_shape(last_custom_layer)\n",
    "                        n_new_units = n_new_units * convolutional_shape[0] * convolutional_shape[1]\n",
    "                    else:\n",
    "                        raise Exception(\"Incorrect order of custom layer types.\")\n",
    "                n_new_units = layer.grow(n_new_units, params.growth_percentage, min_new_units=params.min_new_neurons, scaling_factor=params.pruning_threshold)\n",
    "                last_custom_layer = layer\n",
    "    \n",
    "    def mutate(self, mutation_strength):\n",
    "        for layer in self.lrs:\n",
    "            if isinstance(layer, DASLayer):\n",
    "                layer.mutate(mutation_strength)\n",
    "    \n",
    "    @staticmethod\n",
    "    def convert_channel_indices_to_flattened_indices(channel_indices, convolutional_shape):\n",
    "        dense_indices = list()\n",
    "        units_per_channel = convolutional_shape[0] * convolutional_shape[1]\n",
    "        for channel_index in channel_indices:\n",
    "            for iter in range(units_per_channel):\n",
    "                dense_indices.append(channel_index * units_per_channel + iter)\n",
    "        return dense_indices\n",
    "    \n",
    "    def print_neurons(self):\n",
    "        for layer in self.lrs[:-1]:\n",
    "            print(layer.get_param_string())\n",
    "    \n",
    "    def evaluate(self, params, summed_training_loss, summed_training_metric):\n",
    "        # Calculate training loss and metric\n",
    "        if summed_training_loss is not None:\n",
    "            loss = summed_training_loss / params.x.shape[0]\n",
    "        else:\n",
    "            loss = None\n",
    "        \n",
    "        if summed_training_metric is not None:\n",
    "            metric = summed_training_metric / params.x.shape[0]\n",
    "        else:\n",
    "            metric = None\n",
    "        \n",
    "        # Calculate val loss and metric\n",
    "        summed_val_loss = 0\n",
    "        summed_val_metric = 0\n",
    "        n_val_instances = 0\n",
    "        \n",
    "        for step, (x_batch, y_batch) in enumerate(params.val_dataset):\n",
    "            # y_pred = tf.reshape(self(x_batch, training=False), y_batch.shape)\n",
    "            y_pred = self(x_batch, training=False)\n",
    "            summed_val_loss += tf.reduce_sum(params.loss_fn(y_batch, y_pred))\n",
    "            summed_val_metric += float(tf.reduce_sum(params.metric_fn(y_batch, y_pred)))\n",
    "            n_val_instances += x_batch.shape[0]\n",
    "        \n",
    "        val_loss = summed_val_loss / n_val_instances\n",
    "        val_metric = summed_val_metric / n_val_instances\n",
    "\n",
    "        return loss, metric, val_loss, val_metric\n",
    "\n",
    "    def list_params(self):\n",
    "        trainable_count = np.sum([K.count_params(w) for w in self.trainable_weights])\n",
    "        non_trainable_count = np.sum([K.count_params(w) for w in self.non_trainable_weights])\n",
    "        total_count = trainable_count + non_trainable_count\n",
    "\n",
    "        print('Total params: {:,}'.format(total_count))\n",
    "        print('Trainable params: {:,}'.format(trainable_count))\n",
    "        print('Non-trainable params: {:,}'.format(non_trainable_count))\n",
    "\n",
    "        return total_count, trainable_count, non_trainable_count\n",
    "    \n",
    "    def print_epoch_statistics(self, params, summed_training_loss, summed_training_metric, message=None, require_result=False):\n",
    "        if not params.verbose:\n",
    "            if require_result:\n",
    "                return self.evaluate(params, summed_training_loss, summed_training_metric)\n",
    "            else:\n",
    "                return\n",
    "        \n",
    "        loss, metric, val_loss, val_metric = self.evaluate(params, summed_training_loss, summed_training_metric)  \n",
    "\n",
    "        if message is not None:\n",
    "            print(message)\n",
    "        \n",
    "        print(f\"loss: {loss} - metric: {metric} - val_loss: {val_loss} - val_metric: {val_metric} - penalty: {self.get_regularization_penalty()}\")\n",
    "        hidden_layer_sizes = self.get_hidden_layer_sizes()\n",
    "        print(f\"hidden layer sizes: {hidden_layer_sizes}, total units: {sum(hidden_layer_sizes)}\")\n",
    "        if params.print_neurons:\n",
    "            self.print_neurons()\n",
    "        \n",
    "        if require_result:\n",
    "            return loss, metric, val_loss, val_metric\n",
    "    \n",
    "    def update_history(self, params, loss, metric, val_loss, val_metric):\n",
    "        params.history['loss'].append(float(loss))\n",
    "        params.history['metric'].append(float(metric))\n",
    "        params.history['val_loss'].append(float(val_loss))\n",
    "        params.history['val_metric'].append(float(val_metric))\n",
    "        params.history['hidden_layer_sizes'].append(self.get_hidden_layer_sizes())\n",
    "    \n",
    "    @staticmethod\n",
    "    def prepare_datasets(x, y, batch_size, validation_data):\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "        train_dataset = train_dataset.shuffle(buffer_size=20000).batch(batch_size)\n",
    "        val_dataset = tf.data.Dataset.from_tensor_slices(validation_data).batch(batch_size)\n",
    "        return train_dataset.prefetch(tf.data.AUTOTUNE), val_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    def manage_dynamic_regularization(self, params, val_loss):\n",
    "        if val_loss >= params.best_conditional_val_loss * params.stall_coefficient:\n",
    "            # Training is currently in stall\n",
    "            if not params.training_stalled:\n",
    "                penalty = self.get_regularization_penalty() * params.regularization_penalty_multiplier\n",
    "                print(\"Changing penalty...\")\n",
    "                # TODO this must be modified, penalty can differ for each layer\n",
    "                self.set_regularization_penalty(penalty)\n",
    "                params.training_stalled = True\n",
    "        else:\n",
    "            params.best_conditional_val_loss = val_loss\n",
    "            params.training_stalled = False\n",
    "    \n",
    "    def grow_wrapper(self, params):\n",
    "        dynamic_reqularization_active = params.regularization_penalty_multiplier != 1.\n",
    "        if dynamic_reqularization_active:\n",
    "            loss, metric, val_loss, val_metric = self.print_epoch_statistics(params, None, None, \"Before growing:\", require_result=True)\n",
    "            self.manage_dynamic_regularization(params, val_loss)\n",
    "        else:\n",
    "            self.print_epoch_statistics(params, None, None, \"Before growing:\")\n",
    "\n",
    "        self.grow(params)\n",
    "        self.print_epoch_statistics(params, None, None, \"After growing:\")\n",
    "    \n",
    "    def prune_wrapper(self, params, summed_loss, summed_metric):\n",
    "        loss, metric, _, _ = self.print_epoch_statistics(params, summed_loss, summed_metric, \"Before pruning:\", require_result=True)\n",
    "        self.prune(params)\n",
    "        _, _, val_loss, val_metric = self.print_epoch_statistics(params, None, None, \"After pruning:\", require_result=True)\n",
    "        self.update_history(params, loss, metric, val_loss, val_metric)\n",
    "    \n",
    "    class ParameterContainer:\n",
    "        def __init__(self, x, y, optimizer, batch_size, min_new_neurons, validation_data, pruning_threshold, regularization_penalty_multiplier, \n",
    "                     stall_coefficient, growth_percentage, mini_epochs_per_epoch, verbose, print_neurons, use_static_graph, loss_fn, metric_fn):\n",
    "            self.x = x\n",
    "            self.y = y\n",
    "            self.optimizer = optimizer\n",
    "            self.batch_size = batch_size\n",
    "            self.min_new_neurons = min_new_neurons\n",
    "            self.validation_data = validation_data\n",
    "            self.pruning_threshold = pruning_threshold\n",
    "            self.regularization_penalty_multiplier = regularization_penalty_multiplier\n",
    "            self.stall_coefficient = stall_coefficient\n",
    "            self.growth_percentage = growth_percentage\n",
    "            self.mini_epochs_per_epoch = mini_epochs_per_epoch\n",
    "            self.verbose = verbose\n",
    "            self.print_neurons = print_neurons\n",
    "            self.use_static_graph = use_static_graph\n",
    "            self.loss_fn = loss_fn\n",
    "            self.metric_fn = metric_fn\n",
    "\n",
    "            self.train_dataset, self.val_dataset = Sequential.prepare_datasets(x, y, batch_size, validation_data)\n",
    "            self.history = self.prepare_history()\n",
    "\n",
    "            self.best_conditional_val_loss = np.inf\n",
    "            self.training_stalled = False\n",
    "        \n",
    "        @staticmethod\n",
    "        def prepare_history():\n",
    "            history = {\n",
    "                'loss': list(),\n",
    "                'metric': list(),\n",
    "                'val_loss': list(),\n",
    "                'val_metric': list(),\n",
    "                'hidden_layer_sizes': list(),\n",
    "            }\n",
    "            return history\n",
    "    \n",
    "    def fit_single_step(self, x_batch, y_batch, optimizer, loss_fn, metric_fn):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # y_pred = tf.reshape(self(x_batch, training=True), y_batch.shape)\n",
    "            y_pred = self(x_batch, training=True)\n",
    "            raw_loss = loss_fn(y_batch, y_pred)\n",
    "            loss_value = tf.reduce_mean(raw_loss)\n",
    "            loss_value += sum(self.losses)  # Add losses registered by model.add_loss\n",
    "\n",
    "            loss = tf.reduce_sum(raw_loss)\n",
    "            metric = float(tf.reduce_sum(metric_fn(y_batch, y_pred)))\n",
    "\n",
    "        grads = tape.gradient(loss_value, self.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "\n",
    "        return loss, metric\n",
    "    \n",
    "    def fit_single_epoch(self, params):\n",
    "        summed_loss = 0\n",
    "        summed_metric = 0\n",
    "        \n",
    "        for mini_epoch in range(params.mini_epochs_per_epoch):\n",
    "            summed_loss = 0\n",
    "            summed_metric = 0\n",
    "\n",
    "            if params.use_static_graph:\n",
    "                fit_single_step_function = tf.function(self.fit_single_step)\n",
    "            else:\n",
    "                fit_single_step_function = self.fit_single_step\n",
    "            for step, (x_batch, y_batch) in enumerate(params.train_dataset):\n",
    "                loss, metric = fit_single_step_function(x_batch, y_batch, params.optimizer, params.loss_fn, params.metric_fn)\n",
    "                summed_loss += loss\n",
    "                summed_metric += metric\n",
    "        \n",
    "        return summed_loss, summed_metric\n",
    "\n",
    "    def fit(self, x, y, optimizer, schedule, batch_size, min_new_neurons, validation_data, pruning_threshold=0.001, regularization_penalty_multiplier=1., \n",
    "            stall_coefficient=1, growth_percentage=0.2, mini_epochs_per_epoch=1, verbose=True, print_neurons=False, use_static_graph=True, \n",
    "            loss_fn=tf.keras.losses.sparse_categorical_crossentropy, metric_fn=tf.keras.metrics.sparse_categorical_accuracy):\n",
    "        params = self.ParameterContainer(x=x, y=y, optimizer=optimizer, batch_size=batch_size, min_new_neurons=min_new_neurons, validation_data=validation_data, \n",
    "                                         pruning_threshold=pruning_threshold, regularization_penalty_multiplier=regularization_penalty_multiplier, stall_coefficient=stall_coefficient, \n",
    "                                         growth_percentage=growth_percentage, mini_epochs_per_epoch=mini_epochs_per_epoch, verbose=verbose, print_neurons=print_neurons, \n",
    "                                         use_static_graph=use_static_graph, loss_fn=loss_fn, metric_fn=metric_fn)\n",
    "        self.build(x.shape)  # Necessary when verbose == False\n",
    "\n",
    "        for epoch_no, epoch in enumerate(schedule):\n",
    "            if verbose:\n",
    "                print(\"##########################################################\")\n",
    "                print(f\"Epoch {epoch_no + 1}/{len(schedule)}\")\n",
    "            \n",
    "            self.set_regularization_penalty(epoch.regularization_penalty)\n",
    "            self.set_regularization_method(epoch.regularization_method)\n",
    "\n",
    "            if epoch.grow:\n",
    "                self.grow_wrapper(params)\n",
    "\n",
    "            summed_loss, summed_metric = self.fit_single_epoch(params)\n",
    "            \n",
    "            if epoch.prune:\n",
    "                self.prune_wrapper(params, summed_loss, summed_metric)\n",
    "            else:\n",
    "                loss, metric, val_loss, val_metric = self.print_epoch_statistics(params, summed_loss, summed_metric, require_result=True)\n",
    "                self.update_history(params, loss, metric, val_loss, val_metric)\n",
    "        \n",
    "        return params.history\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# HELPER FUNCTIONS\n",
    "################################################################################\n",
    "\n",
    "\n",
    "def get_statistics_from_history(history):\n",
    "    best_epoch_number = np.argmax(history['val_metric'])\n",
    "    best_val_loss = history['val_loss'][best_epoch_number]\n",
    "    best_val_metric = history['val_metric'][best_epoch_number]\n",
    "    best_hidden_layer_sizes = history['hidden_layer_sizes'][best_epoch_number]\n",
    "    return best_val_loss, best_val_metric, best_hidden_layer_sizes\n",
    "\n",
    "\n",
    "def get_statistics_from_histories(histories):\n",
    "    best_val_losses = list()\n",
    "    best_val_metrics = list()\n",
    "    all_best_hidden_layer_sizes = list()\n",
    "\n",
    "    for history in histories:\n",
    "        best_val_loss, best_val_metric, best_hidden_layer_sizes = get_statistics_from_history(history)\n",
    "        best_val_losses.append(best_val_loss)\n",
    "        best_val_metrics.append(best_val_metric)\n",
    "        all_best_hidden_layer_sizes.append(best_hidden_layer_sizes)\n",
    "    \n",
    "    mean_best_val_loss = np.mean(best_val_losses)\n",
    "    mean_best_val_metric = np.mean(best_val_metrics)\n",
    "    mean_best_hidden_layer_sizes = [np.mean(layer) for layer in list(zip(*all_best_hidden_layer_sizes))]\n",
    "    \n",
    "    return mean_best_val_loss, mean_best_val_metric, mean_best_hidden_layer_sizes\n",
    "\n",
    "\n",
    "def cross_validate(train_fn, x, y, n_splits, random_state=42, *args, **kwargs):\n",
    "    from sklearn.model_selection import KFold\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "    histories = list()\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(x)):\n",
    "        xtrain, xtest = x[train_index], x[test_index]\n",
    "        ytrain, ytest = y[train_index], y[test_index]\n",
    "\n",
    "        history = train_fn(xtrain, ytrain, validation_data=(xtest, ytest), *args, **kwargs)\n",
    "        histories.append(history)\n",
    "\n",
    "        best_val_loss, best_val_metric, best_hidden_layer_sizes = get_statistics_from_history(history)\n",
    "        print(f\"Run {i} completed, best_val_loss: {best_val_loss}, best_val_metric: {best_val_metric}, best_hidden_layer_sizes: {best_hidden_layer_sizes}\")\n",
    "\n",
    "    mean_best_val_loss, mean_best_val_metric, mean_best_hidden_layer_sizes = get_statistics_from_histories(histories)\n",
    "    print(f'mean_best_val_loss: {mean_best_val_loss}')\n",
    "    print(f'mean_best_val_metric: {mean_best_val_metric}')\n",
    "    print(f'mean_best_hidden_layer_sizes: {mean_best_hidden_layer_sizes}')\n",
    "\n",
    "    return histories, mean_best_hidden_layer_sizes\n",
    "\n",
    "\n",
    "def hyperparameter_search(train_fn, x, y, validation_data, *args, **kwargs):\n",
    "    from itertools import product\n",
    "\n",
    "    all_params = [*args] + list(kwargs.values())\n",
    "    histories = list()\n",
    "\n",
    "    best_overall_val_loss = np.inf\n",
    "    best_overall_val_metric = None\n",
    "    best_overall_combination = None\n",
    "\n",
    "    for combination in product(*all_params):\n",
    "        combination_args = combination[:len(args)]\n",
    "\n",
    "        combination_kwargs_values = combination[len(args):]\n",
    "        combination_kwargs = dict(zip(kwargs.keys(), combination_kwargs_values))\n",
    "\n",
    "        history = train_fn(x, y, validation_data, *combination_args, **combination_kwargs)\n",
    "        history['parameters'] = combination\n",
    "        histories.append(history)\n",
    "\n",
    "        best_val_loss, best_val_metric, best_hidden_layer_sizes = get_statistics_from_history(history)\n",
    "        print(f\"Run with parameters {combination} completed, best_val_loss: {best_val_loss}, best_val_metric: {best_val_metric}, best_hidden_layer_sizes: {best_hidden_layer_sizes}\")\n",
    "\n",
    "        if best_val_loss < best_overall_val_loss:\n",
    "            best_overall_val_loss = best_val_loss\n",
    "            best_overall_val_metric = best_val_metric\n",
    "            best_overall_combination = combination\n",
    "    \n",
    "    print(f'Best overall combination: {best_overall_combination}, val_metric: {best_overall_val_metric}')\n",
    "\n",
    "    return histories, best_overall_combination\n",
    "\n",
    "\n",
    "def interruptible(f):\n",
    "    def function(*args, **kwargs):\n",
    "        try:\n",
    "            return f(*args, **kwargs)\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Interrupted by user.\")\n",
    "    return function\n",
    "\n",
    "\n",
    "class AnytimeAlgorithm:\n",
    "    def __init__(self):\n",
    "        self.val_metrics = list()\n",
    "        self.best_val_metric = -np.inf\n",
    "        self.start_time = None\n",
    "    \n",
    "    def log_result(self, val_metric):\n",
    "        if val_metric > self.best_val_metric:\n",
    "            self.best_val_metric = val_metric\n",
    "        _time = time.time() - self.start_time\n",
    "        self.val_metrics.append((_time, self.best_val_metric))\n",
    "    \n",
    "    def run(self):\n",
    "        self.start_time = time.time()\n",
    "\n",
    "\n",
    "class Range:\n",
    "    def __init__(self, min_value, max_value, transformation=None):\n",
    "        self.min_value = min_value\n",
    "        self.max_value = max_value\n",
    "        self.transformation = transformation\n",
    "        \n",
    "    @abc.abstractmethod\n",
    "    def sample():\n",
    "        pass\n",
    "\n",
    "\n",
    "class UniformRange(Range):\n",
    "    def sample(self):\n",
    "        x = np.random.uniform(self.min_value, self.max_value)\n",
    "        if self.transformation is not None:\n",
    "            result = self.transformation(x)\n",
    "            print(f\"Transformed value {x} to {result}.\")\n",
    "            return result\n",
    "        return x\n",
    "\n",
    "\n",
    "class PowerRange(Range):\n",
    "    def sample(self):\n",
    "        exponent = np.random.uniform(self.min_value, self.max_value)\n",
    "        x = 10. ** exponent\n",
    "        if self.transformation is not None:\n",
    "            result = self.transformation(x)\n",
    "            print(f\"Transformed value {x} to {result}.\")\n",
    "            return result\n",
    "        return x\n",
    "\n",
    "\n",
    "class RandomSearch(AnytimeAlgorithm):\n",
    "    @staticmethod\n",
    "    def sample_combination(values):\n",
    "        combination = list()\n",
    "        for value in values:\n",
    "            if isinstance(value, Range):\n",
    "                combination.append(value.sample())\n",
    "            else:\n",
    "                combination.append(value)\n",
    "        return tuple(combination)\n",
    "    \n",
    "    @interruptible\n",
    "    def run(self, train_fn, x, y, validation_data, *args, **kwargs):        \n",
    "        super().run()\n",
    "\n",
    "        start_time = time.time()\n",
    "        histories = list()\n",
    "        improvements = list()\n",
    "\n",
    "        best_overall_val_metric = -np.inf\n",
    "        best_overall_combination = None\n",
    "\n",
    "        while True:\n",
    "            combination_args = self.sample_combination([*args])\n",
    "\n",
    "            combination_kwargs_values = self.sample_combination(list(kwargs.values()))\n",
    "            combination_kwargs = dict(zip(kwargs.keys(), combination_kwargs_values))\n",
    "\n",
    "            combination = combination_args + combination_kwargs_values\n",
    "\n",
    "            print(f\"Run with parameters {combination} started...\")\n",
    "\n",
    "            history = train_fn(x, y, validation_data, *combination_args, **combination_kwargs)\n",
    "            history['parameters'] = combination\n",
    "            histories.append(history)\n",
    "\n",
    "            best_val_loss, best_val_metric, best_hidden_layer_sizes = get_statistics_from_history(history)\n",
    "            print(f\"Run with parameters {combination} completed, best_val_loss: {best_val_loss}, best_val_metric: {best_val_metric}, best_hidden_layer_sizes: {best_hidden_layer_sizes}\")\n",
    "\n",
    "            if best_val_metric > best_overall_val_metric:\n",
    "                best_overall_val_metric = best_val_metric\n",
    "                best_overall_combination = combination\n",
    "            \n",
    "            self.log_result(best_overall_val_metric)\n",
    "\n",
    "        print(f'Best overall combination: {best_overall_combination}, val_metric: {best_overall_val_metric}')\n",
    "\n",
    "        return improvements, histories, best_overall_combination\n",
    "\n",
    "\n",
    "def get_convolutional_model(x, layer_sizes, output_neurons=10):\n",
    "    model = Sequential([\n",
    "        Conv2D(layer_sizes[0], filter_size=(3, 3), activation='selu', strides=(1, 1), padding='SAME', kernel_initializer='lecun_normal', input_shape=x[0,:,:,:].shape),\n",
    "        Conv2D(layer_sizes[1], filter_size=(3, 3), activation='selu', strides=(2, 2), padding='SAME', kernel_initializer='lecun_normal'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        Conv2D(layer_sizes[2], filter_size=(3, 3), activation='selu', strides=(1, 1), padding='SAME', kernel_initializer='lecun_normal'),\n",
    "        Conv2D(layer_sizes[3], filter_size=(3, 3), activation='selu', strides=(2, 2), padding='SAME', kernel_initializer='lecun_normal'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        Flatten(),\n",
    "        Dense(layer_sizes[4], activation='selu', kernel_initializer='lecun_normal'),\n",
    "        Dense(output_neurons, activation='softmax', fixed_size=True),\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_dense_model(x, layer_sizes):\n",
    "    layers = list()\n",
    "    \n",
    "    layers.append(Dense(layer_sizes[0], activation='selu', kernel_initializer='lecun_normal', input_shape=x[0, :].shape))\n",
    "    for layer_size in layer_sizes[1:]:\n",
    "        layers.append(Dense(layer_size, activation='selu', kernel_initializer='lecun_normal'))\n",
    "    layers.append(Dense(1, activation=None, kernel_initializer='lecun_normal', fixed_size=True))\n",
    "    \n",
    "    model = Sequential(layers)\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_fn_conv(x, y, validation_data, learning_rate, schedule, layer_sizes, output_neurons=10, min_new_neurons=20, \n",
    "             growth_percentage=0.2, verbose=False, use_static_graph=True):\n",
    "    batch_size = 128\n",
    "\n",
    "    model = get_convolutional_model(x, layer_sizes, output_neurons)\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    history = model.fit(x=x, y=y, optimizer=optimizer, schedule=schedule, batch_size=batch_size, min_new_neurons=min_new_neurons, \n",
    "                        validation_data=validation_data, growth_percentage=growth_percentage, verbose=verbose, use_static_graph=use_static_graph)\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "def squared_error(y_true, y_pred):\n",
    "    return (y_true - y_pred) ** 2\n",
    "\n",
    "\n",
    "def train_fn_dense(x, y, validation_data, learning_rate, schedule, layer_sizes, min_new_neurons=20, \n",
    "             growth_percentage=0.2, verbose=False, use_static_graph=True):\n",
    "    batch_size = 128\n",
    "\n",
    "    model = get_dense_model(x, layer_sizes)\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    history = model.fit(x=x, y=y, optimizer=optimizer, schedule=schedule, batch_size=batch_size, min_new_neurons=min_new_neurons, \n",
    "                        validation_data=validation_data, growth_percentage=growth_percentage, verbose=verbose, use_static_graph=use_static_graph,\n",
    "                        loss_fn=squared_error, metric_fn=squared_error)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Individual:\n",
    "    def __init__(self, genome, model, optimizer, history=None, age=0):\n",
    "        self.genome = genome\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        if history is not None:\n",
    "            self.history = history\n",
    "        else:\n",
    "            self.history = Sequential.ParameterContainer.prepare_history()\n",
    "        self.age = age\n",
    "    \n",
    "    def copy(self):\n",
    "        genome_copy = self.genome.copy()\n",
    "        model_copy = self.model.copy()\n",
    "        optimizer_copy = copy.deepcopy(self.optimizer)\n",
    "        history_copy = copy.deepcopy(self.history)\n",
    "        individual_copy = Individual(genome_copy, model_copy, optimizer_copy, history_copy, self.age)\n",
    "        return individual_copy\n",
    "    \n",
    "    def mutate(self, mutation_strength):\n",
    "        self.model.mutate(mutation_strength)\n",
    "        self.age = 0\n",
    "    \n",
    "    def correct(self):\n",
    "        self.genome[0] = max(self.genome[0], 2.5)  # Regularization penalty\n",
    "        self.genome[1] = np.clip(self.genome[1], 0.1, 1)  # Dataset sample size\n",
    "    \n",
    "    def get_val_metric(self):\n",
    "        return self.history['val_metric'][-1]\n",
    "    \n",
    "    def get_age(self):\n",
    "        return len(self.history['val_metric'])\n",
    "    \n",
    "    def get_age_penalty_coefficient(self, age_penalty_period):\n",
    "        age = self.get_age()\n",
    "        return 1 / (2 ** max(0, (age - age_penalty_period) / age_penalty_period))\n",
    "    \n",
    "    def get_fitness(self, age_penalty_period):\n",
    "        if age_penalty_period is None:\n",
    "            return self.get_val_metric()\n",
    "        return self.get_val_metric() * self.get_age_penalty_coefficient(age_penalty_period)\n",
    "    \n",
    "    def get_regularization_penalty(self):\n",
    "        return 10. ** -self.genome[0]\n",
    "    \n",
    "    def get_dataset_sample_size(self):\n",
    "        return self.genome[1]\n",
    "    \n",
    "    def get_hidden_layer_sizes(self):\n",
    "        return self.history['hidden_layer_sizes'][-1]\n",
    "\n",
    "class Evolution(AnytimeAlgorithm):\n",
    "    @staticmethod\n",
    "    def create_new_individual(x, layer_sizes, output_neurons, learning_rate):\n",
    "        genome = np.array([3, 0.1])\n",
    "        model = get_convolutional_model(x, layer_sizes, output_neurons=output_neurons)\n",
    "        model.build(x.shape)\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        individual = Individual(genome, model, optimizer)\n",
    "        return individual\n",
    "    \n",
    "    @staticmethod\n",
    "    def initialize_population(population_size, x, layer_sizes, output_neurons, learning_rate):\n",
    "        population = [Evolution.create_new_individual(x, layer_sizes, output_neurons, learning_rate) for _ in range(population_size)]\n",
    "        return population\n",
    "\n",
    "    @staticmethod\n",
    "    def introduce_new_individuals(population, n_introduced, x, layer_sizes, output_neurons, learning_rate):\n",
    "        introduced_individuals = [Evolution.create_new_individual(x, layer_sizes, output_neurons, learning_rate) for _ in range(n_introduced)]\n",
    "        return population + introduced_individuals\n",
    "\n",
    "    @staticmethod\n",
    "    def get_best_individual(population, age_penalty_period):\n",
    "        best_individual = None\n",
    "        best_fitness = - np.inf\n",
    "        for individual in population:\n",
    "            fitness = individual.get_fitness(age_penalty_period)\n",
    "            if fitness > best_fitness:\n",
    "                best_individual = individual\n",
    "                best_fitness = fitness\n",
    "        return best_individual\n",
    "\n",
    "    @staticmethod\n",
    "    def crossover(population, n_parents, strategy):\n",
    "        novel_population = list()\n",
    "        for individual in population:\n",
    "            parents_selection = np.random.choice(list(range(len(population))), size=n_parents, replace=False)\n",
    "            parent_genomes = [population[index].genome for index in parents_selection]\n",
    "            offspring_genome = np.mean(np.vstack(parent_genomes), axis=0)\n",
    "            offspring = individual.copy()\n",
    "            offspring.genome = offspring_genome\n",
    "            offspring.genome += np.random.normal(0, 1, offspring.genome.shape) * strategy\n",
    "            novel_population.append(offspring)\n",
    "        return population + novel_population\n",
    "\n",
    "    @staticmethod\n",
    "    def correct(population):\n",
    "        for individual in population:\n",
    "            individual.correct()\n",
    "        return population\n",
    "\n",
    "    # @staticmethod\n",
    "    # def mutation(population, mutation_strength):\n",
    "    #     new_population = list()\n",
    "    #     for individual in population:\n",
    "    #         individual_copy = individual.copy()\n",
    "    #         individual_copy.mutate(mutation_strength)\n",
    "    #         new_population.extend([individual, individual_copy])\n",
    "    #     return new_population\n",
    "\n",
    "    @staticmethod\n",
    "    def extend_history(old_history, new_history):\n",
    "        for key in old_history.keys():\n",
    "            old_history[key].extend(new_history[key])\n",
    "\n",
    "    @staticmethod\n",
    "    def training(population, x, y, validation_data, batch_size, min_new_neurons, growth_percentage, verbose, use_static_graph):\n",
    "        for individual in population:\n",
    "            model = individual.model\n",
    "            optimizer = individual.optimizer\n",
    "            # schedule = Schedule([StaticEpochNoRegularization()])\n",
    "            schedule = Schedule([DynamicEpoch(individual.get_regularization_penalty(), 'weighted_l1')])\n",
    "            # x_train_sample, y_train_sample = get_dataset_sample(x, y, individual.get_dataset_sample_size())\n",
    "    #         x_train_sample, y_train_sample = get_dataset_sample(x, y, 0.1)\n",
    "            # x_test_sample, y_test_sample = get_dataset_sample(validation_data[0], validation_data[1], individual.get_dataset_sample_size())\n",
    "    #         x_test_sample, y_test_sample = get_dataset_sample(validation_data[0], validation_data[1], 0.1, seed=42)\n",
    "            x_train_sample, y_train_sample = x, y\n",
    "            x_test_sample, y_test_sample = validation_data[0], validation_data[1]\n",
    "            history = model.fit(x=x_train_sample, y=y_train_sample, optimizer=optimizer, schedule=schedule, batch_size=batch_size, \n",
    "                                min_new_neurons=min_new_neurons, validation_data=(x_test_sample, y_test_sample), growth_percentage=growth_percentage, \n",
    "                                verbose=verbose, use_static_graph=use_static_graph)\n",
    "            Evolution.extend_history(individual.history, history)\n",
    "            individual.age += 1\n",
    "        return population\n",
    "\n",
    "    @staticmethod\n",
    "    def tournament_selection(population, population_size, tournament_size, age_penalty_period):\n",
    "        new_population = list()\n",
    "\n",
    "        while len(new_population) < population_size:\n",
    "            selection = np.random.choice(list(range(len(population))), size=tournament_size, replace=False)\n",
    "            best_individual = None\n",
    "            best_fitness = - np.inf\n",
    "            for individual_index in selection:\n",
    "                individual = population[individual_index]\n",
    "                fitness = individual.get_fitness(age_penalty_period)\n",
    "                if fitness > best_fitness:\n",
    "                    best_individual = individual\n",
    "                    best_fitness = fitness\n",
    "            new_population.append(best_individual.copy())\n",
    "\n",
    "        return new_population\n",
    "\n",
    "    @staticmethod\n",
    "    def measure_fitnesses(population, age_penalty_period):\n",
    "        fitnesses = list()\n",
    "        for individual in population:\n",
    "            fitnesses.append(individual.get_fitness(age_penalty_period))\n",
    "        return fitnesses\n",
    "\n",
    "    def print_generation_statistics(self, generation, population, duration, age_penalty_period):\n",
    "        population_sorted_by_fitness = sorted(population, key=lambda x: x.get_fitness(age_penalty_period), reverse=True)\n",
    "        individuals = [(individual.get_age(), round(individual.get_val_metric(), 4), round(individual.get_fitness(age_penalty_period), 4), round(individual.genome[0], 1), round(individual.genome[1], 2), individual.get_hidden_layer_sizes()) for individual in population_sorted_by_fitness]\n",
    "        population_sorted_by_val_metric = sorted(population, key=lambda x: x.get_val_metric(), reverse=True)\n",
    "        best_val_metric = population_sorted_by_val_metric[0].get_val_metric()\n",
    "        self.log_result(best_val_metric)\n",
    "        print(f\"Generation {generation}: {round(duration, 1)} s, best val metric {round(best_val_metric, 4)}, {individuals}\")\n",
    "\n",
    "    @interruptible\n",
    "    def run(self, x, y, validation_data, batch_size, layer_sizes, output_neurons, learning_rate, n_parents, strategy, population_size=10, n_generations=10, \n",
    "            tournament_size=3, elitism=True, n_introduced=0, age_penalty_period=None, min_new_neurons=20, growth_percentage=0.2, use_static_graph=True):\n",
    "        super().run()\n",
    "        \n",
    "        population = self.initialize_population(population_size, x, layer_sizes, output_neurons, learning_rate)\n",
    "        best_individual = None\n",
    "        fitnesses_history = list()\n",
    "        for generation in range(n_generations):\n",
    "            start_time = time.time()\n",
    "            population = self.crossover(population, n_parents, strategy)\n",
    "            # population = mutation(population, mutation_strength)\n",
    "            population = self.introduce_new_individuals(population, n_introduced, x, layer_sizes, output_neurons, learning_rate)\n",
    "            population = self.training(population, x, y, validation_data, batch_size, min_new_neurons, \n",
    "                                  growth_percentage, verbose=False, use_static_graph=use_static_graph)\n",
    "            population = self.tournament_selection(population, population_size, tournament_size, age_penalty_period)\n",
    "            if elitism:\n",
    "                if best_individual is not None:\n",
    "                    population.append(best_individual)\n",
    "                best_individual = self.get_best_individual(population, age_penalty_period).copy()\n",
    "            fitnesses = self.measure_fitnesses(population, age_penalty_period)\n",
    "            duration = time.time() - start_time\n",
    "            self.print_generation_statistics(generation, population, duration, age_penalty_period)\n",
    "            # fitnesses_history.append(fitnesses)\n",
    "\n",
    "        # best_individual = get_best_individual(population)\n",
    "        # fitness = best_individual.get_fitness()\n",
    "\n",
    "        # return fitness, fitnesses_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground for standalone models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = get_fashion_mnist_dataset(fraction=0.005)\n",
    "# fashion_mnist = get_fashion_mnist_dataset(fraction=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stats time: 0.04924893379211426\n",
      "pruning time: 0.09215831756591797\n",
      "stats time: 0.028137922286987305\n",
      "update history time: 0.0002963542938232422\n",
      "total pruning time: 0.17015743255615234\n",
      "2.089315176010132\n",
      "[20, 20, 20, 20, 20]\n",
      "Interrupted by user.\n"
     ]
    }
   ],
   "source": [
    "evolution(x=fashion_mnist.X_train_norm, y=fashion_mnist.y_train, validation_data=(fashion_mnist.X_test_norm, fashion_mnist.y_test),\n",
    "          batch_size=32, layer_sizes=[20, 20, 20, 20, 20], output_neurons=10, learning_rate=0.0004, n_parents=5, strategy=[0.5, 0.05], \n",
    "          population_size=10, n_generations=50, tournament_size=3, n_introduced=2, age_penalty_period=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 0: 22.9 s, [(1, 0.5263, 0.5263, 3.2, 0.17, [20, 20, 20, 20, 20]), (1, 0.4737, 0.4737, 3.0, 0.1, [20, 20, 20, 20, 20]), (1, 0.4737, 0.4737, 3.4, 0.11, [20, 20, 20, 20, 20]), (1, 0.4737, 0.4737, 3.4, 0.11, [20, 20, 20, 20, 20]), (1, 0.4737, 0.4737, 3.0, 0.1, [20, 20, 20, 20, 20]), (1, 0.4211, 0.4211, 3.0, 0.1, [20, 20, 20, 20, 20]), (1, 0.3947, 0.3947, 3.0, 0.1, [20, 20, 20, 20, 20]), (1, 0.3947, 0.3947, 3.0, 0.1, [20, 20, 20, 20, 20]), (1, 0.3947, 0.3947, 3.0, 0.1, [20, 20, 20, 20, 20]), (1, 0.3684, 0.3684, 2.8, 0.11, [20, 20, 20, 20, 20])]\n",
      "Generation 1: 27.5 s, [(2, 0.6053, 0.6053, 2.9, 0.1, [20, 20, 20, 20, 20]), (2, 0.6053, 0.6053, 2.9, 0.1, [20, 20, 20, 20, 20]), (2, 0.6053, 0.6053, 2.9, 0.1, [20, 20, 20, 20, 20]), (2, 0.5526, 0.5526, 3.6, 0.16, [20, 20, 20, 20, 20]), (2, 0.5526, 0.5526, 3.0, 0.1, [20, 20, 20, 20, 20]), (2, 0.5526, 0.5526, 3.0, 0.1, [20, 20, 20, 20, 20]), (2, 0.5526, 0.5526, 3.0, 0.1, [20, 20, 20, 20, 20]), (2, 0.5526, 0.5526, 3.5, -0.02, [20, 20, 20, 20, 20]), (2, 0.5263, 0.5263, 3.1, 0.04, [20, 20, 20, 20, 20]), (1, 0.5263, 0.5263, 3.2, 0.17, [20, 20, 20, 20, 20]), (2, 0.4737, 0.4737, 3.4, 0.11, [20, 20, 20, 20, 20])]\n",
      "Generation 2: 30.9 s, [(3, 0.7105, 0.7105, 2.9, 0.1, [20, 20, 20, 20, 20]), (3, 0.7105, 0.7105, 2.9, 0.1, [20, 20, 20, 20, 20]), (3, 0.7105, 0.7105, 3.0, 0.1, [20, 20, 20, 20, 20]), (3, 0.6842, 0.6842, 3.0, 0.0, [20, 20, 20, 20, 20]), (3, 0.6579, 0.6579, 2.9, 0.1, [20, 20, 20, 20, 20]), (3, 0.6579, 0.6579, 3.6, 0.06, [20, 20, 20, 20, 20]), (3, 0.6579, 0.6579, 3.6, 0.06, [20, 20, 20, 20, 20]), (3, 0.6579, 0.6579, 3.5, 0.11, [20, 20, 20, 20, 20]), (3, 0.6316, 0.6316, 2.8, 0.11, [20, 20, 20, 20, 20]), (3, 0.6053, 0.6053, 3.4, 0.11, [20, 20, 20, 20, 20]), (2, 0.6053, 0.6053, 2.9, 0.1, [20, 20, 20, 20, 20])]\n",
      "Generation 3: 33.6 s, [(4, 0.7105, 0.7105, 2.9, 0.1, [20, 20, 20, 20, 20]), (4, 0.7105, 0.7105, 4.0, 0.17, [20, 20, 20, 20, 20]), (3, 0.7105, 0.7105, 2.9, 0.1, [20, 20, 20, 20, 20]), (4, 0.6842, 0.6842, 3.6, 0.06, [20, 20, 20, 20, 20]), (4, 0.6842, 0.6842, 3.4, 0.01, [20, 20, 20, 20, 20]), (4, 0.6842, 0.6842, 3.6, 0.06, [20, 20, 20, 20, 20]), (4, 0.6842, 0.6842, 2.9, 0.1, [20, 20, 20, 20, 20]), (4, 0.6842, 0.6842, 2.8, -0.01, [20, 20, 20, 20, 20]), (4, 0.6842, 0.6842, 3.0, 0.1, [20, 20, 20, 20, 20]), (4, 0.6842, 0.6842, 3.5, 0.11, [20, 20, 20, 20, 20]), (4, 0.6579, 0.6579, 3.4, 0.11, [20, 20, 20, 20, 20])]\n",
      "Generation 4: 36.4 s, [(5, 0.7632, 0.7632, 3.1, 0.12, [20, 20, 20, 20, 20]), (5, 0.7632, 0.7632, 3.1, 0.12, [20, 20, 20, 20, 20]), (5, 0.7632, 0.7632, 3.9, 0.09, [20, 20, 20, 20, 20]), (5, 0.7632, 0.7632, 3.4, 0.11, [20, 20, 20, 20, 20]), (5, 0.7368, 0.7368, 2.9, 0.15, [20, 20, 20, 20, 20]), (5, 0.7105, 0.7105, 2.8, 0.03, [20, 20, 20, 20, 20]), (4, 0.7105, 0.7105, 2.9, 0.1, [20, 20, 20, 20, 20]), (5, 0.6842, 0.6842, 2.8, -0.01, [20, 20, 20, 20, 20]), (4, 0.6842, 0.6842, 2.9, 0.1, [20, 20, 20, 20, 20]), (5, 0.6842, 0.6842, 2.8, -0.01, [20, 20, 20, 20, 20]), (4, 0.6842, 0.6842, 2.9, 0.1, [20, 20, 20, 20, 20])]\n",
      "Generation 5: 38.1 s, [(5, 0.8158, 0.8158, 2.8, 0.08, [20, 20, 20, 20, 20]), (5, 0.7632, 0.7632, 3.1, 0.12, [20, 20, 20, 20, 20]), (5, 0.7368, 0.7368, 3.3, 0.13, [20, 20, 20, 20, 20]), (6, 0.7368, 0.7368, 2.8, -0.01, [20, 20, 20, 20, 20]), (6, 0.7105, 0.7105, 2.9, 0.15, [20, 20, 20, 20, 20]), (6, 0.7105, 0.7105, 3.1, 0.11, [20, 20, 20, 20, 20]), (6, 0.7105, 0.7105, 3.4, 0.11, [20, 20, 20, 20, 20]), (6, 0.7105, 0.7105, 2.9, 0.15, [20, 20, 20, 20, 20]), (6, 0.7105, 0.7105, 3.4, 0.11, [20, 20, 20, 20, 20]), (6, 0.7105, 0.7105, 3.1, 0.12, [20, 20, 20, 20, 20]), (6, 0.7105, 0.7105, 3.1, 0.11, [20, 20, 20, 20, 20])]\n",
      "Generation 6: 41.5 s, [(7, 0.8158, 0.8158, 2.9, 0.15, [20, 20, 20, 20, 20]), (7, 0.8158, 0.8158, 2.9, 0.15, [20, 20, 20, 20, 20]), (5, 0.8158, 0.8158, 2.8, 0.08, [20, 20, 20, 20, 20]), (7, 0.7632, 0.7632, 2.8, -0.01, [20, 20, 20, 20, 20]), (7, 0.7632, 0.7632, 2.8, -0.01, [20, 20, 20, 20, 20]), (6, 0.7632, 0.7632, 3.1, 0.12, [20, 20, 20, 20, 20]), (7, 0.7632, 0.7632, 2.9, 0.15, [20, 20, 20, 20, 20]), (7, 0.7632, 0.7632, 2.8, -0.01, [20, 20, 20, 20, 20]), (7, 0.7368, 0.7368, 3.1, 0.11, [20, 20, 20, 20, 20]), (7, 0.7368, 0.7368, 3.4, 0.15, [20, 20, 20, 20, 20]), (7, 0.7368, 0.7368, 3.1, 0.14, [20, 20, 20, 20, 20])]\n",
      "Generation 7: 43.7 s, [(7, 0.8158, 0.8158, 2.9, 0.15, [20, 20, 20, 20, 20]), (8, 0.7895, 0.7895, 2.8, -0.01, [20, 20, 20, 20, 20]), (8, 0.7632, 0.7632, 2.9, 0.15, [20, 20, 20, 20, 20]), (8, 0.7632, 0.7632, 3.4, 0.15, [20, 20, 20, 20, 20]), (7, 0.7632, 0.7632, 3.1, 0.12, [20, 20, 20, 20, 20]), (8, 0.7632, 0.7632, 3.1, 0.14, [20, 20, 20, 20, 20]), (8, 0.7632, 0.7632, 2.8, 0.03, [20, 20, 20, 20, 20]), (8, 0.7632, 0.7632, 2.5, 0.18, [20, 20, 20, 20, 20]), (8, 0.7632, 0.7632, 3.1, 0.14, [20, 20, 20, 20, 20]), (7, 0.7632, 0.7632, 3.1, 0.12, [20, 20, 20, 20, 20]), (6, 0.7368, 0.7368, 2.8, 0.08, [20, 20, 20, 20, 20])]\n",
      "Generation 8: 45.7 s, [(8, 0.8158, 0.8158, 3.1, 0.03, [20, 20, 20, 20, 20]), (9, 0.8158, 0.8158, 2.8, -0.01, [20, 20, 20, 20, 20]), (8, 0.8158, 0.8158, 3.1, 0.03, [20, 20, 20, 20, 20]), (9, 0.8158, 0.8158, 2.8, -0.01, [20, 20, 20, 20, 20]), (9, 0.8158, 0.8158, 2.8, 0.07, [20, 20, 20, 20, 20]), (7, 0.8158, 0.8158, 2.9, 0.15, [20, 20, 20, 20, 20]), (9, 0.7895, 0.7895, 2.5, 0.18, [20, 20, 20, 20, 20]), (8, 0.7632, 0.7632, 2.8, 0.1, [20, 20, 20, 20, 20]), (8, 0.7632, 0.7632, 2.8, 0.1, [20, 20, 20, 20, 20]), (7, 0.7632, 0.7632, 2.8, 0.08, [20, 20, 20, 20, 20]), (9, 0.7632, 0.7632, 2.9, 0.13, [20, 20, 20, 20, 20])]\n",
      "Generation 9: 51.4 s, [(10, 0.8421, 0.8421, 2.5, 0.18, [20, 20, 20, 20, 20]), (9, 0.8158, 0.8158, 2.8, 0.1, [20, 20, 20, 20, 20]), (10, 0.8158, 0.8158, 2.9, 0.11, [20, 20, 20, 20, 20]), (10, 0.8158, 0.8158, 2.9, 0.11, [20, 20, 20, 20, 20]), (9, 0.8158, 0.8158, 2.8, 0.1, [20, 20, 20, 20, 20]), (9, 0.8158, 0.8158, 2.8, 0.1, [20, 20, 20, 20, 20]), (10, 0.8158, 0.8158, 2.9, 0.11, [20, 20, 20, 20, 20]), (8, 0.8158, 0.8158, 3.1, 0.03, [20, 20, 20, 20, 20]), (10, 0.7895, 0.7895, 3.5, 0.15, [20, 20, 20, 20, 20]), (10, 0.7895, 0.7895, 3.5, 0.15, [20, 20, 20, 20, 20]), (10, 0.7632, 0.7632, 3.2, 0.06, [20, 20, 20, 20, 20])]\n",
      "Interrupted by user.\n"
     ]
    }
   ],
   "source": [
    "evolution(x=fashion_mnist.X_train_norm, y=fashion_mnist.y_train, validation_data=(fashion_mnist.X_test_norm, fashion_mnist.y_test),\n",
    "          batch_size=32, layer_sizes=[20, 20, 20, 20, 20], output_neurons=10, learning_rate=0.0004, n_parents=5, strategy=[0.5, 0.05], \n",
    "          population_size=10, n_generations=50, tournament_size=3, n_introduced=2, age_penalty_period=10, use_static_graph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 0: 23.8 s, [(1, 0.4737, 0.4737, 3.1, 0.09, [20, 20, 20, 20, 20]), (1, 0.4474, 0.4474, 2.8, 0.07, [20, 20, 20, 20, 20]), (1, 0.4474, 0.4474, 3.0, 0.1, [20, 20, 20, 20, 20]), (1, 0.4474, 0.4474, 3.0, 0.1, [20, 20, 20, 20, 20]), (1, 0.4474, 0.4474, 2.8, 0.07, [20, 20, 20, 20, 20]), (1, 0.4211, 0.4211, 3.0, 0.1, [20, 20, 20, 20, 20]), (1, 0.3947, 0.3947, 2.6, -0.1, [20, 20, 20, 20, 20]), (1, 0.3947, 0.3947, 3.0, 0.1, [20, 20, 20, 20, 20]), (1, 0.2895, 0.2895, 3.0, 0.1, [20, 20, 20, 20, 20]), (1, 0.2895, 0.2895, 3.0, 0.1, [20, 20, 20, 20, 20])]\n",
      "Generation 1: 26.1 s, [(2, 0.5526, 0.5526, 3.0, 0.1, [20, 20, 20, 20, 20]), (1, 0.5526, 0.5526, 3.0, 0.1, [20, 20, 20, 20, 20]), (1, 0.5526, 0.5526, 3.0, 0.1, [20, 20, 20, 20, 20]), (2, 0.5263, 0.5263, 3.1, 0.09, [20, 20, 20, 20, 20]), (2, 0.5, 0.5, 2.8, 0.07, [20, 20, 20, 20, 20]), (2, 0.5, 0.5, 3.0, 0.1, [20, 20, 20, 20, 20]), (2, 0.5, 0.5, 3.0, 0.1, [20, 20, 20, 20, 20]), (2, 0.5, 0.5, 2.8, 0.05, [20, 20, 20, 20, 20]), (2, 0.4737, 0.4737, 3.0, 0.15, [20, 20, 20, 20, 20]), (2, 0.4737, 0.4737, 3.0, 0.1, [20, 20, 20, 20, 20]), (1, 0.4737, 0.4737, 3.1, 0.09, [20, 20, 20, 20, 20])]\n",
      "Generation 2: 29.9 s, [(3, 0.6053, 0.6053, 3.0, 0.17, [20, 20, 20, 20, 20]), (3, 0.6053, 0.6053, 3.0, 0.17, [20, 20, 20, 20, 20]), (3, 0.6053, 0.6053, 2.8, 0.05, [20, 20, 20, 20, 20]), (3, 0.6053, 0.6053, 3.0, 0.17, [20, 20, 20, 20, 20]), (3, 0.5789, 0.5789, 2.9, 0.18, [20, 20, 20, 20, 20]), (3, 0.5789, 0.5789, 2.9, 0.18, [20, 20, 20, 20, 20]), (3, 0.5789, 0.5789, 2.9, 0.18, [20, 20, 20, 20, 20]), (3, 0.5789, 0.5789, 3.3, 0.26, [20, 20, 20, 20, 20]), (3, 0.5789, 0.5789, 3.3, 0.26, [20, 20, 20, 20, 20]), (2, 0.5526, 0.5526, 3.0, 0.1, [20, 20, 20, 20, 20]), (3, 0.5, 0.5, 3.0, 0.1, [20, 20, 20, 20, 20])]\n",
      "Generation 3: 37.0 s, [(4, 0.6316, 0.6316, 2.7, 0.11, [20, 20, 20, 20, 20]), (4, 0.6316, 0.6316, 3.6, 0.26, [20, 20, 20, 20, 20]), (4, 0.6316, 0.6316, 2.7, 0.11, [20, 20, 20, 20, 20]), (4, 0.6316, 0.6316, 2.9, 0.18, [20, 20, 20, 20, 20]), (4, 0.6316, 0.6316, 2.9, 0.18, [20, 20, 20, 20, 20]), (4, 0.6316, 0.6316, 3.6, 0.26, [20, 20, 20, 20, 20]), (4, 0.6053, 0.6053, 3.0, 0.17, [20, 20, 20, 20, 20]), (4, 0.6053, 0.6053, 2.9, 0.18, [20, 20, 20, 20, 20]), (4, 0.6053, 0.6053, 3.9, 0.07, [20, 20, 20, 20, 20]), (3, 0.6053, 0.6053, 3.0, 0.17, [20, 20, 20, 20, 20]), (3, 0.5526, 0.5526, 3.0, 0.1, [20, 20, 20, 20, 20])]\n",
      "Generation 4: 40.9 s, [(5, 0.6842, 0.6842, 3.0, 0.17, [20, 20, 20, 20, 20]), (5, 0.6842, 0.6842, 2.7, 0.11, [20, 20, 20, 20, 20]), (5, 0.6842, 0.6842, 3.0, 0.17, [20, 20, 20, 20, 20]), (5, 0.6579, 0.6579, 2.9, 0.15, [20, 20, 20, 20, 20]), (5, 0.6579, 0.6579, 2.4, 0.13, [20, 20, 20, 20, 20]), (5, 0.6316, 0.6316, 4.1, 0.09, [20, 20, 20, 20, 20]), (5, 0.6316, 0.6316, 3.4, 0.14, [20, 20, 20, 20, 20]), (5, 0.6316, 0.6316, 2.0, 0.19, [20, 20, 20, 20, 20]), (5, 0.6316, 0.6316, 2.9, 0.18, [20, 20, 20, 20, 20]), (4, 0.6316, 0.6316, 2.7, 0.11, [20, 20, 20, 20, 20]), (5, 0.6053, 0.6053, 2.8, 0.24, [20, 20, 20, 20, 20])]\n",
      "Generation 5: 42.6 s, [(6, 0.7105, 0.7105, 3.4, 0.14, [20, 20, 20, 20, 20]), (6, 0.6842, 0.6842, 2.8, 0.24, [20, 20, 20, 20, 20]), (6, 0.6842, 0.6842, 2.8, 0.24, [20, 20, 20, 20, 20]), (6, 0.6842, 0.6842, 3.1, 0.12, [20, 20, 20, 20, 20]), (6, 0.6842, 0.6842, 3.4, 0.15, [20, 20, 20, 20, 20]), (5, 0.6842, 0.6842, 3.0, 0.17, [20, 20, 20, 20, 20]), (6, 0.6579, 0.6579, 3.2, 0.24, [20, 20, 20, 20, 20]), (6, 0.6579, 0.6579, 2.4, 0.13, [20, 20, 20, 20, 20]), (6, 0.6579, 0.6579, 2.5, 0.15, [20, 20, 20, 20, 20]), (6, 0.6316, 0.6316, 3.0, 0.17, [20, 20, 20, 20, 20]), (6, 0.6316, 0.6316, 2.8, 0.2, [20, 20, 20, 20, 20])]\n",
      "Generation 6: 45.2 s, [(7, 0.7105, 0.7105, 2.8, 0.2, [20, 20, 20, 20, 20]), (7, 0.7105, 0.7105, 3.4, 0.15, [20, 20, 20, 20, 20]), (6, 0.7105, 0.7105, 3.4, 0.14, [20, 20, 20, 20, 20]), (7, 0.6842, 0.6842, 3.5, 0.18, [20, 20, 20, 20, 20]), (6, 0.6842, 0.6842, 3.0, 0.17, [20, 20, 20, 20, 20]), (7, 0.6842, 0.6842, 2.8, 0.24, [20, 20, 20, 20, 20]), (7, 0.6842, 0.6842, 2.5, 0.15, [20, 20, 20, 20, 20]), (7, 0.6842, 0.6842, 3.4, 0.19, [20, 20, 20, 20, 20]), (7, 0.6842, 0.6842, 3.5, 0.18, [20, 20, 20, 20, 20]), (7, 0.6579, 0.6579, 3.4, 0.14, [20, 20, 20, 20, 20]), (7, 0.6579, 0.6579, 3.6, 0.2, [20, 20, 20, 20, 20])]\n",
      "Generation 7: 49.5 s, [(8, 0.7368, 0.7368, 3.0, 0.17, [20, 20, 20, 20, 20]), (8, 0.7368, 0.7368, 3.0, 0.17, [20, 20, 20, 20, 20]), (8, 0.7368, 0.7368, 3.4, 0.19, [20, 20, 20, 20, 20]), (8, 0.7368, 0.7368, 3.8, 0.17, [20, 20, 20, 20, 20]), (8, 0.7368, 0.7368, 3.0, 0.17, [20, 20, 20, 20, 20]), (7, 0.7105, 0.7105, 3.9, 0.18, [20, 20, 20, 20, 20]), (8, 0.7105, 0.7105, 3.4, 0.14, [20, 20, 20, 20, 20]), (8, 0.7105, 0.7105, 3.4, 0.14, [20, 20, 20, 20, 20]), (8, 0.7105, 0.7105, 3.4, 0.15, [20, 20, 20, 20, 20]), (7, 0.7105, 0.7105, 2.8, 0.2, [20, 20, 20, 20, 20]), (8, 0.6842, 0.6842, 3.5, 0.18, [20, 20, 20, 20, 20])]\n",
      "Generation 8: 50.6 s, [(9, 0.7632, 0.7632, 3.2, 0.23, [20, 20, 20, 20, 20]), (9, 0.7632, 0.7632, 3.5, 0.18, [20, 20, 20, 20, 20]), (9, 0.7368, 0.7368, 3.0, 0.17, [20, 20, 20, 20, 20]), (8, 0.7368, 0.7368, 3.0, 0.17, [20, 20, 20, 20, 20]), (9, 0.7105, 0.7105, 3.4, 0.17, [20, 20, 20, 20, 20]), (9, 0.7105, 0.7105, 3.0, 0.17, [20, 20, 20, 20, 20]), (9, 0.7105, 0.7105, 3.4, 0.17, [20, 20, 20, 20, 20]), (8, 0.6842, 0.6842, 3.4, 0.12, [20, 20, 20, 20, 20]), (8, 0.6842, 0.6842, 2.8, 0.2, [20, 20, 20, 20, 20]), (9, 0.6842, 0.6842, 4.2, 0.14, [20, 20, 20, 25, 20]), (8, 0.6579, 0.6579, 3.7, 0.19, [20, 20, 20, 20, 20])]\n",
      "Generation 9: 53.9 s, [(9, 0.7895, 0.7895, 3.4, 0.12, [20, 20, 20, 20, 20]), (10, 0.7632, 0.7632, 3.0, 0.17, [20, 20, 20, 20, 20]), (9, 0.7632, 0.7632, 3.2, 0.23, [20, 20, 20, 20, 20]), (9, 0.7368, 0.7368, 3.6, 0.04, [20, 20, 20, 20, 20]), (9, 0.7368, 0.7368, 3.6, 0.04, [20, 20, 20, 20, 20]), (10, 0.7368, 0.7368, 2.4, 0.22, [20, 20, 20, 20, 20]), (9, 0.7368, 0.7368, 3.0, 0.17, [20, 20, 20, 20, 20]), (9, 0.7368, 0.7368, 3.0, 0.17, [20, 20, 20, 20, 20]), (9, 0.7368, 0.7368, 3.0, 0.17, [20, 20, 20, 20, 20]), (10, 0.7368, 0.7368, 2.4, 0.22, [20, 20, 20, 20, 20]), (10, 0.7105, 0.7105, 3.2, 0.23, [20, 20, 20, 20, 20])]\n",
      "Generation 10: 55.9 s, [(9, 0.7895, 0.7895, 3.4, 0.12, [20, 20, 20, 20, 20]), (10, 0.7632, 0.7632, 3.0, 0.17, [20, 20, 20, 20, 20]), (10, 0.7632, 0.7632, 3.6, 0.04, [20, 20, 20, 20, 20]), (10, 0.7632, 0.7632, 3.0, 0.17, [20, 20, 20, 20, 20]), (10, 0.7632, 0.7632, 3.0, 0.17, [20, 20, 20, 20, 20]), (10, 0.7368, 0.7368, 3.7, 0.24, [20, 20, 20, 20, 20]), (10, 0.7368, 0.7368, 3.7, 0.24, [20, 20, 20, 20, 20]), (11, 0.7632, 0.7121, 3.0, 0.19, [20, 20, 20, 20, 20]), (10, 0.7105, 0.7105, 3.1, 0.22, [20, 20, 20, 20, 20]), (10, 0.7105, 0.7105, 3.3, 0.15, [20, 20, 20, 20, 20]), (10, 0.7105, 0.7105, 3.1, 0.22, [20, 20, 20, 20, 20])]\n",
      "Generation 11: 59.3 s, [(9, 0.7895, 0.7895, 3.4, 0.12, [20, 20, 20, 20, 20]), (10, 0.7368, 0.7368, 2.9, 0.13, [20, 20, 20, 20, 20]), (10, 0.7368, 0.7368, 2.9, 0.13, [20, 20, 20, 20, 20]), (11, 0.7632, 0.7121, 3.2, 0.24, [20, 20, 20, 20, 20]), (11, 0.7632, 0.7121, 3.0, 0.17, [20, 20, 20, 20, 20]), (11, 0.7632, 0.7121, 3.2, 0.24, [20, 20, 20, 20, 20]), (11, 0.7632, 0.7121, 3.2, 0.24, [20, 20, 20, 20, 20]), (11, 0.7632, 0.7121, 3.0, 0.21, [20, 20, 20, 20, 20]), (11, 0.7632, 0.7121, 3.2, 0.24, [20, 20, 20, 20, 20]), (11, 0.7368, 0.6875, 2.6, 0.18, [20, 20, 20, 20, 20]), (11, 0.7105, 0.6629, 3.1, 0.22, [20, 20, 20, 20, 20])]\n",
      "Generation 12: 62.8 s, [(9, 0.7895, 0.7895, 3.4, 0.12, [20, 20, 20, 20, 20]), (10, 0.7632, 0.7632, 3.2, 0.17, [20, 20, 20, 20, 20]), (11, 0.7632, 0.7121, 2.6, 0.22, [20, 20, 20, 20, 20]), (11, 0.7632, 0.7121, 2.6, 0.22, [20, 20, 20, 20, 20]), (11, 0.7632, 0.7121, 2.6, 0.22, [20, 20, 20, 20, 20]), (12, 0.7895, 0.6873, 3.2, 0.24, [20, 20, 20, 20, 20]), (12, 0.7895, 0.6873, 3.7, 0.18, [20, 20, 20, 20, 20]), (12, 0.7895, 0.6873, 3.7, 0.18, [20, 20, 20, 20, 20]), (11, 0.7105, 0.6629, 2.9, 0.13, [20, 20, 20, 20, 20]), (11, 0.7105, 0.6629, 2.9, 0.13, [20, 20, 20, 20, 20]), (11, 0.7105, 0.6629, 2.9, 0.13, [20, 20, 20, 20, 20])]\n",
      "Generation 13: 66.7 s, [(9, 0.7895, 0.7895, 3.4, 0.12, [20, 20, 20, 20, 20]), (10, 0.7368, 0.7368, 2.3, 0.22, [20, 20, 20, 20, 20]), (10, 0.7368, 0.7368, 2.3, 0.22, [20, 20, 20, 20, 20]), (11, 0.7895, 0.7366, 3.2, 0.17, [20, 20, 20, 20, 20]), (11, 0.7895, 0.7366, 3.2, 0.17, [20, 20, 20, 20, 20]), (10, 0.7105, 0.7105, 3.4, 0.12, [20, 20, 20, 20, 20]), (11, 0.7368, 0.6875, 3.6, 0.19, [20, 20, 20, 20, 20]), (12, 0.7368, 0.6415, 3.6, 0.18, [20, 20, 20, 20, 20]), (13, 0.7895, 0.6413, 2.8, 0.07, [20, 20, 20, 20, 20]), (12, 0.7105, 0.6185, 3.7, 0.17, [20, 20, 20, 20, 20]), (13, 0.7368, 0.5985, 3.7, 0.18, [20, 20, 20, 20, 20])]\n",
      "Interrupted by user.\n"
     ]
    }
   ],
   "source": [
    "evolution(x=fashion_mnist.X_train_norm, y=fashion_mnist.y_train, validation_data=(fashion_mnist.X_test_norm, fashion_mnist.y_test),\n",
    "          batch_size=32, layer_sizes=[20, 20, 20, 20, 20], output_neurons=10, learning_rate=0.0004, n_parents=5, strategy=[0.5, 0.05], \n",
    "          population_size=10, n_generations=50, tournament_size=3, n_introduced=2, age_penalty_period=10, use_static_graph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 0: 12.4 s, [(1, 0.5263, 0.5263, 3.0, 0.1, [20, 20, 20, 20, 20]), (1, 0.5263, 0.5263, 3.0, 0.1, [20, 20, 20, 20, 20]), (1, 0.5263, 0.5263, 3.0, 0.1, [20, 20, 20, 20, 20]), (1, 0.4737, 0.4737, 3.1, 0.07, [20, 20, 20, 20, 20]), (1, 0.4474, 0.4474, 3.8, 0.07, [20, 20, 25, 30, 20]), (1, 0.3947, 0.3947, 2.7, 0.14, [20, 20, 20, 20, 20]), (1, 0.3947, 0.3947, 3.0, 0.1, [20, 20, 20, 20, 20]), (1, 0.3947, 0.3947, 3.0, 0.1, [20, 20, 20, 20, 20]), (1, 0.3684, 0.3684, 3.0, 0.1, [20, 20, 20, 20, 20]), (1, 0.2895, 0.2895, 3.0, 0.1, [20, 20, 20, 20, 20])]\n",
      "Generation 1: 14.0 s, [(2, 0.6053, 0.6053, 3.0, 0.1, [20, 20, 20, 20, 20]), (2, 0.6053, 0.6053, 3.0, 0.1, [20, 20, 20, 20, 20]), (2, 0.5789, 0.5789, 3.5, 0.1, [20, 20, 20, 20, 20]), (2, 0.5789, 0.5789, 3.3, 0.05, [20, 20, 20, 20, 20]), (2, 0.5789, 0.5789, 3.5, 0.1, [20, 20, 20, 20, 20]), (2, 0.5526, 0.5526, 2.9, 0.13, [20, 20, 20, 20, 20]), (2, 0.5263, 0.5263, 3.0, 0.1, [20, 20, 20, 20, 20]), (2, 0.5263, 0.5263, 3.0, 0.1, [20, 20, 20, 20, 20]), (2, 0.5263, 0.5263, 3.0, 0.1, [20, 20, 20, 20, 20]), (1, 0.5263, 0.5263, 3.0, 0.1, [20, 20, 20, 20, 20]), (2, 0.3947, 0.3947, 2.7, 0.14, [20, 20, 20, 20, 20])]\n",
      "Generation 2: 16.2 s, [(3, 0.6842, 0.6842, 3.5, 0.1, [20, 20, 20, 20, 20]), (3, 0.6842, 0.6842, 3.5, 0.1, [20, 20, 20, 20, 20]), (3, 0.6842, 0.6842, 3.5, 0.1, [20, 20, 20, 20, 20]), (3, 0.6579, 0.6579, 3.5, 0.1, [20, 20, 20, 20, 20]), (2, 0.6316, 0.6316, 3.0, 0.1, [20, 20, 20, 20, 20]), (3, 0.6316, 0.6316, 3.8, 0.14, [20, 20, 20, 20, 20]), (3, 0.6316, 0.6316, 3.8, 0.14, [20, 20, 20, 20, 20]), (3, 0.6053, 0.6053, 3.0, 0.1, [20, 20, 20, 20, 20]), (3, 0.6053, 0.6053, 3.0, 0.02, [20, 20, 20, 20, 20]), (2, 0.6053, 0.6053, 3.0, 0.1, [20, 20, 20, 20, 20]), (3, 0.5789, 0.5789, 3.0, 0.1, [20, 20, 20, 20, 20])]\n",
      "Generation 3: 17.6 s, [(4, 0.6842, 0.6842, 1.9, 0.07, [20, 20, 20, 20, 20]), (4, 0.6842, 0.6842, 3.4, 0.05, [20, 20, 20, 20, 20]), (4, 0.6842, 0.6842, 1.9, 0.07, [20, 20, 20, 20, 20]), (4, 0.6842, 0.6842, 3.4, 0.05, [20, 20, 20, 20, 20]), (3, 0.6842, 0.6842, 3.5, 0.1, [20, 20, 20, 20, 20]), (4, 0.6579, 0.6579, 3.0, 0.14, [20, 20, 20, 20, 20]), (3, 0.6579, 0.6579, 3.0, 0.1, [20, 20, 20, 20, 20]), (4, 0.6316, 0.6316, 3.5, 0.1, [20, 20, 20, 20, 20]), (3, 0.6316, 0.6316, 3.6, 0.15, [20, 20, 20, 20, 20]), (3, 0.6316, 0.6316, 3.6, 0.15, [20, 20, 20, 20, 20]), (3, 0.6316, 0.6316, 4.1, 0.17, [20, 20, 20, 20, 20])]\n",
      "Generation 4: 19.0 s, [(5, 0.7368, 0.7368, 3.5, 0.1, [20, 20, 20, 20, 20]), (5, 0.7368, 0.7368, 3.5, 0.1, [20, 20, 20, 20, 20]), (5, 0.6842, 0.6842, 2.4, 0.12, [20, 20, 20, 20, 20]), (5, 0.6842, 0.6842, 3.6, 0.16, [20, 20, 20, 20, 20]), (5, 0.6842, 0.6842, 3.6, 0.16, [20, 20, 20, 20, 20]), (5, 0.6842, 0.6842, 2.4, 0.12, [20, 20, 20, 20, 20]), (4, 0.6842, 0.6842, 1.9, 0.07, [20, 20, 20, 20, 20]), (5, 0.6579, 0.6579, 1.9, 0.07, [20, 20, 20, 20, 20]), (4, 0.6579, 0.6579, 3.0, 0.1, [20, 20, 20, 20, 20]), (5, 0.6316, 0.6316, 3.0, 0.14, [20, 20, 20, 20, 20]), (5, 0.6316, 0.6316, 3.1, 0.18, [20, 20, 20, 20, 20])]\n",
      "Generation 5: 20.2 s, [(6, 0.7368, 0.7368, 2.7, 0.14, [20, 20, 20, 20, 20]), (6, 0.7368, 0.7368, 3.6, 0.16, [20, 20, 20, 20, 20]), (6, 0.7368, 0.7368, 3.9, 0.33, [20, 20, 20, 20, 20]), (6, 0.7368, 0.7368, 3.5, 0.1, [20, 20, 20, 20, 20]), (6, 0.7368, 0.7368, 3.6, 0.16, [20, 20, 20, 20, 20]), (5, 0.7368, 0.7368, 3.5, 0.1, [20, 20, 20, 20, 20]), (6, 0.7105, 0.7105, 3.0, 0.14, [20, 20, 20, 20, 20]), (6, 0.7105, 0.7105, 3.3, 0.05, [20, 20, 20, 20, 20]), (6, 0.7105, 0.7105, 3.3, 0.05, [20, 20, 20, 20, 20]), (5, 0.6842, 0.6842, 3.0, 0.1, [20, 20, 20, 20, 20]), (5, 0.6842, 0.6842, 3.0, 0.1, [20, 20, 20, 20, 20])]\n",
      "Generation 6: 21.7 s, [(6, 0.7632, 0.7632, 3.5, 0.1, [20, 20, 20, 20, 20]), (6, 0.7632, 0.7632, 3.5, 0.1, [20, 20, 20, 20, 20]), (7, 0.7632, 0.7632, 3.5, 0.1, [20, 20, 20, 20, 20]), (6, 0.7368, 0.7368, 2.8, 0.04, [20, 20, 20, 20, 20]), (7, 0.7368, 0.7368, 2.6, 0.15, [20, 20, 20, 20, 20]), (7, 0.7368, 0.7368, 4.2, 0.16, [20, 20, 20, 23, 20]), (7, 0.7368, 0.7368, 4.2, 0.16, [20, 20, 20, 23, 20]), (6, 0.7368, 0.7368, 2.8, 0.04, [20, 20, 20, 20, 20]), (7, 0.7368, 0.7368, 4.2, 0.16, [20, 20, 20, 23, 20]), (7, 0.7368, 0.7368, 3.9, 0.33, [20, 20, 20, 20, 20]), (6, 0.7368, 0.7368, 2.7, 0.14, [20, 20, 20, 20, 20])]\n",
      "Generation 7: 22.8 s, [(8, 0.7895, 0.7895, 3.9, 0.33, [20, 20, 20, 20, 20]), (8, 0.7895, 0.7895, 2.6, 0.15, [20, 20, 20, 20, 20]), (7, 0.7895, 0.7895, 3.5, 0.1, [20, 20, 20, 20, 20]), (8, 0.7895, 0.7895, 3.9, 0.33, [20, 20, 20, 20, 20]), (7, 0.7895, 0.7895, 3.4, 0.19, [20, 20, 20, 20, 20]), (8, 0.7895, 0.7895, 2.6, 0.15, [20, 20, 20, 20, 20]), (8, 0.7895, 0.7895, 2.6, 0.15, [20, 20, 20, 20, 20]), (8, 0.7895, 0.7895, 3.9, 0.33, [20, 20, 20, 20, 20]), (8, 0.7632, 0.7632, 3.6, 0.08, [20, 20, 20, 20, 20]), (6, 0.7632, 0.7632, 3.5, 0.1, [20, 20, 20, 20, 20]), (8, 0.7368, 0.7368, 3.3, 0.1, [20, 20, 20, 20, 20])]\n",
      "Generation 8: 24.4 s, [(9, 0.8158, 0.8158, 2.6, 0.15, [20, 20, 20, 20, 20]), (9, 0.8158, 0.8158, 3.9, 0.33, [20, 20, 20, 20, 20]), (9, 0.7895, 0.7895, 3.0, 0.11, [20, 20, 20, 20, 20]), (8, 0.7895, 0.7895, 3.4, 0.19, [20, 20, 20, 20, 20]), (8, 0.7895, 0.7895, 2.0, 0.2, [20, 20, 20, 20, 20]), (9, 0.7895, 0.7895, 3.0, 0.11, [20, 20, 20, 20, 20]), (8, 0.7895, 0.7895, 3.4, 0.19, [20, 20, 20, 20, 20]), (9, 0.7895, 0.7895, 2.6, 0.15, [20, 20, 20, 20, 20]), (8, 0.7895, 0.7895, 3.9, 0.33, [20, 20, 20, 20, 20]), (8, 0.7632, 0.7632, 4.1, 0.24, [20, 20, 20, 28, 20]), (9, 0.7368, 0.7368, 3.6, 0.08, [20, 20, 20, 20, 20])]\n",
      "Generation 9: 25.3 s, [(10, 0.8158, 0.8158, 1.8, 0.23, [20, 20, 20, 20, 20]), (10, 0.8158, 0.8158, 3.0, 0.11, [20, 20, 20, 20, 20]), (9, 0.8158, 0.8158, 2.6, 0.15, [20, 20, 20, 20, 20]), (10, 0.7895, 0.7895, 3.0, 0.16, [20, 20, 20, 20, 20]), (10, 0.7895, 0.7895, 3.0, 0.11, [20, 20, 20, 20, 20]), (10, 0.7895, 0.7895, 3.0, 0.11, [20, 20, 20, 20, 20]), (9, 0.7895, 0.7895, 3.8, 0.18, [20, 20, 20, 20, 20]), (9, 0.7895, 0.7895, 3.8, 0.18, [20, 20, 20, 20, 20]), (10, 0.7632, 0.7632, 3.9, 0.33, [20, 20, 20, 20, 20]), (10, 0.7632, 0.7632, 4.1, 0.23, [20, 20, 20, 25, 20]), (9, 0.7632, 0.7632, 3.2, 0.13, [20, 20, 20, 20, 20])]\n",
      "Generation 10: 27.7 s, [(10, 0.8158, 0.8158, 3.2, 0.13, [20, 20, 20, 20, 20]), (10, 0.8158, 0.8158, 1.8, 0.23, [20, 20, 20, 20, 20]), (10, 0.7895, 0.7895, 3.8, 0.18, [20, 20, 20, 20, 20]), (11, 0.8421, 0.7857, 1.8, 0.23, [20, 20, 20, 20, 20]), (11, 0.8421, 0.7857, 1.8, 0.23, [20, 20, 20, 20, 20]), (10, 0.7368, 0.7368, 3.8, 0.18, [20, 20, 20, 20, 20]), (10, 0.7368, 0.7368, 3.7, 0.11, [20, 20, 20, 20, 20]), (10, 0.7368, 0.7368, 3.7, 0.11, [20, 20, 20, 20, 20]), (10, 0.7368, 0.7368, 2.6, 0.15, [20, 20, 20, 20, 20]), (11, 0.7632, 0.7121, 3.0, 0.11, [20, 20, 20, 20, 20]), (11, 0.7632, 0.7121, 2.7, 0.08, [20, 20, 20, 20, 20])]\n",
      "Generation 11: 28.9 s, [(10, 0.8158, 0.8158, 3.2, 0.13, [20, 20, 20, 20, 20]), (11, 0.8421, 0.7857, 1.8, 0.23, [20, 20, 20, 20, 20]), (11, 0.8421, 0.7857, 2.6, 0.12, [20, 20, 20, 20, 20]), (11, 0.8158, 0.7612, 2.6, 0.15, [20, 20, 20, 20, 20]), (11, 0.8158, 0.7612, 2.6, 0.15, [20, 20, 20, 20, 20]), (11, 0.7895, 0.7366, 2.3, 0.18, [20, 20, 20, 20, 20]), (11, 0.7895, 0.7366, 2.3, 0.18, [20, 20, 20, 20, 20]), (11, 0.7895, 0.7366, 3.8, 0.18, [20, 20, 20, 20, 20]), (11, 0.7895, 0.7366, 2.9, 0.2, [20, 20, 20, 20, 20]), (12, 0.8421, 0.7331, 1.8, 0.23, [20, 20, 20, 20, 20]), (11, 0.7368, 0.6875, 3.7, 0.11, [20, 20, 20, 20, 20])]\n",
      "Generation 12: 30.1 s, [(10, 0.8158, 0.8158, 3.2, 0.13, [20, 20, 20, 20, 20]), (11, 0.8158, 0.7612, 3.2, 0.13, [20, 20, 20, 20, 20]), (12, 0.8421, 0.7331, 2.5, 0.19, [20, 20, 20, 20, 20]), (12, 0.8421, 0.7331, 2.5, 0.19, [20, 20, 20, 20, 20]), (12, 0.8421, 0.7331, 2.5, 0.19, [20, 20, 20, 20, 20]), (12, 0.8158, 0.7102, 1.8, 0.23, [20, 20, 20, 20, 20]), (12, 0.8158, 0.7102, 2.6, 0.2, [20, 20, 20, 20, 20]), (12, 0.8158, 0.7102, 2.3, 0.18, [20, 20, 20, 20, 20]), (12, 0.8158, 0.7102, 2.6, 0.2, [20, 20, 20, 20, 20]), (12, 0.8158, 0.7102, 2.6, 0.2, [20, 20, 20, 20, 20]), (12, 0.7895, 0.6873, 3.8, 0.18, [20, 20, 20, 20, 20])]\n",
      "Generation 13: 32.8 s, [(10, 0.8158, 0.8158, 3.2, 0.13, [20, 20, 20, 20, 20]), (11, 0.7895, 0.7366, 2.7, 0.19, [20, 20, 20, 20, 20]), (13, 0.8421, 0.684, 2.5, 0.19, [20, 20, 20, 20, 20]), (13, 0.8421, 0.684, 2.4, 0.16, [20, 20, 20, 20, 20]), (11, 0.7105, 0.6629, 3.2, 0.13, [20, 20, 20, 20, 20]), (13, 0.8158, 0.6626, 2.5, 0.19, [20, 20, 20, 20, 20]), (13, 0.8158, 0.6626, 2.5, 0.19, [20, 20, 20, 20, 20]), (13, 0.8158, 0.6626, 2.0, 0.23, [20, 20, 20, 20, 20]), (13, 0.7632, 0.6199, 2.7, 0.13, [20, 20, 20, 20, 20]), (13, 0.7632, 0.6199, 2.6, 0.2, [20, 20, 20, 20, 20]), (13, 0.7632, 0.6199, 1.8, 0.23, [20, 20, 20, 20, 20])]\n",
      "Generation 14: 32.5 s, [(10, 0.8158, 0.8158, 3.2, 0.13, [20, 20, 20, 20, 20]), (11, 0.7895, 0.7366, 2.5, 0.16, [20, 20, 20, 20, 20]), (11, 0.7632, 0.7121, 3.2, 0.13, [20, 20, 20, 20, 20]), (12, 0.8158, 0.7102, 3.2, 0.13, [20, 20, 20, 20, 20]), (12, 0.8158, 0.7102, 1.9, 0.14, [20, 20, 20, 20, 20]), (12, 0.8158, 0.7102, 1.9, 0.14, [20, 20, 20, 20, 20]), (12, 0.7632, 0.6644, 2.7, 0.19, [20, 20, 20, 20, 20]), (12, 0.7632, 0.6644, 2.7, 0.19, [20, 20, 20, 20, 20]), (12, 0.7632, 0.6644, 2.7, 0.19, [20, 20, 20, 20, 20]), (14, 0.8158, 0.6183, 2.5, 0.14, [20, 20, 20, 20, 20]), (14, 0.7895, 0.5983, 1.1, 0.17, [20, 20, 20, 20, 20])]\n",
      "Generation 15: 34.1 s, [(10, 0.8158, 0.8158, 3.2, 0.13, [20, 20, 20, 20, 20]), (11, 0.8158, 0.7612, 3.2, 0.13, [20, 20, 20, 20, 20]), (11, 0.7632, 0.7121, 2.0, 0.15, [20, 20, 20, 20, 20]), (11, 0.7632, 0.7121, 2.0, 0.15, [20, 20, 20, 20, 20]), (12, 0.7895, 0.6873, 3.2, 0.13, [20, 20, 20, 20, 20]), (13, 0.8421, 0.684, 1.9, 0.21, [20, 20, 20, 20, 20]), (13, 0.8421, 0.684, 1.9, 0.21, [20, 20, 20, 20, 20]), (12, 0.7632, 0.6644, 2.9, 0.15, [20, 20, 20, 20, 20]), (13, 0.7895, 0.6413, 1.9, 0.14, [20, 20, 20, 20, 20]), (13, 0.7895, 0.6413, 2.7, 0.19, [20, 20, 20, 20, 20]), (13, 0.7632, 0.6199, 2.5, 0.1, [20, 20, 20, 20, 20])]\n",
      "Generation 16: 35.0 s, [(10, 0.8158, 0.8158, 3.2, 0.13, [20, 20, 20, 20, 20]), (11, 0.8421, 0.7857, 3.0, 0.13, [20, 20, 20, 20, 20]), (11, 0.7895, 0.7366, 3.2, 0.13, [20, 20, 20, 20, 20]), (11, 0.7895, 0.7366, 3.2, 0.13, [20, 20, 20, 20, 20]), (12, 0.8421, 0.7331, 2.2, 0.21, [20, 20, 20, 20, 20]), (12, 0.8421, 0.7331, 2.2, 0.21, [20, 20, 20, 20, 20]), (12, 0.8421, 0.7331, 3.2, 0.13, [20, 20, 20, 20, 20]), (12, 0.8421, 0.7331, 2.2, 0.21, [20, 20, 20, 20, 20]), (12, 0.8158, 0.7102, 2.0, 0.15, [20, 20, 20, 20, 20]), (12, 0.8158, 0.7102, 2.5, 0.13, [20, 20, 20, 20, 20]), (13, 0.7632, 0.6199, 2.8, 0.13, [20, 20, 20, 20, 20])]\n",
      "Generation 17: 36.1 s, [(10, 0.8158, 0.8158, 3.2, 0.13, [20, 20, 20, 20, 20]), (11, 0.7895, 0.7366, 2.8, 0.13, [20, 20, 20, 20, 20]), (11, 0.7632, 0.7121, 3.2, 0.13, [20, 20, 20, 20, 20]), (12, 0.7895, 0.6873, 3.0, 0.13, [20, 20, 20, 20, 20]), (12, 0.7895, 0.6873, 2.0, 0.11, [20, 20, 20, 20, 20]), (13, 0.8421, 0.684, 3.7, 0.21, [20, 20, 20, 20, 20]), (12, 0.7632, 0.6644, 3.0, 0.16, [20, 20, 20, 20, 20]), (12, 0.7632, 0.6644, 2.9, 0.18, [20, 20, 20, 20, 20]), (13, 0.8158, 0.6626, 2.1, 0.21, [20, 20, 20, 20, 20]), (13, 0.8158, 0.6626, 2.2, 0.21, [20, 20, 20, 20, 20]), (13, 0.8158, 0.6626, 2.5, 0.13, [20, 20, 20, 20, 20])]\n",
      "Interrupted by user.\n"
     ]
    }
   ],
   "source": [
    "evolution(x=fashion_mnist.X_train_norm, y=fashion_mnist.y_train, validation_data=(fashion_mnist.X_test_norm, fashion_mnist.y_test),\n",
    "          batch_size=32, layer_sizes=[20, 20, 20, 20, 20], output_neurons=10, learning_rate=0.0004, n_parents=5, strategy=[0.5, 0.05], \n",
    "          population_size=10, n_generations=50, tournament_size=3, n_introduced=2, age_penalty_period=10, use_static_graph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 0: 12.9 s, [(1, 0.4737, 0.4737, 3.8, 0.17, [20, 21, 21, 38, 20]), (1, 0.4737, 0.4737, 3.8, 0.17, [20, 21, 21, 38, 20]), (1, 0.4737, 0.4737, 3.8, 0.17, [20, 21, 21, 38, 20]), (1, 0.4737, 0.4737, 3.8, 0.17, [20, 21, 21, 38, 20]), (1, 0.4211, 0.4211, 3.0, 0.1, [20, 20, 20, 20, 20]), (1, 0.4211, 0.4211, 3.0, 0.1, [20, 20, 20, 20, 20]), (1, 0.4211, 0.4211, 3.0, 0.1, [20, 20, 20, 20, 20]), (1, 0.4211, 0.4211, 3.0, 0.1, [20, 20, 20, 20, 20]), (1, 0.3684, 0.3684, 2.8, 0.08, [20, 20, 20, 20, 20]), (1, 0.3421, 0.3421, 3.0, 0.1, [20, 20, 20, 20, 20])]\n",
      "Generation 1: 13.9 s, [(2, 0.5789, 0.5789, 3.8, 0.17, [20, 20, 20, 38, 20]), (2, 0.5789, 0.5789, 3.2, 0.08, [20, 20, 20, 30, 20]), (2, 0.5526, 0.5526, 2.8, 0.08, [20, 20, 20, 20, 20]), (2, 0.5526, 0.5526, 2.8, 0.08, [20, 20, 20, 20, 20]), (2, 0.5526, 0.5526, 2.8, 0.08, [20, 20, 20, 20, 20]), (2, 0.5526, 0.5526, 2.6, 0.11, [20, 20, 20, 20, 20]), (2, 0.5526, 0.5526, 2.6, 0.11, [20, 20, 20, 20, 20]), (2, 0.5263, 0.5263, 3.4, 0.17, [20, 20, 20, 20, 20]), (2, 0.5263, 0.5263, 3.4, 0.17, [20, 20, 20, 20, 20]), (2, 0.4737, 0.4737, 4.1, 0.15, [20, 21, 21, 38, 20]), (1, 0.4737, 0.4737, 3.8, 0.17, [20, 21, 21, 38, 20])]\n",
      "Generation 2: 15.2 s, [(3, 0.6579, 0.6579, 3.4, 0.17, [20, 20, 20, 20, 20]), (3, 0.6579, 0.6579, 3.1, 0.17, [20, 20, 20, 20, 20]), (3, 0.6579, 0.6579, 2.8, 0.08, [20, 20, 20, 20, 20]), (3, 0.6579, 0.6579, 2.6, 0.11, [20, 20, 20, 20, 20]), (3, 0.6579, 0.6579, 2.8, 0.08, [20, 20, 20, 20, 20]), (3, 0.6579, 0.6579, 2.8, 0.08, [20, 20, 20, 20, 20]), (3, 0.6316, 0.6316, 2.6, 0.11, [20, 20, 20, 20, 20]), (2, 0.6316, 0.6316, 3.4, 0.13, [20, 20, 20, 35, 20]), (2, 0.5789, 0.5789, 3.8, 0.17, [20, 20, 20, 38, 20]), (3, 0.5526, 0.5526, 2.8, 0.08, [20, 20, 20, 20, 20]), (3, 0.5526, 0.5526, 3.3, 0.18, [20, 20, 20, 20, 20])]\n",
      "Generation 3: 14.6 s, [(4, 0.7368, 0.7368, 3.0, -0.03, [20, 20, 20, 20, 20]), (4, 0.7368, 0.7368, 3.4, 0.17, [20, 20, 20, 20, 20]), (4, 0.7105, 0.7105, 2.8, 0.08, [20, 20, 20, 20, 20]), (4, 0.6842, 0.6842, 3.8, 0.08, [20, 20, 20, 20, 20]), (4, 0.6579, 0.6579, 3.2, 0.08, [20, 20, 20, 20, 20]), (4, 0.6579, 0.6579, 2.8, 0.08, [20, 20, 20, 20, 20]), (4, 0.6579, 0.6579, 3.1, 0.17, [20, 20, 20, 20, 20]), (4, 0.6579, 0.6579, 3.1, 0.17, [20, 20, 20, 20, 20]), (3, 0.6579, 0.6579, 3.4, 0.17, [20, 20, 20, 20, 20]), (4, 0.6316, 0.6316, 2.9, 0.1, [20, 20, 20, 20, 20]), (3, 0.6316, 0.6316, 3.4, 0.13, [20, 20, 20, 35, 20])]\n",
      "Generation 4: 14.6 s, [(5, 0.7368, 0.7368, 3.0, -0.03, [20, 20, 20, 20, 20]), (5, 0.7368, 0.7368, 3.0, -0.03, [20, 20, 20, 20, 20]), (4, 0.7368, 0.7368, 3.0, -0.03, [20, 20, 20, 20, 20]), (5, 0.7105, 0.7105, 3.8, 0.06, [20, 20, 20, 20, 20]), (5, 0.7105, 0.7105, 2.7, 0.11, [20, 20, 20, 20, 20]), (5, 0.7105, 0.7105, 2.7, 0.11, [20, 20, 20, 20, 20]), (5, 0.6842, 0.6842, 2.9, 0.1, [20, 20, 20, 20, 20]), (5, 0.6842, 0.6842, 4.0, 0.21, [20, 20, 20, 20, 20]), (5, 0.6842, 0.6842, 3.1, 0.17, [20, 20, 20, 20, 20]), (5, 0.6842, 0.6842, 4.0, 0.21, [20, 20, 20, 20, 20]), (5, 0.6579, 0.6579, 3.4, 0.06, [20, 20, 20, 20, 20])]\n",
      "Generation 5: 15.1 s, [(6, 0.7368, 0.7368, 3.4, 0.06, [20, 20, 20, 20, 20]), (6, 0.7368, 0.7368, 2.4, 0.01, [20, 20, 20, 20, 20]), (6, 0.7368, 0.7368, 3.4, 0.06, [20, 20, 20, 20, 20]), (5, 0.7368, 0.7368, 3.0, -0.03, [20, 20, 20, 20, 20]), (6, 0.7105, 0.7105, 2.6, 0.01, [20, 20, 20, 20, 20]), (6, 0.7105, 0.7105, 2.6, 0.01, [20, 20, 20, 20, 20]), (6, 0.7105, 0.7105, 4.0, 0.21, [20, 20, 20, 20, 20]), (6, 0.6842, 0.6842, 2.9, 0.2, [20, 20, 20, 20, 20]), (6, 0.6842, 0.6842, 2.9, 0.1, [20, 20, 20, 20, 20]), (6, 0.6842, 0.6842, 3.0, -0.03, [20, 20, 20, 20, 20]), (5, 0.6842, 0.6842, 3.3, 0.11, [20, 20, 20, 20, 20])]\n",
      "Generation 6: 14.8 s, [(7, 0.7895, 0.7895, 2.9, 0.2, [20, 20, 20, 20, 20]), (7, 0.7895, 0.7895, 2.9, 0.2, [20, 20, 20, 20, 20]), (7, 0.7632, 0.7632, 3.7, 0.04, [20, 20, 20, 20, 20]), (7, 0.7632, 0.7632, 3.5, 0.15, [20, 20, 20, 20, 20]), (7, 0.7632, 0.7632, 3.2, 0.11, [20, 20, 20, 20, 20]), (7, 0.7632, 0.7632, 3.5, 0.15, [20, 20, 20, 20, 20]), (6, 0.7368, 0.7368, 3.4, 0.06, [20, 20, 20, 20, 20]), (6, 0.7105, 0.7105, 3.0, -0.01, [20, 20, 20, 20, 20]), (7, 0.7105, 0.7105, 3.0, 0.07, [20, 20, 20, 20, 20]), (7, 0.7105, 0.7105, 3.0, 0.07, [20, 20, 20, 20, 20]), (7, 0.6842, 0.6842, 2.6, 0.01, [20, 20, 20, 20, 20])]\n",
      "Generation 7: 15.5 s, [(8, 0.7895, 0.7895, 2.4, 0.01, [20, 20, 20, 20, 20]), (8, 0.7895, 0.7895, 2.4, 0.01, [20, 20, 20, 20, 20]), (8, 0.7895, 0.7895, 2.6, 0.07, [20, 20, 20, 20, 20]), (7, 0.7895, 0.7895, 2.9, 0.2, [20, 20, 20, 20, 20]), (8, 0.7632, 0.7632, 3.5, 0.15, [20, 20, 20, 20, 20]), (8, 0.7632, 0.7632, 3.5, 0.15, [20, 20, 20, 20, 20]), (8, 0.7632, 0.7632, 2.9, 0.2, [20, 20, 20, 20, 20]), (8, 0.7368, 0.7368, 3.0, 0.07, [20, 20, 20, 20, 20]), (8, 0.7368, 0.7368, 3.5, 0.15, [20, 20, 20, 20, 20]), (8, 0.7368, 0.7368, 3.0, 0.07, [20, 20, 20, 20, 20]), (7, 0.7105, 0.7105, 3.0, -0.01, [20, 20, 20, 20, 20])]\n",
      "Generation 8: 15.5 s, [(9, 0.8158, 0.8158, 3.0, 0.07, [20, 20, 20, 20, 20]), (9, 0.8158, 0.8158, 3.0, 0.07, [20, 20, 20, 20, 20]), (9, 0.8158, 0.8158, 3.0, 0.07, [20, 20, 20, 20, 20]), (9, 0.8158, 0.8158, 3.0, 0.07, [20, 20, 20, 20, 20]), (9, 0.8158, 0.8158, 3.8, 0.12, [20, 20, 20, 20, 20]), (9, 0.7895, 0.7895, 2.8, 0.15, [20, 20, 20, 20, 20]), (8, 0.7895, 0.7895, 2.4, 0.01, [20, 20, 20, 20, 20]), (9, 0.7632, 0.7632, 2.4, 0.01, [20, 20, 20, 20, 20]), (9, 0.7632, 0.7632, 3.5, 0.15, [20, 20, 20, 20, 20]), (9, 0.7368, 0.7368, 2.4, 0.01, [20, 20, 20, 20, 20]), (9, 0.6842, 0.6842, 2.3, 0.04, [20, 20, 20, 20, 20])]\n",
      "Generation 9: 15.7 s, [(9, 0.8158, 0.8158, 3.0, 0.07, [20, 20, 20, 20, 20]), (10, 0.7895, 0.7895, 2.4, 0.01, [20, 20, 20, 20, 20]), (10, 0.7895, 0.7895, 3.5, -0.03, [20, 20, 20, 20, 20]), (10, 0.7895, 0.7895, 3.5, 0.15, [20, 20, 20, 20, 20]), (10, 0.7895, 0.7895, 3.5, 0.15, [20, 20, 20, 20, 20]), (10, 0.7632, 0.7632, 3.8, 0.12, [20, 20, 20, 20, 20]), (10, 0.7632, 0.7632, 2.8, 0.15, [20, 20, 20, 20, 20]), (10, 0.7632, 0.7632, 2.8, 0.15, [20, 20, 20, 20, 20]), (10, 0.7632, 0.7632, 2.8, 0.15, [20, 20, 20, 20, 20]), (10, 0.7105, 0.7105, 3.4, 0.05, [20, 20, 20, 20, 20]), (10, 0.7105, 0.7105, 3.0, 0.07, [20, 20, 20, 20, 20])]\n",
      "Generation 10: 15.9 s, [(9, 0.8158, 0.8158, 3.0, 0.07, [20, 20, 20, 20, 20]), (10, 0.7895, 0.7895, 3.3, 0.2, [20, 20, 20, 20, 20]), (11, 0.8158, 0.7612, 3.8, 0.12, [20, 20, 20, 22, 20]), (11, 0.8158, 0.7612, 2.5, 0.01, [20, 20, 20, 20, 20]), (11, 0.8158, 0.7612, 3.0, 0.08, [20, 20, 20, 20, 20]), (11, 0.8158, 0.7612, 2.5, 0.01, [20, 20, 20, 20, 20]), (11, 0.8158, 0.7612, 3.0, 0.08, [20, 20, 20, 20, 20]), (11, 0.7895, 0.7366, 3.5, -0.03, [20, 20, 20, 20, 20]), (11, 0.7895, 0.7366, 3.5, -0.03, [20, 20, 20, 20, 20]), (11, 0.7895, 0.7366, 3.5, -0.03, [20, 20, 20, 20, 20]), (11, 0.7368, 0.6875, 2.9, 0.11, [20, 20, 20, 20, 20])]\n",
      "Generation 11: 15.9 s, [(9, 0.8158, 0.8158, 3.0, 0.07, [20, 20, 20, 20, 20]), (10, 0.7632, 0.7632, 2.5, 0.1, [20, 20, 20, 20, 20]), (10, 0.7368, 0.7368, 3.0, 0.07, [20, 20, 20, 20, 20]), (10, 0.7368, 0.7368, 3.0, 0.07, [20, 20, 20, 20, 20]), (10, 0.7368, 0.7368, 3.0, 0.07, [20, 20, 20, 20, 20]), (12, 0.8158, 0.7102, 3.5, -0.03, [20, 20, 20, 20, 20]), (12, 0.7895, 0.6873, 3.0, 0.08, [20, 20, 20, 20, 20]), (12, 0.7895, 0.6873, 2.2, 0.01, [20, 20, 20, 20, 20]), (12, 0.7895, 0.6873, 2.2, 0.01, [20, 20, 20, 20, 20]), (12, 0.7895, 0.6873, 2.2, 0.01, [20, 20, 20, 20, 20]), (12, 0.7632, 0.6644, 3.5, -0.03, [20, 20, 20, 20, 20])]\n",
      "Generation 12: 15.8 s, [(9, 0.8158, 0.8158, 3.0, 0.07, [20, 20, 20, 20, 20]), (10, 0.7632, 0.7632, 2.9, -0.06, [20, 20, 20, 20, 20]), (10, 0.7632, 0.7632, 2.9, -0.06, [20, 20, 20, 20, 20]), (11, 0.7895, 0.7366, 3.0, 0.07, [20, 20, 20, 20, 20]), (11, 0.7895, 0.7366, 3.0, 0.07, [20, 20, 20, 20, 20]), (10, 0.7105, 0.7105, 3.0, 0.07, [20, 20, 20, 20, 20]), (11, 0.7368, 0.6875, 2.9, 0.06, [20, 20, 20, 20, 20]), (11, 0.7368, 0.6875, 2.8, 0.0, [20, 20, 20, 20, 20]), (11, 0.7368, 0.6875, 2.9, 0.06, [20, 20, 20, 20, 20]), (11, 0.7368, 0.6875, 2.9, 0.06, [20, 20, 20, 20, 20]), (13, 0.7895, 0.6413, 2.8, -0.05, [20, 20, 20, 20, 20])]\n",
      "Generation 13: 15.6 s, [(9, 0.8158, 0.8158, 3.0, 0.07, [20, 20, 20, 20, 20]), (10, 0.7895, 0.7895, 3.0, 0.07, [20, 20, 20, 20, 20]), (10, 0.7632, 0.7632, 2.8, 0.09, [20, 20, 20, 20, 20]), (11, 0.7632, 0.7121, 3.0, -0.01, [20, 20, 20, 20, 20]), (12, 0.8158, 0.7102, 2.3, -0.01, [20, 20, 20, 20, 20]), (11, 0.7368, 0.6875, 2.9, -0.06, [20, 20, 20, 20, 20]), (11, 0.7368, 0.6875, 3.0, 0.05, [20, 20, 20, 20, 20]), (11, 0.7368, 0.6875, 2.7, 0.11, [20, 20, 20, 20, 20]), (12, 0.7632, 0.6644, 3.0, 0.07, [20, 20, 20, 20, 20]), (12, 0.7632, 0.6644, 3.3, -0.05, [20, 20, 20, 20, 20]), (14, 0.7895, 0.5983, 2.8, 0.05, [20, 20, 20, 20, 20])]\n",
      "Generation 14: 16.0 s, [(9, 0.8158, 0.8158, 3.0, 0.07, [20, 20, 20, 20, 20]), (10, 0.7632, 0.7632, 2.3, 0.04, [20, 20, 20, 20, 20]), (10, 0.7632, 0.7632, 2.3, 0.04, [20, 20, 20, 20, 20]), (11, 0.7632, 0.7121, 3.1, 0.05, [20, 20, 20, 20, 20]), (11, 0.7632, 0.7121, 3.8, 0.02, [20, 20, 20, 20, 20]), (12, 0.7895, 0.6873, 2.7, 0.11, [20, 20, 20, 20, 20]), (10, 0.6842, 0.6842, 3.0, 0.07, [20, 20, 20, 20, 20]), (12, 0.7632, 0.6644, 3.8, 0.16, [20, 20, 20, 20, 20]), (13, 0.7895, 0.6413, 3.3, -0.05, [20, 20, 20, 20, 20]), (13, 0.7632, 0.6199, 2.5, -0.06, [20, 20, 20, 20, 20]), (13, 0.7368, 0.5985, 2.8, 0.02, [20, 20, 20, 20, 20])]\n",
      "Generation 15: 15.9 s, [(9, 0.8158, 0.8158, 3.0, 0.07, [20, 20, 20, 20, 20]), (11, 0.7632, 0.7121, 3.0, 0.03, [20, 20, 20, 20, 20]), (11, 0.7632, 0.7121, 3.0, 0.03, [20, 20, 20, 20, 20]), (10, 0.7105, 0.7105, 2.1, 0.01, [20, 20, 20, 20, 20]), (11, 0.7368, 0.6875, 4.3, 0.03, [20, 20, 20, 33, 21]), (11, 0.7368, 0.6875, 4.3, 0.03, [20, 20, 20, 33, 21]), (11, 0.7368, 0.6875, 4.3, 0.03, [20, 20, 20, 33, 21]), (11, 0.7368, 0.6875, 3.0, 0.07, [20, 20, 20, 20, 20]), (12, 0.7895, 0.6873, 2.5, 0.1, [20, 20, 20, 20, 20]), (12, 0.7895, 0.6873, 3.8, 0.02, [20, 20, 20, 20, 20]), (11, 0.6842, 0.6384, 2.3, 0.04, [20, 20, 20, 20, 20])]\n",
      "Interrupted by user.\n"
     ]
    }
   ],
   "source": [
    "evolution(x=fashion_mnist.X_train_norm, y=fashion_mnist.y_train, validation_data=(fashion_mnist.X_test_norm, fashion_mnist.y_test),\n",
    "          batch_size=32, layer_sizes=[20, 20, 20, 20, 20], output_neurons=10, learning_rate=0.0004, n_parents=5, strategy=[0.5, 0.05], \n",
    "          population_size=10, n_generations=50, tournament_size=3, n_introduced=2, age_penalty_period=10, use_static_graph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 0: 12.9 s, best val metric 0.4737, [(1, 0.4737, 0.4737, 3.0, 0.1, [20, 20, 20, 20, 20]), (1, 0.4737, 0.4737, 3.4, 0.16, [20, 20, 20, 20, 20]), (1, 0.4737, 0.4737, 3.4, 0.16, [20, 20, 20, 20, 20]), (1, 0.3947, 0.3947, 3.0, 0.1, [20, 20, 20, 20, 20]), (1, 0.3947, 0.3947, 3.0, 0.1, [20, 20, 20, 20, 20]), (1, 0.3947, 0.3947, 3.0, 0.1, [20, 20, 20, 20, 20]), (1, 0.3684, 0.3684, 3.0, 0.1, [20, 20, 20, 20, 20]), (1, 0.3684, 0.3684, 3.0, 0.1, [20, 20, 20, 20, 20]), (1, 0.3421, 0.3421, 3.3, 0.15, [20, 20, 20, 20, 20]), (1, 0.3421, 0.3421, 3.3, 0.12, [20, 20, 20, 20, 20])]\n",
      "Generation 1: 13.4 s, best val metric 0.5789, [(2, 0.5789, 0.5789, 4.0, 0.05, [20, 20, 20, 20, 20]), (2, 0.5526, 0.5526, 2.9, 0.22, [20, 20, 20, 20, 20]), (2, 0.5526, 0.5526, 3.4, 0.16, [20, 20, 20, 20, 20]), (2, 0.5526, 0.5526, 2.9, 0.22, [20, 20, 20, 20, 20]), (2, 0.5526, 0.5526, 3.0, 0.1, [20, 20, 20, 20, 20]), (2, 0.5526, 0.5526, 2.9, 0.22, [20, 20, 20, 20, 20]), (2, 0.5526, 0.5526, 3.4, 0.16, [20, 20, 20, 20, 20]), (2, 0.5, 0.5, 2.2, 0.1, [20, 20, 20, 20, 20]), (2, 0.4737, 0.4737, 3.3, 0.12, [20, 20, 20, 20, 20]), (1, 0.4737, 0.4737, 3.0, 0.1, [20, 20, 20, 20, 20]), (2, 0.4211, 0.4211, 3.3, 0.15, [20, 20, 20, 20, 20])]\n",
      "Generation 2: 14.1 s, best val metric 0.6053, [(2, 0.6053, 0.6053, 3.0, 0.1, [20, 20, 20, 20, 20]), (2, 0.6053, 0.6053, 3.0, 0.1, [20, 20, 20, 20, 20]), (3, 0.5789, 0.5789, 2.9, 0.22, [20, 20, 20, 20, 20]), (2, 0.5789, 0.5789, 4.0, 0.05, [20, 20, 20, 20, 20]), (3, 0.5526, 0.5526, 3.3, 0.18, [20, 20, 20, 20, 20]), (3, 0.5526, 0.5526, 3.4, 0.13, [20, 20, 20, 20, 20]), (3, 0.5526, 0.5526, 3.8, 0.21, [20, 20, 20, 20, 20]), (3, 0.5526, 0.5526, 3.6, 0.25, [20, 20, 20, 20, 20]), (3, 0.5526, 0.5526, 3.6, 0.25, [20, 20, 20, 20, 20]), (3, 0.5526, 0.5526, 3.6, 0.25, [20, 20, 20, 20, 20]), (3, 0.5263, 0.5263, 3.0, 0.1, [20, 20, 20, 20, 20])]\n",
      "Generation 3: 14.4 s, best val metric 0.7105, [(4, 0.7105, 0.7105, 3.8, 0.21, [20, 20, 20, 20, 20]), (4, 0.6842, 0.6842, 4.2, 0.16, [20, 20, 20, 25, 20]), (4, 0.6579, 0.6579, 3.5, 0.18, [20, 20, 20, 20, 20]), (4, 0.6579, 0.6579, 3.5, 0.18, [20, 20, 20, 20, 20]), (4, 0.6579, 0.6579, 3.5, 0.18, [20, 20, 20, 20, 20]), (4, 0.6316, 0.6316, 3.4, 0.2, [20, 20, 20, 20, 20]), (4, 0.6053, 0.6053, 3.1, 0.15, [20, 20, 20, 20, 20]), (3, 0.6053, 0.6053, 3.1, 0.24, [20, 20, 20, 20, 20]), (4, 0.6053, 0.6053, 2.9, 0.22, [20, 20, 20, 20, 20]), (3, 0.6053, 0.6053, 3.0, 0.19, [20, 20, 20, 20, 20]), (2, 0.6053, 0.6053, 3.0, 0.1, [20, 20, 20, 20, 20])]\n",
      "Generation 4: 14.8 s, best val metric 0.7368, [(5, 0.7368, 0.7368, 3.6, 0.13, [20, 20, 20, 20, 20]), (5, 0.7368, 0.7368, 3.6, 0.13, [20, 20, 20, 20, 20]), (5, 0.7105, 0.7105, 3.4, 0.26, [20, 20, 20, 20, 20]), (5, 0.7105, 0.7105, 2.9, 0.15, [20, 20, 20, 20, 20]), (5, 0.7105, 0.7105, 3.5, 0.18, [20, 20, 20, 20, 20]), (5, 0.7105, 0.7105, 4.2, 0.16, [20, 20, 20, 26, 20]), (5, 0.7105, 0.7105, 3.5, 0.18, [20, 20, 20, 20, 20]), (4, 0.7105, 0.7105, 3.8, 0.21, [20, 20, 20, 20, 20]), (5, 0.6842, 0.6842, 3.1, 0.23, [20, 20, 20, 20, 20]), (5, 0.6579, 0.6579, 3.1, 0.15, [20, 20, 20, 20, 20]), (5, 0.6316, 0.6316, 2.9, 0.21, [20, 20, 20, 20, 20])]\n",
      "Generation 5: 15.3 s, best val metric 0.7368, [(6, 0.7368, 0.7368, 3.6, 0.13, [20, 20, 20, 20, 20]), (5, 0.7368, 0.7368, 3.6, 0.13, [20, 20, 20, 20, 20]), (6, 0.6842, 0.6842, 2.9, 0.2, [20, 20, 20, 20, 20]), (6, 0.6842, 0.6842, 2.9, 0.13, [20, 20, 20, 20, 20]), (6, 0.6842, 0.6842, 3.5, 0.18, [20, 20, 20, 20, 20]), (6, 0.6842, 0.6842, 4.2, 0.16, [20, 20, 20, 27, 20]), (6, 0.6842, 0.6842, 3.1, 0.23, [20, 20, 20, 20, 20]), (6, 0.6842, 0.6842, 3.0, 0.17, [20, 20, 20, 22, 20]), (6, 0.6842, 0.6842, 3.6, 0.07, [20, 20, 20, 20, 20]), (6, 0.6842, 0.6842, 2.9, 0.13, [20, 20, 20, 20, 20]), (6, 0.6579, 0.6579, 3.9, 0.22, [20, 20, 20, 20, 20])]\n",
      "Generation 6: 15.2 s, best val metric 0.7368, [(7, 0.7368, 0.7368, 3.6, 0.12, [20, 20, 20, 20, 20]), (7, 0.7368, 0.7368, 3.1, 0.23, [20, 20, 20, 20, 20]), (7, 0.7368, 0.7368, 3.1, 0.23, [20, 20, 20, 20, 20]), (7, 0.7368, 0.7368, 4.2, 0.16, [20, 20, 20, 40, 20]), (7, 0.7368, 0.7368, 3.6, 0.12, [20, 20, 20, 20, 20]), (7, 0.7368, 0.7368, 2.7, 0.17, [20, 20, 20, 20, 20]), (7, 0.7368, 0.7368, 3.6, 0.07, [20, 20, 20, 20, 20]), (6, 0.7368, 0.7368, 3.6, 0.13, [20, 20, 20, 20, 20]), (7, 0.7105, 0.7105, 3.1, 0.17, [20, 20, 20, 25, 20]), (7, 0.7105, 0.7105, 3.9, 0.22, [20, 20, 20, 20, 20]), (7, 0.6842, 0.6842, 3.1, 0.17, [20, 20, 20, 20, 20])]\n",
      "Generation 7: 15.1 s, best val metric 0.7895, [(8, 0.7895, 0.7895, 4.0, 0.05, [20, 20, 20, 38, 20]), (8, 0.7895, 0.7895, 4.0, 0.05, [20, 20, 20, 38, 20]), (8, 0.7895, 0.7895, 4.0, 0.05, [20, 20, 20, 38, 20]), (8, 0.7895, 0.7895, 3.3, 0.15, [20, 20, 20, 21, 20]), (8, 0.7895, 0.7895, 3.3, 0.15, [20, 20, 20, 21, 20]), (8, 0.7632, 0.7632, 3.7, 0.16, [20, 20, 20, 20, 20]), (8, 0.7368, 0.7368, 3.9, 0.22, [20, 20, 20, 20, 20]), (8, 0.7368, 0.7368, 3.9, 0.1, [20, 20, 20, 20, 20]), (8, 0.7368, 0.7368, 3.4, 0.18, [20, 20, 20, 20, 20]), (7, 0.7368, 0.7368, 3.6, 0.12, [20, 20, 20, 20, 20]), (8, 0.7105, 0.7105, 3.4, 0.15, [20, 20, 20, 20, 20])]\n",
      "Interrupted by user.\n"
     ]
    }
   ],
   "source": [
    "evolution = Evolution()\n",
    "evolution.run(x=fashion_mnist.X_train_norm, y=fashion_mnist.y_train, validation_data=(fashion_mnist.X_test_norm, fashion_mnist.y_test),\n",
    "              batch_size=32, layer_sizes=[20, 20, 20, 20, 20], output_neurons=10, learning_rate=0.0004, n_parents=5, strategy=[0.5, 0.05], \n",
    "              population_size=10, n_generations=50, tournament_size=3, n_introduced=2, age_penalty_period=10, use_static_graph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc531473430>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 244,
       "width": 371
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = [0] + [x[0] for x in evolution.val_metrics]\n",
    "y = [0] + [x[1] for x in evolution.val_metrics]\n",
    "plt.step(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_convolutional_model(fashion_mnist.X_train_norm, [20, 20, 20, 20, 20], output_neurons=10)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################################################\n",
      "Epoch 1/1\n",
      "Before growing:\n",
      "loss: None - metric: None - val_loss: 2.8670735359191895 - val_metric: 0.05263157894736842 - penalty: 0.01\n",
      "hidden layer sizes: [20, 20, 20, 20, 20], total units: 100\n",
      "After growing:\n",
      "loss: None - metric: None - val_loss: 2.8670742511749268 - val_metric: 0.05263157894736842 - penalty: 0.01\n",
      "hidden layer sizes: [40, 40, 40, 40, 40], total units: 200\n",
      "losses: 6\n",
      "losses: 6\n",
      "losses: 6\n",
      "Before pruning:\n",
      "loss: 2.855534791946411 - metric: 0.10175438225269318 - val_loss: 2.181877374649048 - val_metric: 0.15789473684210525 - penalty: 0.01\n",
      "hidden layer sizes: [40, 40, 40, 40, 40], total units: 200\n",
      "After pruning:\n",
      "loss: None - metric: None - val_loss: 2.1818511486053467 - val_metric: 0.15789473684210525 - penalty: 0.01\n",
      "hidden layer sizes: [20, 20, 20, 20, 20], total units: 100\n",
      "1.674736738204956\n",
      "##########################################################\n",
      "Epoch 1/1\n",
      "Before growing:\n",
      "loss: None - metric: None - val_loss: 2.1818511486053467 - val_metric: 0.15789473684210525 - penalty: 0.01\n",
      "hidden layer sizes: [20, 20, 20, 20, 20], total units: 100\n",
      "After growing:\n",
      "loss: None - metric: None - val_loss: 2.1818509101867676 - val_metric: 0.15789473684210525 - penalty: 0.01\n",
      "hidden layer sizes: [40, 40, 40, 40, 40], total units: 200\n",
      "losses: 6\n",
      "losses: 6\n",
      "losses: 6\n",
      "Before pruning:\n",
      "loss: 2.527128219604492 - metric: 0.15438596904277802 - val_loss: 1.9077192544937134 - val_metric: 0.2894736842105263 - penalty: 0.01\n",
      "hidden layer sizes: [40, 40, 40, 40, 40], total units: 200\n",
      "After pruning:\n",
      "loss: None - metric: None - val_loss: 1.9077236652374268 - val_metric: 0.2894736842105263 - penalty: 0.01\n",
      "hidden layer sizes: [20, 20, 20, 20, 20], total units: 100\n",
      "1.2735157012939453\n",
      "##########################################################\n",
      "Epoch 1/1\n",
      "Before growing:\n",
      "loss: None - metric: None - val_loss: 1.9077236652374268 - val_metric: 0.2894736842105263 - penalty: 0.01\n",
      "hidden layer sizes: [20, 20, 20, 20, 20], total units: 100\n",
      "After growing:\n",
      "loss: None - metric: None - val_loss: 1.9077236652374268 - val_metric: 0.2894736842105263 - penalty: 0.01\n",
      "hidden layer sizes: [40, 40, 40, 40, 40], total units: 200\n",
      "losses: 6\n",
      "losses: 6\n",
      "losses: 6\n",
      "Before pruning:\n",
      "loss: 2.3401598930358887 - metric: 0.15789473056793213 - val_loss: 1.7814252376556396 - val_metric: 0.39473684210526316 - penalty: 0.01\n",
      "hidden layer sizes: [40, 40, 40, 40, 40], total units: 200\n",
      "After pruning:\n",
      "loss: None - metric: None - val_loss: 1.7814143896102905 - val_metric: 0.39473684210526316 - penalty: 0.01\n",
      "hidden layer sizes: [20, 20, 20, 20, 20], total units: 100\n",
      "1.164116382598877\n",
      "##########################################################\n",
      "Epoch 1/1\n",
      "Before growing:\n",
      "loss: None - metric: None - val_loss: 1.7814143896102905 - val_metric: 0.39473684210526316 - penalty: 0.01\n",
      "hidden layer sizes: [20, 20, 20, 20, 20], total units: 100\n",
      "After growing:\n",
      "loss: None - metric: None - val_loss: 1.781414270401001 - val_metric: 0.39473684210526316 - penalty: 0.01\n",
      "hidden layer sizes: [40, 40, 40, 40, 40], total units: 200\n",
      "losses: 6\n",
      "losses: 6\n",
      "losses: 6\n",
      "Before pruning:\n",
      "loss: 2.0183157920837402 - metric: 0.27719298005104065 - val_loss: 1.6985992193222046 - val_metric: 0.47368421052631576 - penalty: 0.01\n",
      "hidden layer sizes: [40, 40, 40, 40, 40], total units: 200\n",
      "After pruning:\n",
      "loss: None - metric: None - val_loss: 1.6986045837402344 - val_metric: 0.47368421052631576 - penalty: 0.01\n",
      "hidden layer sizes: [20, 20, 20, 20, 20], total units: 100\n",
      "1.1845152378082275\n",
      "##########################################################\n",
      "Epoch 1/1\n",
      "Before growing:\n",
      "loss: None - metric: None - val_loss: 1.6986045837402344 - val_metric: 0.47368421052631576 - penalty: 0.01\n",
      "hidden layer sizes: [20, 20, 20, 20, 20], total units: 100\n",
      "After growing:\n",
      "loss: None - metric: None - val_loss: 1.6986043453216553 - val_metric: 0.47368421052631576 - penalty: 0.01\n",
      "hidden layer sizes: [40, 40, 40, 40, 40], total units: 200\n",
      "losses: 6\n",
      "losses: 6\n",
      "losses: 6\n",
      "Before pruning:\n",
      "loss: 1.8457173109054565 - metric: 0.2912280559539795 - val_loss: 1.5904316902160645 - val_metric: 0.5 - penalty: 0.01\n",
      "hidden layer sizes: [40, 40, 40, 40, 40], total units: 200\n",
      "After pruning:\n",
      "loss: None - metric: None - val_loss: 1.5904513597488403 - val_metric: 0.5 - penalty: 0.01\n",
      "hidden layer sizes: [20, 20, 20, 20, 20], total units: 100\n",
      "1.1621110439300537\n",
      "##########################################################\n",
      "Epoch 1/1\n",
      "Before growing:\n",
      "loss: None - metric: None - val_loss: 1.5904513597488403 - val_metric: 0.5 - penalty: 0.01\n",
      "hidden layer sizes: [20, 20, 20, 20, 20], total units: 100\n",
      "After growing:\n",
      "loss: None - metric: None - val_loss: 1.5904518365859985 - val_metric: 0.5 - penalty: 0.01\n",
      "hidden layer sizes: [40, 40, 40, 40, 40], total units: 200\n",
      "losses: 6\n",
      "losses: 6\n",
      "losses: 6\n",
      "Before pruning:\n",
      "loss: 1.8168267011642456 - metric: 0.3263157904148102 - val_loss: 1.5638577938079834 - val_metric: 0.5263157894736842 - penalty: 0.01\n",
      "hidden layer sizes: [40, 40, 40, 40, 40], total units: 200\n",
      "After pruning:\n",
      "loss: None - metric: None - val_loss: 1.5638601779937744 - val_metric: 0.5263157894736842 - penalty: 0.01\n",
      "hidden layer sizes: [20, 20, 20, 20, 20], total units: 100\n",
      "1.1616668701171875\n",
      "##########################################################\n",
      "Epoch 1/1\n",
      "Before growing:\n",
      "loss: None - metric: None - val_loss: 1.5638601779937744 - val_metric: 0.5263157894736842 - penalty: 0.01\n",
      "hidden layer sizes: [20, 20, 20, 20, 20], total units: 100\n",
      "After growing:\n",
      "loss: None - metric: None - val_loss: 1.5638597011566162 - val_metric: 0.5263157894736842 - penalty: 0.01\n",
      "hidden layer sizes: [40, 40, 40, 40, 40], total units: 200\n",
      "losses: 6\n",
      "losses: 6\n",
      "losses: 6\n",
      "Before pruning:\n",
      "loss: 1.7761001586914062 - metric: 0.34385964274406433 - val_loss: 1.5737823247909546 - val_metric: 0.5789473684210527 - penalty: 0.01\n",
      "hidden layer sizes: [40, 40, 40, 40, 40], total units: 200\n",
      "After pruning:\n",
      "loss: None - metric: None - val_loss: 1.5737930536270142 - val_metric: 0.5789473684210527 - penalty: 0.01\n",
      "hidden layer sizes: [20, 20, 20, 20, 20], total units: 100\n",
      "1.400970458984375\n",
      "##########################################################\n",
      "Epoch 1/1\n",
      "Before growing:\n",
      "loss: None - metric: None - val_loss: 1.5737930536270142 - val_metric: 0.5789473684210527 - penalty: 0.01\n",
      "hidden layer sizes: [20, 20, 20, 20, 20], total units: 100\n",
      "After growing:\n",
      "loss: None - metric: None - val_loss: 1.5737930536270142 - val_metric: 0.5789473684210527 - penalty: 0.01\n",
      "hidden layer sizes: [40, 40, 40, 40, 40], total units: 200\n",
      "losses: 6\n",
      "losses: 6\n",
      "losses: 6\n",
      "Before pruning:\n",
      "loss: 1.7161710262298584 - metric: 0.42105263471603394 - val_loss: 1.594118595123291 - val_metric: 0.6052631578947368 - penalty: 0.01\n",
      "hidden layer sizes: [40, 40, 40, 40, 40], total units: 200\n",
      "After pruning:\n",
      "loss: None - metric: None - val_loss: 1.5941472053527832 - val_metric: 0.6052631578947368 - penalty: 0.01\n",
      "hidden layer sizes: [20, 20, 20, 20, 20], total units: 100\n",
      "1.1535968780517578\n",
      "##########################################################\n",
      "Epoch 1/1\n",
      "Before growing:\n",
      "loss: None - metric: None - val_loss: 1.5941472053527832 - val_metric: 0.6052631578947368 - penalty: 0.01\n",
      "hidden layer sizes: [20, 20, 20, 20, 20], total units: 100\n",
      "After growing:\n",
      "loss: None - metric: None - val_loss: 1.5941473245620728 - val_metric: 0.6052631578947368 - penalty: 0.01\n",
      "hidden layer sizes: [40, 40, 40, 40, 40], total units: 200\n",
      "losses: 6\n",
      "losses: 6\n",
      "losses: 6\n",
      "Before pruning:\n",
      "loss: 1.6867527961730957 - metric: 0.4526315927505493 - val_loss: 1.611056923866272 - val_metric: 0.6052631578947368 - penalty: 0.01\n",
      "hidden layer sizes: [40, 40, 40, 40, 40], total units: 200\n",
      "After pruning:\n",
      "loss: None - metric: None - val_loss: 1.6110868453979492 - val_metric: 0.6052631578947368 - penalty: 0.01\n",
      "hidden layer sizes: [20, 20, 20, 20, 20], total units: 100\n",
      "1.189634084701538\n",
      "##########################################################\n",
      "Epoch 1/1\n",
      "Before growing:\n",
      "loss: None - metric: None - val_loss: 1.6110868453979492 - val_metric: 0.6052631578947368 - penalty: 0.01\n",
      "hidden layer sizes: [20, 20, 20, 20, 20], total units: 100\n",
      "After growing:\n",
      "loss: None - metric: None - val_loss: 1.6110872030258179 - val_metric: 0.6052631578947368 - penalty: 0.01\n",
      "hidden layer sizes: [40, 40, 40, 40, 40], total units: 200\n",
      "losses: 6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-5b3056f7548f>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, optimizer, schedule, batch_size, min_new_neurons, validation_data, pruning_threshold, regularization_penalty_multiplier, stall_coefficient, growth_percentage, mini_epochs_per_epoch, verbose, print_neurons, use_static_graph, loss_fn, metric_fn)\u001b[0m\n\u001b[1;32m    953\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrow_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m             \u001b[0msummed_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummed_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_single_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    956\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprune\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-5b3056f7548f>\u001b[0m in \u001b[0;36mfit_single_epoch\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    927\u001b[0m                 \u001b[0mfit_single_step_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_single_step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_single_step_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetric_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    930\u001b[0m                 \u001b[0msummed_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m                 \u001b[0msummed_metric\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/self-scaling-nets/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/self-scaling-nets/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/self-scaling-nets/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    961\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    964\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/self-scaling-nets/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m--> 785\u001b[0;31m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    786\u001b[0m             *args, **kwds))\n\u001b[1;32m    787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/self-scaling-nets/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2981\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2982\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2983\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2984\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/self-scaling-nets/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_call_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3292\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3293\u001b[0m           self._function_cache.add(cache_key, cache_key_deletion_observer,\n\u001b[1;32m   3294\u001b[0m                                    graph_function)\n",
      "\u001b[0;32m~/.miniconda3/envs/self-scaling-nets/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3128\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3129\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3130\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3131\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3132\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/self-scaling-nets/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1200\u001b[0m         if x is not None)\n\u001b[1;32m   1201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1202\u001b[0;31m     \u001b[0mfunc_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1204\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0madd_control_dependencies\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/self-scaling-nets/lib/python3.8/site-packages/tensorflow/python/framework/auto_control_deps.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, unused_type, unused_value, unused_traceback)\u001b[0m\n\u001b[1;32m    584\u001b[0m           \u001b[0;31m# function outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m             \u001b[0mcontrol_output_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontrol_flow_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             control_output_op._set_attr(\"_acd_function_control_output\",\n\u001b[1;32m    588\u001b[0m                                         attr_value_pb2.AttrValue(b=True))\n",
      "\u001b[0;32m~/.miniconda3/envs/self-scaling-nets/lib/python3.8/site-packages/tensorflow/python/ops/gen_control_flow_ops.py\u001b[0m in \u001b[0;36mno_op\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    508\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m     _, _, _op, _outputs = _op_def_library._apply_op_helper(\n\u001b[0m\u001b[1;32m    511\u001b[0m         \"NoOp\", name=name)\n\u001b[1;32m    512\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/self-scaling-nets/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    748\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m     \u001b[0;31m# Conditionally invoke tfdbg v2's op callback(s).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mop_callbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_invoke_op_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m       callback_outputs = op_callbacks.invoke_op_callbacks(\n\u001b[1;32m    752\u001b[0m           \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/self-scaling-nets/lib/python3.8/site-packages/tensorflow/python/framework/op_callbacks.py\u001b[0m in \u001b[0;36mshould_invoke_op_callbacks\u001b[0;34m()\u001b[0m\n\u001b[1;32m    120\u001b[0m   \"\"\"\n\u001b[1;32m    121\u001b[0m   \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop_callbacks\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoking_op_callbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/self-scaling-nets/lib/python3.8/site-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mop_callbacks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mop_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop_callbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for _ in range(3000):\n",
    "    start_time = time.time()\n",
    "    schedule = Schedule([DynamicEpoch(0.01, 'weighted_l1')] * 1)\n",
    "    model.fit(x=fashion_mnist.X_train_norm, y=fashion_mnist.y_train, optimizer=optimizer, schedule=schedule, batch_size=32, min_new_neurons=20, \n",
    "                        validation_data=(fashion_mnist.X_test_norm, fashion_mnist.y_test), growth_percentage=0.2, verbose=True, \n",
    "                        use_static_graph=True)\n",
    "    print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed value 0.00041534645368884667 to 6ac1742dea.\n",
      "Run with parameters (0.0004, 6ac1742dea, [20, 20, 20, 20, 20], 10, 20, 0.2) started...\n",
      "Run with parameters (0.0004, 6ac1742dea, [20, 20, 20, 20, 20], 10, 20, 0.2) completed, best_val_loss: 1.0397995710372925, best_val_metric: 0.6842105263157895, best_hidden_layer_sizes: [20, 20, 20, 20, 20]\n",
      "Transformed value 0.0013678179300379248 to cdc55f9239.\n",
      "Run with parameters (0.0004, cdc55f9239, [20, 20, 20, 20, 20], 10, 20, 0.2) started...\n",
      "Run with parameters (0.0004, cdc55f9239, [20, 20, 20, 20, 20], 10, 20, 0.2) completed, best_val_loss: 0.8996500968933105, best_val_metric: 0.7105263157894737, best_hidden_layer_sizes: [20, 20, 20, 20, 20]\n",
      "Transformed value 0.000487782514601775 to 2eddbf0c7c.\n",
      "Run with parameters (0.0004, 2eddbf0c7c, [20, 20, 20, 20, 20], 10, 20, 0.2) started...\n",
      "Run with parameters (0.0004, 2eddbf0c7c, [20, 20, 20, 20, 20], 10, 20, 0.2) completed, best_val_loss: 0.8693010210990906, best_val_metric: 0.631578947368421, best_hidden_layer_sizes: [20, 20, 20, 20, 20]\n",
      "Transformed value 0.00011441483511794696 to f2955a0fe5.\n",
      "Run with parameters (0.0004, f2955a0fe5, [20, 20, 20, 20, 20], 10, 20, 0.2) started...\n",
      "Run with parameters (0.0004, f2955a0fe5, [20, 20, 20, 20, 20], 10, 20, 0.2) completed, best_val_loss: 0.8831897377967834, best_val_metric: 0.7105263157894737, best_hidden_layer_sizes: [20, 20, 20, 20, 20]\n",
      "Transformed value 0.0004694789752938634 to d9724a9a99.\n",
      "Run with parameters (0.0004, d9724a9a99, [20, 20, 20, 20, 20], 10, 20, 0.2) started...\n",
      "Run with parameters (0.0004, d9724a9a99, [20, 20, 20, 20, 20], 10, 20, 0.2) completed, best_val_loss: 0.8598389625549316, best_val_metric: 0.7631578947368421, best_hidden_layer_sizes: [20, 20, 20, 20, 20]\n",
      "Transformed value 0.0010269961867697178 to 6b38632ca6.\n",
      "Run with parameters (0.0004, 6b38632ca6, [20, 20, 20, 20, 20], 10, 20, 0.2) started...\n",
      "Run with parameters (0.0004, 6b38632ca6, [20, 20, 20, 20, 20], 10, 20, 0.2) completed, best_val_loss: 0.8992174863815308, best_val_metric: 0.6842105263157895, best_hidden_layer_sizes: [20, 20, 20, 20, 20]\n",
      "Transformed value 0.0009707670038514038 to 1210dc71b3.\n",
      "Run with parameters (0.0004, 1210dc71b3, [20, 20, 20, 20, 20], 10, 20, 0.2) started...\n",
      "Run with parameters (0.0004, 1210dc71b3, [20, 20, 20, 20, 20], 10, 20, 0.2) completed, best_val_loss: 0.9041606783866882, best_val_metric: 0.6578947368421053, best_hidden_layer_sizes: [20, 20, 20, 20, 20]\n",
      "Transformed value 0.0006781580596374959 to cae00569c5.\n",
      "Run with parameters (0.0004, cae00569c5, [20, 20, 20, 20, 20], 10, 20, 0.2) started...\n",
      "Interrupted by user.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "histories, best_overall_combination = random_search(\n",
    "    train_fn_conv, x=fashion_mnist.X_train_norm, y=fashion_mnist.y_train, validation_data=(fashion_mnist.X_test_norm, fashion_mnist.y_test), \n",
    "    learning_rate=0.0004, schedule=PowerRange(-4, -2.5, lambda x: Schedule([DynamicEpoch(x, 'weighted_l1')] * 20)), layer_sizes=[20, 20, 20, 20, 20], \n",
    "    output_neurons=10, min_new_neurons=20, growth_percentage=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed value 0.001439351576826643 to 30be2b3482.\n",
      "Run with parameters (0.0004, 30be2b3482, [20, 20, 20, 20, 20], 10, 20, 0.2) started...\n",
      "Run with parameters (0.0004, 30be2b3482, [20, 20, 20, 20, 20], 10, 20, 0.2) completed, best_val_loss: 0.95672208070755, best_val_metric: 0.6842105263157895, best_hidden_layer_sizes: [20, 20, 20, 20, 20]\n",
      "Transformed value 0.00025685508461890423 to bdcb7b6396.\n",
      "Run with parameters (0.0004, bdcb7b6396, [20, 20, 20, 20, 20], 10, 20, 0.2) started...\n",
      "Run with parameters (0.0004, bdcb7b6396, [20, 20, 20, 20, 20], 10, 20, 0.2) completed, best_val_loss: 0.7996328473091125, best_val_metric: 0.6578947368421053, best_hidden_layer_sizes: [20, 20, 20, 20, 20]\n",
      "Transformed value 0.0031164959282910234 to cd16e3065c.\n",
      "Run with parameters (0.0004, cd16e3065c, [20, 20, 20, 20, 20], 10, 20, 0.2) started...\n",
      "Run with parameters (0.0004, cd16e3065c, [20, 20, 20, 20, 20], 10, 20, 0.2) completed, best_val_loss: 1.1780805587768555, best_val_metric: 0.5789473684210527, best_hidden_layer_sizes: [20, 20, 20, 20, 20]\n",
      "Transformed value 0.00039122798800858045 to 9c60aa4e6a.\n",
      "Run with parameters (0.0004, 9c60aa4e6a, [20, 20, 20, 20, 20], 10, 20, 0.2) started...\n",
      "Run with parameters (0.0004, 9c60aa4e6a, [20, 20, 20, 20, 20], 10, 20, 0.2) completed, best_val_loss: 0.910222589969635, best_val_metric: 0.7368421052631579, best_hidden_layer_sizes: [20, 20, 20, 20, 20]\n",
      "Transformed value 0.00016212355399062043 to dab48fe66b.\n",
      "Run with parameters (0.0004, dab48fe66b, [20, 20, 20, 20, 20], 10, 20, 0.2) started...\n",
      "Run with parameters (0.0004, dab48fe66b, [20, 20, 20, 20, 20], 10, 20, 0.2) completed, best_val_loss: 0.9001054167747498, best_val_metric: 0.7105263157894737, best_hidden_layer_sizes: [20, 20, 20, 20, 20]\n",
      "Transformed value 0.0023621582857491306 to eed005961e.\n",
      "Run with parameters (0.0004, eed005961e, [20, 20, 20, 20, 20], 10, 20, 0.2) started...\n",
      "Interrupted by user.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "histories, best_overall_combination = random_search(\n",
    "    train_fn_conv, x=fashion_mnist.X_train_norm, y=fashion_mnist.y_train, validation_data=(fashion_mnist.X_test_norm, fashion_mnist.y_test), \n",
    "    learning_rate=0.0004, schedule=PowerRange(-4, -2.5, lambda x: Schedule([DynamicEpoch(x, 'weighted_l1')] * 20)), layer_sizes=[20, 20, 20, 20, 20], \n",
    "    output_neurons=10, min_new_neurons=20, growth_percentage=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed value 0.00042521825721349133 to fdf215cf84.\n",
      "Run with parameters (0.0004, fdf215cf84, [20, 20, 20, 20, 20], 10, 20, 0.2) started...\n",
      "WARNING:tensorflow:5 out of the last 7 calls to <bound method Sequential.fit_single_step of <__main__.Sequential object at 0x7f4bedb603d0>> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 9 calls to <bound method Sequential.fit_single_step of <__main__.Sequential object at 0x7f4bedb603d0>> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Run with parameters (0.0004, fdf215cf84, [20, 20, 20, 20, 20], 10, 20, 0.2) completed, best_val_loss: 0.9520576596260071, best_val_metric: 0.6842105263157895, best_hidden_layer_sizes: [20, 20, 20, 20, 20]\n",
      "Transformed value 0.0008922229271696954 to 1789056010.\n",
      "Run with parameters (0.0004, 1789056010, [20, 20, 20, 20, 20], 10, 20, 0.2) started...\n",
      "Run with parameters (0.0004, 1789056010, [20, 20, 20, 20, 20], 10, 20, 0.2) completed, best_val_loss: 0.9162994027137756, best_val_metric: 0.7631578947368421, best_hidden_layer_sizes: [20, 20, 20, 20, 20]\n",
      "Transformed value 0.0010076891993239365 to fedfb3f038.\n",
      "Run with parameters (0.0004, fedfb3f038, [20, 20, 20, 20, 20], 10, 20, 0.2) started...\n",
      "Run with parameters (0.0004, fedfb3f038, [20, 20, 20, 20, 20], 10, 20, 0.2) completed, best_val_loss: 1.0485434532165527, best_val_metric: 0.7105263157894737, best_hidden_layer_sizes: [20, 20, 20, 20, 20]\n",
      "Transformed value 0.002091455785870768 to 0baa2d3c61.\n",
      "Run with parameters (0.0004, 0baa2d3c61, [20, 20, 20, 20, 20], 10, 20, 0.2) started...\n",
      "Run with parameters (0.0004, 0baa2d3c61, [20, 20, 20, 20, 20], 10, 20, 0.2) completed, best_val_loss: 0.9740272164344788, best_val_metric: 0.7105263157894737, best_hidden_layer_sizes: [20, 20, 20, 20, 20]\n",
      "Transformed value 0.00016785810302952562 to 90717144e8.\n",
      "Run with parameters (0.0004, 90717144e8, [20, 20, 20, 20, 20], 10, 20, 0.2) started...\n",
      "Run with parameters (0.0004, 90717144e8, [20, 20, 20, 20, 20], 10, 20, 0.2) completed, best_val_loss: 0.7982632517814636, best_val_metric: 0.7631578947368421, best_hidden_layer_sizes: [20, 20, 20, 20, 20]\n",
      "Transformed value 0.00040009195727464626 to 0fbf479b7d.\n",
      "Run with parameters (0.0004, 0fbf479b7d, [20, 20, 20, 20, 20], 10, 20, 0.2) started...\n",
      "Run with parameters (0.0004, 0fbf479b7d, [20, 20, 20, 20, 20], 10, 20, 0.2) completed, best_val_loss: 1.1521357297897339, best_val_metric: 0.7105263157894737, best_hidden_layer_sizes: [20, 20, 20, 20, 20]\n",
      "Transformed value 0.00027008488381032833 to 860ccde53c.\n",
      "Run with parameters (0.0004, 860ccde53c, [20, 20, 20, 20, 20], 10, 20, 0.2) started...\n",
      "Run with parameters (0.0004, 860ccde53c, [20, 20, 20, 20, 20], 10, 20, 0.2) completed, best_val_loss: 0.8630073070526123, best_val_metric: 0.7368421052631579, best_hidden_layer_sizes: [20, 20, 20, 20, 20]\n",
      "Transformed value 0.0002815967117190191 to b6a4f82635.\n",
      "Run with parameters (0.0004, b6a4f82635, [20, 20, 20, 20, 20], 10, 20, 0.2) started...\n",
      "Run with parameters (0.0004, b6a4f82635, [20, 20, 20, 20, 20], 10, 20, 0.2) completed, best_val_loss: 1.0314395427703857, best_val_metric: 0.7368421052631579, best_hidden_layer_sizes: [20, 20, 20, 20, 20]\n",
      "Transformed value 0.00013232409394243645 to 864ecfe8ae.\n",
      "Run with parameters (0.0004, 864ecfe8ae, [20, 20, 20, 20, 20], 10, 20, 0.2) started...\n",
      "Run with parameters (0.0004, 864ecfe8ae, [20, 20, 20, 20, 20], 10, 20, 0.2) completed, best_val_loss: 1.0115591287612915, best_val_metric: 0.7105263157894737, best_hidden_layer_sizes: [20, 20, 20, 21, 20]\n",
      "Transformed value 0.0016032890361876884 to 48d5819f9e.\n",
      "Run with parameters (0.0004, 48d5819f9e, [20, 20, 20, 20, 20], 10, 20, 0.2) started...\n",
      "Interrupted by user.\n"
     ]
    }
   ],
   "source": [
    "random_search = RandomSearch()\n",
    "random_search.run(\n",
    "    train_fn_conv, x=fashion_mnist.X_train_norm, y=fashion_mnist.y_train, validation_data=(fashion_mnist.X_test_norm, fashion_mnist.y_test), \n",
    "    learning_rate=0.0004, schedule=PowerRange(-4, -2.5, lambda x: Schedule([DynamicEpoch(x, 'weighted_l1')] * 20)), layer_sizes=[20, 20, 20, 20, 20], \n",
    "    output_neurons=10, min_new_neurons=20, growth_percentage=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(21.270880222320557, 0.6842105263157895),\n",
       " (42.808112382888794, 0.7631578947368421),\n",
       " (64.3229489326477, 0.7631578947368421),\n",
       " (85.85527348518372, 0.7631578947368421),\n",
       " (107.26386427879333, 0.7631578947368421),\n",
       " (128.31746411323547, 0.7631578947368421),\n",
       " (150.12859892845154, 0.7631578947368421),\n",
       " (171.3938684463501, 0.7631578947368421),\n",
       " (192.60103964805603, 0.7631578947368421)]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f4bedefb4f0>]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 248,
       "width": 371
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = [0] + [x[0] for x in random_search.results]\n",
    "y = [0] + [x[1] for x in random_search.results]\n",
    "plt.step(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "machine_shape": "hm",
   "name": "tf_multi_layer_ssnet_inverse.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
