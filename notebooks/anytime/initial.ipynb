{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f_deAUKlniFk",
    "outputId": "434f0bdd-1358-46dc-adc2-aee14230789c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Mar 21 22:42:29 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 495.29.05    Driver Version: 495.29.05    CUDA Version: 11.5     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  On   | 00000000:01:01.0 Off |                    0 |\n",
      "| N/A   31C    P0    36W / 250W |  19160MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-PCIE...  On   | 00000000:01:02.0 Off |                    0 |\n",
      "| N/A   28C    P0    23W / 250W |      8MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      2534      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    0   N/A  N/A    195794      C   ...oks/Tomas/venv/bin/python     2243MiB |\n",
      "|    0   N/A  N/A    601346      C   ...3/envs/jupyter/bin/python     3069MiB |\n",
      "|    0   N/A  N/A   1044380      C   ...3/envs/jupyter/bin/python     5895MiB |\n",
      "|    0   N/A  N/A   1226612      C   ...3/envs/jupyter/bin/python     4111MiB |\n",
      "|    0   N/A  N/A   2645362      C   ...oks/Tomas/venv/bin/python     1481MiB |\n",
      "|    0   N/A  N/A   3859600      C   ...3/envs/jupyter/bin/python     2351MiB |\n",
      "|    1   N/A  N/A      2534      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
    "  print('and then re-execute this cell.')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "yKwUwV_NneIo"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-21 22:42:39.086469: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-21 22:42:39.087319: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-21 22:42:39.146963: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-21 22:42:39.147591: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-21 22:42:39.147924: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-21 22:42:39.148213: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from enum import Enum\n",
    "import imageio\n",
    "import hashlib\n",
    "import copy\n",
    "import time\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "dtype = 'float32'\n",
    "tf.keras.backend.set_floatx(dtype)\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "UTZq4KMpneIv"
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# DATASETS\n",
    "################################################################################\n",
    "\n",
    "\n",
    "def get_dataset_sample(X, y, fraction, seed=None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)  # Set random seed\n",
    "    selection = np.random.choice([True, False], len(X), p=[fraction, 1 - fraction])\n",
    "    if seed is not None:\n",
    "        np.random.seed()  # Unset random seed\n",
    "    X_sampled = X[selection]\n",
    "    y_sampled = y[selection]\n",
    "    return X_sampled, y_sampled\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, X_train, y_train, X_test, y_test, shape, shape_flattened, fraction, vision=True, standardize=True):\n",
    "        if fraction is not None:\n",
    "            X_train, y_train = get_dataset_sample(X_train, y_train, fraction, seed=42)\n",
    "            X_test, y_test = get_dataset_sample(X_test, y_test, fraction, seed=42)\n",
    "        \n",
    "        X_train = X_train.astype(dtype)\n",
    "        y_train = y_train.astype(dtype)\n",
    "        X_test = X_test.astype(dtype)\n",
    "        y_test = y_test.astype(dtype)\n",
    "\n",
    "        if vision:\n",
    "            X_train = X_train / 255.0\n",
    "            X_test = X_test / 255.0\n",
    "\n",
    "        X_train = np.reshape(X_train, shape_flattened)\n",
    "        X_test = np.reshape(X_test, shape_flattened)\n",
    "\n",
    "        X = np.concatenate((X_train, X_test), axis=0)\n",
    "        y = np.concatenate((y_train, y_test), axis=0)\n",
    "\n",
    "        if standardize:\n",
    "            from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "            scaler = StandardScaler()\n",
    "            scaler.fit(X_train)  # Scaling each feature independently\n",
    "\n",
    "            X_norm = scaler.transform(X)\n",
    "            X_train_norm = scaler.transform(X_train)\n",
    "            X_test_norm = scaler.transform(X_test)\n",
    "        else:\n",
    "            X_norm = X.copy()\n",
    "            X_train_norm = X_train.copy()\n",
    "            X_test_norm = X_test.copy()\n",
    "\n",
    "        X_norm = np.reshape(X_norm, shape)\n",
    "        X_train_norm = np.reshape(X_train_norm, shape)\n",
    "        X_test_norm = np.reshape(X_test_norm, shape)\n",
    "\n",
    "        del X, X_train, X_test\n",
    "\n",
    "        self.X_norm = X_norm\n",
    "        self.y = y\n",
    "        self.X_train_norm = X_train_norm\n",
    "        self.y_train = y_train\n",
    "        self.X_test_norm = X_test_norm\n",
    "        self.y_test = y_test\n",
    "\n",
    "\n",
    "def get_cifar_10_dataset(fraction=None):\n",
    "    cifar10 = tf.keras.datasets.cifar10\n",
    "    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "    shape = (-1, 32, 32, 3)\n",
    "    shape_flattened = (-1, 3072)  # Scaling each feature independently\n",
    "    return Dataset(X_train, y_train, X_test, y_test, shape=shape, shape_flattened=shape_flattened, fraction=fraction)\n",
    "\n",
    "\n",
    "def get_cifar_100_dataset(fraction=None):\n",
    "    cifar100 = tf.keras.datasets.cifar100\n",
    "    (X_train, y_train), (X_test, y_test) = cifar100.load_data()\n",
    "    shape = (-1, 32, 32, 3)\n",
    "    shape_flattened = (-1, 3072)  # Scaling each feature independently\n",
    "    return Dataset(X_train, y_train, X_test, y_test, shape=shape, shape_flattened=shape_flattened, fraction=fraction)\n",
    "\n",
    "\n",
    "def get_svhn_dataset(fraction=None):\n",
    "    from urllib.request import urlretrieve\n",
    "    from scipy import io\n",
    "\n",
    "    train_filename, _ = urlretrieve('http://ufldl.stanford.edu/housenumbers/train_32x32.mat')\n",
    "    test_filename, _ = urlretrieve('http://ufldl.stanford.edu/housenumbers/test_32x32.mat')\n",
    "\n",
    "    X_train = io.loadmat(train_filename, variable_names='X').get('X')\n",
    "    y_train = io.loadmat(train_filename, variable_names='y').get('y')\n",
    "    X_test = io.loadmat(test_filename, variable_names='X').get('X')\n",
    "    y_test = io.loadmat(test_filename, variable_names='y').get('y')\n",
    "\n",
    "    X_train = np.moveaxis(X_train, -1, 0)\n",
    "    y_train -= 1\n",
    "    X_test = np.moveaxis(X_test, -1, 0)\n",
    "    y_test -= 1\n",
    "\n",
    "    shape = (-1, 32, 32, 3)\n",
    "    shape_flattened = (-1, 3072)  # Scaling each feature independently\n",
    "    return Dataset(X_train, y_train, X_test, y_test, shape=shape, shape_flattened=shape_flattened, fraction=fraction)\n",
    "\n",
    "\n",
    "def get_tiny_imagenet_dataset(fraction=None):\n",
    "    \"\"\"\n",
    "    Original source: https://github.com/sonugiri1043/Train_ResNet_On_Tiny_ImageNet/blob/master/Train_ResNet_On_Tiny_ImageNet.ipynb\n",
    "    Original author: sonugiri1043@gmail.com\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.isdir('IMagenet'):\n",
    "        ! git clone https://github.com/seshuad/IMagenet\n",
    "\n",
    "    print(\"Processing the downloaded dataset...\")\n",
    "\n",
    "    path = 'IMagenet/tiny-imagenet-200/'\n",
    "\n",
    "    id_dict = {}\n",
    "    for i, line in enumerate(open(path + 'wnids.txt', 'r')):\n",
    "        id_dict[line.replace('\\n', '')] = i\n",
    "\n",
    "    train_data = list()\n",
    "    test_data = list()\n",
    "    train_labels = list()\n",
    "    test_labels = list()\n",
    "\n",
    "    for key, value in id_dict.items():\n",
    "        train_data += [imageio.imread(path + 'train/{}/images/{}_{}.JPEG'.format(key, key, str(i)), pilmode='RGB') for i in range(500)]\n",
    "        train_labels_ = np.array([[0]*200]*500)\n",
    "        train_labels_[:, value] = 1\n",
    "        train_labels += train_labels_.tolist()\n",
    "\n",
    "    for line in open(path + 'val/val_annotations.txt'):\n",
    "        img_name, class_id = line.split('\\t')[:2]\n",
    "        test_data.append(imageio.imread(path + 'val/images/{}'.format(img_name), pilmode='RGB'))\n",
    "        test_labels_ = np.array([[0]*200])\n",
    "        test_labels_[0, id_dict[class_id]] = 1\n",
    "        test_labels += test_labels_.tolist()\n",
    "\n",
    "    X_train = np.array(train_data)\n",
    "    y_train = np.argmax(np.array(train_labels), axis=1)\n",
    "    X_test = np.array(test_data)\n",
    "    y_test = np.argmax(np.array(test_labels), axis=1)\n",
    "\n",
    "    shape = (-1, 64, 64, 3)\n",
    "    shape_flattened = (-1, 12288)  # Scaling each feature independently\n",
    "    return Dataset(X_train, y_train, X_test, y_test, shape=shape, shape_flattened=shape_flattened, fraction=fraction)\n",
    "\n",
    "\n",
    "def get_mnist_dataset(fraction=None):\n",
    "    mnist = tf.keras.datasets.mnist\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "    shape = (-1, 28, 28, 1)\n",
    "    shape_flattened = (-1, 1)  # Scaling all features together\n",
    "    return Dataset(X_train, y_train, X_test, y_test, shape=shape, shape_flattened=shape_flattened, fraction=fraction)\n",
    "\n",
    "\n",
    "def get_fashion_mnist_dataset(fraction=None):\n",
    "    fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "    (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "    shape = (-1, 28, 28, 1)\n",
    "    shape_flattened = (-1, 1)  # Scaling all features together\n",
    "    return Dataset(X_train, y_train, X_test, y_test, shape=shape, shape_flattened=shape_flattened, fraction=fraction)\n",
    "\n",
    "\n",
    "def get_fifteen_puzzle_dataset(path=None, fraction=None):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    if path is None:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/gdrive')\n",
    "        path = 'gdrive/MyDrive/15-costs-v3.csv'\n",
    "    costs = pd.read_csv(path)\n",
    "\n",
    "    X_raw = costs.iloc[:,:-1].values\n",
    "    y = costs['cost'].values\n",
    "    X = np.apply_along_axis(lambda x: np.eye(16)[x].ravel(), 1, X_raw)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    del X, X_raw, y\n",
    "\n",
    "    shape = (-1, 256)\n",
    "    shape_flattened = (-1, 256)  # Scaling all features together\n",
    "    return Dataset(X_train, y_train, X_test, y_test, shape=shape, shape_flattened=shape_flattened, vision=False, fraction=fraction)\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# REGULARIZERS\n",
    "################################################################################\n",
    "\n",
    "\n",
    "class Regularizer(tf.keras.regularizers.Regularizer):\n",
    "    def __init__(self):\n",
    "        self.n_new_neurons = 0\n",
    "        self.scaling_tensor = None\n",
    "        self.set_regularization_penalty(0.)\n",
    "        self.set_regularization_method(None)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if self.regularization_method is None or self.regularization_penalty == 0:\n",
    "            return 0\n",
    "        elif self.regularization_method == 'weighted_l1':\n",
    "            return self.weighted_l1(x)\n",
    "        elif self.regularization_method == 'weighted_l1_reordered':\n",
    "            return self.weighted_l1_reordered(x)\n",
    "        elif self.regularization_method == 'group_sparsity':\n",
    "            return self.group_sparsity(x)\n",
    "        elif self.regularization_method == 'l1':\n",
    "            return self.l1(x)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Unknown regularization method {self.regularization_method}\")\n",
    "    \n",
    "    def weighted_l1(self, x):\n",
    "        # I.e. for a parameter matrix of 4 input and 10 output neurons:\n",
    "        #\n",
    "        # [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
    "        #  [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
    "        #  [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
    "        #  [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]\n",
    "        #\n",
    "        # the scaling tensor, as well as the resulting weighted values, could be:\n",
    "        #\n",
    "        # [[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n",
    "        #  [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n",
    "        #  [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n",
    "        #  [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]]\n",
    "        #\n",
    "        # Therefore every additional output neuron is regularized more.\n",
    "\n",
    "        scaling_tensor = tf.cumsum(tf.constant(self.regularization_penalty, shape=x.shape, dtype=dtype), axis=-1)\n",
    "        weighted_values = scaling_tensor * tf.abs(x)\n",
    "        return tf.reduce_sum(weighted_values)\n",
    "    \n",
    "    def weighted_l1_reordered(self, x):\n",
    "        # I.e. for a parameter matrix of 4 input and 10 output neurons:\n",
    "        #\n",
    "        # [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
    "        #  [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
    "        #  [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
    "        #  [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]\n",
    "        #\n",
    "        # the scaling tensor, as well as the resulting weighted values, could be:\n",
    "        #\n",
    "        # [[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n",
    "        #  [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n",
    "        #  [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n",
    "        #  [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]]\n",
    "        #\n",
    "        # Therefore every additional output neuron is regularized more.\n",
    "\n",
    "        if self.update_scaling_tensor:\n",
    "            scaling_tensor_raw = tf.cumsum(tf.constant(self.regularization_penalty, shape=x.shape, dtype=dtype), axis=-1)\n",
    "\n",
    "            scaling_tensor_old_neurons = scaling_tensor_raw[:, :-self.n_new_neurons]\n",
    "            scaling_tensor_new_neurons = scaling_tensor_raw[:, -self.n_new_neurons:]\n",
    "            scaling_tensor_old_neurons_shuffled = tf.transpose(tf.random.shuffle(tf.transpose(scaling_tensor_old_neurons)))\n",
    "            self.scaling_tensor = tf.concat([scaling_tensor_old_neurons_shuffled, scaling_tensor_new_neurons], axis=-1)\n",
    "            self.update_scaling_tensor = False\n",
    "\n",
    "        weighted_values = self.scaling_tensor * tf.abs(x)\n",
    "        return tf.reduce_sum(weighted_values)\n",
    "    \n",
    "    def group_sparsity(self, x):\n",
    "        # I.e. for a parameter matrix of 3 input and 5 output neurons:\n",
    "        #\n",
    "        # [[1., 1., 1., 1., 1.],\n",
    "        #  [1., 2., 2., 1., 2.],\n",
    "        #  [2., 2., 3., 1., 3.]]\n",
    "        #\n",
    "        # The resulting vector of group norms is [2., 2., 3., 1., 3.], therefore for\n",
    "        # every output neuron, its incoming connections form a group.\n",
    "\n",
    "        group_norms = tf.norm(x, ord=2, axis=0)\n",
    "        # assert group_norms.shape[0] == x.shape[1]\n",
    "        return self.regularization_penalty * tf.reduce_sum(group_norms)\n",
    "    \n",
    "    def l1(self, x):\n",
    "        weighted_values = self.regularization_penalty * tf.abs(x)\n",
    "        return tf.reduce_sum(weighted_values)\n",
    "    \n",
    "    def prune(self):\n",
    "        self.n_new_neurons = 0\n",
    "        if self.regularization_method == 'weighted_l1_reordered':\n",
    "            self.update_scaling_tensor = True\n",
    "    \n",
    "    def grow(self, n_new_neurons):\n",
    "        self.n_new_neurons = n_new_neurons\n",
    "        if self.regularization_method == 'weighted_l1_reordered':\n",
    "            self.update_scaling_tensor = True\n",
    "    \n",
    "    def set_regularization_penalty(self, regularization_penalty):\n",
    "        self.regularization_penalty = regularization_penalty\n",
    "    \n",
    "    def set_regularization_method(self, regularization_method):\n",
    "        self.regularization_method = regularization_method\n",
    "        if self.regularization_method == 'weighted_l1_reordered':\n",
    "            self.update_scaling_tensor = True\n",
    "        else:\n",
    "            self.update_scaling_tensor = None\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'regularization_penalty': float(self.regularization_penalty)}\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# LAYERS\n",
    "################################################################################\n",
    "\n",
    "\n",
    "class DASLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_shape):\n",
    "        super().__init__()\n",
    "\n",
    "        self._input_shape = input_shape\n",
    "\n",
    "\n",
    "class Dense(DASLayer):\n",
    "    def __init__(self, units, activation, kernel_initializer='glorot_uniform', \n",
    "                 bias_initializer='zeros', input_shape=None, fixed_size=False,\n",
    "                 regularizer=None):\n",
    "        super().__init__(input_shape)\n",
    "\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.bias_initializer = bias_initializer\n",
    "        self.fixed_size = fixed_size\n",
    "        \n",
    "        self.A = tf.keras.activations.get(activation)\n",
    "        self.W_init = tf.keras.initializers.get(kernel_initializer)\n",
    "        self.b_init = tf.keras.initializers.get(bias_initializer)\n",
    "        if regularizer is not None:\n",
    "            self.regularizer = regularizer\n",
    "        else:\n",
    "            self.regularizer = Regularizer()\n",
    "    \n",
    "    # def copy(self):\n",
    "    #     input_shape = self._input_shape\n",
    "    #     regularizer_copy = self.regularizer.copy()\n",
    "    #     layer_copy = Dense(self.units, self.activation, self.kernel_initializer, \n",
    "    #                        self.bias_initializer, input_shape, self.fixed_size, \n",
    "    #                        regularizer_copy)\n",
    "    #     return layer_copy\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        input_units = input_shape[-1]\n",
    "\n",
    "        self.W = tf.Variable(\n",
    "            name='W',\n",
    "            initial_value=self.W_init(shape=(input_units, self.units), dtype=dtype),\n",
    "            trainable=True)\n",
    "        \n",
    "        self.b = tf.Variable(\n",
    "            name='b',\n",
    "            initial_value=self.b_init(shape=(self.units,), dtype=dtype),\n",
    "            trainable=True)\n",
    "        \n",
    "        self.add_regularizer_loss()\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        return self.A(tf.matmul(inputs, self.W) + self.b)\n",
    "    \n",
    "    def add_regularizer_loss(self):\n",
    "        self.add_loss(lambda: self.regularizer(tf.concat([self.W, tf.reshape(self.b, (1, -1))], axis=0)))\n",
    "\n",
    "    def get_size(self):\n",
    "        return self.W.shape[0], self.W.shape[1]\n",
    "    \n",
    "    def prune(self, threshold, active_input_units_indices):\n",
    "        # Remove connections from pruned units in previous layer\n",
    "        new_W = tf.gather(self.W.value(), active_input_units_indices, axis=0)\n",
    "\n",
    "        if self.fixed_size:\n",
    "            active_output_neurons_indices = list(range(new_W.shape[1]))\n",
    "        else:\n",
    "            # Prune units in this layer\n",
    "            weights_with_biases = tf.concat([new_W, tf.reshape(self.b.value(), (1, -1))], axis=0)\n",
    "            neurons_are_active = tf.math.reduce_max(tf.abs(weights_with_biases), axis=0) >= threshold\n",
    "            active_output_neurons_indices = tf.reshape(tf.where(neurons_are_active), (-1,))\n",
    "            \n",
    "            new_W = tf.gather(new_W, active_output_neurons_indices, axis=1)\n",
    "            new_b = tf.gather(self.b.value(), active_output_neurons_indices, axis=0)\n",
    "\n",
    "            self.b = tf.Variable(name='b', initial_value=new_b, trainable=True)\n",
    "\n",
    "        self.W = tf.Variable(name='W', initial_value=new_W, trainable=True)\n",
    "\n",
    "        self.regularizer.prune()\n",
    "        return active_output_neurons_indices\n",
    "    \n",
    "    def grow(self, n_new_input_units, percentage, min_new_units, scaling_factor):\n",
    "        if n_new_input_units > 0:\n",
    "            # Add connections to grown units in previous layer\n",
    "            W_growth = self.W_init(shape=(self.W.shape[0] + n_new_input_units, self.W.shape[1]), dtype=dtype)[-n_new_input_units:, :] * scaling_factor  # TODO is it better to be multiplying here by scaling_factor? It does help with not increasing the max weights of existing neurons when new neurons are added.\n",
    "            new_W = tf.concat([self.W.value(), W_growth], axis=0)\n",
    "        else:\n",
    "            new_W = self.W.value()\n",
    "\n",
    "        if self.fixed_size:\n",
    "            n_new_output_units = 0\n",
    "        else:\n",
    "            # Grow new units in this layer\n",
    "            n_new_output_units = max(min_new_units, int(new_W.shape[1] * percentage))\n",
    "            if n_new_output_units > 0:\n",
    "                W_growth = self.W_init(shape=(new_W.shape[0], new_W.shape[1] + n_new_output_units), dtype=dtype)[:, -n_new_output_units:] * scaling_factor\n",
    "                b_growth = self.b_init(shape=(n_new_output_units,), dtype=dtype)  # TODO for all possible bias initializers to work properly, the whole bias vector should be initialized at once\n",
    "                new_W = tf.concat([new_W, W_growth], axis=1)\n",
    "                new_b = tf.concat([self.b.value(), b_growth], axis=0)\n",
    "\n",
    "                self.b = tf.Variable(name='b', initial_value=new_b, trainable=True)\n",
    "\n",
    "        self.W = tf.Variable(name='W', initial_value=new_W, trainable=True)\n",
    "\n",
    "        self.regularizer.grow(n_new_output_units)\n",
    "        return n_new_output_units\n",
    "    \n",
    "    def mutate(self, mutation_strength):\n",
    "        self.W.assign_add(tf.random.normal(self.W.shape, mean=0.0, stddev=mutation_strength))\n",
    "        self.b.assign_add(tf.random.normal(self.b.shape, mean=0.0, stddev=mutation_strength))\n",
    "    \n",
    "    def set_regularization_penalty(self, regularization_penalty):\n",
    "        if not self.fixed_size:\n",
    "            self.regularizer.set_regularization_penalty(regularization_penalty)\n",
    "    \n",
    "    def set_regularization_method(self, regularization_method):\n",
    "        if not self.fixed_size:\n",
    "            self.regularizer.set_regularization_method(regularization_method)\n",
    "    \n",
    "    def get_param_string():\n",
    "        param_string = \"\"\n",
    "        weights_with_bias = tf.concat([self.W, tf.reshape(self.b, (1, -1))], axis=0)\n",
    "        max_parameters = tf.math.reduce_max(tf.abs(weights_with_bias), axis=0).numpy()\n",
    "        magnitudes = np.floor(np.log10(max_parameters))\n",
    "        for m in magnitudes:\n",
    "            if m > 0:\n",
    "                m = 0\n",
    "            param_string += str(int(-m))\n",
    "        return param_string\n",
    "\n",
    "\n",
    "class Conv2D(DASLayer):\n",
    "    def __init__(self, filters, filter_size, activation, strides=(1, 1), \n",
    "                 padding='SAME', kernel_initializer='glorot_uniform',\n",
    "                 bias_initializer='zeros', input_shape=None, fixed_size=False,\n",
    "                 regularizer=None):\n",
    "        super().__init__(input_shape)\n",
    "    \n",
    "        self.filters = filters\n",
    "        self.filter_size = filter_size\n",
    "        self.activation = activation\n",
    "        self.strides = strides\n",
    "        self.padding = padding\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.bias_initializer = bias_initializer\n",
    "        self.fixed_size = fixed_size\n",
    "        \n",
    "        self.A = tf.keras.activations.get(activation)\n",
    "        self.F_init = tf.keras.initializers.get(kernel_initializer)\n",
    "        self.b_init = tf.keras.initializers.get(bias_initializer)\n",
    "        if regularizer is not None:\n",
    "            self.regularizer = regularizer\n",
    "        else:\n",
    "            self.regularizer = Regularizer()            \n",
    "        \n",
    "    # def copy(self):\n",
    "    #     input_shape = self._input_shape\n",
    "    #     regularizer_copy = self.regularizer.copy()\n",
    "    #     layer_copy = Conv2D(self.filters, self.filter_size, self.activation, self.strides, \n",
    "    #                         self.padding, self.kernel_initializer, self.bias_initializer, \n",
    "    #                         input_shape, self.fixed_size, regularizer_copy)\n",
    "    #     return layer_copy\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        input_filters = input_shape[-1]\n",
    "\n",
    "        self.F = tf.Variable(\n",
    "            name='F',\n",
    "            initial_value=self.F_init(\n",
    "                shape=(self.filter_size[0], self.filter_size[1], input_filters, self.filters), dtype=dtype\n",
    "            ),\n",
    "            trainable=True)\n",
    "        \n",
    "        self.b = tf.Variable(\n",
    "            name='b',\n",
    "            initial_value=self.b_init(shape=(self.filters,), dtype=dtype),\n",
    "            trainable=True)\n",
    "\n",
    "        self.add_regularizer_loss()\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        y = tf.nn.conv2d(inputs, self.F, strides=self.strides, padding=self.padding)\n",
    "        y = tf.nn.bias_add(y, self.b)\n",
    "        y = self.A(y)\n",
    "        return y\n",
    "    \n",
    "    def add_regularizer_loss(self):\n",
    "        self.add_loss(lambda: self.regularizer(tf.concat([tf.reshape(self.F, (-1, self.F.shape[-1])), tf.reshape(self.b, (1, -1))], axis=0)))\n",
    "    \n",
    "    def get_size(self):\n",
    "        return self.F.shape[-2], self.F.shape[-1]\n",
    "    \n",
    "    def prune(self, threshold, active_input_units_indices):\n",
    "        # Remove connections from pruned units in previous layer\n",
    "        new_F = tf.gather(self.F.value(), active_input_units_indices, axis=-2)\n",
    "\n",
    "        if self.fixed_size:\n",
    "            active_output_filters_indices = list(range(new_F.shape[-1]))\n",
    "        else:\n",
    "            # Prune units in this layer\n",
    "            F_reduced_max = tf.reshape(tf.math.reduce_max(tf.abs(new_F), axis=(0, 1, 2)), (1, -1))\n",
    "            F_reduced_max_with_biases = tf.concat([F_reduced_max, tf.reshape(self.b.value(), (1, -1))], axis=0)\n",
    "            filters_are_active = tf.math.reduce_max(tf.abs(F_reduced_max_with_biases), axis=0) >= threshold\n",
    "            active_output_filters_indices = tf.reshape(tf.where(filters_are_active), (-1,))\n",
    "            \n",
    "            new_F = tf.gather(new_F, active_output_filters_indices, axis=-1)\n",
    "            new_b = tf.gather(self.b.value(), active_output_filters_indices, axis=0)\n",
    "\n",
    "            self.b = tf.Variable(name='b', initial_value=new_b, trainable=True)\n",
    "\n",
    "        self.F = tf.Variable(name='F', initial_value=new_F, trainable=True)\n",
    "\n",
    "        self.regularizer.prune()\n",
    "        return active_output_filters_indices\n",
    "\n",
    "    def grow(self, n_new_input_units, percentage, min_new_units, scaling_factor):\n",
    "        if n_new_input_units > 0:\n",
    "            # Add connections to grown units in previous layer\n",
    "            F_growth = self.F_init(shape=(self.F.shape[0], self.F.shape[1], self.F.shape[2] + n_new_input_units, self.F.shape[3]), dtype=dtype)[:, :, -n_new_input_units:, :] * scaling_factor  # TODO is it better to be multiplying here by scaling_factor? It does help with not increasing the max weights of existing neurons when new neurons are added.\n",
    "            new_F = tf.concat([self.F.value(), F_growth], axis=-2)\n",
    "        else:\n",
    "            new_F = self.F.value()\n",
    "\n",
    "        if self.fixed_size:\n",
    "            n_new_output_units = 0\n",
    "        else:\n",
    "            # Grow new units in this layer\n",
    "            n_new_output_units = max(min_new_units, int(new_F.shape[-1] * percentage))\n",
    "            if n_new_output_units > 0:\n",
    "                F_growth = self.F_init(shape=(new_F.shape[0], new_F.shape[1], new_F.shape[2], new_F.shape[3] + n_new_output_units), dtype=dtype)[:, :, :, -n_new_output_units:] * scaling_factor\n",
    "                b_growth = self.b_init(shape=(n_new_output_units,), dtype=dtype)  # TODO for all possible bias initializers to work properly, the whole bias vector should be initialized at once\n",
    "                new_F = tf.concat([new_F, F_growth], axis=-1)\n",
    "                new_b = tf.concat([self.b.value(), b_growth], axis=0)\n",
    "\n",
    "                self.b = tf.Variable(name='b', initial_value=new_b, trainable=True)\n",
    "\n",
    "        self.F = tf.Variable(name='F', initial_value=new_F, trainable=True)\n",
    "\n",
    "        self.regularizer.grow(n_new_output_units)\n",
    "        return n_new_output_units\n",
    "    \n",
    "    def mutate(self, mutation_strength):\n",
    "        self.F.assign_add(tf.random.normal(self.F.shape, mean=0.0, stddev=mutation_strength))\n",
    "        self.b.assign_add(tf.random.normal(self.b.shape, mean=0.0, stddev=mutation_strength))\n",
    "    \n",
    "    def set_regularization_penalty(self, regularization_penalty):\n",
    "        if not self.fixed_size:\n",
    "            self.regularizer.set_regularization_penalty(regularization_penalty)\n",
    "    \n",
    "    def set_regularization_method(self, regularization_method):\n",
    "        if not self.fixed_size:\n",
    "            self.regularizer.set_regularization_method(regularization_method)\n",
    "\n",
    "    def get_param_string():\n",
    "        param_string = \"\"\n",
    "        # TODO\n",
    "        return param_string\n",
    "\n",
    "\n",
    "class Flatten(tf.keras.layers.Layer):\n",
    "    def call(self, inputs, training=None):\n",
    "        return tf.reshape(tf.transpose(inputs, perm=[0, 3, 1, 2]), (inputs.shape[0], -1))\n",
    "    \n",
    "    # def copy(self):\n",
    "    #     return Flatten()\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# MODELS\n",
    "################################################################################\n",
    "\n",
    "\n",
    "class Epoch:\n",
    "    def __init__(self, grow, prune, regularization_penalty, regularization_method):\n",
    "        self.grow = grow\n",
    "        self.prune = prune\n",
    "        self.regularization_penalty = regularization_penalty\n",
    "        self.regularization_method = regularization_method\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f'{int(self.grow)}{int(self.prune)}{self.regularization_penalty}{self.regularization_method}'\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "\n",
    "class DynamicEpoch(Epoch):\n",
    "    def __init__(self, regularization_penalty, regularization_method):\n",
    "        super().__init__(True, True, regularization_penalty, regularization_method)\n",
    "\n",
    "\n",
    "class StaticEpoch(Epoch):\n",
    "    def __init__(self, regularization_penalty, regularization_method):\n",
    "        super().__init__(False, False, regularization_penalty, regularization_method)\n",
    "\n",
    "\n",
    "class StaticEpochNoRegularization(StaticEpoch):\n",
    "    def __init__(self):\n",
    "        super().__init__(0., None)\n",
    "\n",
    "\n",
    "class Schedule:\n",
    "    def __init__(self, epochs):\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self.epochs.__iter__()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.epochs)\n",
    "    \n",
    "    def __str__(self):\n",
    "        text = ''.join([str(epoch) for epoch in self.epochs])\n",
    "        return hashlib.sha1(text.encode('utf-8')).hexdigest()[:10]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "\n",
    "class Sequential(tf.keras.Model):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lrs = layers\n",
    "        \n",
    "    def call(self, inputs, training=None):\n",
    "        x = inputs\n",
    "        for layer in self.lrs:\n",
    "            x = layer(x, training=training)\n",
    "        return x\n",
    "    \n",
    "    def copy(self):\n",
    "        copied_layers = list()\n",
    "        for layer in self.lrs:\n",
    "            if isinstance(layer, DASLayer):\n",
    "                # layer_copy = layer.copy()\n",
    "                layer_copy = copy.deepcopy(layer)\n",
    "                layer_copy.add_regularizer_loss()\n",
    "            else:\n",
    "                layer_copy = copy.deepcopy(layer)\n",
    "            copied_layers.append(layer_copy)\n",
    "        \n",
    "        model_copy = Sequential(copied_layers)\n",
    "        return model_copy\n",
    "    \n",
    "    def get_layer_input_shape(self, target_layer):\n",
    "        if target_layer._input_shape is not None:\n",
    "            return target_layer._input_shape\n",
    "\n",
    "        input = np.random.normal(size=(1,) + self.lrs[0]._input_shape)\n",
    "        for layer in self.lrs:\n",
    "            if layer is target_layer:\n",
    "                return tuple(input.shape[1:])\n",
    "            input = layer(input)\n",
    "        raise Exception(\"Layer not found in the model.\")\n",
    "\n",
    "    def get_layer_output_shape(self, target_layer):\n",
    "        input = np.random.normal(size=(1,) + self.lrs[0]._input_shape)\n",
    "        for layer in self.lrs:\n",
    "            output = layer(input)\n",
    "            if layer is target_layer:\n",
    "                return tuple(output.shape[1:])\n",
    "            input = output\n",
    "        raise Exception(\"Layer not found in the model.\")\n",
    "    \n",
    "    def get_layer_sizes(self):\n",
    "        \"\"\"\n",
    "        Returns the sizes of all layers in the model, including the input and output layer.\n",
    "        \"\"\"\n",
    "        layer_sizes = list()\n",
    "        first_layer = True\n",
    "        for l in range(len(self.lrs)):\n",
    "            layer = self.lrs[l]\n",
    "            if isinstance(layer, DASLayer):\n",
    "                layer_size = layer.get_size()\n",
    "                if first_layer:\n",
    "                    layer_sizes.append(layer_size[0])\n",
    "                    first_layer = False\n",
    "                layer_sizes.append(layer_size[1])\n",
    "        return layer_sizes\n",
    "    \n",
    "    def get_hidden_layer_sizes(self):\n",
    "        return self.get_layer_sizes()[1:-1]\n",
    "    \n",
    "    def get_regularization_penalty(self):\n",
    "        #TODO improve\n",
    "        return self.lrs[-2].regularizer.regularization_penalty\n",
    "    \n",
    "    def set_regularization_penalty(self, regularization_penalty):\n",
    "        for layer in self.lrs:\n",
    "            if isinstance(layer, DASLayer) and not layer.fixed_size:\n",
    "                layer.set_regularization_penalty(regularization_penalty)\n",
    "    \n",
    "    def set_regularization_method(self, regularization_method):\n",
    "        for layer in self.lrs:\n",
    "            if isinstance(layer, DASLayer) and not layer.fixed_size:\n",
    "                layer.set_regularization_method(regularization_method)\n",
    "\n",
    "    def prune(self, params):\n",
    "        input_shape = self.get_layer_input_shape(self.lrs[0])\n",
    "        n_input_units = input_shape[-1]\n",
    "        active_units_indices = list(range(n_input_units))\n",
    "\n",
    "        last_custom_layer = None\n",
    "        for layer in self.lrs:\n",
    "            if isinstance(layer, DASLayer):\n",
    "                if last_custom_layer is not None and type(last_custom_layer) != type(layer):\n",
    "                    if type(last_custom_layer) == Conv2D and type(layer) == Dense:\n",
    "                        convolutional_shape = self.get_layer_output_shape(last_custom_layer)\n",
    "                        active_units_indices = self.convert_channel_indices_to_flattened_indices(active_units_indices, convolutional_shape)\n",
    "                    else:\n",
    "                        raise Exception(\"Incorrect order of custom layer types.\")\n",
    "                active_units_indices = layer.prune(params.pruning_threshold, active_units_indices)\n",
    "                last_custom_layer = layer\n",
    "    \n",
    "    def grow(self, params):   \n",
    "        n_new_units = 0\n",
    "\n",
    "        last_custom_layer = None\n",
    "        for layer in self.lrs:\n",
    "            if isinstance(layer, DASLayer):\n",
    "                if last_custom_layer is not None and type(last_custom_layer) != type(layer):\n",
    "                    if type(last_custom_layer) == Conv2D and type(layer) == Dense:\n",
    "                        convolutional_shape = self.get_layer_output_shape(last_custom_layer)\n",
    "                        n_new_units = n_new_units * convolutional_shape[0] * convolutional_shape[1]\n",
    "                    else:\n",
    "                        raise Exception(\"Incorrect order of custom layer types.\")\n",
    "                n_new_units = layer.grow(n_new_units, params.growth_percentage, min_new_units=params.min_new_neurons, scaling_factor=params.pruning_threshold)\n",
    "                last_custom_layer = layer\n",
    "    \n",
    "    def mutate(self, mutation_strength):\n",
    "        for layer in self.lrs:\n",
    "            if isinstance(layer, DASLayer):\n",
    "                layer.mutate(mutation_strength)\n",
    "    \n",
    "    @staticmethod\n",
    "    def convert_channel_indices_to_flattened_indices(channel_indices, convolutional_shape):\n",
    "        dense_indices = list()\n",
    "        units_per_channel = convolutional_shape[0] * convolutional_shape[1]\n",
    "        for channel_index in channel_indices:\n",
    "            for iter in range(units_per_channel):\n",
    "                dense_indices.append(channel_index * units_per_channel + iter)\n",
    "        return dense_indices\n",
    "    \n",
    "    def print_neurons(self):\n",
    "        for layer in self.lrs[:-1]:\n",
    "            print(layer.get_param_string())\n",
    "    \n",
    "    def evaluate(self, params, summed_training_loss, summed_training_metric):\n",
    "        # Calculate training loss and metric\n",
    "        if summed_training_loss is not None:\n",
    "            loss = summed_training_loss / params.x.shape[0]\n",
    "        else:\n",
    "            loss = None\n",
    "        \n",
    "        if summed_training_metric is not None:\n",
    "            metric = summed_training_metric / params.x.shape[0]\n",
    "        else:\n",
    "            metric = None\n",
    "        \n",
    "        # Calculate val loss and metric\n",
    "        summed_val_loss = 0\n",
    "        summed_val_metric = 0\n",
    "        n_val_instances = 0\n",
    "        \n",
    "        for step, (x_batch, y_batch) in enumerate(params.val_dataset):\n",
    "            # y_pred = tf.reshape(self(x_batch, training=False), y_batch.shape)\n",
    "            y_pred = self(x_batch, training=False)\n",
    "            summed_val_loss += tf.reduce_sum(params.loss_fn(y_batch, y_pred))\n",
    "            summed_val_metric += float(tf.reduce_sum(params.metric_fn(y_batch, y_pred)))\n",
    "            n_val_instances += x_batch.shape[0]\n",
    "        \n",
    "        val_loss = summed_val_loss / n_val_instances\n",
    "        val_metric = summed_val_metric / n_val_instances\n",
    "\n",
    "        return loss, metric, val_loss, val_metric\n",
    "\n",
    "    def list_params(self):\n",
    "        trainable_count = np.sum([K.count_params(w) for w in self.trainable_weights])\n",
    "        non_trainable_count = np.sum([K.count_params(w) for w in self.non_trainable_weights])\n",
    "        total_count = trainable_count + non_trainable_count\n",
    "\n",
    "        print('Total params: {:,}'.format(total_count))\n",
    "        print('Trainable params: {:,}'.format(trainable_count))\n",
    "        print('Non-trainable params: {:,}'.format(non_trainable_count))\n",
    "\n",
    "        return total_count, trainable_count, non_trainable_count\n",
    "    \n",
    "    def print_epoch_statistics(self, params, summed_training_loss, summed_training_metric, message=None, require_result=False):\n",
    "        if not params.verbose:\n",
    "            if require_result:\n",
    "                return self.evaluate(params, summed_training_loss, summed_training_metric)\n",
    "            else:\n",
    "                return\n",
    "        \n",
    "        loss, metric, val_loss, val_metric = self.evaluate(params, summed_training_loss, summed_training_metric)  \n",
    "\n",
    "        if message is not None:\n",
    "            print(message)\n",
    "        \n",
    "        print(f\"loss: {loss} - metric: {metric} - val_loss: {val_loss} - val_metric: {val_metric} - penalty: {self.get_regularization_penalty()}\")\n",
    "        hidden_layer_sizes = self.get_hidden_layer_sizes()\n",
    "        print(f\"hidden layer sizes: {hidden_layer_sizes}, total units: {sum(hidden_layer_sizes)}\")\n",
    "        if params.print_neurons:\n",
    "            self.print_neurons()\n",
    "        \n",
    "        if require_result:\n",
    "            return loss, metric, val_loss, val_metric\n",
    "    \n",
    "    def update_history(self, params, loss, metric, val_loss, val_metric):\n",
    "        params.history['loss'].append(float(loss))\n",
    "        params.history['metric'].append(float(metric))\n",
    "        params.history['val_loss'].append(float(val_loss))\n",
    "        params.history['val_metric'].append(float(val_metric))\n",
    "        params.history['hidden_layer_sizes'].append(self.get_hidden_layer_sizes())\n",
    "    \n",
    "    @staticmethod\n",
    "    def prepare_datasets(x, y, batch_size, validation_data):\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "        train_dataset = train_dataset.shuffle(buffer_size=20000).batch(batch_size)\n",
    "        val_dataset = tf.data.Dataset.from_tensor_slices(validation_data).batch(batch_size)\n",
    "        return train_dataset.prefetch(tf.data.AUTOTUNE), val_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    def manage_dynamic_regularization(self, params, val_loss):\n",
    "        if val_loss >= params.best_conditional_val_loss * params.stall_coefficient:\n",
    "            # Training is currently in stall\n",
    "            if not params.training_stalled:\n",
    "                penalty = self.get_regularization_penalty() * params.regularization_penalty_multiplier\n",
    "                print(\"Changing penalty...\")\n",
    "                # TODO this must be modified, penalty can differ for each layer\n",
    "                self.set_regularization_penalty(penalty)\n",
    "                params.training_stalled = True\n",
    "        else:\n",
    "            params.best_conditional_val_loss = val_loss\n",
    "            params.training_stalled = False\n",
    "    \n",
    "    def grow_wrapper(self, params):\n",
    "        dynamic_reqularization_active = params.regularization_penalty_multiplier != 1.\n",
    "        if dynamic_reqularization_active:\n",
    "            loss, metric, val_loss, val_metric = self.print_epoch_statistics(params, None, None, \"Before growing:\", require_result=True)\n",
    "            self.manage_dynamic_regularization(params, val_loss)\n",
    "        else:\n",
    "            self.print_epoch_statistics(params, None, None, \"Before growing:\")\n",
    "\n",
    "        self.grow(params)\n",
    "        self.print_epoch_statistics(params, None, None, \"After growing:\")\n",
    "    \n",
    "    def prune_wrapper(self, params, summed_loss, summed_metric):\n",
    "        loss, metric, _, _ = self.print_epoch_statistics(params, summed_loss, summed_metric, \"Before pruning:\", require_result=True)\n",
    "        self.prune(params)\n",
    "        _, _, val_loss, val_metric = self.print_epoch_statistics(params, None, None, \"After pruning:\", require_result=True)\n",
    "\n",
    "        self.update_history(params, loss, metric, val_loss, val_metric)\n",
    "    \n",
    "    class ParameterContainer:\n",
    "        def __init__(self, x, y, optimizer, batch_size, min_new_neurons, validation_data, pruning_threshold, regularization_penalty_multiplier, \n",
    "                     stall_coefficient, growth_percentage, mini_epochs_per_epoch, verbose, print_neurons, use_static_graph, loss_fn, metric_fn):\n",
    "            self.x = x\n",
    "            self.y = y\n",
    "            self.optimizer = optimizer\n",
    "            self.batch_size = batch_size\n",
    "            self.min_new_neurons = min_new_neurons\n",
    "            self.validation_data = validation_data\n",
    "            self.pruning_threshold = pruning_threshold\n",
    "            self.regularization_penalty_multiplier = regularization_penalty_multiplier\n",
    "            self.stall_coefficient = stall_coefficient\n",
    "            self.growth_percentage = growth_percentage\n",
    "            self.mini_epochs_per_epoch = mini_epochs_per_epoch\n",
    "            self.verbose = verbose\n",
    "            self.print_neurons = print_neurons\n",
    "            self.use_static_graph = use_static_graph\n",
    "            self.loss_fn = loss_fn\n",
    "            self.metric_fn = metric_fn\n",
    "\n",
    "            self.train_dataset, self.val_dataset = Sequential.prepare_datasets(x, y, batch_size, validation_data)\n",
    "            self.history = self.prepare_history()\n",
    "\n",
    "            self.best_conditional_val_loss = np.inf\n",
    "            self.training_stalled = False\n",
    "        \n",
    "        @staticmethod\n",
    "        def prepare_history():\n",
    "            history = {\n",
    "                'loss': list(),\n",
    "                'metric': list(),\n",
    "                'val_loss': list(),\n",
    "                'val_metric': list(),\n",
    "                'hidden_layer_sizes': list(),\n",
    "            }\n",
    "            return history\n",
    "    \n",
    "    def fit_single_step(self, params, x_batch, y_batch):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # y_pred = tf.reshape(self(x_batch, training=True), y_batch.shape)\n",
    "            y_pred = self(x_batch, training=True)\n",
    "            raw_loss = params.loss_fn(y_batch, y_pred)\n",
    "            loss_value = tf.reduce_mean(raw_loss)\n",
    "            loss_value += sum(self.losses)  # Add losses registered by model.add_loss\n",
    "\n",
    "            loss = tf.reduce_sum(raw_loss)\n",
    "            metric = float(tf.reduce_sum(params.metric_fn(y_batch, y_pred)))\n",
    "\n",
    "        grads = tape.gradient(loss_value, self.trainable_variables)\n",
    "        params.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "\n",
    "        return loss, metric\n",
    "    \n",
    "    def fit_single_epoch(self, params):\n",
    "        summed_loss = 0\n",
    "        summed_metric = 0\n",
    "        \n",
    "        for mini_epoch in range(params.mini_epochs_per_epoch):\n",
    "            summed_loss = 0\n",
    "            summed_metric = 0\n",
    "\n",
    "            if params.use_static_graph:\n",
    "                fit_single_step_function = tf.function(self.fit_single_step)\n",
    "            else:\n",
    "                fit_single_step_function = self.fit_single_step\n",
    "            for step, (x_batch, y_batch) in enumerate(params.train_dataset):\n",
    "                loss, metric = fit_single_step_function(params, x_batch, y_batch)\n",
    "                summed_loss += loss\n",
    "                summed_metric += metric\n",
    "        \n",
    "        return summed_loss, summed_metric\n",
    "\n",
    "    def fit(self, x, y, optimizer, schedule, batch_size, min_new_neurons, validation_data, pruning_threshold=0.001, regularization_penalty_multiplier=1., \n",
    "            stall_coefficient=1, growth_percentage=0.2, mini_epochs_per_epoch=1, verbose=True, print_neurons=False, use_static_graph=True, \n",
    "            loss_fn=tf.keras.losses.sparse_categorical_crossentropy, metric_fn=tf.keras.metrics.sparse_categorical_accuracy):\n",
    "        params = self.ParameterContainer(x=x, y=y, optimizer=optimizer, batch_size=batch_size, min_new_neurons=min_new_neurons, validation_data=validation_data, \n",
    "                                         pruning_threshold=pruning_threshold, regularization_penalty_multiplier=regularization_penalty_multiplier, stall_coefficient=stall_coefficient, \n",
    "                                         growth_percentage=growth_percentage, mini_epochs_per_epoch=mini_epochs_per_epoch, verbose=verbose, print_neurons=print_neurons, \n",
    "                                         use_static_graph=use_static_graph, loss_fn=loss_fn, metric_fn=metric_fn)\n",
    "        self.build(x.shape)  # Necessary when verbose == False\n",
    "\n",
    "        for epoch_no, epoch in enumerate(schedule):\n",
    "            if verbose:\n",
    "                print(\"##########################################################\")\n",
    "                print(f\"Epoch {epoch_no + 1}/{len(schedule)}\")\n",
    "            \n",
    "            self.set_regularization_penalty(epoch.regularization_penalty)\n",
    "            self.set_regularization_method(epoch.regularization_method)\n",
    "\n",
    "            if epoch.grow:\n",
    "                self.grow_wrapper(params)\n",
    "            \n",
    "            summed_loss, summed_metric = self.fit_single_epoch(params)\n",
    "\n",
    "            if epoch.prune:\n",
    "                self.prune_wrapper(params, summed_loss, summed_metric)\n",
    "            else:\n",
    "                loss, metric, val_loss, val_metric = self.print_epoch_statistics(params, summed_loss, summed_metric, require_result=True)\n",
    "                self.update_history(params, loss, metric, val_loss, val_metric)\n",
    "        \n",
    "        return params.history\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# HELPER FUNCTIONS\n",
    "################################################################################\n",
    "\n",
    "\n",
    "def get_statistics_from_history(history):\n",
    "    best_epoch_number = np.argmin(history['val_loss'])\n",
    "    best_val_loss = history['val_loss'][best_epoch_number]\n",
    "    best_val_metric = history['val_metric'][best_epoch_number]\n",
    "    best_hidden_layer_sizes = history['hidden_layer_sizes'][best_epoch_number]\n",
    "    return best_val_loss, best_val_metric, best_hidden_layer_sizes\n",
    "\n",
    "\n",
    "def get_statistics_from_histories(histories):\n",
    "    best_val_losses = list()\n",
    "    best_val_metrics = list()\n",
    "    all_best_hidden_layer_sizes = list()\n",
    "\n",
    "    for history in histories:\n",
    "        best_val_loss, best_val_metric, best_hidden_layer_sizes = get_statistics_from_history(history)\n",
    "        best_val_losses.append(best_val_loss)\n",
    "        best_val_metrics.append(best_val_metric)\n",
    "        all_best_hidden_layer_sizes.append(best_hidden_layer_sizes)\n",
    "    \n",
    "    mean_best_val_loss = np.mean(best_val_losses)\n",
    "    mean_best_val_metric = np.mean(best_val_metrics)\n",
    "    mean_best_hidden_layer_sizes = [np.mean(layer) for layer in list(zip(*all_best_hidden_layer_sizes))]\n",
    "    \n",
    "    return mean_best_val_loss, mean_best_val_metric, mean_best_hidden_layer_sizes\n",
    "\n",
    "\n",
    "def cross_validate(train_fn, x, y, n_splits, random_state=42, *args, **kwargs):\n",
    "    from sklearn.model_selection import KFold\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "    histories = list()\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(x)):\n",
    "        xtrain, xtest = x[train_index], x[test_index]\n",
    "        ytrain, ytest = y[train_index], y[test_index]\n",
    "\n",
    "        history = train_fn(xtrain, ytrain, validation_data=(xtest, ytest), *args, **kwargs)\n",
    "        histories.append(history)\n",
    "\n",
    "        best_val_loss, best_val_metric, best_hidden_layer_sizes = get_statistics_from_history(history)\n",
    "        print(f\"Run {i} completed, best_val_loss: {best_val_loss}, best_val_metric: {best_val_metric}, best_hidden_layer_sizes: {best_hidden_layer_sizes}\")\n",
    "\n",
    "    mean_best_val_loss, mean_best_val_metric, mean_best_hidden_layer_sizes = get_statistics_from_histories(histories)\n",
    "    print(f'mean_best_val_loss: {mean_best_val_loss}')\n",
    "    print(f'mean_best_val_metric: {mean_best_val_metric}')\n",
    "    print(f'mean_best_hidden_layer_sizes: {mean_best_hidden_layer_sizes}')\n",
    "\n",
    "    return histories, mean_best_hidden_layer_sizes\n",
    "\n",
    "\n",
    "def hyperparameter_search(train_fn, x, y, validation_data, *args, **kwargs):\n",
    "    from itertools import product\n",
    "\n",
    "    all_params = [*args] + list(kwargs.values())\n",
    "    histories = list()\n",
    "\n",
    "    best_overall_val_loss = np.inf\n",
    "    best_overall_val_metric = None\n",
    "    best_overall_combination = None\n",
    "\n",
    "    for combination in product(*all_params):\n",
    "        combination_args = combination[:len(args)]\n",
    "\n",
    "        combination_kwargs_values = combination[len(args):]\n",
    "        combination_kwargs = dict(zip(kwargs.keys(), combination_kwargs_values))\n",
    "\n",
    "        history = train_fn(x, y, validation_data, *combination_args, **combination_kwargs)\n",
    "        history['parameters'] = combination\n",
    "        histories.append(history)\n",
    "\n",
    "        best_val_loss, best_val_metric, best_hidden_layer_sizes = get_statistics_from_history(history)\n",
    "        print(f\"Run with parameters {combination} completed, best_val_loss: {best_val_loss}, best_val_metric: {best_val_metric}, best_hidden_layer_sizes: {best_hidden_layer_sizes}\")\n",
    "\n",
    "        if best_val_loss < best_overall_val_loss:\n",
    "            best_overall_val_loss = best_val_loss\n",
    "            best_overall_val_metric = best_val_metric\n",
    "            best_overall_combination = combination\n",
    "    \n",
    "    print(f'Best overall combination: {best_overall_combination}, val_metric: {best_overall_val_metric}')\n",
    "\n",
    "    return histories, best_overall_combination\n",
    "\n",
    "\n",
    "def get_convolutional_model(x, layer_sizes, output_neurons=10):\n",
    "    model = Sequential([\n",
    "        Conv2D(layer_sizes[0], filter_size=(3, 3), activation='selu', strides=(1, 1), padding='SAME', kernel_initializer='lecun_normal', input_shape=x[0,:,:,:].shape),\n",
    "        Conv2D(layer_sizes[1], filter_size=(3, 3), activation='selu', strides=(2, 2), padding='SAME', kernel_initializer='lecun_normal'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        Conv2D(layer_sizes[2], filter_size=(3, 3), activation='selu', strides=(1, 1), padding='SAME', kernel_initializer='lecun_normal'),\n",
    "        Conv2D(layer_sizes[3], filter_size=(3, 3), activation='selu', strides=(2, 2), padding='SAME', kernel_initializer='lecun_normal'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        Flatten(),\n",
    "        Dense(layer_sizes[4], activation='selu', kernel_initializer='lecun_normal'),\n",
    "        Dense(output_neurons, activation='softmax', fixed_size=True),\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_dense_model(x, layer_sizes):\n",
    "    layers = list()\n",
    "    \n",
    "    layers.append(Dense(layer_sizes[0], activation='selu', kernel_initializer='lecun_normal', input_shape=x[0, :].shape))\n",
    "    for layer_size in layer_sizes[1:]:\n",
    "        layers.append(Dense(layer_size, activation='selu', kernel_initializer='lecun_normal'))\n",
    "    layers.append(Dense(1, activation=None, kernel_initializer='lecun_normal', fixed_size=True))\n",
    "    \n",
    "    model = Sequential(layers)\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_fn_conv(x, y, validation_data, learning_rate, schedule, layer_sizes, output_neurons=10, min_new_neurons=20, \n",
    "             growth_percentage=0.2, verbose=False, use_static_graph=True):\n",
    "    batch_size = 128\n",
    "\n",
    "    model = get_convolutional_model(x, layer_sizes, output_neurons)\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    history = model.fit(x=x, y=y, optimizer=optimizer, schedule=schedule, batch_size=batch_size, min_new_neurons=min_new_neurons, \n",
    "                        validation_data=validation_data, growth_percentage=growth_percentage, verbose=verbose, use_static_graph=use_static_graph)\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "def squared_error(y_true, y_pred):\n",
    "    return (y_true - y_pred) ** 2\n",
    "\n",
    "\n",
    "def train_fn_dense(x, y, validation_data, learning_rate, schedule, layer_sizes, min_new_neurons=20, \n",
    "             growth_percentage=0.2, verbose=False, use_static_graph=True):\n",
    "    batch_size = 128\n",
    "\n",
    "    model = get_dense_model(x, layer_sizes)\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    history = model.fit(x=x, y=y, optimizer=optimizer, schedule=schedule, batch_size=batch_size, min_new_neurons=min_new_neurons, \n",
    "                        validation_data=validation_data, growth_percentage=growth_percentage, verbose=verbose, use_static_graph=use_static_graph,\n",
    "                        loss_fn=squared_error, metric_fn=squared_error)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Individual:\n",
    "    def __init__(self, genome, model, optimizer, history=None, age=0):\n",
    "        self.genome = genome\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        if history is not None:\n",
    "            self.history = history\n",
    "        else:\n",
    "            self.history = Sequential.ParameterContainer.prepare_history()\n",
    "        self.age = age\n",
    "    \n",
    "    def copy(self):\n",
    "        genome_copy = self.genome.copy()\n",
    "        model_copy = self.model.copy()\n",
    "        optimizer_copy = copy.deepcopy(self.optimizer)\n",
    "        history_copy = copy.deepcopy(self.history)\n",
    "        individual_copy = Individual(genome_copy, model_copy, optimizer_copy, history_copy, self.age)\n",
    "        return individual_copy\n",
    "    \n",
    "    def mutate(self, mutation_strength):\n",
    "        self.model.mutate(mutation_strength)\n",
    "        self.age = 0\n",
    "    \n",
    "    def correct(self):\n",
    "        self.genome[0] = max(self.genome[0], 2.5)  # Regularization penalty\n",
    "        self.genome[1] = np.clip(self.genome[1], 0.1, 1)  # Dataset sample size\n",
    "    \n",
    "    def get_fitness(self):\n",
    "        return self.history['val_metric'][-1]\n",
    "    \n",
    "    def get_regularization_penalty(self):\n",
    "        return 10. ** -self.genome[0]\n",
    "    \n",
    "    def get_dataset_sample_size(self):\n",
    "        return self.genome[1]\n",
    "    \n",
    "    def get_hidden_layer_sizes(self):\n",
    "        return self.history['hidden_layer_sizes'][-1]\n",
    "\n",
    "\n",
    "def initialize_population(population_size, x, layer_sizes, output_neurons, learning_rate):\n",
    "    population = list()\n",
    "    for _ in range(population_size):\n",
    "        genome = np.array([3, 0.1])\n",
    "        model = get_convolutional_model(x, layer_sizes, output_neurons=output_neurons)\n",
    "        model.build(x.shape)\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        individual = Individual(genome, model, optimizer)\n",
    "        population.append(individual)\n",
    "    return population\n",
    "\n",
    "\n",
    "def get_best_individual(population):\n",
    "    best_individual = None\n",
    "    best_fitness = - np.inf\n",
    "    for individual in population:\n",
    "        fitness = individual.get_fitness()\n",
    "        if fitness > best_fitness:\n",
    "            best_individual = individual\n",
    "            best_fitness = fitness\n",
    "    return best_individual\n",
    "\n",
    "\n",
    "def crossover(population, n_parents, strategy):\n",
    "    novel_population = list()\n",
    "    for individual in population:\n",
    "        parents_selection = np.random.choice(list(range(len(population))), size=n_parents, replace=False)\n",
    "        parent_genomes = [population[index].genome for index in parents_selection]\n",
    "        offspring_genome = np.mean(np.vstack(parent_genomes), axis=0)\n",
    "        offspring = individual.copy()\n",
    "        offspring.genome = offspring_genome\n",
    "        offspring.genome += np.random.normal(0, 1, offspring.genome.shape) * strategy\n",
    "        novel_population.append(offspring)\n",
    "    return population + novel_population\n",
    "\n",
    "\n",
    "def correct(population):\n",
    "    for individual in population:\n",
    "        individual.correct()\n",
    "    return population\n",
    "\n",
    "\n",
    "# def mutation(population, mutation_strength):\n",
    "#     new_population = list()\n",
    "#     for individual in population:\n",
    "#         individual_copy = individual.copy()\n",
    "#         individual_copy.mutate(mutation_strength)\n",
    "#         new_population.extend([individual, individual_copy])\n",
    "#     return new_population\n",
    "\n",
    "\n",
    "def extend_history(old_history, new_history):\n",
    "    for key in old_history.keys():\n",
    "        old_history[key].extend(new_history[key])\n",
    "\n",
    "\n",
    "def training(population, x, y, validation_data, batch_size, min_new_neurons, growth_percentage, verbose, use_static_graph):\n",
    "    for individual in population:\n",
    "        model = individual.model\n",
    "        optimizer = individual.optimizer\n",
    "        # schedule = Schedule([StaticEpochNoRegularization()])\n",
    "        schedule = Schedule([DynamicEpoch(individual.get_regularization_penalty(), 'weighted_l1')])\n",
    "        # x_train_sample, y_train_sample = get_dataset_sample(x, y, individual.get_dataset_sample_size())\n",
    "        x_train_sample, y_train_sample = get_dataset_sample(x, y, 0.1)\n",
    "        # x_test_sample, y_test_sample = get_dataset_sample(validation_data[0], validation_data[1], individual.get_dataset_sample_size())\n",
    "        x_test_sample, y_test_sample = get_dataset_sample(validation_data[0], validation_data[1], 0.1, seed=42)\n",
    "        history = model.fit(x=x_train_sample, y=y_train_sample, optimizer=optimizer, schedule=schedule, batch_size=batch_size, \n",
    "                            min_new_neurons=min_new_neurons, validation_data=(x_test_sample, y_test_sample), growth_percentage=growth_percentage, \n",
    "                            verbose=verbose, use_static_graph=use_static_graph)\n",
    "        extend_history(individual.history, history)\n",
    "        individual.age += 1\n",
    "    return population\n",
    "\n",
    "\n",
    "def tournament_selection(population, population_size, tournament_size):\n",
    "    new_population = list()\n",
    "\n",
    "    while len(new_population) < population_size:\n",
    "        selection = np.random.choice(list(range(len(population))), size=tournament_size, replace=False)\n",
    "        best_individual = None\n",
    "        best_fitness = - np.inf\n",
    "        for individual_index in selection:\n",
    "            individual = population[individual_index]\n",
    "            fitness = individual.get_fitness()\n",
    "            if fitness > best_fitness:\n",
    "                best_individual = individual\n",
    "                best_fitness = fitness\n",
    "        new_population.append(best_individual.copy())\n",
    "\n",
    "    return new_population\n",
    "\n",
    "\n",
    "def measure_fitnesses(population):\n",
    "    fitnesses = list()\n",
    "    for individual in population:\n",
    "        fitnesses.append(individual.get_fitness())\n",
    "    return fitnesses\n",
    "\n",
    "\n",
    "def interruptible(f):\n",
    "    def function(*args, **kwargs):\n",
    "        try:\n",
    "            return f(*args, **kwargs)\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Interrupted by user.\")\n",
    "    return function\n",
    "\n",
    "\n",
    "def print_generation_statistics(generation, population, duration):\n",
    "    sorted_population = sorted(population, key=lambda x: x.get_fitness(), reverse=True)\n",
    "    individuals = [(individual.age, round(individual.get_fitness(), 4), round(individual.genome[0], 1), round(individual.genome[1], 2), individual.get_hidden_layer_sizes()) for individual in sorted_population]\n",
    "    print(f\"Generation {generation}: {round(duration, 1)} s, {individuals}\")\n",
    "\n",
    "\n",
    "@interruptible\n",
    "def evolution(x, y, validation_data, batch_size, layer_sizes, output_neurons, learning_rate, n_parents, strategy, population_size=10, n_generations=10, \n",
    "              tournament_size=3, elitism=True, min_new_neurons=20, growth_percentage=0.2):\n",
    "    population = initialize_population(population_size, x, layer_sizes, output_neurons, learning_rate)\n",
    "    best_individual = None\n",
    "    fitnesses_history = list()\n",
    "    for generation in range(n_generations):\n",
    "        start_time = time.time()\n",
    "        population = crossover(population, n_parents, strategy)\n",
    "        population = correct(population)\n",
    "        # population = mutation(population, mutation_strength)\n",
    "        population = training(population, x, y, validation_data, batch_size, min_new_neurons, \n",
    "                              growth_percentage, verbose=False, use_static_graph=True)\n",
    "        population = tournament_selection(population, population_size, tournament_size)\n",
    "        if elitism:\n",
    "            if best_individual is not None:\n",
    "                population.append(best_individual)\n",
    "            best_individual = get_best_individual(population).copy()\n",
    "        fitnesses = measure_fitnesses(population)\n",
    "        duration = time.time() - start_time\n",
    "        print_generation_statistics(generation, population, duration)\n",
    "        # fitnesses_history.append(fitnesses)\n",
    "\n",
    "    # best_individual = get_best_individual(population)\n",
    "    # fitness = best_individual.get_fitness()\n",
    "\n",
    "    # return fitness, fitnesses_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground for standalone models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fashion_mnist = get_fashion_mnist_dataset(fraction=0.1)\n",
    "cifar10 = get_cifar_10_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_convolutional_model(cifar10.X_train_norm, [20, 20, 20, 20, 20], output_neurons=10)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build called\n",
      "Build called\n",
      "Build called\n",
      "Build called\n",
      "##########################################################\n",
      "Epoch 1/3\n",
      "Before growing:\n",
      "loss: None - metric: None - val_loss: 3.0269274711608887 - val_metric: 0.09990300678952474 - penalty: 0.001\n",
      "hidden layer sizes: [20, 20, 20, 20, 20], total units: 100\n",
      "After growing:\n",
      "loss: None - metric: None - val_loss: 3.0269272327423096 - val_metric: 0.09990300678952474 - penalty: 0.001\n",
      "hidden layer sizes: [40, 40, 40, 40, 40], total units: 200\n",
      "Before pruning:\n",
      "loss: 2.3111469745635986 - metric: 0.2078595608472824 - val_loss: 1.9085042476654053 - val_metric: 0.30940834141610085 - penalty: 0.001\n",
      "hidden layer sizes: [40, 40, 40, 40, 40], total units: 200\n",
      "After pruning:\n",
      "loss: None - metric: None - val_loss: 1.908506155014038 - val_metric: 0.30940834141610085 - penalty: 0.001\n",
      "hidden layer sizes: [20, 20, 20, 20, 20], total units: 100\n",
      "##########################################################\n",
      "Epoch 2/3\n",
      "Before growing:\n",
      "loss: None - metric: None - val_loss: 1.908506155014038 - val_metric: 0.30940834141610085 - penalty: 0.001\n",
      "hidden layer sizes: [20, 20, 20, 20, 20], total units: 100\n",
      "After growing:\n",
      "loss: None - metric: None - val_loss: 1.908506155014038 - val_metric: 0.30940834141610085 - penalty: 0.001\n",
      "hidden layer sizes: [40, 40, 40, 40, 40], total units: 200\n",
      "Before pruning:\n",
      "loss: 1.9064066410064697 - metric: 0.31757429242134094 - val_loss: 1.8859540224075317 - val_metric: 0.3258971871968962 - penalty: 0.001\n",
      "hidden layer sizes: [40, 40, 40, 40, 40], total units: 200\n",
      "After pruning:\n",
      "loss: None - metric: None - val_loss: 1.8859440088272095 - val_metric: 0.3258971871968962 - penalty: 0.001\n",
      "hidden layer sizes: [20, 20, 20, 20, 20], total units: 100\n",
      "##########################################################\n",
      "Epoch 3/3\n",
      "Before growing:\n",
      "loss: None - metric: None - val_loss: 1.8859440088272095 - val_metric: 0.3258971871968962 - penalty: 0.001\n",
      "hidden layer sizes: [20, 20, 20, 20, 20], total units: 100\n",
      "After growing:\n",
      "loss: None - metric: None - val_loss: 1.8859440088272095 - val_metric: 0.3258971871968962 - penalty: 0.001\n",
      "hidden layer sizes: [40, 40, 40, 40, 40], total units: 200\n",
      "Before pruning:\n",
      "loss: 1.8920725584030151 - metric: 0.3297426700592041 - val_loss: 1.8531503677368164 - val_metric: 0.3501454898157129 - penalty: 0.001\n",
      "hidden layer sizes: [40, 40, 40, 40, 40], total units: 200\n",
      "After pruning:\n",
      "loss: None - metric: None - val_loss: 1.8531984090805054 - val_metric: 0.34917555771096026 - penalty: 0.001\n",
      "hidden layer sizes: [20, 20, 19, 15, 20], total units: 94\n",
      "CPU times: user 20 s, sys: 1.29 s, total: 21.2 s\n",
      "Wall time: 20.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "schedule = Schedule([DynamicEpoch(0.001, 'weighted_l1')] * 3)\n",
    "history = model.fit(x=cifar10.X_train_norm, y=cifar10.y_train, optimizer=optimizer, schedule=schedule, batch_size=32, min_new_neurons=20, \n",
    "                    validation_data=(cifar10.X_test_norm, cifar10.y_test), growth_percentage=0.2, verbose=True, \n",
    "                    use_static_graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 0.3259, 3), (1, 0.3288, 3), (1, 0.3453, 3.108894334009524), (1, 0.3579, 2.9342262850188043), (1, 0.3278, 2.8255903530138333), (1, 0.3366, 3.4943646588916417), (1, 0.3579, 2.9342262850188043), (1, 0.3366, 3.4943646588916417), (1, 0.3579, 2.9342262850188043), (1, 0.3288, 3)], 114.6 s\n",
      "[(2, 0.3725, 2.9342262850188043), (2, 0.3676, 2.799887138119949), (2, 0.3783, 3.4943646588916417), (2, 0.3715, 2.9575499060170576), (2, 0.3705, 3), (2, 0.3948, 2.9342262850188043), (2, 0.3948, 2.9342262850188043), (2, 0.3705, 3), (2, 0.3725, 2.9342262850188043), (2, 0.3695, 3.549870512944016), (1, 0.3579, 2.9342262850188043)], 110.5 s\n",
      "[(3, 0.3744, 2.872232056725549), (3, 0.3851, 2.9126201357924115), (3, 0.387, 3.029669968015206), (3, 0.4016, 2.9342262850188043), (3, 0.4016, 2.9342262850188043), (3, 0.3957, 2.9829794515015466), (3, 0.387, 3.029669968015206), (3, 0.3695, 2.9575499060170576), (3, 0.3851, 2.9126201357924115), (3, 0.4016, 2.9342262850188043), (2, 0.3948, 2.9342262850188043)], 132.7 s\n",
      "[(4, 0.4083, 3.227785019171855), (4, 0.3521, 2.4683878781638846), (4, 0.3841, 2.8642451743703736), (4, 0.4093, 2.9829794515015466), (4, 0.387, 2.7079025246718), (4, 0.4035, 2.9342262850188043), (4, 0.3802, 2.9126201357924115), (4, 0.4035, 2.9342262850188043), (4, 0.4083, 3.227785019171855), (4, 0.4093, 2.9829794515015466), (3, 0.4016, 2.9342262850188043)], 144.0 s\n",
      "[(5, 0.3919, 3.227785019171855), (4, 0.3831, 2.9342262850188043), (5, 0.387, 3.112981018868467), (5, 0.3938, 2.9829794515015466), (5, 0.3802, 3.430950364608639), (5, 0.4016, 2.971296843844073), (5, 0.3919, 3.227785019171855), (4, 0.3831, 2.9342262850188043), (4, 0.3831, 2.9342262850188043), (5, 0.3899, 2.807157215303264), (4, 0.4093, 2.9829794515015466)], 130.7 s\n",
      "[(5, 0.3569, 3.1949051730734794), (5, 0.3598, 3.509979859982562), (5, 0.3899, 2.9342262850188043), (5, 0.3899, 2.9342262850188043), (5, 0.3889, 2.9089853736502342), (6, 0.4054, 2.971296843844073), (6, 0.3666, 3.224042687957389), (6, 0.4054, 2.971296843844073), (5, 0.3899, 2.9342262850188043), (5, 0.3899, 2.9342262850188043), (4, 0.4093, 2.9829794515015466)], 147.6 s\n",
      "[(6, 0.3919, 3.3621560083853352), (6, 0.3618, 2.9342262850188043), (6, 0.3792, 3.1497732406101364), (5, 0.3919, 2.9829794515015466), (5, 0.3666, 3.184964423813257), (7, 0.3521, 2.971296843844073), (5, 0.3666, 3.184964423813257), (5, 0.3919, 2.9829794515015466), (7, 0.3521, 2.971296843844073), (6, 0.3783, 2.9342262850188043), (4, 0.4093, 2.9829794515015466)], 133.5 s\n",
      "[(7, 0.3637, 2.9342262850188043), (7, 0.3637, 2.9342262850188043), (7, 0.3395, 3.102596406706587), (6, 0.3404, 3.4121671731833683), (6, 0.3404, 3.4121671731833683), (7, 0.3385, 3.1497732406101364), (6, 0.3453, 2.9829794515015466), (7, 0.3424, 2.6627877505019195), (5, 0.4035, 3.2363781780535326), (6, 0.3288, 3.3505097253202463), (4, 0.4093, 2.9829794515015466)], 132.8 s\n",
      "[(6, 0.3007, 2.9514281224285965), (5, 0.3812, 2.9829794515015466), (6, 0.3007, 2.9514281224285965), (7, 0.3676, 3.3505097253202463), (5, 0.354, 3.2615033451706408), (6, 0.3007, 2.9514281224285965), (7, 0.2629, 3.229097151099772), (7, 0.3259, 2.874101608839575), (5, 0.3812, 2.9829794515015466), (7, 0.3259, 2.874101608839575), (4, 0.4093, 2.9829794515015466)], 162.7 s\n",
      "[(6, 0.3851, 2.9829794515015466), (7, 0.2842, 2.9514281224285965), (5, 0.3725, 2.9829794515015466), (8, 0.2619, 3.5229569794767626), (6, 0.3763, 2.8155047776592084), (5, 0.3725, 2.9829794515015466), (6, 0.3763, 2.8155047776592084), (5, 0.3725, 2.9829794515015466), (6, 0.3851, 2.9829794515015466), (5, 0.3802, 3.465511369288253), (4, 0.4093, 2.9829794515015466)], 150.8 s\n",
      "[(6, 0.3725, 2.8963892362600046), (6, 0.3948, 2.880195641918802), (5, 0.3589, 2.9829794515015466), (7, 0.324, 2.9829794515015466), (5, 0.3589, 2.9829794515015466), (6, 0.3948, 2.880195641918802), (7, 0.2696, 3.373978628713137), (7, 0.2881, 2.8155047776592084), (6, 0.3948, 2.880195641918802), (5, 0.3589, 2.9829794515015466), (4, 0.4093, 2.9829794515015466)], 136.5 s\n",
      "[(6, 0.3676, 2.946547150799923), (5, 0.3618, 3.153293763218284), (6, 0.3637, 2.9829794515015466), (6, 0.3637, 2.9829794515015466), (7, 0.3608, 3.0884545149968), (6, 0.3676, 2.8232476515797527), (6, 0.2919, 2.9829794515015466), (5, 0.3618, 3.153293763218284), (7, 0.3094, 2.6948208167643135), (7, 0.2386, 2.880195641918802), (4, 0.4093, 2.9829794515015466)], 150.8 s\n",
      "[(7, 0.3899, 2.9829794515015466), (5, 0.3531, 3.05763661852348), (5, 0.3783, 2.9829794515015466), (7, 0.2745, 2.9829794515015466), (6, 0.3249, 3.153293763218284), (7, 0.3899, 2.9829794515015466), (5, 0.3783, 2.9829794515015466), (7, 0.3492, 2.9829794515015466), (7, 0.3725, 2.8232476515797527), (7, 0.3899, 2.9829794515015466), (4, 0.4093, 2.9829794515015466)], 142.1 s\n",
      "Interrupted by user.\n"
     ]
    }
   ],
   "source": [
    "evolution(x=cifar10.X_train_norm, y=cifar10.y_train, validation_data=(cifar10.X_test_norm, cifar10.y_test),\n",
    "          batch_size=32, layer_sizes=[20, 20, 20, 20, 20], output_neurons=10, learning_rate=0.0004, n_parents=5, mutation_strength=0.25, \n",
    "          population_size=10, n_generations=50, tournament_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 0.3191, 3), (1, 0.3375, 2.8411915083977264), (1, 0.3191, 3), (1, 0.3249, 3), (1, 0.356, 2.6695199944380263), (1, 0.3337, 2.710941564524781), (1, 0.3375, 2.8411915083977264), (1, 0.3337, 3.471949620719608), (1, 0.3249, 3), (1, 0.356, 2.6695199944380263)], 122.2 s\n",
      "[(2, 0.3851, 3.471949620719608), (2, 0.3666, 2.6420132063649913), (2, 0.3744, 3.294924807125864), (2, 0.3608, 2.797353499448219), (2, 0.3851, 3.471949620719608), (2, 0.3744, 3.294924807125864), (2, 0.3851, 3.471949620719608), (2, 0.3647, 2.710941564524781), (2, 0.3851, 3.471949620719608), (2, 0.3608, 2.797353499448219), (1, 0.356, 2.6695199944380263)], 129.8 s\n",
      "[(3, 0.3686, 2.7929601049219506), (3, 0.4054, 3.3158819187250446), (3, 0.3967, 2.6420132063649913), (3, 0.4054, 3.3158819187250446), (3, 0.3754, 2.9826830627293273), (3, 0.3928, 3.471949620719608), (3, 0.4054, 3.3158819187250446), (3, 0.3928, 3.471949620719608), (3, 0.3754, 3.471949620719608), (3, 0.3841, 3.294924807125864), (2, 0.3851, 3.471949620719608)], 124.5 s\n",
      "[(4, 0.3967, 3.3158819187250446), (4, 0.3996, 3.268966700756461), (4, 0.4151, 4.499666243883857), (4, 0.3919, 3.294924807125864), (4, 0.3889, 3.471949620719608), (4, 0.3996, 3.268966700756461), (4, 0.4142, 2.7370397763421024), (4, 0.3967, 3.3158819187250446), (4, 0.3889, 3.471949620719608), (4, 0.3754, 3.1101487322138603), (3, 0.4054, 3.3158819187250446)], 138.8 s\n",
      "[(5, 0.419, 3.99954267921139), (4, 0.4113, 3.3158819187250446), (5, 0.4151, 3.268966700756461), (5, 0.419, 3.99954267921139), (5, 0.4151, 3.268966700756461), (5, 0.4151, 3.268966700756461), (5, 0.4151, 3.268966700756461), (4, 0.4016, 2.9389896596209635), (5, 0.4113, 2.9066580640634037), (5, 0.4384, 3.8721899379193516), (4, 0.4151, 4.499666243883857)], 143.5 s\n",
      "[(6, 0.4025, 3.4959432908728654), (6, 0.4219, 3.268966700756461), (6, 0.4054, 4.004760316065422), (5, 0.4161, 3.738866056794125), (6, 0.4074, 2.8249858133162697), (6, 0.4025, 3.4959432908728654), (5, 0.4132, 4.499666243883857), (5, 0.4132, 4.499666243883857), (6, 0.418, 3.1688782007578786), (5, 0.4161, 3.738866056794125), (5, 0.4384, 3.8721899379193516)], 140.2 s\n",
      "[(6, 0.4103, 4.529514747578008), (7, 0.3948, 3.573091130023536), (7, 0.355, 3.43436826824461), (6, 0.4064, 3.738866056794125), (6, 0.4151, 4.499666243883857), (6, 0.4103, 4.529514747578008), (6, 0.3792, 3.738866056794125), (6, 0.4074, 4.0748957455051364), (7, 0.3948, 3.573091130023536), (6, 0.4151, 4.499666243883857), (5, 0.4384, 3.8721899379193516)], 137.5 s\n",
      "[(7, 0.3695, 4.529514747578008), (7, 0.3366, 3.738866056794125), (6, 0.3957, 3.8721899379193516), (7, 0.3734, 4.207459109786479), (6, 0.3957, 3.8721899379193516), (6, 0.3957, 3.8721899379193516), (7, 0.3734, 4.207459109786479), (7, 0.3783, 4.499666243883857), (7, 0.3783, 4.499666243883857), (6, 0.3957, 3.8721899379193516), (5, 0.4384, 3.8721899379193516)], 144.4 s\n",
      "[(7, 0.3948, 3.8787655450129144), (7, 0.3763, 3.8721899379193516), (8, 0.3375, 4.499666243883857), (7, 0.3763, 3.8721899379193516), (7, 0.4045, 3.8721899379193516), (8, 0.3608, 4.529514747578008), (6, 0.4006, 3.8721899379193516), (8, 0.3143, 4.207459109786479), (6, 0.4006, 3.8721899379193516), (7, 0.3948, 3.8787655450129144), (5, 0.4384, 3.8721899379193516)], 150.0 s\n",
      "[(8, 0.4045, 3.4545013092264534), (8, 0.3851, 3.8787655450129144), (7, 0.4239, 3.7295191297915413), (8, 0.3851, 3.8787655450129144), (7, 0.4239, 3.7295191297915413), (8, 0.3851, 3.8787655450129144), (6, 0.3957, 4.291350654370489), (8, 0.4045, 3.4545013092264534), (8, 0.3065, 3.6854780971286365), (9, 0.2929, 3.9113573246621494), (5, 0.4384, 3.8721899379193516)], 160.5 s\n",
      "[(9, 0.322, 3.6438792446347463), (8, 0.3744, 3.014170967962674), (8, 0.3763, 4.240225464195209), (7, 0.1746, 4.291350654370489), (8, 0.3763, 4.240225464195209), (6, 0.4035, 3.8721899379193516), (7, 0.3356, 3.988842487812914), (7, 0.1746, 4.291350654370489), (9, 0.1843, 3.8787655450129144), (9, 0.1057, 3.0163021148012796), (5, 0.4384, 3.8721899379193516)], 157.4 s\n",
      "[(6, 0.3773, 3.8721899379193516), (8, 0.1523, 4.229036340217173), (9, 0.3482, 3.014170967962674), (7, 0.3851, 3.8721899379193516), (10, 0.1707, 4.367291528549237), (6, 0.3773, 3.8721899379193516), (7, 0.3851, 3.8721899379193516), (9, 0.1668, 3.692466650173598), (6, 0.3986, 3.509467645865594), (9, 0.3482, 3.014170967962674), (5, 0.4384, 3.8721899379193516)], 159.8 s\n",
      "[(9, 0.194, 4.229036340217173), (7, 0.42, 3.509467645865594), (10, 0.1959, 4.454673958931548), (7, 0.3511, 3.8721899379193516), (7, 0.3511, 3.8721899379193516), (7, 0.3511, 3.8721899379193516), (7, 0.3472, 3.9871518477796726), (7, 0.42, 3.509467645865594), (7, 0.387, 3.425495411424366), (7, 0.387, 3.425495411424366), (5, 0.4384, 3.8721899379193516)], 171.9 s\n",
      "[(6, 0.3986, 3.8721899379193516), (6, 0.3986, 3.8721899379193516), (8, 0.2444, 3.509467645865594), (8, 0.3385, 4.31048279436771), (8, 0.2629, 3.9871518477796726), (8, 0.3162, 4.320815302082309), (8, 0.193, 3.425495411424366), (8, 0.3385, 4.31048279436771), (6, 0.3986, 3.8721899379193516), (6, 0.3986, 3.8721899379193516), (5, 0.4384, 3.8721899379193516)], 155.7 s\n",
      "[(9, 0.2211, 4.31048279436771), (7, 0.3569, 3.7355462004362336), (7, 0.3977, 3.8721899379193516), (6, 0.4035, 3.565542079738601), (6, 0.4035, 3.565542079738601), (7, 0.3482, 4.710065836394907), (9, 0.1232, 4.320815302082309), (6, 0.3666, 3.8721899379193516), (7, 0.3977, 3.8721899379193516), (6, 0.3666, 3.8721899379193516), (5, 0.4384, 3.8721899379193516)], 163.1 s\n",
      "[(8, 0.3385, 3.8721899379193516), (6, 0.4074, 3.52245178907164), (8, 0.356, 3.9548915532484568), (7, 0.356, 4.041192676178096), (6, 0.4074, 3.52245178907164), (7, 0.356, 4.041192676178096), (8, 0.356, 3.9548915532484568), (8, 0.3385, 3.8721899379193516), (7, 0.3773, 3.565542079738601), (6, 0.4074, 3.52245178907164), (5, 0.4384, 3.8721899379193516)], 170.0 s\n",
      "[(6, 0.2803, 3.8721899379193516), (7, 0.3822, 4.4304134922828915), (9, 0.3298, 3.590179439843744), (7, 0.4016, 3.52245178907164), (9, 0.3123, 3.9548915532484568), (7, 0.3822, 4.4304134922828915), (8, 0.3395, 3.4376281209774726), (7, 0.3957, 3.52245178907164), (8, 0.3404, 4.041192676178096), (9, 0.3123, 3.9548915532484568), (5, 0.4384, 3.8721899379193516)], 153.6 s\n",
      "[(8, 0.3356, 4.4304134922828915), (6, 0.3812, 3.8721899379193516), (6, 0.3967, 3.839554765195468), (8, 0.3356, 4.4304134922828915), (6, 0.3812, 3.8721899379193516), (8, 0.3181, 3.817855946989185), (9, 0.3317, 4.041192676178096), (8, 0.3356, 4.4304134922828915), (8, 0.2735, 3.52245178907164), (9, 0.3317, 4.041192676178096), (5, 0.4384, 3.8721899379193516)], 148.7 s\n",
      "[(6, 0.4132, 4.833064688013414), (7, 0.4306, 3.8721899379193516), (7, 0.3754, 3.03704879567952), (9, 0.1736, 4.121739196704333), (6, 0.4132, 4.833064688013414), (6, 0.4113, 3.8721899379193516), (6, 0.4132, 4.833064688013414), (9, 0.1785, 4.4304134922828915), (7, 0.3919, 5.498539192404672), (6, 0.4113, 3.8721899379193516), (5, 0.4384, 3.8721899379193516)], 160.6 s\n",
      "[(7, 0.42, 4.611885828896562), (7, 0.3851, 3.734881007383988), (7, 0.4093, 4.249565124956177), (7, 0.3967, 3.8721899379193516), (7, 0.4258, 3.8721899379193516), (6, 0.4074, 3.8148770421876406), (7, 0.42, 4.611885828896562), (7, 0.4258, 3.8721899379193516), (7, 0.3657, 4.833064688013414), (7, 0.3647, 4.833064688013414), (5, 0.4384, 3.8721899379193516)], 153.1 s\n",
      "[(8, 0.3618, 4.611885828896562), (6, 0.3686, 4.028711087971086), (8, 0.3113, 4.611885828896562), (8, 0.3618, 4.611885828896562), (8, 0.3831, 3.5184507073040967), (6, 0.3686, 4.028711087971086), (8, 0.3666, 3.8721899379193516), (6, 0.3686, 4.028711087971086), (7, 0.3657, 4.503996942207121), (8, 0.3831, 3.5184507073040967), (5, 0.4384, 3.8721899379193516)], 155.0 s\n",
      "[(6, 0.4268, 4.6129921215029155), (9, 0.2299, 3.5345966367359174), (7, 0.3521, 4.028711087971086), (7, 0.3725, 4.028711087971086), (7, 0.3715, 4.089970515574152), (6, 0.4268, 4.6129921215029155), (6, 0.4268, 4.6129921215029155), (8, 0.2551, 4.503996942207121), (7, 0.3715, 4.089970515574152), (7, 0.3919, 4.028711087971086), (5, 0.4384, 3.8721899379193516)], 156.7 s\n",
      "[(7, 0.354, 3.7897334410152874), (7, 0.3511, 4.6129921215029155), (7, 0.3511, 4.6129921215029155), (8, 0.3812, 4.028711087971086), (7, 0.354, 3.7897334410152874), (8, 0.3812, 4.028711087971086), (8, 0.3754, 4.089970515574152), (7, 0.3919, 4.6129921215029155), (8, 0.3812, 4.028711087971086), (7, 0.3919, 4.6129921215029155), (5, 0.4384, 3.8721899379193516)], 149.7 s\n",
      "Interrupted by user.\n"
     ]
    }
   ],
   "source": [
    "evolution(x=cifar10.X_train_norm, y=cifar10.y_train, validation_data=(cifar10.X_test_norm, cifar10.y_test),\n",
    "          batch_size=32, layer_sizes=[20, 20, 20, 20, 20], output_neurons=10, learning_rate=0.0004, n_parents=5, mutation_strength=0.5, \n",
    "          population_size=10, n_generations=50, tournament_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "bMrz149vtoST"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 0: 114.1 s, [(1, 0.3569, 2.4963026418315666, [40, 40, 40, 40, 40]), (1, 0.3569, 2.4963026418315666, [40, 40, 40, 40, 40]), (1, 0.356, 3.4262987166427443, [40, 40, 40, 40, 40]), (1, 0.356, 3.4262987166427443, [40, 40, 40, 40, 40]), (1, 0.356, 3.4262987166427443, [40, 40, 40, 40, 40]), (1, 0.3531, 3.1420714040802062, [40, 40, 40, 40, 40]), (1, 0.3453, 2.4988843363508497, [40, 40, 40, 40, 40]), (1, 0.3404, 3, [20, 20, 20, 20, 20]), (1, 0.3375, 3.0469875583662356, [40, 40, 40, 40, 40]), (1, 0.3366, 3, [20, 20, 20, 20, 20])]\n",
      "Generation 1: 110.7 s, [(2, 0.3889, 3.4262987166427443, [60, 60, 60, 60, 60]), (2, 0.3889, 3.4262987166427443, [60, 60, 60, 60, 60]), (2, 0.3889, 3.4262987166427443, [60, 60, 60, 60, 60]), (2, 0.3889, 3.4262987166427443, [60, 60, 60, 60, 60]), (2, 0.3773, 3.4262987166427443, [60, 60, 60, 60, 60]), (2, 0.3763, 3, [40, 40, 40, 40, 40]), (2, 0.3734, 2.4963026418315666, [60, 60, 60, 60, 60]), (2, 0.3734, 2.4963026418315666, [60, 60, 60, 60, 60]), (2, 0.3608, 2.7498754734324113, [40, 40, 40, 40, 40]), (2, 0.3598, 3.7021126820013013, [60, 60, 60, 60, 60]), (1, 0.3569, 2.4963026418315666, [40, 40, 40, 40, 40])]\n",
      "Generation 2: 116.0 s, [(3, 0.3977, 3.8881652351966687, [80, 80, 80, 80, 80]), (3, 0.3977, 3.8881652351966687, [80, 80, 80, 80, 80]), (3, 0.3948, 2.1910692635165283, [80, 80, 80, 80, 80]), (2, 0.3889, 3.4262987166427443, [60, 60, 60, 60, 60]), (3, 0.387, 2.4963026418315666, [80, 80, 80, 80, 80]), (3, 0.3802, 3.166392845294562, [80, 80, 80, 80, 80]), (3, 0.3802, 3.166392845294562, [80, 80, 80, 80, 80]), (3, 0.3763, 3.4262987166427443, [80, 80, 80, 80, 80]), (3, 0.3763, 2.7799070012545304, [80, 80, 80, 80, 80]), (3, 0.3763, 2.7799070012545304, [80, 80, 80, 80, 80]), (3, 0.3734, 3.4262987166427443, [80, 80, 80, 80, 80])]\n",
      "Generation 3: 128.1 s, [(4, 0.4035, 2.8815363582513958, [100, 100, 100, 100, 100]), (4, 0.4035, 2.8815363582513958, [100, 100, 100, 100, 100]), (3, 0.4025, 2.9938264253997358, [80, 80, 80, 80, 80]), (3, 0.4025, 2.9938264253997358, [80, 80, 80, 80, 80]), (3, 0.3977, 3.8881652351966687, [80, 80, 80, 80, 80]), (4, 0.3967, 2.3891565202782212, [100, 100, 100, 100, 100]), (4, 0.3889, 3.8881652351966687, [100, 100, 100, 100, 100]), (4, 0.3889, 3.8881652351966687, [100, 100, 100, 100, 100]), (4, 0.3831, 3.018046036297736, [100, 100, 100, 100, 100]), (4, 0.3822, 2.5045280463129287, [100, 100, 100, 100, 100]), (4, 0.3569, 2.7799070012545304, [100, 100, 100, 100, 100])]\n",
      "Generation 4: 118.6 s, [(5, 0.4093, 2.3891565202782212, [120, 120, 120, 120, 120]), (4, 0.4035, 2.8815363582513958, [100, 100, 100, 100, 100]), (5, 0.4006, 3.318410364163591, [120, 120, 120, 120, 120]), (5, 0.3967, 2.6141146285998484, [120, 120, 120, 120, 120]), (5, 0.387, 2.8815363582513958, [120, 120, 120, 120, 120]), (5, 0.3773, 2.0436038795127667, [120, 120, 120, 120, 120]), (4, 0.3773, 3.6856790279503935, [100, 100, 100, 100, 100]), (5, 0.3773, 2.0436038795127667, [120, 120, 120, 120, 120]), (4, 0.3754, 3.8881652351966687, [100, 100, 100, 100, 100]), (4, 0.3686, 2.9938264253997358, [100, 100, 100, 100, 100]), (5, 0.3579, 2.7799070012545304, [120, 120, 120, 120, 120])]\n",
      "Generation 5: 133.5 s, [(5, 0.4151, 2.767337069320188, [120, 120, 120, 120, 120]), (5, 0.4151, 2.767337069320188, [120, 120, 120, 120, 120]), (5, 0.4093, 2.3891565202782212, [120, 120, 120, 120, 120]), (6, 0.4016, 2.4816338644276073, [144, 144, 144, 144, 144]), (5, 0.3899, 2.9938264253997358, [120, 120, 120, 120, 120]), (5, 0.3899, 2.9938264253997358, [120, 120, 120, 120, 120]), (5, 0.3889, 3.8881652351966687, [120, 120, 120, 120, 120]), (6, 0.3802, 2.391440630727115, [144, 144, 144, 144, 144]), (5, 0.3773, 3.07449614659215, [120, 120, 120, 120, 120]), (5, 0.3773, 3.07449614659215, [120, 120, 120, 120, 120]), (5, 0.3511, 3.6856790279503935, [120, 120, 120, 120, 120])]\n",
      "Generation 6: 133.1 s, [(5, 0.4151, 2.767337069320188, [120, 120, 120, 120, 120]), (6, 0.388, 4.247550456296834, [144, 144, 144, 144, 144]), (6, 0.388, 4.247550456296834, [144, 144, 144, 144, 144]), (6, 0.3822, 2.9938264253997358, [144, 144, 144, 144, 144]), (6, 0.3492, 2.767337069320188, [144, 144, 144, 144, 144]), (6, 0.3453, 2.7864594503431794, [144, 144, 144, 144, 144]), (6, 0.3453, 2.7864594503431794, [144, 144, 144, 144, 144]), (6, 0.3443, 2.911330960445173, [144, 144, 144, 144, 144]), (7, 0.3317, 2.391440630727115, [172, 172, 172, 172, 172]), (6, 0.3201, 3.0115299367300072, [144, 144, 144, 144, 144]), (6, 0.3162, 3.132379846845131, [144, 144, 144, 144, 144])]\n",
      "Generation 7: 125.8 s, [(5, 0.4151, 2.767337069320188, [120, 120, 120, 120, 120]), (6, 0.354, 2.767337069320188, [144, 144, 144, 144, 144]), (6, 0.354, 2.767337069320188, [144, 144, 144, 144, 144]), (7, 0.3356, 2.9938264253997358, [172, 172, 172, 172, 172]), (7, 0.3356, 2.9938264253997358, [172, 172, 172, 172, 172]), (7, 0.3356, 2.9938264253997358, [172, 172, 172, 172, 172]), (7, 0.3152, 2.743270429393758, [172, 172, 172, 172, 172]), (7, 0.3036, 3.382178318644421, [172, 172, 172, 172, 172]), (7, 0.3036, 3.382178318644421, [172, 172, 172, 172, 172]), (7, 0.3007, 2.911330960445173, [172, 172, 172, 172, 172]), (7, 0.2522, 3.0115299367300072, [172, 172, 172, 172, 172])]\n",
      "Interrupted by user.\n"
     ]
    }
   ],
   "source": [
    "evolution(x=cifar10.X_train_norm, y=cifar10.y_train, validation_data=(cifar10.X_test_norm, cifar10.y_test),\n",
    "          batch_size=32, layer_sizes=[20, 20, 20, 20, 20], output_neurons=10, learning_rate=0.0004, n_parents=5, mutation_strength=0.5, \n",
    "          population_size=10, n_generations=50, tournament_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 0: 53.1 s, [(1, 0.3143, 3.2271322561109588, [20, 20, 20, 20, 20]), (1, 0.3104, 3.2296534408001984, [20, 20, 20, 20, 20]), (1, 0.3104, 3.4002216809995107, [20, 20, 20, 20, 20]), (1, 0.3055, 2.9638846470638054, [20, 20, 20, 20, 20]), (1, 0.3046, 2.81665912453963, [20, 20, 20, 20, 20]), (1, 0.2919, 2.884814783800078, [20, 20, 20, 20, 20]), (1, 0.2667, 2.5830946056579953, [20, 20, 20, 20, 20]), (1, 0.2367, 2.46487256174084, [20, 20, 20, 20, 20]), (1, 0.1862, 1.7808410753455155, [20, 20, 20, 20, 20]), (1, 0.1086, 1.4815408043116105, [20, 20, 20, 20, 20])]\n",
      "Generation 1: 52.3 s, [(2, 0.3608, 3.332707853826636, [20, 20, 20, 20, 20]), (2, 0.3472, 3.265129093010425, [20, 20, 20, 20, 20]), (2, 0.3356, 3.200016116022627, [20, 20, 20, 20, 20]), (2, 0.3327, 3.4195007865782054, [20, 20, 20, 20, 20]), (2, 0.3278, 2.7791936827722936, [20, 20, 20, 20, 20]), (2, 0.322, 3.9191264397952983, [20, 20, 20, 32, 35]), (2, 0.3172, 3.0819722447553786, [20, 20, 20, 20, 20]), (2, 0.3046, 3.099329038685261, [20, 20, 20, 20, 20]), (2, 0.3007, 2.8921526716276507, [20, 20, 20, 20, 20]), (2, 0.2638, 2.4554219552392307, [20, 20, 20, 20, 20])]\n",
      "Generation 2: 56.3 s, [(3, 0.3744, 3.2942267028107066, [20, 15, 19, 19, 22]), (3, 0.3657, 3.255013969352772, [20, 20, 17, 17, 20]), (3, 0.3657, 3.172965999860139, [20, 20, 20, 20, 20]), (3, 0.3647, 3.3334908943792634, [20, 20, 20, 20, 20]), (3, 0.3637, 3.7686998507518634, [20, 24, 20, 29, 33]), (3, 0.356, 3.1649895880610117, [20, 20, 20, 20, 20]), (3, 0.3501, 3.7016569845110845, [20, 20, 18, 21, 36]), (3, 0.3395, 3.3051621527528794, [20, 20, 17, 19, 21]), (3, 0.322, 2.9039362100107184, [20, 18, 17, 15, 20]), (3, 0.2949, 2.5120633097294736, [20, 4, 8, 4, 20])]\n",
      "Interrupted by user.\n"
     ]
    }
   ],
   "source": [
    "evolution(x=cifar10.X_train_norm, y=cifar10.y_train, validation_data=(cifar10.X_test_norm, cifar10.y_test),\n",
    "          batch_size=32, layer_sizes=[20, 20, 20, 20, 20], output_neurons=10, learning_rate=0.0004, n_parents=5, mutation_strength=0.5, \n",
    "          population_size=10, n_generations=50, tournament_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 0: 118.7 s, [(1, 0.3298, 3, [20, 20, 20, 20, 20]), (1, 0.3269, 3.154565372812397, [20, 20, 20, 20, 20]), (1, 0.3259, 3, [20, 20, 20, 20, 20]), (1, 0.3259, 3, [20, 20, 20, 20, 20]), (1, 0.3201, 2.691522902774604, [20, 20, 20, 20, 20]), (1, 0.3191, 3.55763229302829, [20, 20, 20, 20, 20]), (1, 0.3191, 3.55763229302829, [20, 20, 20, 20, 20]), (1, 0.3152, 3, [20, 20, 20, 20, 20]), (1, 0.3152, 3, [20, 20, 20, 20, 20]), (1, 0.3094, 3.506301457088, [20, 20, 20, 20, 20])]\n",
      "Generation 1: 134.2 s, [(2, 0.3773, 3.767503190943379, [21, 23, 20, 20, 30]), (2, 0.3773, 3.767503190943379, [21, 23, 20, 20, 30]), (2, 0.3754, 3.722830530857286, [20, 21, 20, 20, 20]), (2, 0.3754, 3.722830530857286, [20, 21, 20, 20, 20]), (2, 0.3618, 4.1395795306043945, [24, 37, 27, 40, 38]), (2, 0.3618, 4.1395795306043945, [24, 37, 27, 40, 38]), (2, 0.3598, 3, [20, 20, 20, 20, 20]), (2, 0.3531, 3.55763229302829, [20, 20, 20, 20, 20]), (2, 0.3482, 3, [20, 20, 20, 20, 20]), (2, 0.3463, 3.506301457088, [20, 20, 20, 20, 20]), (1, 0.3298, 3, [20, 20, 20, 20, 20])]\n",
      "Generation 2: 175.2 s, [(3, 0.3938, 4.88922458641407, [40, 41, 40, 40, 40]), (3, 0.3938, 4.88922458641407, [40, 41, 40, 40, 40]), (3, 0.3938, 4.88922458641407, [40, 41, 40, 40, 40]), (3, 0.386, 3.55763229302829, [20, 20, 20, 20, 26]), (3, 0.3831, 3.8120299123006025, [20, 26, 20, 33, 37]), (3, 0.3831, 3.8120299123006025, [20, 26, 20, 33, 37]), (3, 0.3831, 3.8120299123006025, [20, 26, 20, 33, 37]), (3, 0.3812, 3.722830530857286, [20, 21, 20, 20, 31]), (2, 0.3773, 3.767503190943379, [21, 23, 20, 20, 30]), (3, 0.3744, 3.7650498205742933, [20, 31, 20, 29, 39]), (2, 0.3521, 3, [20, 20, 20, 20, 20])]\n",
      "Generation 3: 211.0 s, [(4, 0.3986, 4.88922458641407, [60, 61, 60, 60, 60]), (4, 0.3986, 4.88922458641407, [60, 61, 60, 60, 60]), (4, 0.3986, 4.88922458641407, [60, 61, 60, 60, 60]), (4, 0.3957, 3.8120299123006025, [24, 27, 20, 27, 42]), (3, 0.3938, 4.88922458641407, [40, 41, 40, 40, 40]), (4, 0.3928, 4.296869798330454, [35, 45, 38, 53, 57]), (4, 0.3919, 3.720888615535871, [20, 21, 20, 23, 44]), (4, 0.3919, 3.720888615535871, [20, 21, 20, 23, 44]), (4, 0.3919, 3.720888615535871, [20, 21, 20, 23, 44]), (4, 0.3919, 3.720888615535871, [20, 21, 20, 23, 44]), (4, 0.3919, 4.030747764081939, [27, 33, 28, 51, 56])]\n",
      "Generation 4: 252.4 s, [(5, 0.4161, 4.374039538675792, [50, 64, 58, 73, 77]), (5, 0.4161, 4.374039538675792, [50, 64, 58, 73, 77]), (5, 0.4161, 4.374039538675792, [50, 64, 58, 73, 77]), (5, 0.4161, 4.374039538675792, [50, 64, 58, 73, 77]), (5, 0.4151, 4.030747764081939, [32, 48, 35, 62, 65]), (5, 0.4151, 4.030747764081939, [32, 48, 35, 62, 65]), (5, 0.4142, 4.88922458641407, [80, 81, 80, 80, 80]), (5, 0.4142, 4.88922458641407, [80, 81, 80, 80, 80]), (5, 0.4074, 3.815868790558817, [20, 20, 28, 32, 46]), (4, 0.3986, 4.88922458641407, [60, 61, 60, 60, 60]), (5, 0.3938, 3.720888615535871, [20, 22, 23, 28, 51])]\n",
      "Generation 5: 261.4 s, [(6, 0.4336, 3.8610983905162946, [26, 28, 26, 47, 65]), (6, 0.4297, 4.030747764081939, [40, 36, 41, 64, 75]), (6, 0.4297, 4.030747764081939, [40, 36, 41, 64, 75]), (6, 0.4297, 4.030747764081939, [40, 36, 41, 64, 75]), (6, 0.4287, 4.374039538675792, [42, 74, 66, 93, 97]), (6, 0.4258, 4.374039538675792, [39, 70, 68, 93, 96]), (6, 0.4258, 4.374039538675792, [39, 70, 68, 93, 96]), (6, 0.418, 4.374039538675792, [48, 62, 68, 92, 95]), (6, 0.418, 4.374039538675792, [48, 62, 68, 92, 95]), (5, 0.4161, 4.374039538675792, [50, 64, 58, 73, 77]), (6, 0.4064, 4.374039538675792, [60, 81, 74, 93, 97])]\n",
      "Generation 6: 309.2 s, [(7, 0.4413, 4.030747764081939, [44, 38, 44, 66, 90]), (7, 0.4413, 4.030747764081939, [44, 38, 44, 66, 90]), (7, 0.4374, 4.845294493704019, [60, 56, 61, 84, 95]), (7, 0.4374, 4.116079321221229, [21, 27, 39, 64, 84]), (6, 0.4336, 3.8610983905162946, [26, 28, 26, 47, 65]), (7, 0.4316, 4.030747764081939, [28, 35, 45, 56, 91]), (7, 0.4316, 4.030747764081939, [28, 35, 45, 56, 91]), (7, 0.4287, 4.687518929273031, [78, 99, 94, 113, 117]), (7, 0.4277, 4.374039538675792, [53, 77, 80, 111, 117]), (7, 0.4277, 4.374039538675792, [53, 77, 80, 111, 117]), (7, 0.4258, 3.3741932802757884, [20, 20, 20, 20, 38])]\n",
      "Generation 7: 344.8 s, [(8, 0.4597, 4.116079321221229, [29, 29, 43, 77, 102]), (8, 0.4597, 4.116079321221229, [29, 29, 43, 77, 102]), (8, 0.4597, 4.116079321221229, [29, 29, 43, 77, 102]), (8, 0.4597, 4.116079321221229, [29, 29, 43, 77, 102]), (8, 0.4578, 3.5486041133683877, [20, 20, 20, 21, 52]), (8, 0.4578, 3.5486041133683877, [20, 20, 20, 21, 52]), (8, 0.4578, 3.5486041133683877, [20, 20, 20, 21, 52]), (7, 0.4413, 4.030747764081939, [44, 38, 44, 66, 90]), (8, 0.4403, 4.211359856288451, [35, 40, 40, 40, 58]), (8, 0.4306, 4.419719633746534, [40, 46, 59, 84, 104]), (8, 0.4287, 4.030747764081939, [23, 28, 39, 63, 104])]\n",
      "Generation 8: 337.4 s, [(9, 0.4753, 4.030747764081939, [34, 32, 36, 66, 112]), (9, 0.4743, 4.116079321221229, [21, 29, 38, 80, 105]), (9, 0.4704, 4.116079321221229, [34, 35, 46, 84, 112]), (9, 0.4704, 4.116079321221229, [34, 35, 46, 84, 112]), (9, 0.4656, 4.116079321221229, [41, 32, 49, 75, 115]), (9, 0.4656, 4.116079321221229, [41, 32, 49, 75, 115]), (9, 0.4656, 4.116079321221229, [41, 32, 49, 75, 115]), (8, 0.4597, 4.116079321221229, [29, 29, 43, 77, 102]), (9, 0.4588, 4.172448291198385, [39, 42, 57, 82, 118]), (9, 0.4568, 3.772076012496182, [21, 21, 27, 28, 66]), (9, 0.4452, 4.187826731533549, [37, 39, 40, 41, 72])]\n",
      "Generation 9: 396.2 s, [(9, 0.4753, 4.030747764081939, [34, 32, 36, 66, 112]), (10, 0.4704, 4.187826731533549, [41, 46, 57, 61, 92]), (10, 0.4704, 4.187826731533549, [41, 46, 57, 61, 92]), (10, 0.4685, 4.116079321221229, [38, 37, 59, 79, 127]), (10, 0.4685, 4.116079321221229, [38, 37, 59, 79, 127]), (10, 0.4646, 4.172448291198385, [25, 40, 68, 92, 135]), (10, 0.4636, 4.116079321221229, [32, 35, 45, 87, 112]), (10, 0.4636, 4.116079321221229, [32, 35, 45, 87, 112]), (10, 0.4578, 4.116079321221229, [34, 25, 57, 89, 120]), (10, 0.452, 4.130731539238547, [34, 42, 34, 88, 127]), (10, 0.452, 4.9298323475588655, [59, 62, 77, 102, 140])]\n",
      "Interrupted by user.\n"
     ]
    }
   ],
   "source": [
    "evolution(x=cifar10.X_train_norm, y=cifar10.y_train, validation_data=(cifar10.X_test_norm, cifar10.y_test),\n",
    "          batch_size=32, layer_sizes=[20, 20, 20, 20, 20], output_neurons=10, learning_rate=0.0004, n_parents=5, mutation_strength=0.5, \n",
    "          population_size=10, n_generations=50, tournament_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-21 22:43:04.038726: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-21 22:43:04.572366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-21 22:43:04.573003: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-21 22:43:04.573442: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-21 22:43:04.573758: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-21 22:43:04.574047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-21 22:43:04.574340: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-21 22:43:06.747205: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-21 22:43:06.747816: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-21 22:43:06.748177: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-21 22:43:06.748550: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-21 22:43:06.748878: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-21 22:43:06.749378: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 11832 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:01:01.0, compute capability: 7.0\n",
      "2022-03-21 22:43:06.751257: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-21 22:43:06.751623: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 30984 MB memory:  -> device: 1, name: Tesla V100-PCIE-32GB, pci bus id: 0000:01:02.0, compute capability: 7.0\n",
      "2022-03-21 22:43:10.422293: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8301\n",
      "2022-03-21 22:43:12.663409: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 0: 130.2 s, [(1, 0.3366, 3.0, 0.1, [20, 20, 20, 20, 20]), (1, 0.3366, 3.0, 0.1, [20, 20, 20, 20, 20]), (1, 0.3366, 3.0, 0.1, [20, 20, 20, 20, 20]), (1, 0.3346, 2.9, 0.1, [20, 20, 20, 20, 20]), (1, 0.3269, 2.8, 0.1, [20, 20, 20, 20, 20]), (1, 0.3269, 2.8, 0.1, [20, 20, 20, 20, 20]), (1, 0.3269, 2.8, 0.1, [20, 20, 20, 20, 20]), (1, 0.324, 3.0, 0.1, [20, 20, 20, 20, 20]), (1, 0.3201, 2.9, 0.1, [20, 20, 20, 20, 20]), (1, 0.3104, 3.0, 0.1, [20, 20, 20, 20, 20])]\n",
      "Generation 1: 151.9 s, [(2, 0.3831, 3.9, 0.19, [20, 21, 20, 25, 29]), (2, 0.3666, 3.4, 0.1, [20, 20, 20, 20, 20]), (2, 0.3453, 3.0, 0.1, [20, 20, 20, 20, 20]), (2, 0.3453, 3.0, 0.1, [20, 20, 20, 20, 20]), (2, 0.3375, 3.0, 0.1, [20, 20, 20, 20, 20]), (2, 0.3375, 3.0, 0.1, [20, 20, 20, 20, 20]), (2, 0.3375, 3.0, 0.1, [20, 20, 20, 20, 20]), (2, 0.3366, 3.0, 0.1, [20, 20, 20, 20, 20]), (1, 0.3366, 3.0, 0.1, [20, 20, 20, 20, 20]), (2, 0.322, 2.8, 0.1, [20, 20, 20, 20, 20]), (2, 0.322, 2.8, 0.1, [20, 20, 20, 20, 20])]\n",
      "Generation 2: 159.3 s, [(3, 0.4006, 3.9, 0.19, [20, 21, 20, 32, 38]), (3, 0.4006, 3.9, 0.19, [20, 21, 20, 32, 38]), (2, 0.3831, 3.9, 0.19, [20, 21, 20, 25, 29]), (3, 0.3657, 3.4, 0.1, [20, 20, 20, 20, 20]), (3, 0.3657, 3.4, 0.1, [20, 20, 20, 20, 20]), (3, 0.3618, 3.0, 0.1, [20, 20, 19, 18, 20]), (3, 0.3579, 3.6, 0.11, [20, 20, 20, 20, 27]), (3, 0.3569, 3.0, 0.13, [20, 20, 19, 19, 20]), (3, 0.3443, 3.0, 0.1, [20, 19, 16, 17, 20]), (3, 0.3385, 3.0, 0.1, [20, 19, 15, 15, 20]), (3, 0.3375, 3.0, 0.1, [20, 20, 20, 19, 20])]\n",
      "Generation 3: 208.1 s, [(3, 0.4006, 3.9, 0.19, [20, 21, 20, 32, 38]), (4, 0.3977, 3.6, 0.1, [20, 20, 19, 19, 30]), (3, 0.388, 3.9, 0.19, [21, 26, 20, 30, 38]), (3, 0.388, 3.9, 0.19, [21, 26, 20, 30, 38]), (4, 0.387, 3.4, 0.12, [20, 17, 16, 17, 21]), (4, 0.387, 3.4, 0.12, [20, 17, 16, 17, 21]), (4, 0.3802, 3.4, 0.21, [20, 20, 20, 20, 20]), (4, 0.3802, 3.4, 0.21, [20, 20, 20, 20, 20]), (4, 0.3792, 3.9, 0.19, [21, 22, 23, 36, 43]), (4, 0.3628, 3.0, 0.13, [20, 14, 10, 9, 20]), (4, 0.3463, 2.8, 0.2, [18, 10, 8, 7, 20])]\n",
      "Generation 4: 230.2 s, [(5, 0.4239, 4.4, 0.14, [37, 37, 40, 40, 40]), (5, 0.4239, 4.4, 0.14, [37, 37, 40, 40, 40]), (4, 0.4142, 3.9, 0.19, [26, 22, 20, 34, 45]), (4, 0.4142, 3.9, 0.19, [26, 22, 20, 34, 45]), (5, 0.4093, 4.1, 0.21, [25, 29, 37, 53, 61]), (3, 0.4006, 3.9, 0.19, [20, 21, 20, 32, 38]), (4, 0.3919, 3.9, 0.19, [21, 21, 21, 40, 46]), (4, 0.3919, 3.9, 0.19, [21, 21, 21, 40, 46]), (4, 0.388, 3.5, 0.15, [20, 20, 20, 20, 34]), (5, 0.3725, 3.4, 0.14, [20, 18, 15, 12, 24]), (5, 0.3598, 3.4, 0.21, [20, 20, 20, 19, 23])]\n",
      "Generation 5: 262.4 s, [(5, 0.4306, 4.3, 0.17, [29, 41, 40, 54, 65]), (6, 0.4258, 4.4, 0.14, [49, 49, 60, 60, 60]), (6, 0.4258, 4.4, 0.14, [49, 49, 60, 60, 60]), (5, 0.4239, 4.4, 0.14, [37, 37, 40, 40, 40]), (5, 0.4103, 4.1, 0.17, [27, 30, 35, 57, 64]), (5, 0.4093, 3.9, 0.19, [24, 22, 23, 28, 45]), (5, 0.4093, 3.9, 0.19, [24, 22, 23, 28, 45]), (5, 0.4093, 3.7, 0.1, [20, 20, 21, 22, 36]), (6, 0.3986, 3.0, 0.12, [20, 20, 19, 20, 20]), (6, 0.3986, 3.0, 0.12, [20, 20, 19, 20, 20]), (5, 0.3948, 3.9, 0.19, [21, 20, 21, 29, 45])]\n",
      "Generation 6: 288.7 s, [(7, 0.45, 4.4, 0.14, [42, 66, 80, 80, 80]), (6, 0.4326, 3.9, 0.19, [20, 23, 20, 32, 59]), (6, 0.4326, 3.9, 0.19, [20, 23, 20, 32, 59]), (6, 0.4326, 3.9, 0.19, [20, 23, 20, 32, 59]), (6, 0.4326, 3.9, 0.19, [20, 23, 20, 32, 59]), (6, 0.4316, 3.9, 0.19, [20, 27, 25, 35, 54]), (5, 0.4306, 4.3, 0.17, [29, 41, 40, 54, 65]), (6, 0.4287, 4.1, 0.17, [26, 27, 33, 59, 70]), (6, 0.4287, 4.1, 0.17, [26, 27, 33, 59, 70]), (6, 0.4258, 4.2, 0.17, [38, 38, 40, 48, 64]), (6, 0.4258, 4.2, 0.17, [38, 38, 40, 48, 64])]\n",
      "Generation 7: 293.3 s, [(7, 0.4646, 4.2, 0.17, [24, 51, 57, 68, 82]), (7, 0.4607, 4.1, 0.17, [27, 31, 42, 55, 84]), (7, 0.4607, 4.1, 0.17, [27, 31, 42, 55, 84]), (7, 0.453, 4.2, 0.1, [25, 40, 51, 71, 90]), (7, 0.453, 4.1, 0.22, [44, 31, 47, 66, 78]), (7, 0.452, 3.9, 0.19, [22, 27, 34, 29, 62]), (7, 0.451, 3.9, 0.19, [20, 20, 23, 31, 67]), (7, 0.45, 4.4, 0.14, [42, 66, 80, 80, 80]), (7, 0.4365, 4.2, 0.13, [23, 40, 44, 55, 72]), (7, 0.4326, 3.6, 0.22, [20, 20, 20, 20, 50]), (8, 0.4268, 4.4, 0.14, [46, 69, 82, 100, 98])]\n",
      "Generation 8: 319.0 s, [(8, 0.4656, 3.6, 0.22, [21, 21, 20, 20, 50]), (7, 0.4646, 4.2, 0.17, [24, 51, 57, 68, 82]), (8, 0.4636, 4.1, 0.22, [27, 37, 54, 69, 88]), (8, 0.4636, 4.1, 0.22, [27, 37, 54, 69, 88]), (8, 0.4627, 4.1, 0.17, [30, 26, 56, 60, 92]), (8, 0.4627, 4.1, 0.17, [30, 26, 56, 60, 92]), (9, 0.4607, 4.4, 0.14, [41, 71, 88, 120, 111]), (9, 0.4607, 4.4, 0.14, [41, 71, 88, 120, 111]), (8, 0.4568, 4.7, 0.18, [62, 51, 67, 86, 98]), (8, 0.451, 3.9, 0.19, [24, 20, 23, 30, 76]), (8, 0.4394, 4.2, 0.13, [36, 51, 59, 73, 91])]\n",
      "Generation 9: 356.5 s, [(9, 0.4743, 4.2, 0.13, [38, 37, 63, 81, 98]), (9, 0.4743, 4.2, 0.13, [38, 37, 63, 81, 98]), (9, 0.4694, 4.0, 0.28, [35, 37, 40, 49, 89]), (8, 0.4656, 3.6, 0.22, [21, 21, 20, 20, 50]), (9, 0.4636, 4.1, 0.22, [28, 30, 65, 67, 97]), (9, 0.4636, 4.1, 0.22, [28, 30, 65, 67, 97]), (9, 0.4559, 4.3, 0.2, [45, 52, 77, 93, 107]), (10, 0.453, 4.4, 0.14, [47, 62, 87, 139, 130]), (9, 0.4452, 3.9, 0.19, [24, 22, 29, 34, 75]), (9, 0.4452, 5.3, 0.19, [82, 71, 87, 106, 118]), (9, 0.4413, 3.9, 0.19, [25, 22, 35, 46, 83])]\n",
      "Generation 10: 406.7 s, [(10, 0.4772, 4.1, 0.21, [26, 39, 57, 68, 103]), (10, 0.4772, 4.1, 0.21, [26, 39, 57, 68, 103]), (10, 0.4772, 4.3, 0.2, [59, 47, 80, 106, 117]), (10, 0.4772, 4.3, 0.2, [59, 47, 80, 106, 117]), (10, 0.4772, 4.3, 0.2, [59, 47, 80, 106, 117]), (10, 0.4762, 4.1, 0.22, [28, 38, 58, 64, 102]), (10, 0.4762, 4.1, 0.22, [28, 38, 58, 64, 102]), (9, 0.4743, 4.2, 0.13, [38, 37, 63, 81, 98]), (10, 0.4656, 4.0, 0.28, [27, 38, 50, 53, 91]), (10, 0.4636, 4.4, 0.22, [43, 42, 55, 66, 102]), (10, 0.4636, 4.4, 0.22, [43, 42, 55, 66, 102])]\n",
      "Generation 11: 408.7 s, [(10, 0.4772, 4.1, 0.21, [26, 39, 57, 68, 103]), (11, 0.4762, 4.3, 0.2, [51, 52, 94, 98, 128]), (11, 0.4743, 4.6, 0.22, [45, 59, 77, 88, 121]), (11, 0.4724, 4.0, 0.28, [28, 40, 56, 50, 95]), (11, 0.4714, 4.1, 0.21, [34, 47, 73, 75, 117]), (11, 0.4714, 4.1, 0.21, [34, 47, 73, 75, 117]), (11, 0.4685, 4.0, 0.24, [37, 33, 55, 52, 104]), (11, 0.4685, 4.0, 0.24, [37, 33, 55, 52, 104]), (11, 0.4646, 4.1, 0.22, [32, 37, 67, 70, 104]), (10, 0.4627, 4.2, 0.13, [44, 49, 72, 83, 103]), (11, 0.4607, 3.7, 0.13, [20, 21, 24, 30, 66])]\n",
      "Generation 12: 417.6 s, [(12, 0.4791, 4.0, 0.24, [37, 37, 51, 41, 105]), (12, 0.4791, 4.0, 0.24, [37, 37, 51, 41, 105]), (12, 0.4791, 4.0, 0.24, [37, 37, 51, 41, 105]), (10, 0.4772, 4.1, 0.21, [26, 39, 57, 68, 103]), (12, 0.4743, 4.2, 0.3, [36, 39, 43, 50, 86]), (12, 0.4733, 4.4, 0.21, [56, 71, 95, 108, 141]), (12, 0.4714, 4.0, 0.24, [36, 37, 45, 53, 116]), (12, 0.4704, 3.7, 0.13, [26, 20, 24, 22, 68]), (12, 0.4704, 3.7, 0.13, [26, 20, 24, 22, 68]), (11, 0.4694, 4.2, 0.15, [38, 41, 68, 71, 111]), (12, 0.4656, 4.0, 0.28, [30, 26, 52, 48, 110])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function ScopedTFGraph.__del__ at 0x7fd323e795e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/cahlivoj/self-scaling-nets/.venv/lib/python3.8/site-packages/tensorflow/python/framework/c_api_util.py\", line 54, in __del__\n",
      "    self.deleter(self.graph)\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "evolution(x=cifar10.X_train_norm, y=cifar10.y_train, validation_data=(cifar10.X_test_norm, cifar10.y_test),\n",
    "          batch_size=32, layer_sizes=[20, 20, 20, 20, 20], output_neurons=10, learning_rate=0.0004, n_parents=5, strategy=[0.5, 0.05], \n",
    "          population_size=10, n_generations=50, tournament_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "machine_shape": "hm",
   "name": "tf_multi_layer_ssnet_inverse.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
