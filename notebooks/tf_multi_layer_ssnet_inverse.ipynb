{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = 'float32'\n",
    "tf.keras.backend.set_floatx(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "# (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# X_train = X_train.astype(dtype) / 255.0\n",
    "# y_train = y_train.astype(dtype)\n",
    "# X_test = X_test.astype(dtype)  / 255.0\n",
    "# y_test = y_test.astype(dtype)\n",
    "\n",
    "# X_train = np.reshape(X_train, (-1, 784))\n",
    "# X_test = np.reshape(X_test, (-1, 784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10 = tf.keras.datasets.cifar10\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "X_train = X_train.astype(dtype) / 255.0\n",
    "y_train = y_train.astype(dtype)\n",
    "X_test = X_test.astype(dtype)  / 255.0\n",
    "y_test = y_test.astype(dtype)\n",
    "\n",
    "X_train = np.reshape(X_train, (-1, 3072))\n",
    "X_test = np.reshape(X_test, (-1, 3072))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSRegularizer(tf.keras.regularizers.Regularizer):\n",
    "    def __init__(self, l1):\n",
    "        self.l1 = l1\n",
    "\n",
    "    def __call__(self, x):\n",
    "        scaling_vector = tf.cumsum(tf.constant(self.l1, shape=(x.shape[0],), dtype=dtype), axis=0) - self.l1\n",
    "        return tf.reduce_sum(tf.reshape(scaling_vector, (-1, 1)) * tf.abs(x))\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'l1': float(self.l1)}\n",
    "\n",
    "\n",
    "class SSLayer(tf.keras.Model):\n",
    "    def __init__(self, input_units, units, activation, l1, kernel_initializer, bias_initializer, regularize=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_units = input_units\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "        self.l1 = l1\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.bias_initializer = bias_initializer\n",
    "        \n",
    "        self.A = tf.keras.activations.get(activation)\n",
    "        self.W_init = tf.keras.initializers.get(kernel_initializer)\n",
    "        self.b_init = tf.keras.initializers.get(bias_initializer)\n",
    "        self.regularizer = SSRegularizer(self.l1)\n",
    "        \n",
    "        self.W = tf.Variable(\n",
    "            name='W',\n",
    "            initial_value=self.W_init(shape=(input_units, units), dtype=dtype),\n",
    "            trainable=True)\n",
    "        \n",
    "        self.b = tf.Variable(\n",
    "            name='b',\n",
    "            initial_value=self.b_init(shape=(units,), dtype=dtype),\n",
    "            trainable=True)\n",
    "        \n",
    "        if regularize:\n",
    "            self.add_loss(lambda: self.regularizer(self.W))\n",
    "            self.add_loss(lambda: self.regularizer(self.b))\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return self.A(tf.matmul(inputs, self.W) + self.b)\n",
    "    \n",
    "    def copy_without_regularization(self):\n",
    "        copy = SSLayer(self.input_units, self.units, self.activation, self.l1, self.kernel_initializer, self.bias_initializer, regularize=False)\n",
    "        copy.W = self.W\n",
    "        copy.b = self.b\n",
    "        return copy\n",
    "\n",
    "\n",
    "class SSModel(tf.keras.Model):\n",
    "    def __init__(self, layer_sizes, activation=None, l1=0.01, kernel_initializer='glorot_uniform', bias_initializer='zeros'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.sslayers = list()\n",
    "        for l in range(len(layer_sizes) - 1):\n",
    "            input_units = layer_sizes[l]\n",
    "            units = layer_sizes[l + 1]\n",
    "            if l == 0:  # First layer\n",
    "                layer = SSLayer(input_units, units, activation, 0., kernel_initializer, bias_initializer)\n",
    "            elif l == len(layer_sizes) - 2:  # Last layer\n",
    "                layer = SSLayer(input_units, units, 'softmax', l1, kernel_initializer, bias_initializer)\n",
    "            else:  \n",
    "                layer = SSLayer(input_units, units, activation, l1, kernel_initializer, bias_initializer)\n",
    "            self.sslayers.append(layer)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.sslayers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def get_layer_sizes(self):\n",
    "        layer_sizes = list()\n",
    "        for l in range(len(self.sslayers)):\n",
    "            layer = self.sslayers[l]\n",
    "            layer_sizes.append(layer.W.shape[0])\n",
    "            if l == len(self.sslayers) - 1:  # Last layer\n",
    "                layer_sizes.append(layer.W.shape[1])\n",
    "        return layer_sizes\n",
    "    \n",
    "    def print_neurons(self):\n",
    "        for layer in self.sslayers[1:]:\n",
    "            print(get_param_string(layer.W))\n",
    "    \n",
    "    def remove_regularization(self):\n",
    "        for l in range(len(self.sslayers)):\n",
    "            self.sslayers[l] = self.sslayers[l].copy_without_regularization()\n",
    "        \n",
    "    def prune(self, threshold=0.001):\n",
    "        for l in range(len(self.sslayers) - 1):\n",
    "            layer1 = self.sslayers[l]\n",
    "            layer2 = self.sslayers[l + 1]\n",
    "            \n",
    "            W1 = layer1.W.value()\n",
    "            b1 = layer1.b.value()\n",
    "            W2 = layer2.W.value()\n",
    "\n",
    "            neurons_are_active = tf.math.reduce_max(tf.abs(W2), axis=1) >= threshold\n",
    "            active_neurons_indices = tf.reshape(tf.where(neurons_are_active), (-1,))\n",
    "            \n",
    "            new_W1 = tf.gather(W1, active_neurons_indices, axis=1)\n",
    "            new_b1 = tf.gather(b1, active_neurons_indices, axis=0)\n",
    "            new_W2 = tf.gather(W2, active_neurons_indices, axis=0)\n",
    "\n",
    "            layer1.W = tf.Variable(name='W', initial_value=new_W1, trainable=True)\n",
    "            layer1.b = tf.Variable(name='b', initial_value=new_b1, trainable=True)\n",
    "            layer2.W = tf.Variable(name='W', initial_value=new_W2, trainable=True)\n",
    "    \n",
    "    def grow(self, min_new_neurons=5, scaling_factor=0.001):   \n",
    "        for l in range(len(self.sslayers) - 1):\n",
    "            layer1 = self.sslayers[l]\n",
    "            layer2 = self.sslayers[l + 1]\n",
    "       \n",
    "            W1 = layer1.W.value()\n",
    "            b1 = layer1.b.value()\n",
    "            W2 = layer2.W.value()\n",
    "\n",
    "            n_new_neurons = max(min_new_neurons, int(W1.shape[1] * 0.2))\n",
    "\n",
    "            W1_growth = layer1.W_init(shape=(W1.shape[0], W1.shape[1] + n_new_neurons), dtype=dtype)[:, -n_new_neurons:] * scaling_factor\n",
    "            b1_growth = layer1.b_init(shape=(n_new_neurons,), dtype=dtype)\n",
    "            W2_growth = layer2.W_init(shape=(W2.shape[0] + n_new_neurons, W2.shape[1]), dtype=dtype)[-n_new_neurons:, :]\n",
    "\n",
    "            new_W1 = tf.concat([W1, W1_growth], axis=1)\n",
    "            new_b1 = tf.concat([b1, b1_growth], axis=0)\n",
    "            new_W2 = tf.concat([W2, W2_growth], axis=0)\n",
    "\n",
    "            layer1.W = tf.Variable(name='W1', initial_value=new_W1, trainable=True)\n",
    "            layer1.b = tf.Variable(name='b1', initial_value=new_b1, trainable=True)\n",
    "            layer2.W = tf.Variable(name='W2', initial_value=new_W2, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_param_string(weights):\n",
    "    param_string = \"\"\n",
    "    max_parameters = tf.math.reduce_max(tf.abs(weights), axis=1).numpy()\n",
    "    magnitudes = np.floor(np.log10(max_parameters))\n",
    "    for m in magnitudes:\n",
    "        if m > 0:\n",
    "            m = 0\n",
    "        param_string += str(int(-m))\n",
    "    return param_string\n",
    "\n",
    "\n",
    "def print_epoch_statistics(model):\n",
    "    y_pred = model(X_train)\n",
    "    loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(y_train, y_pred))\n",
    "    accuracy = tf.reduce_mean(tf.keras.metrics.sparse_categorical_accuracy(y_train, y_pred))\n",
    "    \n",
    "    y_pred_val = model(X_test)\n",
    "    val_loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(y_test, y_pred_val))\n",
    "    val_accuracy = tf.reduce_mean(tf.keras.metrics.sparse_categorical_accuracy(y_test, y_pred_val))\n",
    "    print(f\"loss: {loss} - accuracy: {accuracy} - val_loss: {val_loss} - val_accuracy: {val_accuracy}\")\n",
    "    print(f\"layer sizes: {model.get_layer_sizes()}\")\n",
    "    model.print_neurons()\n",
    "\n",
    "\n",
    "def train_model(model, optimizer, epochs, batch_size, train_dataset):\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        \n",
    "        print(\"Before growing:\")\n",
    "        print_epoch_statistics(model)\n",
    "        model.grow(min_new_neurons=10, scaling_factor=0.001)\n",
    "        print(\"After growing:\")\n",
    "        print_epoch_statistics(model)\n",
    "\n",
    "        for step, (x_batch, y_batch) in enumerate(train_dataset):\n",
    "            with tf.GradientTape() as tape:\n",
    "                y_pred = model(x_batch, training=True)\n",
    "                loss_value = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(y_batch, y_pred))\n",
    "                loss_value += sum(model.losses)\n",
    "\n",
    "            grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        \n",
    "        print(\"Before pruning:\")\n",
    "        print_epoch_statistics(model)\n",
    "        model.prune(threshold=0.001)\n",
    "        print(\"After pruning:\")\n",
    "        print_epoch_statistics(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "Before growing:\n",
      "loss: 2.443971633911133 - accuracy: 0.10277999937534332 - val_loss: 2.4407873153686523 - val_accuracy: 0.10570000112056732\n",
      "layer sizes: [3072, 100, 100, 10]\n",
      "1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
      "1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
      "After growing:\n",
      "loss: 2.4439027309417725 - accuracy: 0.10270000249147415 - val_loss: 2.4407172203063965 - val_accuracy: 0.10570000112056732\n",
      "layer sizes: [3072, 120, 120, 10]\n",
      "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
      "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
      "Before pruning:\n",
      "loss: 1.7542556524276733 - accuracy: 0.3724200129508972 - val_loss: 1.7534072399139404 - val_accuracy: 0.37279999256134033\n",
      "layer sizes: [3072, 120, 120, 10]\n",
      "121444444434442444141444444144444444444414441444443444444444444444144444144444441414444444444444444444414441424444441424\n",
      "111111111114111111421111414342311311243144311413144434142451133342141444123523424444445332414444411444444444444444444444\n",
      "After pruning:\n",
      "loss: 1.754254698753357 - accuracy: 0.3724200129508972 - val_loss: 1.7534070014953613 - val_accuracy: 0.37279999256134033\n",
      "layer sizes: [3072, 20, 63, 10]\n",
      "12132111113111111212\n",
      "111111111111111112111113231131123131113131211333211123232332111\n",
      "Epoch 2/20\n",
      "Before growing:\n",
      "loss: 1.754254698753357 - accuracy: 0.3724200129508972 - val_loss: 1.7534070014953613 - val_accuracy: 0.37279999256134033\n",
      "layer sizes: [3072, 20, 63, 10]\n",
      "12132111113111111212\n",
      "111111111111111112111113231131123131113131211333211123232332111\n",
      "After growing:\n",
      "loss: 1.7543753385543823 - accuracy: 0.37237998843193054 - val_loss: 1.753549575805664 - val_accuracy: 0.37229999899864197\n",
      "layer sizes: [3072, 30, 75, 10]\n",
      "121321111131111112121111111111\n",
      "111111111111111112111113231131123131113131211333211123232332111111111111111\n",
      "Before pruning:\n",
      "loss: 1.7117598056793213 - accuracy: 0.3818199932575226 - val_loss: 1.7093638181686401 - val_accuracy: 0.3799999952316284\n",
      "layer sizes: [3072, 30, 75, 10]\n",
      "141441111144111414141414144441\n",
      "111114111111111115111225351243544142454144444444414244444545243434444444444\n",
      "After pruning:\n",
      "loss: 1.7117595672607422 - accuracy: 0.3818199932575226 - val_loss: 1.7093636989593506 - val_accuracy: 0.3799999952316284\n",
      "layer sizes: [3072, 16, 33, 10]\n",
      "1111111111111111\n",
      "111111111111111111122312312112233\n",
      "Epoch 3/20\n",
      "Before growing:\n",
      "loss: 1.7117595672607422 - accuracy: 0.3818199932575226 - val_loss: 1.7093636989593506 - val_accuracy: 0.3799999952316284\n",
      "layer sizes: [3072, 16, 33, 10]\n",
      "1111111111111111\n",
      "111111111111111111122312312112233\n",
      "After growing:\n",
      "loss: 1.7117596864700317 - accuracy: 0.38185998797416687 - val_loss: 1.7093579769134521 - val_accuracy: 0.38019999861717224\n",
      "layer sizes: [3072, 26, 43, 10]\n",
      "11111111111111111111111111\n",
      "1111111111111111111223123121122331111111111\n",
      "Before pruning:\n",
      "loss: 1.6686034202575684 - accuracy: 0.4021199941635132 - val_loss: 1.668116807937622 - val_accuracy: 0.40369999408721924\n",
      "layer sizes: [3072, 26, 43, 10]\n",
      "11411111111134444444424444\n",
      "1111111112111112131244144251333444444444442\n",
      "After pruning:\n",
      "loss: 1.6686047315597534 - accuracy: 0.4021199941635132 - val_loss: 1.6681183576583862 - val_accuracy: 0.40369999408721924\n",
      "layer sizes: [3072, 13, 27, 10]\n",
      "1111111111132\n",
      "111111111211111213121213332\n",
      "Epoch 4/20\n",
      "Before growing:\n",
      "loss: 1.6686047315597534 - accuracy: 0.4021199941635132 - val_loss: 1.6681183576583862 - val_accuracy: 0.40369999408721924\n",
      "layer sizes: [3072, 13, 27, 10]\n",
      "1111111111132\n",
      "111111111211111213121213332\n",
      "After growing:\n",
      "loss: 1.6686406135559082 - accuracy: 0.40202000737190247 - val_loss: 1.6681840419769287 - val_accuracy: 0.40380001068115234\n",
      "layer sizes: [3072, 23, 37, 10]\n",
      "11111111111321111111111\n",
      "1111111112111112131212133321111111111\n",
      "Before pruning:\n",
      "loss: 1.6896930932998657 - accuracy: 0.39149999618530273 - val_loss: 1.690864086151123 - val_accuracy: 0.39100000262260437\n",
      "layer sizes: [3072, 23, 37, 10]\n",
      "11111111111441444444444\n",
      "1111111113111112141413154444444444144\n",
      "After pruning:\n",
      "loss: 1.6896930932998657 - accuracy: 0.3914799988269806 - val_loss: 1.6908642053604126 - val_accuracy: 0.39100000262260437\n",
      "layer sizes: [3072, 12, 22, 10]\n",
      "111111111111\n",
      "1111111113111112111311\n",
      "Epoch 5/20\n",
      "Before growing:\n",
      "loss: 1.6896930932998657 - accuracy: 0.3914799988269806 - val_loss: 1.6908642053604126 - val_accuracy: 0.39100000262260437\n",
      "layer sizes: [3072, 12, 22, 10]\n",
      "111111111111\n",
      "1111111113111112111311\n",
      "After growing:\n",
      "loss: 1.689278483390808 - accuracy: 0.3918600082397461 - val_loss: 1.6904542446136475 - val_accuracy: 0.39160001277923584\n",
      "layer sizes: [3072, 22, 32, 10]\n",
      "1111111111111111111111\n",
      "11111111131111121113111111111111\n",
      "Before pruning:\n",
      "loss: 1.6463667154312134 - accuracy: 0.40790000557899475 - val_loss: 1.6491533517837524 - val_accuracy: 0.4115999937057495\n",
      "layer sizes: [3072, 22, 32, 10]\n",
      "1141111111144144141244\n",
      "11111111151121141114134424444444\n",
      "After pruning:\n",
      "loss: 1.6463667154312134 - accuracy: 0.40790000557899475 - val_loss: 1.6491533517837524 - val_accuracy: 0.4115999937057495\n",
      "layer sizes: [3072, 14, 20, 10]\n",
      "11111111111112\n",
      "11111111111211111132\n",
      "Epoch 6/20\n",
      "Before growing:\n",
      "loss: 1.6463667154312134 - accuracy: 0.40790000557899475 - val_loss: 1.6491533517837524 - val_accuracy: 0.4115999937057495\n",
      "layer sizes: [3072, 14, 20, 10]\n",
      "11111111111112\n",
      "11111111111211111132\n",
      "After growing:\n",
      "loss: 1.6464512348175049 - accuracy: 0.4076800048351288 - val_loss: 1.649206280708313 - val_accuracy: 0.4115000069141388\n",
      "layer sizes: [3072, 24, 30, 10]\n",
      "111111111111121111111111\n",
      "111111111112111111321111111111\n",
      "Before pruning:\n",
      "loss: 1.6534479856491089 - accuracy: 0.4051800072193146 - val_loss: 1.6607693433761597 - val_accuracy: 0.4016999900341034\n",
      "layer sizes: [3072, 24, 30, 10]\n",
      "111111111144444444414444\n",
      "111111111112111111444444544434\n",
      "After pruning:\n",
      "loss: 1.6534479856491089 - accuracy: 0.4051800072193146 - val_loss: 1.6607693433761597 - val_accuracy: 0.4016999900341034\n",
      "layer sizes: [3072, 11, 19, 10]\n",
      "11111111111\n",
      "1111111111121111113\n",
      "Epoch 7/20\n",
      "Before growing:\n",
      "loss: 1.6534479856491089 - accuracy: 0.4051800072193146 - val_loss: 1.6607693433761597 - val_accuracy: 0.4016999900341034\n",
      "layer sizes: [3072, 11, 19, 10]\n",
      "11111111111\n",
      "1111111111121111113\n",
      "After growing:\n",
      "loss: 1.6528940200805664 - accuracy: 0.4057599902153015 - val_loss: 1.6602706909179688 - val_accuracy: 0.4016000032424927\n",
      "layer sizes: [3072, 21, 29, 10]\n",
      "111111111111111111111\n",
      "11111111111211111131111111111\n",
      "Before pruning:\n",
      "loss: 1.641068935394287 - accuracy: 0.40755999088287354 - val_loss: 1.6480238437652588 - val_accuracy: 0.4074999988079071\n",
      "layer sizes: [3072, 21, 29, 10]\n",
      "111111111141414142444\n",
      "11111111111111111154444444444\n",
      "After pruning:\n",
      "loss: 1.641068935394287 - accuracy: 0.40755999088287354 - val_loss: 1.6480238437652588 - val_accuracy: 0.4074999988079071\n",
      "layer sizes: [3072, 14, 18, 10]\n",
      "11111111111112\n",
      "111111111111111111\n",
      "Epoch 8/20\n",
      "Before growing:\n",
      "loss: 1.641068935394287 - accuracy: 0.40755999088287354 - val_loss: 1.6480238437652588 - val_accuracy: 0.4074999988079071\n",
      "layer sizes: [3072, 14, 18, 10]\n",
      "11111111111112\n",
      "111111111111111111\n",
      "After growing:\n",
      "loss: 1.6409062147140503 - accuracy: 0.4075999855995178 - val_loss: 1.6478902101516724 - val_accuracy: 0.40720000863075256\n",
      "layer sizes: [3072, 24, 28, 10]\n",
      "111111111111121111111111\n",
      "1111111111111111111111111111\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-ac0346e8e3ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-87-cf9bc1028b7c>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, epochs, batch_size, train_dataset)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Before pruning:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/self-scaling-nets/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, name, experimental_aggregate_gradients)\u001b[0m\n\u001b[1;32m    623\u001b[0m             \"ParameterServerStrategy and CentralStorageStrategy\")\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m       \u001b[0mapply_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mexperimental_aggregate_gradients\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform_unaggregated_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/self-scaling-nets/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36m_prepare\u001b[0;34m(self, var_list)\u001b[0m\n\u001b[1;32m    877\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mvar_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_dtype\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m       \u001b[0mapply_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    880\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "model = SSModel(layer_sizes=[3072, 100, 100, 10], activation='relu', l1=0.0001, kernel_initializer='he_normal')\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "train_model(model, optimizer, epochs, batch_size, train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
