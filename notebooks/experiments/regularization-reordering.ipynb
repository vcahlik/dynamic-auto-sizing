{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf_multi_layer_ssnet_inverse.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "background_execution": "on"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_deAUKlniFk",
        "outputId": "41e0c962-8b67-4263-997b-f2f1b6be2c44"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Jan  9 14:14:42 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0    28W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKwUwV_NneIo"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "dtype = 'float32'\n",
        "tf.keras.backend.set_floatx(dtype)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTZq4KMpneIv"
      },
      "source": [
        "################################################################################\n",
        "# DATASETS\n",
        "################################################################################\n",
        "\n",
        "\n",
        "class Dataset:\n",
        "    def __init__(self, X_train, y_train, X_test, y_test, shape, shape_flattened):\n",
        "        X_train = X_train.astype(dtype) / 255.0\n",
        "        y_train = y_train.astype(dtype)\n",
        "        X_test = X_test.astype(dtype)  / 255.0\n",
        "        y_test = y_test.astype(dtype)\n",
        "\n",
        "        X_train = np.reshape(X_train, shape_flattened)\n",
        "        X_test = np.reshape(X_test, shape_flattened)\n",
        "\n",
        "        X = np.concatenate((X_train, X_test), axis=0)\n",
        "        y = np.concatenate((y_train, y_test), axis=0)\n",
        "\n",
        "        from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(X_train)  # Scaling each feature independently\n",
        "\n",
        "        X_norm = scaler.transform(X)\n",
        "        X_train_norm = scaler.transform(X_train)\n",
        "        X_test_norm = scaler.transform(X_test)\n",
        "\n",
        "        X_norm = np.reshape(X_norm, shape)\n",
        "        X_train_norm = np.reshape(X_train_norm, shape)\n",
        "        X_test_norm = np.reshape(X_test_norm, shape)\n",
        "\n",
        "        self.X_norm = X_norm\n",
        "        self.y = y\n",
        "        self.X_train_norm = X_train_norm\n",
        "        self.y_train = y_train\n",
        "        self.X_test_norm = X_test_norm\n",
        "        self.y_test = y_test\n",
        "\n",
        "\n",
        "def get_cifar_10_dataset():\n",
        "    cifar10 = tf.keras.datasets.cifar10\n",
        "    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "    shape = (-1, 32, 32, 3)\n",
        "    shape_flattened = (-1, 3072)  # Scaling each feature independently\n",
        "    return Dataset(X_train, y_train, X_test, y_test, shape=shape, shape_flattened=shape_flattened)\n",
        "\n",
        "\n",
        "def get_cifar_100_dataset():\n",
        "    cifar100 = tf.keras.datasets.cifar100\n",
        "    (X_train, y_train), (X_test, y_test) = cifar100.load_data()\n",
        "    shape = (-1, 32, 32, 3)\n",
        "    shape_flattened = (-1, 3072)  # Scaling each feature independently\n",
        "    return Dataset(X_train, y_train, X_test, y_test, shape=shape, shape_flattened=shape_flattened)\n",
        "\n",
        "\n",
        "def get_svhn_dataset():\n",
        "    from scipy import io\n",
        "\n",
        "    X_train = io.loadmat(train_filename, variable_names='X').get('X')\n",
        "    y_train = io.loadmat(train_filename, variable_names='y').get('y')\n",
        "    X_test = io.loadmat(test_filename, variable_names='X').get('X')\n",
        "    y_test = io.loadmat(test_filename, variable_names='y').get('y')\n",
        "\n",
        "    X_train = np.moveaxis(X_train, -1, 0)\n",
        "    y_train -= 1\n",
        "    X_test = np.moveaxis(X_test, -1, 0)\n",
        "    y_test -= 1\n",
        "\n",
        "    shape = (-1, 32, 32, 3)\n",
        "    shape_flattened = (-1, 3072)  # Scaling each feature independently\n",
        "    return Dataset(X_train, y_train, X_test, y_test, shape=shape, shape_flattened=shape_flattened)\n",
        "\n",
        "\n",
        "def get_mnist_dataset():\n",
        "    mnist = tf.keras.datasets.mnist\n",
        "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "    shape = (-1, 28, 28, 1)\n",
        "    shape_flattened = (-1, 1)  # Scaling all features together\n",
        "    return Dataset(X_train, y_train, X_test, y_test, shape=shape, shape_flattened=shape_flattened)\n",
        "\n",
        "\n",
        "def get_fashion_mnist_dataset():\n",
        "    fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "    (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "    shape = (-1, 28, 28, 1)\n",
        "    shape_flattened = (-1, 1)  # Scaling all features together\n",
        "    return Dataset(X_train, y_train, X_test, y_test, shape=shape, shape_flattened=shape_flattened)\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# REGULARIZERS\n",
        "################################################################################\n",
        "\n",
        "\n",
        "class Regularizer(tf.keras.regularizers.Regularizer):\n",
        "    def __init__(self, regularization_penalty, regularization_method):\n",
        "        self.regularization_penalty = regularization_penalty\n",
        "        self.regularization_method = regularization_method\n",
        "        self.n_new_neurons = 0\n",
        "        self.scaling_tensor = None\n",
        "        if self.regularization_method == 'weighted_l1_reordered':\n",
        "            self.update_scaling_tensor = True\n",
        "        else:\n",
        "            self.update_scaling_tensor = None\n",
        "\n",
        "    def __call__(self, x):\n",
        "        if self.regularization_method == 'weighted_l1':\n",
        "            return self.weighted_l1(x)\n",
        "        elif self.regularization_method == 'weighted_l1_reordered':\n",
        "            return self.weighted_l1_reordered(x)\n",
        "        elif self.regularization_method == 'group_sparsity':\n",
        "            return self.group_sparsity(x)\n",
        "        else:\n",
        "            raise NotImplementedError(f\"Unknown regularization method {self.regularization_method}\")\n",
        "    \n",
        "    def weighted_l1(self, x):\n",
        "        # I.e. for a parameter matrix of 4 input and 10 output neurons:\n",
        "        #\n",
        "        # [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
        "        #  [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
        "        #  [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
        "        #  [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]\n",
        "        #\n",
        "        # the scaling tensor, as well as the resulting weighted values, could be:\n",
        "        #\n",
        "        # [[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n",
        "        #  [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n",
        "        #  [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n",
        "        #  [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]]\n",
        "        #\n",
        "        # Therefore every additional output neuron is regularized more.\n",
        "\n",
        "        scaling_tensor = tf.cumsum(tf.constant(self.regularization_penalty, shape=x.shape, dtype=dtype), axis=-1)\n",
        "        weighted_values = scaling_tensor * tf.abs(x)\n",
        "        return tf.reduce_sum(weighted_values)\n",
        "    \n",
        "    def weighted_l1_reordered(self, x):\n",
        "        # I.e. for a parameter matrix of 4 input and 10 output neurons:\n",
        "        #\n",
        "        # [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
        "        #  [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
        "        #  [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
        "        #  [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]\n",
        "        #\n",
        "        # the scaling tensor, as well as the resulting weighted values, could be:\n",
        "        #\n",
        "        # [[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n",
        "        #  [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n",
        "        #  [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n",
        "        #  [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]]\n",
        "        #\n",
        "        # Therefore every additional output neuron is regularized more.\n",
        "\n",
        "        if self.update_scaling_tensor:\n",
        "            scaling_tensor_raw = tf.cumsum(tf.constant(self.regularization_penalty, shape=x.shape, dtype=dtype), axis=-1)\n",
        "\n",
        "            scaling_tensor_old_neurons = scaling_tensor_raw[:, :-self.n_new_neurons]\n",
        "            scaling_tensor_new_neurons = scaling_tensor_raw[:, -self.n_new_neurons:]\n",
        "            scaling_tensor_old_neurons_shuffled = tf.transpose(tf.random.shuffle(tf.transpose(scaling_tensor_old_neurons)))\n",
        "            self.scaling_tensor = tf.concat([scaling_tensor_old_neurons_shuffled, scaling_tensor_new_neurons], axis=-1)\n",
        "            self.update_scaling_tensor = False\n",
        "\n",
        "        weighted_values = self.scaling_tensor * tf.abs(x)\n",
        "        return tf.reduce_sum(weighted_values)\n",
        "    \n",
        "    def group_sparsity(self, x):\n",
        "        # I.e. for a parameter matrix of 3 input and 5 output neurons:\n",
        "        #\n",
        "        # [[1., 1., 1., 1., 1.],\n",
        "        #  [1., 2., 2., 1., 2.],\n",
        "        #  [2., 2., 3., 1., 3.]]\n",
        "        #\n",
        "        # The resulting vector of group norms is [2., 2., 3., 1., 3.], therefore for\n",
        "        # every output neuron, its incoming connections form a group.\n",
        "\n",
        "        group_norms = tf.norm(x, ord=2, axis=0)\n",
        "        # assert group_norms.shape[0] == x.shape[1]\n",
        "        return self.regularization_penalty * tf.reduce_sum(group_norms)\n",
        "    \n",
        "    def prune(self):\n",
        "        self.n_new_neurons = 0\n",
        "        if self.regularization_method == 'weighted_l1_reordered':\n",
        "            self.update_scaling_tensor = True\n",
        "    \n",
        "    def grow(self, n_new_neurons):\n",
        "        self.n_new_neurons = n_new_neurons\n",
        "        if self.regularization_method == 'weighted_l1_reordered':\n",
        "            self.update_scaling_tensor = True\n",
        "\n",
        "    def get_config(self):\n",
        "        return {'regularization_penalty': float(self.regularization_penalty)}\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# LAYERS\n",
        "################################################################################\n",
        "\n",
        "\n",
        "class CustomLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_shape):\n",
        "        super().__init__()\n",
        "\n",
        "        self.inpt_shp = input_shape\n",
        "\n",
        "\n",
        "class Dense(CustomLayer):\n",
        "    def __init__(self, units, activation, regularization_penalty=0.01, \n",
        "                 regularization_method='weighted_l1', kernel_initializer='glorot_uniform', \n",
        "                 bias_initializer='zeros', input_shape=None, fixed_size=False):\n",
        "        super().__init__(input_shape)\n",
        "\n",
        "        self.units = units\n",
        "        self.activation = activation\n",
        "        self.regularization_penalty = regularization_penalty\n",
        "        self.regularization_method = regularization_method\n",
        "        self.kernel_initializer = kernel_initializer\n",
        "        self.bias_initializer = bias_initializer\n",
        "        self.fixed_size = fixed_size\n",
        "        \n",
        "        self.A = tf.keras.activations.get(activation)\n",
        "        self.W_init = tf.keras.initializers.get(kernel_initializer)\n",
        "        self.b_init = tf.keras.initializers.get(bias_initializer)\n",
        "        self.regularizer = Regularizer(self.regularization_penalty, self.regularization_method)\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        input_units = input_shape[-1]\n",
        "\n",
        "        self.W = tf.Variable(\n",
        "            name='W',\n",
        "            initial_value=self.W_init(shape=(input_units, self.units), dtype=dtype),\n",
        "            trainable=True)\n",
        "        \n",
        "        self.b = tf.Variable(\n",
        "            name='b',\n",
        "            initial_value=self.b_init(shape=(self.units,), dtype=dtype),\n",
        "            trainable=True)\n",
        "        \n",
        "        if self.regularization_method is not None:\n",
        "            self.add_loss(lambda: self.regularizer(tf.concat([self.W, tf.reshape(self.b, (1, -1))], axis=0)))\n",
        "    \n",
        "    def call(self, inputs, training=None):\n",
        "        return self.A(tf.matmul(inputs, self.W) + self.b)\n",
        "\n",
        "    def get_size(self):\n",
        "        return self.W.shape[0], self.W.shape[1]\n",
        "    \n",
        "    def prune(self, threshold, active_input_units_indices):\n",
        "        # Remove connections from pruned units in previous layer\n",
        "        new_W = tf.gather(self.W.value(), active_input_units_indices, axis=0)\n",
        "\n",
        "        if self.fixed_size:\n",
        "            active_output_neurons_indices = list(range(new_W.shape[1]))\n",
        "        else:\n",
        "            # Prune units in this layer\n",
        "            weights_with_biases = tf.concat([new_W, tf.reshape(self.b.value(), (1, -1))], axis=0)\n",
        "            neurons_are_active = tf.math.reduce_max(tf.abs(weights_with_biases), axis=0) >= threshold\n",
        "            active_output_neurons_indices = tf.reshape(tf.where(neurons_are_active), (-1,))\n",
        "            \n",
        "            new_W = tf.gather(new_W, active_output_neurons_indices, axis=1)\n",
        "            new_b = tf.gather(self.b.value(), active_output_neurons_indices, axis=0)\n",
        "\n",
        "            self.b = tf.Variable(name='b', initial_value=new_b, trainable=True)\n",
        "\n",
        "        self.W = tf.Variable(name='W', initial_value=new_W, trainable=True)\n",
        "\n",
        "        self.regularizer.prune()\n",
        "        return active_output_neurons_indices\n",
        "    \n",
        "    def grow(self, n_new_input_units, percentage, min_new_units, scaling_factor):\n",
        "        if n_new_input_units > 0:\n",
        "            # Add connections to grown units in previous layer\n",
        "            W_growth = self.W_init(shape=(self.W.shape[0] + n_new_input_units, self.W.shape[1]), dtype=dtype)[-n_new_input_units:, :] * scaling_factor  # TODO is it better to be multiplying here by scaling_factor? It does help with not increasing the max weights of existing neurons when new neurons are added.\n",
        "            new_W = tf.concat([self.W.value(), W_growth], axis=0)\n",
        "        else:\n",
        "            new_W = self.W.value()\n",
        "\n",
        "        if self.fixed_size:\n",
        "            n_new_output_units = 0\n",
        "        else:\n",
        "            # Grow new units in this layer\n",
        "            n_new_output_units = max(min_new_units, int(new_W.shape[1] * percentage))\n",
        "            W_growth = self.W_init(shape=(new_W.shape[0], new_W.shape[1] + n_new_output_units), dtype=dtype)[:, -n_new_output_units:] * scaling_factor\n",
        "            b_growth = self.b_init(shape=(n_new_output_units,), dtype=dtype)  # TODO for all possible bias initializers to work properly, the whole bias vector should be initialized at once\n",
        "            new_W = tf.concat([new_W, W_growth], axis=1)\n",
        "            new_b = tf.concat([self.b.value(), b_growth], axis=0)\n",
        "\n",
        "            self.b = tf.Variable(name='b', initial_value=new_b, trainable=True)\n",
        "\n",
        "        self.W = tf.Variable(name='W', initial_value=new_W, trainable=True)\n",
        "\n",
        "        self.regularizer.grow(n_new_output_units)\n",
        "        return n_new_output_units\n",
        "    \n",
        "    def get_param_string():\n",
        "        param_string = \"\"\n",
        "        weights_with_bias = tf.concat([self.W, tf.reshape(self.b, (1, -1))], axis=0)\n",
        "        max_parameters = tf.math.reduce_max(tf.abs(weights_with_bias), axis=0).numpy()\n",
        "        magnitudes = np.floor(np.log10(max_parameters))\n",
        "        for m in magnitudes:\n",
        "            if m > 0:\n",
        "                m = 0\n",
        "            param_string += str(int(-m))\n",
        "        return param_string\n",
        "\n",
        "\n",
        "class Conv2D(CustomLayer):\n",
        "    def __init__(self, filters, filter_size, activation, strides=(1, 1), \n",
        "                 padding='SAME', regularization_penalty=0.01, \n",
        "                 regularization_method='weighted_l1', kernel_initializer='glorot_uniform',\n",
        "                 bias_initializer='zeros', input_shape=None, fixed_size=False):\n",
        "        super().__init__(input_shape)\n",
        "    \n",
        "        self.filters = filters\n",
        "        self.filter_size = filter_size\n",
        "        self.activation = activation\n",
        "        self.strides = strides\n",
        "        self.padding = padding\n",
        "        self.regularization_penalty = regularization_penalty\n",
        "        self.regularization_method = regularization_method\n",
        "        self.kernel_initializer = kernel_initializer\n",
        "        self.bias_initializer = bias_initializer\n",
        "        self.fixed_size = fixed_size\n",
        "        \n",
        "        self.A = tf.keras.activations.get(activation)\n",
        "        self.F_init = tf.keras.initializers.get(kernel_initializer)\n",
        "        self.b_init = tf.keras.initializers.get(bias_initializer)\n",
        "        self.regularizer = Regularizer(self.regularization_penalty, self.regularization_method)\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        input_filters = input_shape[-1]\n",
        "\n",
        "        self.F = tf.Variable(\n",
        "            name='F',\n",
        "            initial_value=self.F_init(\n",
        "                shape=(self.filter_size[0], self.filter_size[1], input_filters, self.filters), dtype=dtype\n",
        "            ),\n",
        "            trainable=True)\n",
        "        \n",
        "        self.b = tf.Variable(\n",
        "            name='b',\n",
        "            initial_value=self.b_init(shape=(self.filters,), dtype=dtype),\n",
        "            trainable=True)\n",
        "\n",
        "        if self.regularization_method is not None:\n",
        "            self.add_loss(lambda: self.regularizer(tf.concat([tf.reshape(self.F, (-1, self.F.shape[-1])), tf.reshape(self.b, (1, -1))], axis=0)))\n",
        "    \n",
        "    def call(self, inputs, training=None):\n",
        "        y = tf.nn.conv2d(inputs, self.F, strides=self.strides, padding=self.padding)\n",
        "        y = tf.nn.bias_add(y, self.b)\n",
        "        y = self.A(y)\n",
        "        return y\n",
        "    \n",
        "    def get_size(self):\n",
        "        return self.F.shape[-2], self.F.shape[-1]\n",
        "    \n",
        "    def prune(self, threshold, active_input_units_indices):\n",
        "        # Remove connections from pruned units in previous layer\n",
        "        new_F = tf.gather(self.F.value(), active_input_units_indices, axis=-2)\n",
        "\n",
        "        if self.fixed_size:\n",
        "            active_output_filters_indices = list(range(new_F.shape[-1]))\n",
        "        else:\n",
        "            # Prune units in this layer\n",
        "            F_reduced_max = tf.reshape(tf.math.reduce_max(tf.abs(new_F), axis=(0, 1, 2)), (1, -1))\n",
        "            F_reduced_max_with_biases = tf.concat([F_reduced_max, tf.reshape(self.b.value(), (1, -1))], axis=0)\n",
        "            filters_are_active = tf.math.reduce_max(tf.abs(F_reduced_max_with_biases), axis=0) >= threshold\n",
        "            active_output_filters_indices = tf.reshape(tf.where(filters_are_active), (-1,))\n",
        "            \n",
        "            new_F = tf.gather(new_F, active_output_filters_indices, axis=-1)\n",
        "            new_b = tf.gather(self.b.value(), active_output_filters_indices, axis=0)\n",
        "\n",
        "            self.b = tf.Variable(name='b', initial_value=new_b, trainable=True)\n",
        "\n",
        "        self.F = tf.Variable(name='F', initial_value=new_F, trainable=True)\n",
        "\n",
        "        self.regularizer.prune()\n",
        "        return active_output_filters_indices\n",
        "\n",
        "    def grow(self, n_new_input_units, percentage, min_new_units, scaling_factor):\n",
        "        if n_new_input_units > 0:\n",
        "            # Add connections to grown units in previous layer\n",
        "            F_growth = self.F_init(shape=(self.F.shape[0], self.F.shape[1], self.F.shape[2] + n_new_input_units, self.F.shape[3]), dtype=dtype)[:, :, -n_new_input_units:, :] * scaling_factor  # TODO is it better to be multiplying here by scaling_factor? It does help with not increasing the max weights of existing neurons when new neurons are added.\n",
        "            new_F = tf.concat([self.F.value(), F_growth], axis=-2)\n",
        "        else:\n",
        "            new_F = self.F.value()\n",
        "\n",
        "        if self.fixed_size:\n",
        "            n_new_output_units = 0\n",
        "        else:\n",
        "            # Grow new units in this layer\n",
        "            n_new_output_units = max(min_new_units, int(new_F.shape[-1] * percentage))\n",
        "            F_growth = self.F_init(shape=(new_F.shape[0], new_F.shape[1], new_F.shape[2], new_F.shape[3] + n_new_output_units), dtype=dtype)[:, :, :, -n_new_output_units:] * scaling_factor\n",
        "            b_growth = self.b_init(shape=(n_new_output_units,), dtype=dtype)  # TODO for all possible bias initializers to work properly, the whole bias vector should be initialized at once\n",
        "            new_F = tf.concat([new_F, F_growth], axis=-1)\n",
        "            new_b = tf.concat([self.b.value(), b_growth], axis=0)\n",
        "\n",
        "            self.b = tf.Variable(name='b', initial_value=new_b, trainable=True)\n",
        "\n",
        "        self.F = tf.Variable(name='F', initial_value=new_F, trainable=True)\n",
        "\n",
        "        self.regularizer.grow(n_new_output_units)\n",
        "        return n_new_output_units\n",
        "\n",
        "    def get_param_string():\n",
        "        param_string = \"\"\n",
        "        # TODO\n",
        "        return param_string\n",
        "\n",
        "\n",
        "class Flatten(tf.keras.Model):\n",
        "    def call(self, inputs, training=None):\n",
        "        return tf.reshape(tf.transpose(inputs, perm=[0, 3, 1, 2]), (inputs.shape[0], -1))\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# MODELS\n",
        "################################################################################\n",
        "\n",
        "\n",
        "class Sequential(tf.keras.Model):\n",
        "    def __init__(self, layers, activation=None):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.lrs = layers\n",
        "        \n",
        "    def call(self, inputs, training=None):\n",
        "        x = inputs\n",
        "        for layer in self.lrs:\n",
        "            x = layer(x, training=training)\n",
        "        return x\n",
        "    \n",
        "    def get_layer_input_shape(self, target_layer):\n",
        "        if target_layer.inpt_shp is not None:\n",
        "            return target_layer.inpt_shp\n",
        "\n",
        "        input = np.random.normal(size=(1,) + self.lrs[0].inpt_shp)\n",
        "        for layer in self.lrs:\n",
        "            if layer is target_layer:\n",
        "                return tuple(input.shape[1:])\n",
        "            input = layer(input)\n",
        "        raise Exception(\"Layer not found in the model.\")\n",
        "\n",
        "    def get_layer_output_shape(self, target_layer):\n",
        "        input = np.random.normal(size=(1,) + self.lrs[0].inpt_shp)\n",
        "        for layer in self.lrs:\n",
        "            output = layer(input)\n",
        "            if layer is target_layer:\n",
        "                return tuple(output.shape[1:])\n",
        "            input = output\n",
        "        raise Exception(\"Layer not found in the model.\")\n",
        "    \n",
        "    def get_layer_sizes(self):\n",
        "        \"\"\"\n",
        "        Returns the sizes of all layers in the model, including the input and output layer.\n",
        "        \"\"\"\n",
        "        layer_sizes = list()\n",
        "        first_layer = True\n",
        "        for l in range(len(self.lrs)):\n",
        "            layer = self.lrs[l]\n",
        "            if isinstance(layer, CustomLayer):\n",
        "                layer_size = layer.get_size()\n",
        "                if first_layer:\n",
        "                    layer_sizes.append(layer_size[0])\n",
        "                    first_layer = False\n",
        "                layer_sizes.append(layer_size[1])\n",
        "        return layer_sizes\n",
        "    \n",
        "    def get_hidden_layer_sizes(self):\n",
        "        return self.get_layer_sizes()[1:-1]\n",
        "    \n",
        "    def remove_regularization(self):\n",
        "        self.set_regularization_penalty(0.)\n",
        "    \n",
        "    def get_regularization_penalty(self):\n",
        "        #TODO improve\n",
        "        return self.lrs[-2].regularizer.regularization_penalty\n",
        "    \n",
        "    def set_regularization_penalty(self, regularization_penalty):\n",
        "        for layer in self.lrs:\n",
        "            if isinstance(layer, CustomLayer) and not layer.fixed_size:\n",
        "                layer.regularizer.regularization_penalty = regularization_penalty\n",
        "    \n",
        "    def prune(self, params):\n",
        "        input_shape = self.get_layer_input_shape(self.lrs[0])\n",
        "        n_input_units = input_shape[-1]\n",
        "        active_units_indices = list(range(n_input_units))\n",
        "\n",
        "        last_custom_layer = None\n",
        "        for layer in self.lrs:\n",
        "            if isinstance(layer, CustomLayer):\n",
        "                if last_custom_layer is not None and type(last_custom_layer) != type(layer):\n",
        "                    if type(last_custom_layer) == Conv2D and type(layer) == Dense:\n",
        "                        convolutional_shape = self.get_layer_output_shape(last_custom_layer)\n",
        "                        active_units_indices = self.convert_channel_indices_to_flattened_indices(active_units_indices, convolutional_shape)\n",
        "                    else:\n",
        "                        raise Exception(\"Incorrect order of custom layer types.\")\n",
        "                active_units_indices = layer.prune(params.pruning_threshold, active_units_indices)\n",
        "                last_custom_layer = layer\n",
        "    \n",
        "    def grow(self, params):   \n",
        "        n_new_units = 0\n",
        "\n",
        "        last_custom_layer = None\n",
        "        for layer in self.lrs:\n",
        "            if isinstance(layer, CustomLayer):\n",
        "                if last_custom_layer is not None and type(last_custom_layer) != type(layer):\n",
        "                    if type(last_custom_layer) == Conv2D and type(layer) == Dense:\n",
        "                        convolutional_shape = self.get_layer_output_shape(last_custom_layer)\n",
        "                        n_new_units = n_new_units * convolutional_shape[0] * convolutional_shape[1]\n",
        "                    else:\n",
        "                        raise Exception(\"Incorrect order of custom layer types.\")\n",
        "                n_new_units = layer.grow(n_new_units, params.growth_percentage, min_new_units=params.min_new_neurons, scaling_factor=params.pruning_threshold)\n",
        "                last_custom_layer = layer\n",
        "    \n",
        "    @staticmethod\n",
        "    def convert_channel_indices_to_flattened_indices(channel_indices, convolutional_shape):\n",
        "        dense_indices = list()\n",
        "        units_per_channel = convolutional_shape[0] * convolutional_shape[1]\n",
        "        for channel_index in channel_indices:\n",
        "            for iter in range(units_per_channel):\n",
        "                dense_indices.append(channel_index * units_per_channel + iter)\n",
        "        return dense_indices\n",
        "    \n",
        "    def print_neurons(self):\n",
        "        for layer in self.lrs[:-1]:\n",
        "            print(layer.get_param_string())\n",
        "    \n",
        "    def evaluate(self, params, summed_training_loss, summed_training_accuracy):\n",
        "        # Calculate training loss and accuracy\n",
        "        if summed_training_loss is not None:\n",
        "            loss = summed_training_loss / params.x.shape[0]\n",
        "        else:\n",
        "            loss = None\n",
        "        \n",
        "        if summed_training_accuracy is not None:\n",
        "            accuracy = summed_training_accuracy / params.x.shape[0]\n",
        "        else:\n",
        "            accuracy = None\n",
        "        \n",
        "        # Calculate val loss and accuracy\n",
        "        summed_val_loss = 0\n",
        "        summed_val_accuracy = 0\n",
        "        n_val_instances = 0\n",
        "        \n",
        "        for step, (x_batch, y_batch) in enumerate(params.val_dataset):\n",
        "            y_pred = self(x_batch, training=False)\n",
        "            summed_val_loss += tf.reduce_sum(tf.keras.losses.sparse_categorical_crossentropy(y_batch, y_pred))\n",
        "            summed_val_accuracy += float(tf.reduce_sum(tf.keras.metrics.sparse_categorical_accuracy(y_batch, y_pred)))\n",
        "            n_val_instances += x_batch.shape[0]\n",
        "        \n",
        "        val_loss = summed_val_loss / n_val_instances\n",
        "        val_accuracy = summed_val_accuracy / n_val_instances\n",
        "\n",
        "        return loss, accuracy, val_loss, val_accuracy\n",
        "    \n",
        "    def print_epoch_statistics(self, params, summed_training_loss, summed_training_accuracy, message=None, require_result=False):\n",
        "        if not params.verbose:\n",
        "            if require_result:\n",
        "                return self.evaluate(params, summed_training_loss, summed_training_accuracy)\n",
        "            else:\n",
        "                return\n",
        "        \n",
        "        loss, accuracy, val_loss, val_accuracy = self.evaluate(params, summed_training_loss, summed_training_accuracy)  \n",
        "\n",
        "        if message is not None:\n",
        "            print(message)\n",
        "        \n",
        "        print(f\"loss: {loss} - accuracy: {accuracy} - val_loss: {val_loss} - val_accuracy: {val_accuracy} - penalty: {self.get_regularization_penalty()}\")\n",
        "        hidden_layer_sizes = self.get_hidden_layer_sizes()\n",
        "        print(f\"hidden layer sizes: {hidden_layer_sizes}, total units: {sum(hidden_layer_sizes)}\")\n",
        "        if params.print_neurons:\n",
        "            self.print_neurons()\n",
        "        \n",
        "        if require_result:\n",
        "            return loss, accuracy, val_loss, val_accuracy\n",
        "    \n",
        "    def update_history(self, params, loss, accuracy, val_loss, val_accuracy):\n",
        "        params.history['loss'].append(loss)\n",
        "        params.history['accuracy'].append(accuracy)\n",
        "        params.history['val_loss'].append(val_loss)\n",
        "        params.history['val_accuracy'].append(val_accuracy)\n",
        "        params.history['hidden_layer_sizes'].append(self.get_hidden_layer_sizes())\n",
        "    \n",
        "    @staticmethod\n",
        "    def prepare_datasets(x, y, batch_size, validation_data):\n",
        "        train_dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
        "        train_dataset = train_dataset.shuffle(buffer_size=20000).batch(batch_size)\n",
        "        val_dataset = tf.data.Dataset.from_tensor_slices(validation_data).batch(batch_size)\n",
        "        return train_dataset.prefetch(tf.data.AUTOTUNE), val_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "    \n",
        "    def manage_dynamic_regularization(self, params, val_loss):\n",
        "        if val_loss >= params.best_conditional_val_loss * params.stall_coefficient:\n",
        "            # Training is currently in stall\n",
        "            if not params.training_stalled:\n",
        "                penalty = self.get_regularization_penalty() * params.regularization_penalty_multiplier\n",
        "                print(\"Changing penalty...\")\n",
        "                # TODO this must be modified, penalty can differ for each layer\n",
        "                self.set_regularization_penalty(penalty)\n",
        "                params.training_stalled = True\n",
        "        else:\n",
        "            params.best_conditional_val_loss = val_loss\n",
        "            params.training_stalled = False\n",
        "    \n",
        "    def grow_wrapper(self, params):\n",
        "        dynamic_reqularization_active = params.regularization_penalty_multiplier != 1.\n",
        "        if dynamic_reqularization_active:\n",
        "            loss, accuracy, val_loss, val_accuracy = self.print_epoch_statistics(params, None, None, \"Before growing:\", require_result=True)\n",
        "            self.manage_dynamic_regularization(params, val_loss)\n",
        "        else:\n",
        "            self.print_epoch_statistics(params, None, None, \"Before growing:\")\n",
        "\n",
        "        self.grow(params)\n",
        "        self.print_epoch_statistics(params, None, None, \"After growing:\")\n",
        "    \n",
        "    def prune_wrapper(self, params, summed_loss, summed_accuracy):\n",
        "        loss, accuracy, _, _ = self.print_epoch_statistics(params, summed_loss, summed_accuracy, \"Before pruning:\", require_result=True)\n",
        "        self.prune(params)\n",
        "        _, _, val_loss, val_accuracy = self.print_epoch_statistics(params, None, None, \"After pruning:\", require_result=True)\n",
        "\n",
        "        self.update_history(params, loss, accuracy, val_loss, val_accuracy)\n",
        "    \n",
        "    class ParameterContainer:\n",
        "        def __init__(self, x, y, optimizer, epochs, self_scaling_epochs, batch_size, min_new_neurons, validation_data, pruning_threshold, \n",
        "                regularization_penalty_multiplier, stall_coefficient, growth_percentage, mini_epochs_per_epoch, verbose, print_neurons, use_static_graph):\n",
        "            self.x = x\n",
        "            self.y = y\n",
        "            self.optimizer = optimizer\n",
        "            self.epochs = epochs\n",
        "            self.self_scaling_epochs = self_scaling_epochs\n",
        "            self.batch_size = batch_size\n",
        "            self.min_new_neurons = min_new_neurons\n",
        "            self.validation_data = validation_data\n",
        "            self.pruning_threshold = pruning_threshold\n",
        "            self.regularization_penalty_multiplier = regularization_penalty_multiplier\n",
        "            self.stall_coefficient = stall_coefficient\n",
        "            self.growth_percentage = growth_percentage\n",
        "            self.mini_epochs_per_epoch = mini_epochs_per_epoch\n",
        "            self.verbose = verbose\n",
        "            self.print_neurons = print_neurons\n",
        "            self.use_static_graph = use_static_graph\n",
        "\n",
        "            self.train_dataset, self.val_dataset = Sequential.prepare_datasets(x, y, batch_size, validation_data)\n",
        "            self.history = self.prepare_history()\n",
        "\n",
        "            self.best_conditional_val_loss = np.inf\n",
        "            self.training_stalled = False\n",
        "        \n",
        "        @staticmethod\n",
        "        def prepare_history():\n",
        "            history = {\n",
        "                'loss': list(),\n",
        "                'accuracy': list(),\n",
        "                'val_loss': list(),\n",
        "                'val_accuracy': list(),\n",
        "                'hidden_layer_sizes': list(),\n",
        "            }\n",
        "            return history\n",
        "    \n",
        "    def fit_single_step(self, params, x_batch, y_batch):\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = self(x_batch, training=True)\n",
        "            raw_loss = tf.keras.losses.sparse_categorical_crossentropy(y_batch, y_pred)\n",
        "            loss_value = tf.reduce_mean(raw_loss)\n",
        "            loss_value += sum(self.losses)  # Add losses registered by model.add_loss\n",
        "\n",
        "            loss = tf.reduce_sum(raw_loss)\n",
        "            accuracy = float(tf.reduce_sum(tf.keras.metrics.sparse_categorical_accuracy(y_batch, y_pred)))\n",
        "\n",
        "        grads = tape.gradient(loss_value, self.trainable_variables)\n",
        "        params.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
        "\n",
        "        return loss, accuracy\n",
        "    \n",
        "    def fit_single_epoch(self, params):\n",
        "        summed_loss = 0\n",
        "        summed_accuracy = 0\n",
        "        \n",
        "        for mini_epoch in range(params.mini_epochs_per_epoch):\n",
        "            summed_loss = 0\n",
        "            summed_accuracy = 0\n",
        "\n",
        "            if params.use_static_graph:\n",
        "                fit_single_step_function = tf.function(self.fit_single_step)\n",
        "            else:\n",
        "                fit_single_step_function = self.fit_single_step\n",
        "            for step, (x_batch, y_batch) in enumerate(params.train_dataset):\n",
        "                loss, accuracy = fit_single_step_function(params, x_batch, y_batch)\n",
        "                summed_loss += loss\n",
        "                summed_accuracy += accuracy\n",
        "        \n",
        "        return summed_loss, summed_accuracy\n",
        "\n",
        "    def fit(self, x, y, optimizer, epochs, self_scaling_epochs, batch_size, min_new_neurons, validation_data, pruning_threshold=0.001, regularization_penalty_multiplier=1., \n",
        "            stall_coefficient=1, growth_percentage=0.2, mini_epochs_per_epoch=1, pruning_only_epochs=0, verbose=True, print_neurons=False, use_static_graph=True):\n",
        "        params = self.ParameterContainer(x, y, optimizer, epochs, self_scaling_epochs, batch_size, min_new_neurons, \n",
        "                                         validation_data, pruning_threshold, regularization_penalty_multiplier, stall_coefficient, \n",
        "                                         growth_percentage, mini_epochs_per_epoch, verbose, print_neurons, use_static_graph)\n",
        "        self.build(x.shape)  # Necessary when verbose == False\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            if verbose:\n",
        "                print(\"##########################################################\")\n",
        "                print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "\n",
        "            if epoch < self_scaling_epochs - pruning_only_epochs:\n",
        "                self.grow_wrapper(params)\n",
        "            \n",
        "            if epoch == self_scaling_epochs:\n",
        "                self.remove_regularization()\n",
        "\n",
        "            summed_loss, summed_accuracy = self.fit_single_epoch(params)\n",
        "            \n",
        "            if epoch < self_scaling_epochs:\n",
        "                self.prune_wrapper(params, summed_loss, summed_accuracy)\n",
        "            else:\n",
        "                loss, accuracy, val_loss, val_accuracy = self.print_epoch_statistics(params, summed_loss, summed_accuracy, require_result=True)\n",
        "                self.update_history(params, loss, accuracy, val_loss, val_accuracy)\n",
        "\n",
        "        return params.history\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# HELPER FUNCTIONS\n",
        "################################################################################\n",
        "\n",
        "\n",
        "def get_statistics_from_history(history):\n",
        "    best_epoch_number = np.argmax(history['val_accuracy'])\n",
        "    best_val_accuracy = history['val_accuracy'][best_epoch_number]\n",
        "    best_hidden_layer_sizes = history['hidden_layer_sizes'][best_epoch_number]\n",
        "    return best_val_accuracy, best_hidden_layer_sizes\n",
        "\n",
        "\n",
        "def get_statistics_from_histories(histories):\n",
        "    best_val_accuracies = list()\n",
        "    all_best_hidden_layer_sizes = list()\n",
        "\n",
        "    for history in histories:\n",
        "        best_val_accuracy, best_hidden_layer_sizes = get_statistics_from_history(history)\n",
        "        best_val_accuracies.append(best_val_accuracy)\n",
        "        all_best_hidden_layer_sizes.append(best_hidden_layer_sizes)\n",
        "    \n",
        "    mean_best_val_accuracy = np.mean(best_val_accuracies)\n",
        "    mean_best_hidden_layer_sizes = [np.mean(layer) for layer in list(zip(*all_best_hidden_layer_sizes))]\n",
        "    \n",
        "    return mean_best_val_accuracy, mean_best_hidden_layer_sizes\n",
        "\n",
        "\n",
        "def cross_validate(train_fn, x, y, n_splits, random_state=42, *args, **kwargs):\n",
        "    from sklearn.model_selection import KFold\n",
        "\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
        "\n",
        "    histories = list()\n",
        "    for i, (train_index, test_index) in enumerate(kf.split(x)):\n",
        "        xtrain, xtest = x[train_index], x[test_index]\n",
        "        ytrain, ytest = y[train_index], y[test_index]\n",
        "\n",
        "        history = train_fn(xtrain, ytrain, validation_data=(xtest, ytest), *args, **kwargs)\n",
        "        histories.append(history)\n",
        "\n",
        "        best_val_accuracy, best_hidden_layer_sizes = get_statistics_from_history(history)\n",
        "        print(f\"Run {i} completed, best_val_accuracy: {best_val_accuracy}, best_hidden_layer_sizes: {best_hidden_layer_sizes}\")\n",
        "\n",
        "    mean_best_val_accuracy, mean_best_hidden_layer_sizes = get_statistics_from_histories(histories)\n",
        "    print(f'mean_best_val_accuracy: {mean_best_val_accuracy}')\n",
        "    print(f'mean_best_hidden_layer_sizes: {mean_best_hidden_layer_sizes}')\n",
        "\n",
        "    return histories\n",
        "\n",
        "\n",
        "def hyperparameter_search(train_fn, x, y, validation_data, *args, **kwargs):\n",
        "    from itertools import product\n",
        "\n",
        "    all_params = [*args] + list(kwargs.values())\n",
        "    histories = list()\n",
        "\n",
        "    best_overall_val_accuracy = -np.inf\n",
        "    best_overall_combination = None\n",
        "\n",
        "    for combination in product(*all_params):\n",
        "        combination_args = combination[:len(args)]\n",
        "\n",
        "        combination_kwargs_values = combination[len(args):]\n",
        "        combination_kwargs = dict(zip(kwargs.keys(), combination_kwargs_values))\n",
        "\n",
        "        history = train_fn(x, y, validation_data, *combination_args, **combination_kwargs)\n",
        "        history['parameters'] = combination\n",
        "        histories.append(history)\n",
        "\n",
        "        best_val_accuracy, best_hidden_layer_sizes = get_statistics_from_history(history)\n",
        "        print(f\"Run with parameters {combination} completed, best_val_accuracy: {best_val_accuracy}, best_hidden_layer_sizes sizes: {best_hidden_layer_sizes}\")\n",
        "\n",
        "        if best_val_accuracy > best_overall_val_accuracy:\n",
        "            best_overall_val_accuracy = best_val_accuracy\n",
        "            best_overall_combination = combination\n",
        "    \n",
        "    print(f'Best overall combination: {best_overall_combination}, val_accuracy: {best_overall_val_accuracy}')\n",
        "\n",
        "    return histories"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1MrQXUTFwOe"
      },
      "source": [
        "# Accuracy benchmark - FF and convolutions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdBhEgOUHfLq"
      },
      "source": [
        "def get_convolutional_model(x, regularization_penalty, regularization_method, layer_sizes, output_neurons=10):\n",
        "    model = Sequential([\n",
        "        Conv2D(layer_sizes[0], filter_size=(3, 3), activation='selu', strides=(1, 1), padding='SAME', \n",
        "            regularization_penalty=regularization_penalty, regularization_method=regularization_method, \n",
        "            kernel_initializer='lecun_normal', input_shape=x[0,:,:,:].shape),\n",
        "        Conv2D(layer_sizes[1], filter_size=(3, 3), activation='selu', strides=(2, 2), padding='SAME', \n",
        "            regularization_penalty=regularization_penalty, regularization_method=regularization_method, \n",
        "            kernel_initializer='lecun_normal'),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        Conv2D(layer_sizes[2], filter_size=(3, 3), activation='selu', strides=(1, 1), padding='SAME', \n",
        "            regularization_penalty=regularization_penalty, regularization_method=regularization_method, \n",
        "            kernel_initializer='lecun_normal'),\n",
        "        Conv2D(layer_sizes[3], filter_size=(3, 3), activation='selu', strides=(2, 2), padding='SAME', \n",
        "            regularization_penalty=regularization_penalty, regularization_method=regularization_method, \n",
        "            kernel_initializer='lecun_normal'),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        Flatten(),\n",
        "        Dense(layer_sizes[4], activation='selu', regularization_penalty=regularization_penalty, \n",
        "            regularization_method=regularization_method, kernel_initializer='lecun_normal'),\n",
        "        Dense(output_neurons, activation='softmax', regularization_penalty=0., \n",
        "            regularization_method=None, fixed_size=True),\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_fn(x, y, validation_data, learning_rate, regularization_penalty, regularization_method, self_scaling_epochs, layer_sizes, output_neurons=10, \n",
        "             epochs=40, pruning_only_epochs=0, min_new_neurons=20, growth_percentage=0.2, verbose=False, use_static_graph=True):\n",
        "    batch_size = 128\n",
        "\n",
        "    model = get_convolutional_model(x, regularization_penalty, regularization_method, layer_sizes, output_neurons)\n",
        "    \n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "    history = model.fit(x, y, optimizer, epochs, self_scaling_epochs, batch_size, \n",
        "                        min_new_neurons, validation_data=validation_data, pruning_only_epochs=pruning_only_epochs, \n",
        "                        growth_percentage=growth_percentage, verbose=verbose, use_static_graph=use_static_graph)\n",
        "    \n",
        "    return history"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsivpauwveEK"
      },
      "source": [
        "## CIFAR10"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cifar10 = get_cifar_10_dataset()"
      ],
      "metadata": {
        "id": "SjJ2e9njMl04"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Misc experiments"
      ],
      "metadata": {
        "id": "mJeDaeqdfnmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "histories = hyperparameter_search(train_fn, x=cifar10.X_train_norm, y=cifar10.y_train, validation_data=(cifar10.X_test_norm, cifar10.y_test), \n",
        "                                  learning_rate=[0.00005, 0.0002, 0.0006], regularization_penalty=[0.00002], regularization_method=['weighted_l1'], \n",
        "                                  self_scaling_epochs=[20], layer_sizes=[[100, 100, 100, 100, 100]])"
      ],
      "metadata": {
        "id": "ikXTxRgtPZSO",
        "outputId": "06dec896-945c-4871-a4e3-d6bd78a9b367",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run with parameters (5e-05, 2e-05, 'weighted_l1', 20, [100, 100, 100, 100, 100]) completed, best_val_accuracy: 0.6983, best_hidden_layer_sizes sizes: [100, 44, 39, 55, 63]\n",
            "Run with parameters (0.0002, 2e-05, 'weighted_l1', 20, [100, 100, 100, 100, 100]) completed, best_val_accuracy: 0.7588, best_hidden_layer_sizes sizes: [52, 19, 26, 47, 96]\n",
            "Run with parameters (0.0006, 2e-05, 'weighted_l1', 20, [100, 100, 100, 100, 100]) completed, best_val_accuracy: 0.768, best_hidden_layer_sizes sizes: [34, 20, 30, 63, 293]\n",
            "Best overall combination: (0.0006, 2e-05, 'weighted_l1', 20, [100, 100, 100, 100, 100]), val_accuracy: 0.768\n",
            "CPU times: user 9min 21s, sys: 19 s, total: 9min 40s\n",
            "Wall time: 8min 38s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "histories = cross_validate(train_fn, x=cifar10.X_norm, y=cifar10.y, n_splits=6, learning_rate=0.0006,\n",
        "                           regularization_penalty=0.00002, regularization_method='weighted_l1',\n",
        "                           self_scaling_epochs=20, layer_sizes=[100, 100, 100, 100, 100])"
      ],
      "metadata": {
        "id": "vzdK7MxpTf4q",
        "outputId": "4e3e3ca9-b5ac-4174-8a94-95c8c9607bd3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run 0 completed, best_val_accuracy: 0.7676, best_hidden_layer_sizes: [36, 18, 41, 54, 286]\n",
            "Run 1 completed, best_val_accuracy: 0.7569, best_hidden_layer_sizes: [36, 19, 33, 91, 250]\n",
            "Run 2 completed, best_val_accuracy: 0.7683, best_hidden_layer_sizes: [34, 18, 26, 67, 261]\n",
            "Run 3 completed, best_val_accuracy: 0.7627, best_hidden_layer_sizes: [36, 15, 23, 71, 227]\n",
            "Run 4 completed, best_val_accuracy: 0.7792, best_hidden_layer_sizes: [32, 15, 37, 59, 272]\n",
            "Run 5 completed, best_val_accuracy: 0.767, best_hidden_layer_sizes: [32, 16, 22, 61, 267]\n",
            "mean_best_val_accuracy: 0.76695\n",
            "mean_best_hidden_layer_sizes: [34.333333333333336, 16.833333333333332, 30.333333333333332, 67.16666666666667, 260.5]\n",
            "CPU times: user 18min 42s, sys: 38 s, total: 19min 20s\n",
            "Wall time: 16min 25s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "histories = hyperparameter_search(train_fn, x=cifar10.X_train_norm, y=cifar10.y_train, validation_data=(cifar10.X_test_norm, cifar10.y_test), \n",
        "                                  learning_rate=[0.0005, 0.002, 0.006], regularization_penalty=[0.], regularization_method=[None], \n",
        "                                  self_scaling_epochs=[0], layer_sizes=[[34, 17, 30, 67, 261]])"
      ],
      "metadata": {
        "id": "SqhKcPiJYRvQ",
        "outputId": "55758517-ca9f-4a7c-f187-be68a165f1b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run with parameters (0.0005, 0.0, None, 0, [34, 17, 30, 67, 261]) completed, best_val_accuracy: 0.6542, best_hidden_layer_sizes sizes: [34, 17, 30, 67, 261]\n",
            "Run with parameters (0.002, 0.0, None, 0, [34, 17, 30, 67, 261]) completed, best_val_accuracy: 0.7267, best_hidden_layer_sizes sizes: [34, 17, 30, 67, 261]\n",
            "Run with parameters (0.006, 0.0, None, 0, [34, 17, 30, 67, 261]) completed, best_val_accuracy: 0.5603, best_hidden_layer_sizes sizes: [34, 17, 30, 67, 261]\n",
            "Best overall combination: (0.002, 0.0, None, 0, [34, 17, 30, 67, 261]), val_accuracy: 0.7267\n",
            "CPU times: user 6min 43s, sys: 16.9 s, total: 7min\n",
            "Wall time: 5min 20s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "histories = hyperparameter_search(train_fn, x=cifar10.X_train_norm, y=cifar10.y_train, validation_data=(cifar10.X_test_norm, cifar10.y_test), \n",
        "                                  learning_rate=[0.00005, 0.0002, 0.0006], regularization_penalty=[0.], regularization_method=[None], \n",
        "                                  self_scaling_epochs=[0], layer_sizes=[[34, 17, 30, 67, 261]])"
      ],
      "metadata": {
        "id": "0GKxUHK6acqB",
        "outputId": "d2146fa6-248c-4cbb-903e-beba3d622c6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run with parameters (5e-05, 0.0, None, 0, [34, 17, 30, 67, 261]) completed, best_val_accuracy: 0.6221, best_hidden_layer_sizes sizes: [34, 17, 30, 67, 261]\n",
            "Run with parameters (0.0002, 0.0, None, 0, [34, 17, 30, 67, 261]) completed, best_val_accuracy: 0.658, best_hidden_layer_sizes sizes: [34, 17, 30, 67, 261]\n",
            "Run with parameters (0.0006, 0.0, None, 0, [34, 17, 30, 67, 261]) completed, best_val_accuracy: 0.6556, best_hidden_layer_sizes sizes: [34, 17, 30, 67, 261]\n",
            "Best overall combination: (0.0002, 0.0, None, 0, [34, 17, 30, 67, 261]), val_accuracy: 0.658\n",
            "CPU times: user 6min 43s, sys: 16.9 s, total: 7min\n",
            "Wall time: 5min 21s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "histories = hyperparameter_search(train_fn, x=cifar10.X_train_norm, y=cifar10.y_train, validation_data=(cifar10.X_test_norm, cifar10.y_test), \n",
        "                                  learning_rate=[0.001, 0.003, 0.004], regularization_penalty=[0.], regularization_method=[None], \n",
        "                                  self_scaling_epochs=[0], layer_sizes=[[34, 17, 30, 67, 261]])"
      ],
      "metadata": {
        "id": "-pOO8wZobKwF",
        "outputId": "7737242b-df92-4da0-fdfc-3f6e3510d07c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run with parameters (0.001, 0.0, None, 0, [34, 17, 30, 67, 261]) completed, best_val_accuracy: 0.6858, best_hidden_layer_sizes sizes: [34, 17, 30, 67, 261]\n",
            "Run with parameters (0.003, 0.0, None, 0, [34, 17, 30, 67, 261]) completed, best_val_accuracy: 0.7017, best_hidden_layer_sizes sizes: [34, 17, 30, 67, 261]\n",
            "Run with parameters (0.004, 0.0, None, 0, [34, 17, 30, 67, 261]) completed, best_val_accuracy: 0.6586, best_hidden_layer_sizes sizes: [34, 17, 30, 67, 261]\n",
            "Best overall combination: (0.003, 0.0, None, 0, [34, 17, 30, 67, 261]), val_accuracy: 0.7017\n",
            "CPU times: user 6min 42s, sys: 16.7 s, total: 6min 58s\n",
            "Wall time: 5min 19s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reordering evaluation "
      ],
      "metadata": {
        "id": "bROnao4KfrVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "\n",
        "model = get_convolutional_model(cifar10.X_train_norm, regularization_penalty=0.00002, regularization_method='weighted_l1', layer_sizes=[100, 100, 100, 100, 100], output_neurons=10)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0006)\n",
        "\n",
        "history = model.fit(cifar10.X_train_norm, cifar10.y_train, optimizer, epochs=20, self_scaling_epochs=20, batch_size=batch_size, \n",
        "                    min_new_neurons=20, validation_data=(cifar10.X_test_norm, cifar10.y_test), pruning_only_epochs=0, \n",
        "                    growth_percentage=0.2, verbose=True)"
      ],
      "metadata": {
        "id": "WnUhPEMBMeGu",
        "outputId": "c6f39666-770a-440a-a6a8-3ef9d3a1d084",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##########################################################\n",
            "Epoch 1/20\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 3.057939291000366 - val_accuracy: 0.0943 - penalty: 2e-05\n",
            "hidden layer sizes: [100, 100, 100, 100, 100], total units: 500\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 3.057939291000366 - val_accuracy: 0.0943 - penalty: 2e-05\n",
            "hidden layer sizes: [120, 120, 120, 120, 120], total units: 600\n",
            "Before pruning:\n",
            "loss: 1.7466492652893066 - accuracy: 0.3958800137042999 - val_loss: 1.5053508281707764 - val_accuracy: 0.4652 - penalty: 2e-05\n",
            "hidden layer sizes: [120, 120, 120, 120, 120], total units: 600\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 1.505497932434082 - val_accuracy: 0.4655 - penalty: 2e-05\n",
            "hidden layer sizes: [100, 100, 100, 100, 105], total units: 505\n",
            "##########################################################\n",
            "Epoch 2/20\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 1.505497932434082 - val_accuracy: 0.4655 - penalty: 2e-05\n",
            "hidden layer sizes: [100, 100, 100, 100, 105], total units: 505\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 1.505497932434082 - val_accuracy: 0.4655 - penalty: 2e-05\n",
            "hidden layer sizes: [120, 120, 120, 120, 126], total units: 606\n",
            "Before pruning:\n",
            "loss: 1.4750251770019531 - accuracy: 0.4690600037574768 - val_loss: 1.2498983144760132 - val_accuracy: 0.5561 - penalty: 2e-05\n",
            "hidden layer sizes: [120, 120, 120, 120, 126], total units: 606\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 1.2499985694885254 - val_accuracy: 0.5562 - penalty: 2e-05\n",
            "hidden layer sizes: [100, 81, 100, 83, 114], total units: 478\n",
            "##########################################################\n",
            "Epoch 3/20\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 1.2499985694885254 - val_accuracy: 0.5562 - penalty: 2e-05\n",
            "hidden layer sizes: [100, 81, 100, 83, 114], total units: 478\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 1.2499985694885254 - val_accuracy: 0.5562 - penalty: 2e-05\n",
            "hidden layer sizes: [120, 101, 120, 103, 136], total units: 580\n",
            "Before pruning:\n",
            "loss: 1.3131948709487915 - accuracy: 0.5279800295829773 - val_loss: 1.1899751424789429 - val_accuracy: 0.5676 - penalty: 2e-05\n",
            "hidden layer sizes: [120, 101, 120, 103, 136], total units: 580\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 1.1897356510162354 - val_accuracy: 0.5677 - penalty: 2e-05\n",
            "hidden layer sizes: [100, 45, 82, 57, 127], total units: 411\n",
            "##########################################################\n",
            "Epoch 4/20\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 1.1897356510162354 - val_accuracy: 0.5677 - penalty: 2e-05\n",
            "hidden layer sizes: [100, 45, 82, 57, 127], total units: 411\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 1.1897358894348145 - val_accuracy: 0.5677 - penalty: 2e-05\n",
            "hidden layer sizes: [120, 65, 102, 77, 152], total units: 516\n",
            "Before pruning:\n",
            "loss: 1.2375969886779785 - accuracy: 0.5590599775314331 - val_loss: 1.0969624519348145 - val_accuracy: 0.6005 - penalty: 2e-05\n",
            "hidden layer sizes: [120, 65, 102, 77, 152], total units: 516\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 1.0970027446746826 - val_accuracy: 0.6004 - penalty: 2e-05\n",
            "hidden layer sizes: [97, 29, 66, 60, 146], total units: 398\n",
            "##########################################################\n",
            "Epoch 5/20\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 1.0970027446746826 - val_accuracy: 0.6004 - penalty: 2e-05\n",
            "hidden layer sizes: [97, 29, 66, 60, 146], total units: 398\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 1.0970027446746826 - val_accuracy: 0.6004 - penalty: 2e-05\n",
            "hidden layer sizes: [117, 49, 86, 80, 175], total units: 507\n",
            "Before pruning:\n",
            "loss: 1.1858901977539062 - accuracy: 0.5779399871826172 - val_loss: 1.0350512266159058 - val_accuracy: 0.6325 - penalty: 2e-05\n",
            "hidden layer sizes: [117, 49, 86, 80, 175], total units: 507\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 1.03499174118042 - val_accuracy: 0.6325 - penalty: 2e-05\n",
            "hidden layer sizes: [91, 19, 53, 60, 160], total units: 383\n",
            "##########################################################\n",
            "Epoch 6/20\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 1.03499174118042 - val_accuracy: 0.6325 - penalty: 2e-05\n",
            "hidden layer sizes: [91, 19, 53, 60, 160], total units: 383\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 1.0349918603897095 - val_accuracy: 0.6325 - penalty: 2e-05\n",
            "hidden layer sizes: [111, 39, 73, 80, 192], total units: 495\n",
            "Before pruning:\n",
            "loss: 1.146216869354248 - accuracy: 0.5902000069618225 - val_loss: 0.9992194175720215 - val_accuracy: 0.6424 - penalty: 2e-05\n",
            "hidden layer sizes: [111, 39, 73, 80, 192], total units: 495\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 0.9992177486419678 - val_accuracy: 0.642 - penalty: 2e-05\n",
            "hidden layer sizes: [69, 17, 42, 60, 178], total units: 366\n",
            "##########################################################\n",
            "Epoch 7/20\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 0.9992177486419678 - val_accuracy: 0.642 - penalty: 2e-05\n",
            "hidden layer sizes: [69, 17, 42, 60, 178], total units: 366\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 0.9992177486419678 - val_accuracy: 0.642 - penalty: 2e-05\n",
            "hidden layer sizes: [89, 37, 62, 80, 213], total units: 481\n",
            "Before pruning:\n",
            "loss: 1.118972897529602 - accuracy: 0.6016600131988525 - val_loss: 0.966903030872345 - val_accuracy: 0.6557 - penalty: 2e-05\n",
            "hidden layer sizes: [89, 37, 62, 80, 213], total units: 481\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 0.9666222929954529 - val_accuracy: 0.6556 - penalty: 2e-05\n",
            "hidden layer sizes: [67, 17, 41, 67, 173], total units: 365\n",
            "##########################################################\n",
            "Epoch 8/20\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 0.9666222929954529 - val_accuracy: 0.6556 - penalty: 2e-05\n",
            "hidden layer sizes: [67, 17, 41, 67, 173], total units: 365\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 0.9666222929954529 - val_accuracy: 0.6556 - penalty: 2e-05\n",
            "hidden layer sizes: [87, 37, 61, 87, 207], total units: 479\n",
            "Before pruning:\n",
            "loss: 1.0923619270324707 - accuracy: 0.6108999848365784 - val_loss: 0.9338831901550293 - val_accuracy: 0.6745 - penalty: 2e-05\n",
            "hidden layer sizes: [87, 37, 61, 87, 207], total units: 479\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 0.933876633644104 - val_accuracy: 0.6746 - penalty: 2e-05\n",
            "hidden layer sizes: [60, 17, 44, 66, 202], total units: 389\n",
            "##########################################################\n",
            "Epoch 9/20\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 0.933876633644104 - val_accuracy: 0.6746 - penalty: 2e-05\n",
            "hidden layer sizes: [60, 17, 44, 66, 202], total units: 389\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 0.9338765740394592 - val_accuracy: 0.6746 - penalty: 2e-05\n",
            "hidden layer sizes: [80, 37, 64, 86, 242], total units: 509\n",
            "Before pruning:\n",
            "loss: 1.066840648651123 - accuracy: 0.6229000091552734 - val_loss: 0.9372082948684692 - val_accuracy: 0.6636 - penalty: 2e-05\n",
            "hidden layer sizes: [80, 37, 64, 86, 242], total units: 509\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 0.93723064661026 - val_accuracy: 0.6636 - penalty: 2e-05\n",
            "hidden layer sizes: [55, 16, 40, 65, 230], total units: 406\n",
            "##########################################################\n",
            "Epoch 10/20\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 0.93723064661026 - val_accuracy: 0.6636 - penalty: 2e-05\n",
            "hidden layer sizes: [55, 16, 40, 65, 230], total units: 406\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 0.9372305870056152 - val_accuracy: 0.6636 - penalty: 2e-05\n",
            "hidden layer sizes: [75, 36, 60, 85, 276], total units: 532\n",
            "Before pruning:\n",
            "loss: 1.0416220426559448 - accuracy: 0.630020022392273 - val_loss: 0.9240490198135376 - val_accuracy: 0.6693 - penalty: 2e-05\n",
            "hidden layer sizes: [75, 36, 60, 85, 276], total units: 532\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 0.9238019585609436 - val_accuracy: 0.6697 - penalty: 2e-05\n",
            "hidden layer sizes: [49, 16, 38, 64, 234], total units: 401\n",
            "##########################################################\n",
            "Epoch 11/20\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 0.9238019585609436 - val_accuracy: 0.6697 - penalty: 2e-05\n",
            "hidden layer sizes: [49, 16, 38, 64, 234], total units: 401\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 0.9238021373748779 - val_accuracy: 0.6697 - penalty: 2e-05\n",
            "hidden layer sizes: [69, 36, 58, 84, 280], total units: 527\n",
            "Before pruning:\n",
            "loss: 1.027788519859314 - accuracy: 0.6372399926185608 - val_loss: 0.8874439597129822 - val_accuracy: 0.6901 - penalty: 2e-05\n",
            "hidden layer sizes: [69, 36, 58, 84, 280], total units: 527\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 0.8872784972190857 - val_accuracy: 0.6906 - penalty: 2e-05\n",
            "hidden layer sizes: [48, 15, 39, 59, 217], total units: 378\n",
            "##########################################################\n",
            "Epoch 12/20\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 0.8872784972190857 - val_accuracy: 0.6906 - penalty: 2e-05\n",
            "hidden layer sizes: [48, 15, 39, 59, 217], total units: 378\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 0.8872783184051514 - val_accuracy: 0.6906 - penalty: 2e-05\n",
            "hidden layer sizes: [68, 35, 59, 79, 260], total units: 501\n",
            "Before pruning:\n",
            "loss: 1.0151690244674683 - accuracy: 0.6410199999809265 - val_loss: 0.869193971157074 - val_accuracy: 0.6961 - penalty: 2e-05\n",
            "hidden layer sizes: [68, 35, 59, 79, 260], total units: 501\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 0.8690743446350098 - val_accuracy: 0.696 - penalty: 2e-05\n",
            "hidden layer sizes: [45, 15, 33, 62, 223], total units: 378\n",
            "##########################################################\n",
            "Epoch 13/20\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 0.8690743446350098 - val_accuracy: 0.696 - penalty: 2e-05\n",
            "hidden layer sizes: [45, 15, 33, 62, 223], total units: 378\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 0.8690743446350098 - val_accuracy: 0.696 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 35, 53, 82, 267], total units: 502\n",
            "Before pruning:\n",
            "loss: 1.0006461143493652 - accuracy: 0.6457399725914001 - val_loss: 0.8903037309646606 - val_accuracy: 0.6842 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 35, 53, 82, 267], total units: 502\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 0.8902047872543335 - val_accuracy: 0.684 - penalty: 2e-05\n",
            "hidden layer sizes: [43, 15, 34, 61, 241], total units: 394\n",
            "##########################################################\n",
            "Epoch 14/20\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 0.8902047872543335 - val_accuracy: 0.684 - penalty: 2e-05\n",
            "hidden layer sizes: [43, 15, 34, 61, 241], total units: 394\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 0.8902047872543335 - val_accuracy: 0.684 - penalty: 2e-05\n",
            "hidden layer sizes: [63, 35, 54, 81, 289], total units: 522\n",
            "Before pruning:\n",
            "loss: 0.9937612414360046 - accuracy: 0.6483200192451477 - val_loss: 0.8812468647956848 - val_accuracy: 0.6884 - penalty: 2e-05\n",
            "hidden layer sizes: [63, 35, 54, 81, 289], total units: 522\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 0.8814318180084229 - val_accuracy: 0.6881 - penalty: 2e-05\n",
            "hidden layer sizes: [43, 15, 35, 64, 235], total units: 392\n",
            "##########################################################\n",
            "Epoch 15/20\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 0.8814318180084229 - val_accuracy: 0.6881 - penalty: 2e-05\n",
            "hidden layer sizes: [43, 15, 35, 64, 235], total units: 392\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 0.8814318180084229 - val_accuracy: 0.6881 - penalty: 2e-05\n",
            "hidden layer sizes: [63, 35, 55, 84, 282], total units: 519\n",
            "Before pruning:\n",
            "loss: 0.9895923733711243 - accuracy: 0.6479600071907043 - val_loss: 0.8822240233421326 - val_accuracy: 0.6968 - penalty: 2e-05\n",
            "hidden layer sizes: [63, 35, 55, 84, 282], total units: 519\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 0.8823312520980835 - val_accuracy: 0.6964 - penalty: 2e-05\n",
            "hidden layer sizes: [46, 14, 38, 66, 241], total units: 405\n",
            "##########################################################\n",
            "Epoch 16/20\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 0.8823312520980835 - val_accuracy: 0.6964 - penalty: 2e-05\n",
            "hidden layer sizes: [46, 14, 38, 66, 241], total units: 405\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 0.882331132888794 - val_accuracy: 0.6964 - penalty: 2e-05\n",
            "hidden layer sizes: [66, 34, 58, 86, 289], total units: 533\n",
            "Before pruning:\n",
            "loss: 0.9788526296615601 - accuracy: 0.652679979801178 - val_loss: 0.8732669949531555 - val_accuracy: 0.6929 - penalty: 2e-05\n",
            "hidden layer sizes: [66, 34, 58, 86, 289], total units: 533\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 0.8726234436035156 - val_accuracy: 0.6934 - penalty: 2e-05\n",
            "hidden layer sizes: [42, 14, 33, 64, 263], total units: 416\n",
            "##########################################################\n",
            "Epoch 17/20\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 0.8726234436035156 - val_accuracy: 0.6934 - penalty: 2e-05\n",
            "hidden layer sizes: [42, 14, 33, 64, 263], total units: 416\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 0.8726235628128052 - val_accuracy: 0.6934 - penalty: 2e-05\n",
            "hidden layer sizes: [62, 34, 53, 84, 315], total units: 548\n",
            "Before pruning:\n",
            "loss: 0.9704590439796448 - accuracy: 0.657260000705719 - val_loss: 0.8364091515541077 - val_accuracy: 0.7036 - penalty: 2e-05\n",
            "hidden layer sizes: [62, 34, 53, 84, 315], total units: 548\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 0.8361420631408691 - val_accuracy: 0.7039 - penalty: 2e-05\n",
            "hidden layer sizes: [43, 15, 31, 68, 238], total units: 395\n",
            "##########################################################\n",
            "Epoch 18/20\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 0.8361420631408691 - val_accuracy: 0.7039 - penalty: 2e-05\n",
            "hidden layer sizes: [43, 15, 31, 68, 238], total units: 395\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 0.8361421823501587 - val_accuracy: 0.7039 - penalty: 2e-05\n",
            "hidden layer sizes: [63, 35, 51, 88, 285], total units: 522\n",
            "Before pruning:\n",
            "loss: 0.963128924369812 - accuracy: 0.6587799787521362 - val_loss: 0.823143720626831 - val_accuracy: 0.7117 - penalty: 2e-05\n",
            "hidden layer sizes: [63, 35, 51, 88, 285], total units: 522\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 0.8231715559959412 - val_accuracy: 0.7116 - penalty: 2e-05\n",
            "hidden layer sizes: [38, 14, 32, 65, 266], total units: 415\n",
            "##########################################################\n",
            "Epoch 19/20\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 0.8231715559959412 - val_accuracy: 0.7116 - penalty: 2e-05\n",
            "hidden layer sizes: [38, 14, 32, 65, 266], total units: 415\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 0.8231714963912964 - val_accuracy: 0.7116 - penalty: 2e-05\n",
            "hidden layer sizes: [58, 34, 52, 85, 319], total units: 548\n",
            "Before pruning:\n",
            "loss: 0.9577078819274902 - accuracy: 0.6590800285339355 - val_loss: 0.8351711630821228 - val_accuracy: 0.7071 - penalty: 2e-05\n",
            "hidden layer sizes: [58, 34, 52, 85, 319], total units: 548\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 0.8354650139808655 - val_accuracy: 0.7069 - penalty: 2e-05\n",
            "hidden layer sizes: [39, 14, 30, 59, 262], total units: 404\n",
            "##########################################################\n",
            "Epoch 20/20\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 0.8354650139808655 - val_accuracy: 0.7069 - penalty: 2e-05\n",
            "hidden layer sizes: [39, 14, 30, 59, 262], total units: 404\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 0.8354649543762207 - val_accuracy: 0.7069 - penalty: 2e-05\n",
            "hidden layer sizes: [59, 34, 50, 79, 314], total units: 536\n",
            "Before pruning:\n",
            "loss: 0.9506619572639465 - accuracy: 0.6647199988365173 - val_loss: 0.829342782497406 - val_accuracy: 0.7115 - penalty: 2e-05\n",
            "hidden layer sizes: [59, 34, 50, 79, 314], total units: 536\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 0.8295055627822876 - val_accuracy: 0.7114 - penalty: 2e-05\n",
            "hidden layer sizes: [39, 14, 29, 62, 278], total units: 422\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "\n",
        "model = get_convolutional_model(regularization_penalty=0.00002, regularization_method='weighted_l1_reordered', layer_sizes=[100, 100, 100, 100, 100], output_neurons=10)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0006)\n",
        "\n",
        "history = model.fit(X_train_norm, y_train, optimizer, epochs=20, self_scaling_epochs=20, batch_size=batch_size, \n",
        "                    min_new_neurons=20, validation_data=(X_test_norm, y_test), pruning_only_epochs=0, \n",
        "                    growth_percentage=0.2, verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNZABZej0HVd",
        "outputId": "d5b485e3-ff9d-4adb-c40a-b010060f6f3d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##########################################################\n",
            "Epoch 1/20\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 2.8939507007598877 - val_accuracy: 0.1005 - penalty: 2e-05\n",
            "hidden layer sizes: [100, 100, 100, 100, 100], total units: 500\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 2.8939504623413086 - val_accuracy: 0.1005 - penalty: 2e-05\n",
            "hidden layer sizes: [120, 120, 120, 120, 120], total units: 600\n",
            "Before pruning:\n",
            "loss: 1.7473422288894653 - accuracy: 0.39658 - val_loss: 1.4361164569854736 - val_accuracy: 0.4886 - penalty: 2e-05\n",
            "hidden layer sizes: [120, 120, 120, 120, 120], total units: 600\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 1.436206579208374 - val_accuracy: 0.4881 - penalty: 2e-05\n",
            "hidden layer sizes: [100, 100, 100, 100, 106], total units: 506\n",
            "##########################################################\n",
            "Epoch 2/20\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 1.436206579208374 - val_accuracy: 0.4881 - penalty: 2e-05\n",
            "hidden layer sizes: [100, 100, 100, 100, 106], total units: 506\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 1.4362064599990845 - val_accuracy: 0.4881 - penalty: 2e-05\n",
            "hidden layer sizes: [120, 120, 120, 120, 127], total units: 607\n",
            "Before pruning:\n",
            "loss: 1.677731990814209 - accuracy: 0.40232 - val_loss: 1.4387489557266235 - val_accuracy: 0.4833 - penalty: 2e-05\n",
            "hidden layer sizes: [120, 120, 120, 120, 127], total units: 607\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 1.4385145902633667 - val_accuracy: 0.4839 - penalty: 2e-05\n",
            "hidden layer sizes: [100, 99, 98, 96, 124], total units: 517\n",
            "##########################################################\n",
            "Epoch 3/20\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 1.4385145902633667 - val_accuracy: 0.4839 - penalty: 2e-05\n",
            "hidden layer sizes: [100, 99, 98, 96, 124], total units: 517\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 1.4385145902633667 - val_accuracy: 0.4839 - penalty: 2e-05\n",
            "hidden layer sizes: [120, 119, 118, 116, 148], total units: 621\n",
            "Before pruning:\n",
            "loss: 1.5032627582550049 - accuracy: 0.45452 - val_loss: 1.3018602132797241 - val_accuracy: 0.5344 - penalty: 2e-05\n",
            "hidden layer sizes: [120, 119, 118, 116, 148], total units: 621\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 1.3016899824142456 - val_accuracy: 0.5344 - penalty: 2e-05\n",
            "hidden layer sizes: [100, 76, 76, 60, 126], total units: 438\n",
            "##########################################################\n",
            "Epoch 4/20\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 1.3016899824142456 - val_accuracy: 0.5344 - penalty: 2e-05\n",
            "hidden layer sizes: [100, 76, 76, 60, 126], total units: 438\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 1.3016901016235352 - val_accuracy: 0.5344 - penalty: 2e-05\n",
            "hidden layer sizes: [120, 96, 96, 80, 151], total units: 543\n",
            "Before pruning:\n",
            "loss: 1.3952487707138062 - accuracy: 0.4962 - val_loss: 1.220427393913269 - val_accuracy: 0.5601 - penalty: 2e-05\n",
            "hidden layer sizes: [120, 96, 96, 80, 151], total units: 543\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 1.220436692237854 - val_accuracy: 0.56 - penalty: 2e-05\n",
            "hidden layer sizes: [97, 28, 47, 80, 134], total units: 386\n",
            "##########################################################\n",
            "Epoch 5/20\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 1.220436692237854 - val_accuracy: 0.56 - penalty: 2e-05\n",
            "hidden layer sizes: [97, 28, 47, 80, 134], total units: 386\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 1.2204365730285645 - val_accuracy: 0.56 - penalty: 2e-05\n",
            "hidden layer sizes: [117, 48, 67, 100, 160], total units: 492\n",
            "Before pruning:\n",
            "loss: 1.3192065954208374 - accuracy: 0.52716 - val_loss: 1.153975486755371 - val_accuracy: 0.5824 - penalty: 2e-05\n",
            "hidden layer sizes: [117, 48, 67, 100, 160], total units: 492\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 1.1539466381072998 - val_accuracy: 0.5824 - penalty: 2e-05\n",
            "hidden layer sizes: [81, 18, 30, 50, 155], total units: 334\n",
            "##########################################################\n",
            "Epoch 6/20\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 1.1539466381072998 - val_accuracy: 0.5824 - penalty: 2e-05\n",
            "hidden layer sizes: [81, 18, 30, 50, 155], total units: 334\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 1.1539466381072998 - val_accuracy: 0.5824 - penalty: 2e-05\n",
            "hidden layer sizes: [101, 38, 50, 70, 186], total units: 445\n",
            "Before pruning:\n",
            "loss: 1.2823785543441772 - accuracy: 0.53958 - val_loss: 1.1207634210586548 - val_accuracy: 0.5982 - penalty: 2e-05\n",
            "hidden layer sizes: [101, 38, 50, 70, 186], total units: 445\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 1.1207762956619263 - val_accuracy: 0.5986 - penalty: 2e-05\n",
            "hidden layer sizes: [73, 17, 26, 37, 170], total units: 323\n",
            "##########################################################\n",
            "Epoch 7/20\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 1.1207762956619263 - val_accuracy: 0.5986 - penalty: 2e-05\n",
            "hidden layer sizes: [73, 17, 26, 37, 170], total units: 323\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 1.1207762956619263 - val_accuracy: 0.5986 - penalty: 2e-05\n",
            "hidden layer sizes: [93, 37, 46, 57, 204], total units: 437\n",
            "Before pruning:\n",
            "loss: 1.2580077648162842 - accuracy: 0.549 - val_loss: 1.1123729944229126 - val_accuracy: 0.6108 - penalty: 2e-05\n",
            "hidden layer sizes: [93, 37, 46, 57, 204], total units: 437\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 1.1123605966567993 - val_accuracy: 0.6107 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 15, 25, 33, 192], total units: 330\n",
            "##########################################################\n",
            "Epoch 8/20\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 1.1123605966567993 - val_accuracy: 0.6107 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 15, 25, 33, 192], total units: 330\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 1.1123605966567993 - val_accuracy: 0.6107 - penalty: 2e-05\n",
            "hidden layer sizes: [85, 35, 45, 53, 230], total units: 448\n",
            "Before pruning:\n",
            "loss: 1.2325387001037598 - accuracy: 0.55746 - val_loss: 1.057499885559082 - val_accuracy: 0.6235 - penalty: 2e-05\n",
            "hidden layer sizes: [85, 35, 45, 53, 230], total units: 448\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 1.0574957132339478 - val_accuracy: 0.6236 - penalty: 2e-05\n",
            "hidden layer sizes: [57, 15, 27, 53, 223], total units: 375\n",
            "##########################################################\n",
            "Epoch 9/20\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 1.0574957132339478 - val_accuracy: 0.6236 - penalty: 2e-05\n",
            "hidden layer sizes: [57, 15, 27, 53, 223], total units: 375\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 1.0574957132339478 - val_accuracy: 0.6236 - penalty: 2e-05\n",
            "hidden layer sizes: [77, 35, 47, 73, 267], total units: 499\n",
            "Before pruning:\n",
            "loss: 1.206843614578247 - accuracy: 0.5693 - val_loss: 1.0597469806671143 - val_accuracy: 0.6176 - penalty: 2e-05\n",
            "hidden layer sizes: [77, 35, 47, 73, 267], total units: 499\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 1.0596445798873901 - val_accuracy: 0.6179 - penalty: 2e-05\n",
            "hidden layer sizes: [51, 15, 30, 73, 249], total units: 418\n",
            "##########################################################\n",
            "Epoch 10/20\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 1.0596445798873901 - val_accuracy: 0.6179 - penalty: 2e-05\n",
            "hidden layer sizes: [51, 15, 30, 73, 249], total units: 418\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 1.0596445798873901 - val_accuracy: 0.6179 - penalty: 2e-05\n",
            "hidden layer sizes: [71, 35, 50, 93, 298], total units: 547\n",
            "Before pruning:\n",
            "loss: 1.185500979423523 - accuracy: 0.57732 - val_loss: 1.0105369091033936 - val_accuracy: 0.6453 - penalty: 2e-05\n",
            "hidden layer sizes: [71, 35, 50, 93, 298], total units: 547\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 1.010555386543274 - val_accuracy: 0.6452 - penalty: 2e-05\n",
            "hidden layer sizes: [48, 15, 26, 56, 265], total units: 410\n",
            "##########################################################\n",
            "Epoch 11/20\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 1.010555386543274 - val_accuracy: 0.6452 - penalty: 2e-05\n",
            "hidden layer sizes: [48, 15, 26, 56, 265], total units: 410\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 1.010555386543274 - val_accuracy: 0.6452 - penalty: 2e-05\n",
            "hidden layer sizes: [68, 35, 46, 76, 318], total units: 543\n",
            "Before pruning:\n",
            "loss: 1.1761764287948608 - accuracy: 0.57728 - val_loss: 0.9887935519218445 - val_accuracy: 0.6441 - penalty: 2e-05\n",
            "hidden layer sizes: [68, 35, 46, 76, 318], total units: 543\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 0.9887959957122803 - val_accuracy: 0.644 - penalty: 2e-05\n",
            "hidden layer sizes: [46, 14, 32, 70, 271], total units: 433\n",
            "##########################################################\n",
            "Epoch 12/20\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 0.9887959957122803 - val_accuracy: 0.644 - penalty: 2e-05\n",
            "hidden layer sizes: [46, 14, 32, 70, 271], total units: 433\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 0.9887959957122803 - val_accuracy: 0.644 - penalty: 2e-05\n",
            "hidden layer sizes: [66, 34, 52, 90, 325], total units: 567\n",
            "Before pruning:\n",
            "loss: 1.1645961999893188 - accuracy: 0.58416 - val_loss: 1.0151164531707764 - val_accuracy: 0.6424 - penalty: 2e-05\n",
            "hidden layer sizes: [66, 34, 52, 90, 325], total units: 567\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 1.0147064924240112 - val_accuracy: 0.6428 - penalty: 2e-05\n",
            "hidden layer sizes: [39, 13, 31, 65, 233], total units: 381\n",
            "##########################################################\n",
            "Epoch 13/20\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 1.0147064924240112 - val_accuracy: 0.6428 - penalty: 2e-05\n",
            "hidden layer sizes: [39, 13, 31, 65, 233], total units: 381\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 1.0147064924240112 - val_accuracy: 0.6428 - penalty: 2e-05\n",
            "hidden layer sizes: [59, 33, 51, 85, 279], total units: 507\n",
            "Before pruning:\n",
            "loss: 1.13856041431427 - accuracy: 0.59534 - val_loss: 0.9703090786933899 - val_accuracy: 0.658 - penalty: 2e-05\n",
            "hidden layer sizes: [59, 33, 51, 85, 279], total units: 507\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 0.9702850580215454 - val_accuracy: 0.6582 - penalty: 2e-05\n",
            "hidden layer sizes: [37, 13, 25, 71, 207], total units: 353\n",
            "##########################################################\n",
            "Epoch 14/20\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 0.9702850580215454 - val_accuracy: 0.6582 - penalty: 2e-05\n",
            "hidden layer sizes: [37, 13, 25, 71, 207], total units: 353\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 0.970285177230835 - val_accuracy: 0.6582 - penalty: 2e-05\n",
            "hidden layer sizes: [57, 33, 45, 91, 248], total units: 474\n",
            "Before pruning:\n",
            "loss: 1.1257309913635254 - accuracy: 0.59814 - val_loss: 0.974070131778717 - val_accuracy: 0.6555 - penalty: 2e-05\n",
            "hidden layer sizes: [57, 33, 45, 91, 248], total units: 474\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 0.974126935005188 - val_accuracy: 0.6559 - penalty: 2e-05\n",
            "hidden layer sizes: [38, 13, 26, 82, 214], total units: 373\n",
            "##########################################################\n",
            "Epoch 15/20\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 0.974126935005188 - val_accuracy: 0.6559 - penalty: 2e-05\n",
            "hidden layer sizes: [38, 13, 26, 82, 214], total units: 373\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 0.9741270542144775 - val_accuracy: 0.6559 - penalty: 2e-05\n",
            "hidden layer sizes: [58, 33, 46, 102, 256], total units: 495\n",
            "Before pruning:\n",
            "loss: 1.1114637851715088 - accuracy: 0.60682 - val_loss: 0.9480355381965637 - val_accuracy: 0.6654 - penalty: 2e-05\n",
            "hidden layer sizes: [58, 33, 46, 102, 256], total units: 495\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 0.9481191635131836 - val_accuracy: 0.6653 - penalty: 2e-05\n",
            "hidden layer sizes: [40, 13, 30, 102, 215], total units: 400\n",
            "##########################################################\n",
            "Epoch 16/20\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 0.9481191635131836 - val_accuracy: 0.6653 - penalty: 2e-05\n",
            "hidden layer sizes: [40, 13, 30, 102, 215], total units: 400\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 0.9481191635131836 - val_accuracy: 0.6653 - penalty: 2e-05\n",
            "hidden layer sizes: [60, 33, 50, 122, 258], total units: 523\n",
            "Before pruning:\n",
            "loss: 1.1002287864685059 - accuracy: 0.6085 - val_loss: 0.9465226531028748 - val_accuracy: 0.6704 - penalty: 2e-05\n",
            "hidden layer sizes: [60, 33, 50, 122, 258], total units: 523\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 0.9466623067855835 - val_accuracy: 0.6704 - penalty: 2e-05\n",
            "hidden layer sizes: [39, 13, 28, 90, 240], total units: 410\n",
            "##########################################################\n",
            "Epoch 17/20\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 0.9466623067855835 - val_accuracy: 0.6704 - penalty: 2e-05\n",
            "hidden layer sizes: [39, 13, 28, 90, 240], total units: 410\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 0.946662187576294 - val_accuracy: 0.6704 - penalty: 2e-05\n",
            "hidden layer sizes: [59, 33, 48, 110, 288], total units: 538\n",
            "Before pruning:\n",
            "loss: 1.0925538539886475 - accuracy: 0.61188 - val_loss: 0.9606273174285889 - val_accuracy: 0.6647 - penalty: 2e-05\n",
            "hidden layer sizes: [59, 33, 48, 110, 288], total units: 538\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 0.9605926871299744 - val_accuracy: 0.6645 - penalty: 2e-05\n",
            "hidden layer sizes: [37, 13, 31, 84, 266], total units: 431\n",
            "##########################################################\n",
            "Epoch 18/20\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 0.9605926871299744 - val_accuracy: 0.6645 - penalty: 2e-05\n",
            "hidden layer sizes: [37, 13, 31, 84, 266], total units: 431\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 0.9605926871299744 - val_accuracy: 0.6645 - penalty: 2e-05\n",
            "hidden layer sizes: [57, 33, 51, 104, 319], total units: 564\n",
            "Before pruning:\n",
            "loss: 1.0852175951004028 - accuracy: 0.61362 - val_loss: 0.920579195022583 - val_accuracy: 0.6731 - penalty: 2e-05\n",
            "hidden layer sizes: [57, 33, 51, 104, 319], total units: 564\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 0.9205239415168762 - val_accuracy: 0.6733 - penalty: 2e-05\n",
            "hidden layer sizes: [37, 13, 31, 89, 273], total units: 443\n",
            "##########################################################\n",
            "Epoch 19/20\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 0.9205239415168762 - val_accuracy: 0.6733 - penalty: 2e-05\n",
            "hidden layer sizes: [37, 13, 31, 89, 273], total units: 443\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 0.920524001121521 - val_accuracy: 0.6733 - penalty: 2e-05\n",
            "hidden layer sizes: [57, 33, 51, 109, 327], total units: 577\n",
            "Before pruning:\n",
            "loss: 1.077634572982788 - accuracy: 0.6164 - val_loss: 0.9266952872276306 - val_accuracy: 0.6774 - penalty: 2e-05\n",
            "hidden layer sizes: [57, 33, 51, 109, 327], total units: 577\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 0.9266478419303894 - val_accuracy: 0.6773 - penalty: 2e-05\n",
            "hidden layer sizes: [40, 13, 30, 100, 308], total units: 491\n",
            "##########################################################\n",
            "Epoch 20/20\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 0.9266478419303894 - val_accuracy: 0.6773 - penalty: 2e-05\n",
            "hidden layer sizes: [40, 13, 30, 100, 308], total units: 491\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 0.9266478419303894 - val_accuracy: 0.6773 - penalty: 2e-05\n",
            "hidden layer sizes: [60, 33, 50, 120, 369], total units: 632\n",
            "Before pruning:\n",
            "loss: 1.0724891424179077 - accuracy: 0.61908 - val_loss: 0.9140296578407288 - val_accuracy: 0.6811 - penalty: 2e-05\n",
            "hidden layer sizes: [60, 33, 50, 120, 369], total units: 632\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 0.9139829874038696 - val_accuracy: 0.6815 - penalty: 2e-05\n",
            "hidden layer sizes: [37, 13, 34, 95, 318], total units: 497\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Xjdr-TaRdui",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84572bce-9e00-49f5-ccc3-3266ea8762e9"
      },
      "source": [
        "%%time\n",
        "\n",
        "histories = hyperparameter_search(train_fn, learning_rate=[0.00005, 0.0002, 0.0006], \n",
        "                                  regularization_penalty=[0.00002], regularization_method=['weighted_l1'], \n",
        "                                  self_scaling_epochs=[20], layer_sizes=[[100, 100, 100, 100, 100]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run with parameters (5e-05, 2e-05, 'weighted_l1', 20, [100, 100, 100, 100, 100]) completed, best_val_accuracy: 0.7074, final_hidden_layer sizes: [100, 46, 47, 53, 71]\n",
            "Run with parameters (0.0002, 2e-05, 'weighted_l1', 20, [100, 100, 100, 100, 100]) completed, best_val_accuracy: 0.7496, final_hidden_layer sizes: [50, 25, 23, 60, 86]\n",
            "Run with parameters (0.0006, 2e-05, 'weighted_l1', 20, [100, 100, 100, 100, 100]) completed, best_val_accuracy: 0.7554, final_hidden_layer sizes: [36, 15, 30, 81, 266]\n",
            "Best overall combination: (0.0006, 2e-05, 'weighted_l1', 20, [100, 100, 100, 100, 100]), val_accuracy: 0.7554\n",
            "CPU times: user 27min 12s, sys: 31.1 s, total: 27min 43s\n",
            "Wall time: 27min 44s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "histories = cross_validate(train_fn, X_norm, y, n_splits=6, learning_rate=0.0006,\n",
        "                           regularization_penalty=0.00002, regularization_method='weighted_l1',\n",
        "                           self_scaling_epochs=20, layer_sizes=[100, 100, 100, 100, 100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFFQHhEIk6_U",
        "outputId": "6a7a98e4-37d1-4472-9f6c-325c8555cf45"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run 0 completed, best_val_accuracy: 0.7628, best_hidden_layer_sizes: [37, 17, 28, 77, 258]\n",
            "Run 1 completed, best_val_accuracy: 0.7695, best_hidden_layer_sizes: [34, 21, 33, 75, 314]\n",
            "Run 2 completed, best_val_accuracy: 0.7704, best_hidden_layer_sizes: [40, 16, 29, 75, 292]\n",
            "Run 3 completed, best_val_accuracy: 0.7652, best_hidden_layer_sizes: [26, 15, 28, 73, 254]\n",
            "Run 4 completed, best_val_accuracy: 0.7672, best_hidden_layer_sizes: [39, 16, 34, 85, 292]\n",
            "Run 5 completed, best_val_accuracy: 0.7641, best_hidden_layer_sizes: [31, 18, 35, 92, 276]\n",
            "mean_best_val_accuracy: 0.7665333333333333\n",
            "mean_best_hidden_layer_sizes: [34.5, 17.166666666666668, 31.166666666666668, 79.5, 281.0]\n",
            "CPU times: user 20min 30s, sys: 44.1 s, total: 21min 14s\n",
            "Wall time: 17min 48s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "histories = cross_validate(train_fn, X_norm, y, n_splits=6, learning_rate=0.0006,\n",
        "                           regularization_penalty=0.00002, regularization_method='weighted_l1_reordered',\n",
        "                           self_scaling_epochs=20, layer_sizes=[100, 100, 100, 100, 100], use_static_graph=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs6wXPoL4MFj",
        "outputId": "f6c8b5ea-ff36-4976-b6d3-8515c4587277"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run 0 completed, best_val_accuracy: 0.7568, best_hidden_layer_sizes: [33, 14, 23, 78, 280]\n",
            "Run 1 completed, best_val_accuracy: 0.7565, best_hidden_layer_sizes: [32, 13, 33, 75, 329]\n",
            "Run 2 completed, best_val_accuracy: 0.7552, best_hidden_layer_sizes: [36, 14, 31, 83, 336]\n",
            "Run 3 completed, best_val_accuracy: 0.7551, best_hidden_layer_sizes: [32, 12, 30, 61, 346]\n",
            "Run 4 completed, best_val_accuracy: 0.7585, best_hidden_layer_sizes: [34, 14, 29, 66, 304]\n",
            "Run 5 completed, best_val_accuracy: 0.7516, best_hidden_layer_sizes: [31, 15, 28, 87, 294]\n",
            "mean_best_val_accuracy: 0.7556166666666666\n",
            "mean_best_hidden_layer_sizes: [33.0, 13.666666666666666, 29.0, 75.0, 314.8333333333333]\n",
            "CPU times: user 44min 29s, sys: 1min 7s, total: 45min 36s\n",
            "Wall time: 44min 25s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzMHztGzKpNG"
      },
      "source": [
        "## CIFAR100"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "histories = hyperparameter_search(train_fn, learning_rate=[0.00005, 0.0002, 0.0006], \n",
        "                                  regularization_penalty=[0.00002], regularization_method=['weighted_l1_reordered'], \n",
        "                                  self_scaling_epochs=[20], layer_sizes=[[100, 100, 100, 100, 100]], output_neurons=[100], use_static_graph=[False])"
      ],
      "metadata": {
        "id": "ppeykoOvyD2r",
        "outputId": "321e1e9a-5513-414c-86d3-b6798e316027",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run with parameters (5e-05, 2e-05, 'weighted_l1_reordered', 20, [100, 100, 100, 100, 100], 100, False) completed, best_val_accuracy: 0.3867, best_hidden_layer_sizes sizes: [94, 21, 39, 37, 100]\n",
            "Run with parameters (0.0002, 2e-05, 'weighted_l1_reordered', 20, [100, 100, 100, 100, 100], 100, False) completed, best_val_accuracy: 0.4364, best_hidden_layer_sizes sizes: [62, 16, 23, 66, 246]\n",
            "Run with parameters (0.0006, 2e-05, 'weighted_l1_reordered', 20, [100, 100, 100, 100, 100], 100, False) completed, best_val_accuracy: 0.4187, best_hidden_layer_sizes sizes: [46, 21, 55, 93, 899]\n",
            "Best overall combination: (0.0002, 2e-05, 'weighted_l1_reordered', 20, [100, 100, 100, 100, 100], 100, False), val_accuracy: 0.4364\n",
            "CPU times: user 21min 43s, sys: 25.1 s, total: 22min 8s\n",
            "Wall time: 20min 47s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "histories = cross_validate(train_fn, X_norm, y, n_splits=6, learning_rate=0.0002,\n",
        "                           regularization_penalty=0.00002, regularization_method='weighted_l1_reordered',\n",
        "                           self_scaling_epochs=20, layer_sizes=[100, 100, 100, 100, 100], output_neurons=100, use_static_graph=False)"
      ],
      "metadata": {
        "id": "CK3PTgETma5u",
        "outputId": "20451996-5b32-4e75-9ef2-a268547ef411",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run 0 completed, best_val_accuracy: 0.4363, best_hidden_layer_sizes: [62, 15, 22, 65, 256]\n",
            "Run 1 completed, best_val_accuracy: 0.453, best_hidden_layer_sizes: [71, 17, 22, 51, 265]\n",
            "Run 2 completed, best_val_accuracy: 0.4458, best_hidden_layer_sizes: [62, 16, 27, 57, 277]\n",
            "Run 3 completed, best_val_accuracy: 0.4425, best_hidden_layer_sizes: [63, 19, 22, 82, 261]\n",
            "Run 4 completed, best_val_accuracy: 0.4468, best_hidden_layer_sizes: [61, 19, 21, 79, 249]\n",
            "Run 5 completed, best_val_accuracy: 0.4441, best_hidden_layer_sizes: [68, 14, 20, 77, 274]\n",
            "mean_best_val_accuracy: 0.44475000000000003\n",
            "mean_best_hidden_layer_sizes: [64.5, 16.666666666666668, 22.333333333333332, 68.5, 263.6666666666667]\n",
            "CPU times: user 44min 10s, sys: 3min 14s, total: 47min 25s\n",
            "Wall time: 44min 35s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DpYmnqKKyb5",
        "outputId": "c7f7806e-4b69-4983-87fc-f47e97afbe23"
      },
      "source": [
        "%%time\n",
        "\n",
        "histories = hyperparameter_search(train_fn, learning_rate=[0.00005, 0.0002, 0.0006], \n",
        "                                  regularization_penalty=[0.00002], regularization_method=['weighted_l1'], \n",
        "                                  self_scaling_epochs=[20], layer_sizes=[[100, 100, 100, 100, 100]], output_neurons=[100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run with parameters (5e-05, 2e-05, 'weighted_l1', 20, [100, 100, 100, 100, 100], 100) completed, best_val_accuracy: 0.4097, final_hidden_layer sizes: [100, 40, 48, 48, 100]\n",
            "Run with parameters (0.0002, 2e-05, 'weighted_l1', 20, [100, 100, 100, 100, 100], 100) completed, best_val_accuracy: 0.451, final_hidden_layer sizes: [69, 21, 33, 67, 193]\n",
            "Run with parameters (0.0006, 2e-05, 'weighted_l1', 20, [100, 100, 100, 100, 100], 100) completed, best_val_accuracy: 0.4063, final_hidden_layer sizes: [52, 21, 100, 124, 808]\n",
            "Best overall combination: (0.0002, 2e-05, 'weighted_l1', 20, [100, 100, 100, 100, 100], 100), val_accuracy: 0.451\n",
            "CPU times: user 18min 25s, sys: 19.1 s, total: 18min 45s\n",
            "Wall time: 20min 34s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DuXxNaoFK9aw",
        "outputId": "fb62749c-0f12-475c-c02b-862fc8fb45c7"
      },
      "source": [
        "%%time\n",
        "\n",
        "histories = cross_validate(train_fn, X_norm, y, n_splits=6, learning_rate=0.0002,\n",
        "                           regularization_penalty=0.00002, regularization_method='weighted_l1',\n",
        "                           self_scaling_epochs=20, layer_sizes=[100, 100, 100, 100, 100], output_neurons=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run 0 completed, best_val_accuracy: 0.4509, final_hidden_layer sizes: [65, 22, 30, 68, 200]\n",
            "Run 1 completed, best_val_accuracy: 0.4497, final_hidden_layer sizes: [63, 24, 31, 69, 206]\n",
            "Run 2 completed, best_val_accuracy: 0.4431, final_hidden_layer sizes: [63, 23, 28, 81, 189]\n",
            "Run 3 completed, best_val_accuracy: 0.4468, final_hidden_layer sizes: [65, 27, 32, 70, 206]\n",
            "Run 4 completed, best_val_accuracy: 0.459, final_hidden_layer sizes: [68, 19, 30, 69, 200]\n",
            "Run 5 completed, best_val_accuracy: 0.4536, final_hidden_layer sizes: [65, 24, 25, 75, 204]\n",
            "mean_best_val_accuracy: 0.4505166666666667\n",
            "mean_final_hidden_layer_sizes: [64.83333333333333, 23.166666666666668, 29.333333333333332, 72.0, 200.83333333333334]\n",
            "CPU times: user 36min 16s, sys: 35.5 s, total: 36min 52s\n",
            "Wall time: 39min 4s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBbX9M1TacAv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}