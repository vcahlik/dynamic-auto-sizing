{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "tf_multi_layer_ssnet_inverse.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_deAUKlniFk",
        "outputId": "250a3695-2e81-450b-be96-d73f23ef2936"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Oct 12 07:58:53 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.74       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKwUwV_NneIo"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOoXBq05neIt"
      },
      "source": [
        "dtype = 'float32'\n",
        "tf.keras.backend.set_floatx(dtype)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BrJPdkBneIv",
        "outputId": "3358936d-96a4-4066-e997-8f86330cac75"
      },
      "source": [
        "cifar10 = tf.keras.datasets.cifar10\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "X_train = X_train.astype(dtype) / 255.0\n",
        "y_train = y_train.astype(dtype)\n",
        "X_test = X_test.astype(dtype)  / 255.0\n",
        "y_test = y_test.astype(dtype)\n",
        "\n",
        "X_train = np.reshape(X_train, (-1, 3072))\n",
        "X_test = np.reshape(X_test, (-1, 3072))\n",
        "\n",
        "X = np.concatenate((X_train, X_test), axis=0)\n",
        "y = np.concatenate((y_train, y_test), axis=0)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n",
            "170508288/170498071 [==============================] - 2s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5uTvu5kxF-b"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "\n",
        "X_norm = scaler.transform(X)\n",
        "X_train_norm = scaler.transform(X_train)\n",
        "X_test_norm = scaler.transform(X_test)\n",
        "\n",
        "X_norm = np.reshape(X_norm, (-1, 32, 32, 3))\n",
        "X_train_norm = np.reshape(X_train_norm, (-1, 32, 32, 3))\n",
        "X_test_norm = np.reshape(X_test_norm, (-1, 32, 32, 3))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTZq4KMpneIv"
      },
      "source": [
        "class Regularizer(tf.keras.regularizers.Regularizer):\n",
        "    def __init__(self, regularization_penalty, regularization_method):\n",
        "        self.regularization_penalty = regularization_penalty\n",
        "        self.regularization_method = regularization_method\n",
        "\n",
        "    def __call__(self, x):\n",
        "        if self.regularization_method == 'weighted_l1':\n",
        "            return self.weighted_l1(x)\n",
        "        elif self.regularization_method == 'group_sparsity':\n",
        "            return self.group_sparsity(x)\n",
        "        else:\n",
        "            raise NotImplementedError(f\"Unknown regularization method {self.regularization_method}\")\n",
        "    \n",
        "    def weighted_l1(self, x):\n",
        "        # I.e. for a parameter matrix of 4 input and 10 output neurons:\n",
        "        #\n",
        "        # [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
        "        #  [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
        "        #  [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
        "        #  [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]\n",
        "        #\n",
        "        # the scaling tensor, as well as the resulting weighted values, could be:\n",
        "        #\n",
        "        # [[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n",
        "        #  [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n",
        "        #  [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n",
        "        #  [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]]\n",
        "        #\n",
        "        # Therefore every additional output neuron is regularized more.\n",
        "\n",
        "        scaling_tensor = tf.cumsum(tf.constant(self.regularization_penalty, shape=x.shape, dtype=dtype), axis=-1)\n",
        "        weighted_values = scaling_tensor * tf.abs(x)\n",
        "        return tf.reduce_sum(weighted_values)\n",
        "    \n",
        "    def group_sparsity(self, x):\n",
        "        # I.e. for a parameter matrix of 3 input and 5 output neurons:\n",
        "        #\n",
        "        # [[1., 1., 1., 1., 1.],\n",
        "        #  [1., 2., 2., 1., 2.],\n",
        "        #  [2., 2., 3., 1., 3.]]\n",
        "        #\n",
        "        # The resulting vector of group norms is [2., 2., 3., 1., 3.], therefore for\n",
        "        # every output neuron, its incoming connections form a group.\n",
        "\n",
        "        # TODO implement for Conv2D layers\n",
        "        group_norms = tf.norm(x, ord=2, axis=0)\n",
        "        # assert group_norms.shape[0] == x.shape[1]\n",
        "        return self.regularization_penalty * tf.reduce_sum(group_norms)\n",
        "\n",
        "    def get_config(self):\n",
        "        return {'regularization_penalty': float(self.regularization_penalty)}\n",
        "\n",
        "\n",
        "class ModelReference:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "\n",
        "\n",
        "class CustomLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_shape):\n",
        "        super().__init__()\n",
        "\n",
        "        self.inpt_shp = input_shape\n",
        "    \n",
        "    # def configure(self, model):\n",
        "    #     self.mr = ModelReference(model)\n",
        "    \n",
        "    # def get_input_shape(self):\n",
        "    #     if self.inpt_shp is not None:\n",
        "    #         return self.inpt_shp\n",
        "        \n",
        "    #     return self.mr.model.get_layer_input_shape(self)\n",
        "    \n",
        "    # def get_output_shape(self):\n",
        "    #     return self.mr.model.get_layer_output_shape(self)\n",
        "\n",
        "\n",
        "class Dense(CustomLayer):\n",
        "    def __init__(self, units, activation, regularization_penalty=0.01, \n",
        "                 regularization_method='weighted_l1', kernel_initializer='glorot_uniform', \n",
        "                 bias_initializer='zeros', input_shape=None, fixed_size=False):\n",
        "        super().__init__(input_shape)\n",
        "\n",
        "        self.units = units\n",
        "        self.activation = activation\n",
        "        self.regularization_penalty = regularization_penalty\n",
        "        self.regularization_method = regularization_method\n",
        "        self.kernel_initializer = kernel_initializer\n",
        "        self.bias_initializer = bias_initializer\n",
        "        self.fixed_size = fixed_size\n",
        "        \n",
        "        self.A = tf.keras.activations.get(activation)\n",
        "        self.W_init = tf.keras.initializers.get(kernel_initializer)\n",
        "        self.b_init = tf.keras.initializers.get(bias_initializer)\n",
        "        self.regularizer = Regularizer(self.regularization_penalty, self.regularization_method)\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        input_units = input_shape[-1]\n",
        "\n",
        "        self.W = tf.Variable(\n",
        "            name='W',\n",
        "            initial_value=self.W_init(shape=(input_units, self.units), dtype=dtype),\n",
        "            trainable=True)\n",
        "        \n",
        "        self.b = tf.Variable(\n",
        "            name='b',\n",
        "            initial_value=self.b_init(shape=(self.units,), dtype=dtype),\n",
        "            trainable=True)\n",
        "        \n",
        "        if self.regularization_method is not None:\n",
        "            self.add_loss(lambda: self.regularizer(tf.concat([self.W, tf.reshape(self.b, (1, -1))], axis=0)))\n",
        "    \n",
        "    def call(self, inputs, training=None):\n",
        "        return self.A(tf.matmul(inputs, self.W) + self.b)\n",
        "\n",
        "    def get_size(self):\n",
        "        return self.W.shape[0], self.W.shape[1]\n",
        "    \n",
        "    def prune(self, threshold, active_input_units_indices):\n",
        "        # Remove connections from pruned units in previous layer\n",
        "        new_W = tf.gather(self.W.value(), active_input_units_indices, axis=0)\n",
        "\n",
        "        if self.fixed_size:\n",
        "            active_output_neurons_indices = list(range(new_W.shape[1]))\n",
        "        else:\n",
        "            # Prune units in this layer\n",
        "            weights_with_biases = tf.concat([new_W, tf.reshape(self.b.value(), (1, -1))], axis=0)\n",
        "            neurons_are_active = tf.math.reduce_max(tf.abs(weights_with_biases), axis=0) >= threshold\n",
        "            active_output_neurons_indices = tf.reshape(tf.where(neurons_are_active), (-1,))\n",
        "            \n",
        "            new_W = tf.gather(new_W, active_output_neurons_indices, axis=1)\n",
        "            new_b = tf.gather(self.b.value(), active_output_neurons_indices, axis=0)\n",
        "\n",
        "            self.b = tf.Variable(name='b', initial_value=new_b, trainable=True)\n",
        "\n",
        "        self.W = tf.Variable(name='W', initial_value=new_W, trainable=True)\n",
        "\n",
        "        return active_output_neurons_indices\n",
        "    \n",
        "    def grow(self, n_new_input_units, percentage, min_new_units, scaling_factor):\n",
        "        if n_new_input_units > 0:\n",
        "            # Add connections to grown units in previous layer\n",
        "            W_growth = self.W_init(shape=(self.W.shape[0] + n_new_input_units, self.W.shape[1]), dtype=dtype)[-n_new_input_units:, :] * scaling_factor  # TODO is it better to be multiplying here by scaling_factor? It does help with not increasing the max weights of existing neurons when new neurons are added.\n",
        "            new_W = tf.concat([self.W.value(), W_growth], axis=0)\n",
        "        else:\n",
        "            new_W = self.W.value()\n",
        "\n",
        "        if self.fixed_size:\n",
        "            n_new_output_units = 0\n",
        "        else:\n",
        "            # Grow new units in this layer\n",
        "            n_new_output_units = max(min_new_units, int(new_W.shape[1] * percentage))\n",
        "            W_growth = self.W_init(shape=(new_W.shape[0], new_W.shape[1] + n_new_output_units), dtype=dtype)[:, -n_new_output_units:] * scaling_factor\n",
        "            b_growth = self.b_init(shape=(n_new_output_units,), dtype=dtype)  # TODO for all possible bias initializers to work properly, the whole bias vector should be initialized at once\n",
        "            new_W = tf.concat([new_W, W_growth], axis=1)\n",
        "            new_b = tf.concat([self.b.value(), b_growth], axis=0)\n",
        "\n",
        "            self.b = tf.Variable(name='b', initial_value=new_b, trainable=True)\n",
        "\n",
        "        self.W = tf.Variable(name='W', initial_value=new_W, trainable=True)\n",
        "\n",
        "        return n_new_output_units\n",
        "    \n",
        "    def get_param_string():\n",
        "        param_string = \"\"\n",
        "        weights_with_bias = tf.concat([self.W, tf.reshape(self.b, (1, -1))], axis=0)\n",
        "        max_parameters = tf.math.reduce_max(tf.abs(weights_with_bias), axis=0).numpy()\n",
        "        magnitudes = np.floor(np.log10(max_parameters))\n",
        "        for m in magnitudes:\n",
        "            if m > 0:\n",
        "                m = 0\n",
        "            param_string += str(int(-m))\n",
        "        return param_string\n",
        "\n",
        "\n",
        "class Conv2D(CustomLayer):\n",
        "    def __init__(self, filters, filter_size, activation, strides=(1, 1), \n",
        "                 padding='SAME', regularization_penalty=0.01, \n",
        "                 regularization_method='weighted_l1', kernel_initializer='glorot_uniform',\n",
        "                 bias_initializer='zeros', input_shape=None, fixed_size=False):\n",
        "        super().__init__(input_shape)\n",
        "    \n",
        "        self.filters = filters\n",
        "        self.filter_size = filter_size\n",
        "        self.activation = activation\n",
        "        self.strides = strides\n",
        "        self.padding = padding\n",
        "        self.regularization_penalty = regularization_penalty\n",
        "        self.regularization_method = regularization_method\n",
        "        self.kernel_initializer = kernel_initializer\n",
        "        self.bias_initializer = bias_initializer\n",
        "        self.fixed_size = fixed_size\n",
        "        \n",
        "        self.A = tf.keras.activations.get(activation)\n",
        "        self.F_init = tf.keras.initializers.get(kernel_initializer)\n",
        "        self.b_init = tf.keras.initializers.get(bias_initializer)\n",
        "        self.regularizer = Regularizer(self.regularization_penalty, self.regularization_method)\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        input_filters = input_shape[-1]\n",
        "\n",
        "        self.F = tf.Variable(\n",
        "            name='F',\n",
        "            initial_value=self.F_init(\n",
        "                shape=(self.filter_size[0], self.filter_size[1], input_filters, self.filters), dtype=dtype\n",
        "            ),\n",
        "            trainable=True)\n",
        "        \n",
        "        self.b = tf.Variable(\n",
        "            name='b',\n",
        "            initial_value=self.b_init(shape=(self.filters,), dtype=dtype),\n",
        "            trainable=True)\n",
        "\n",
        "        if self.regularization_method is not None:\n",
        "            self.add_loss(lambda: self.regularizer(tf.concat([tf.reshape(self.F, (-1, self.F.shape[-1])), tf.reshape(self.b, (1, -1))], axis=0)))\n",
        "    \n",
        "    def call(self, inputs, training=None):\n",
        "        y = tf.nn.conv2d(inputs, self.F, strides=self.strides, padding=self.padding)\n",
        "        y = tf.nn.bias_add(y, self.b)\n",
        "        y = self.A(y)\n",
        "        return y\n",
        "    \n",
        "    def get_size(self):\n",
        "        return self.F.shape[-2], self.F.shape[-1]\n",
        "    \n",
        "    def prune(self, threshold, active_input_units_indices):\n",
        "        # Remove connections from pruned units in previous layer\n",
        "        new_F = tf.gather(self.F.value(), active_input_units_indices, axis=-2)\n",
        "\n",
        "        if self.fixed_size:\n",
        "            active_output_filters_indices = list(range(new_F.shape[-1]))\n",
        "        else:\n",
        "            # Prune units in this layer\n",
        "            F_reduced_max = tf.reshape(tf.math.reduce_max(tf.abs(new_F), axis=(0, 1, 2)), (1, -1))\n",
        "            F_reduced_max_with_biases = tf.concat([F_reduced_max, tf.reshape(self.b.value(), (1, -1))], axis=0)\n",
        "            filters_are_active = tf.math.reduce_max(tf.abs(F_reduced_max_with_biases), axis=0) >= threshold\n",
        "            active_output_filters_indices = tf.reshape(tf.where(filters_are_active), (-1,))\n",
        "            \n",
        "            new_F = tf.gather(new_F, active_output_filters_indices, axis=-1)\n",
        "            new_b = tf.gather(self.b.value(), active_output_filters_indices, axis=0)\n",
        "\n",
        "            self.b = tf.Variable(name='b', initial_value=new_b, trainable=True)\n",
        "\n",
        "        self.F = tf.Variable(name='F', initial_value=new_F, trainable=True)\n",
        "\n",
        "        return active_output_filters_indices\n",
        "\n",
        "    def grow(self, n_new_input_units, percentage, min_new_units, scaling_factor):\n",
        "        if n_new_input_units > 0:\n",
        "            # Add connections to grown units in previous layer\n",
        "            F_growth = self.F_init(shape=(self.F.shape[0], self.F.shape[1], self.F.shape[2] + n_new_input_units, self.F.shape[3]), dtype=dtype)[:, :, -n_new_input_units:, :] * scaling_factor  # TODO is it better to be multiplying here by scaling_factor? It does help with not increasing the max weights of existing neurons when new neurons are added.\n",
        "            new_F = tf.concat([self.F.value(), F_growth], axis=-2)\n",
        "        else:\n",
        "            new_F = self.F.value()\n",
        "\n",
        "        if self.fixed_size:\n",
        "            n_new_output_units = 0\n",
        "        else:\n",
        "            # Grow new units in this layer\n",
        "            n_new_output_units = max(min_new_units, int(new_F.shape[-1] * percentage))\n",
        "            F_growth = self.F_init(shape=(new_F.shape[0], new_F.shape[1], new_F.shape[2], new_F.shape[3] + n_new_output_units), dtype=dtype)[:, :, :, -n_new_output_units:] * scaling_factor\n",
        "            b_growth = self.b_init(shape=(n_new_output_units,), dtype=dtype)  # TODO for all possible bias initializers to work properly, the whole bias vector should be initialized at once\n",
        "            new_F = tf.concat([new_F, F_growth], axis=-1)\n",
        "            new_b = tf.concat([self.b.value(), b_growth], axis=0)\n",
        "\n",
        "            self.b = tf.Variable(name='b', initial_value=new_b, trainable=True)\n",
        "\n",
        "        self.F = tf.Variable(name='F', initial_value=new_F, trainable=True)\n",
        "\n",
        "        return n_new_output_units\n",
        "\n",
        "    def get_param_string():\n",
        "        param_string = \"\"\n",
        "        # TODO\n",
        "        return param_string\n",
        "\n",
        "\n",
        "class Flatten(tf.keras.Model):\n",
        "    def call(self, inputs, training=None):\n",
        "        return tf.reshape(tf.transpose(inputs, perm=[0, 3, 1, 2]), (inputs.shape[0], -1))\n",
        "\n",
        "\n",
        "class Sequential(tf.keras.Model):\n",
        "    def __init__(self, layers, activation=None):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.lrs = list()\n",
        "        for layer in layers:\n",
        "            self.lrs.append(layer)\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        x = inputs\n",
        "        for layer in self.lrs:\n",
        "            x = layer(x, training=training)\n",
        "        return x\n",
        "    \n",
        "    def get_layer_input_shape(self, target_layer):\n",
        "        if target_layer.inpt_shp is not None:\n",
        "            return target_layer.inpt_shp\n",
        "\n",
        "        input = np.random.normal(size=(1,) + self.lrs[0].inpt_shp)\n",
        "        for layer in self.lrs:\n",
        "            if layer is target_layer:\n",
        "                return tuple(input.shape[1:])\n",
        "            input = layer(input)\n",
        "        raise Exception(\"Layer not found in the model.\")\n",
        "\n",
        "    def get_layer_output_shape(self, target_layer):\n",
        "        input = np.random.normal(size=(1,) + self.lrs[0].inpt_shp)\n",
        "        for layer in self.lrs:\n",
        "            output = layer(input)\n",
        "            if layer is target_layer:\n",
        "                return tuple(output.shape[1:])\n",
        "            input = output\n",
        "        raise Exception(\"Layer not found in the model.\")\n",
        "    \n",
        "    def get_layer_sizes(self):\n",
        "        \"\"\"\n",
        "        Returns the sizes of all layers in the model, including the input and output layer.\n",
        "        \"\"\"\n",
        "        layer_sizes = list()\n",
        "        first_layer = True\n",
        "        for l in range(len(self.lrs)):\n",
        "            layer = self.lrs[l]\n",
        "            if isinstance(layer, CustomLayer):\n",
        "                layer_size = layer.get_size()\n",
        "                if first_layer:\n",
        "                    layer_sizes.append(layer_size[0])\n",
        "                    first_layer = False\n",
        "                layer_sizes.append(layer_size[1])\n",
        "        return layer_sizes\n",
        "    \n",
        "    def get_hidden_layer_sizes(self):\n",
        "        return self.get_layer_sizes()[1:-1]\n",
        "    \n",
        "    def remove_regularization(self):\n",
        "        self.set_regularization_penalty(0.)\n",
        "    \n",
        "    def get_regularization_penalty(self):\n",
        "        #TODO improve\n",
        "        return self.lrs[-2].regularizer.regularization_penalty\n",
        "    \n",
        "    def set_regularization_penalty(self, regularization_penalty):\n",
        "        for layer in self.lrs:\n",
        "            if isinstance(layer, CustomLayer) and not layer.fixed_size:\n",
        "                layer.regularizer.regularization_penalty = regularization_penalty\n",
        "    \n",
        "    def prune(self, threshold=0.001):\n",
        "        input_shape = self.get_layer_input_shape(self.lrs[0])\n",
        "        n_input_units = input_shape[-1]\n",
        "        active_units_indices = list(range(n_input_units))\n",
        "\n",
        "        last_custom_layer = None\n",
        "        for layer in self.lrs:\n",
        "            if isinstance(layer, CustomLayer):\n",
        "                if last_custom_layer is not None and type(last_custom_layer) != type(layer):\n",
        "                    if type(last_custom_layer) == Conv2D and type(layer) == Dense:\n",
        "                        convolutional_shape = self.get_layer_output_shape(last_custom_layer)\n",
        "                        active_units_indices = self.convert_channel_indices_to_flattened_indices(active_units_indices, convolutional_shape)\n",
        "                    else:\n",
        "                        raise Exception(\"Incorrect order of custom layer types.\")\n",
        "                active_units_indices = layer.prune(threshold, active_units_indices)\n",
        "                last_custom_layer = layer\n",
        "    \n",
        "    def grow(self, percentage, min_new_neurons=5, scaling_factor=0.001):   \n",
        "        n_new_units = 0\n",
        "\n",
        "        last_custom_layer = None\n",
        "        for layer in self.lrs:\n",
        "            if isinstance(layer, CustomLayer):\n",
        "                if last_custom_layer is not None and type(last_custom_layer) != type(layer):\n",
        "                    if type(last_custom_layer) == Conv2D and type(layer) == Dense:\n",
        "                        convolutional_shape = self.get_layer_output_shape(last_custom_layer)\n",
        "                        n_new_units = n_new_units * convolutional_shape[0] * convolutional_shape[1]\n",
        "                    else:\n",
        "                        raise Exception(\"Incorrect order of custom layer types.\")\n",
        "                n_new_units = layer.grow(n_new_units, percentage, min_new_units=min_new_neurons, scaling_factor=scaling_factor)\n",
        "                last_custom_layer = layer\n",
        "    \n",
        "    @staticmethod\n",
        "    def convert_channel_indices_to_flattened_indices(channel_indices, convolutional_shape):\n",
        "        dense_indices = list()\n",
        "        units_per_channel = convolutional_shape[0] * convolutional_shape[1]\n",
        "        for channel_index in channel_indices:\n",
        "            for iter in range(units_per_channel):\n",
        "                dense_indices.append(channel_index * units_per_channel + iter)\n",
        "        return dense_indices\n",
        "    \n",
        "    def print_neurons(self):\n",
        "        for layer in self.lrs[:-1]:\n",
        "            print(layer.get_param_string())\n",
        "    \n",
        "    def evaluate(self, x, y, summed_training_loss, summed_training_accuracy, val_dataset):\n",
        "        # Calculate training loss and accuracy\n",
        "        if summed_training_loss is not None:\n",
        "            loss = summed_training_loss / x.shape[0]\n",
        "        else:\n",
        "            loss = None\n",
        "        \n",
        "        if summed_training_accuracy is not None:\n",
        "            accuracy = summed_training_accuracy / x.shape[0]\n",
        "        else:\n",
        "            accuracy = None\n",
        "        \n",
        "        # Calculate val loss and accuracy\n",
        "        summed_val_loss = 0\n",
        "        summed_val_accuracy = 0\n",
        "        n_val_instances = 0\n",
        "        \n",
        "        for step, (x_batch, y_batch) in enumerate(val_dataset):\n",
        "            y_pred = self(x_batch, training=False)\n",
        "            summed_val_loss += tf.reduce_sum(tf.keras.losses.sparse_categorical_crossentropy(y_batch, y_pred))\n",
        "            summed_val_accuracy += float(tf.reduce_sum(tf.keras.metrics.sparse_categorical_accuracy(y_batch, y_pred)))\n",
        "            n_val_instances += x_batch.shape[0]\n",
        "        \n",
        "        val_loss = summed_val_loss / n_val_instances\n",
        "        val_accuracy = summed_val_accuracy / n_val_instances\n",
        "\n",
        "        return loss, accuracy, val_loss, val_accuracy\n",
        "    \n",
        "    def print_epoch_statistics(self, x, y, summed_training_loss, summed_training_accuracy, val_dataset, print_neurons):\n",
        "        loss, accuracy, val_loss, val_accuracy = self.evaluate(x, y, summed_training_loss, summed_training_accuracy, val_dataset)\n",
        "        print(f\"loss: {loss} - accuracy: {accuracy} - val_loss: {val_loss} - val_accuracy: {val_accuracy} - penalty: {model.get_regularization_penalty()}\")\n",
        "        hidden_layer_sizes = self.get_hidden_layer_sizes()\n",
        "        print(f\"hidden layer sizes: {hidden_layer_sizes}, total units: {sum(hidden_layer_sizes)}\")\n",
        "        if print_neurons:\n",
        "            self.print_neurons()\n",
        "    \n",
        "    def update_history(self, x, y, summed_loss, summed_accuracy, val_dataset, history):\n",
        "        loss, accuracy, val_loss, val_accuracy = self.evaluate(x, y, summed_loss, summed_accuracy, val_dataset)\n",
        "        history['loss'].append(loss)\n",
        "        history['accuracy'].append(accuracy)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_accuracy'].append(val_accuracy)\n",
        "\n",
        "    def fit(self, x, y, optimizer, epochs, self_scaling_epochs, batch_size, min_new_neurons, validation_data, pruning_threshold=0.001, \n",
        "            regularization_penalty_multiplier=1., stall_coefficient=1, growth_percentage=0.2, mini_epochs_per_epoch=1, verbose=True, print_neurons=False):\n",
        "        train_dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
        "        train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "\n",
        "        val_dataset = tf.data.Dataset.from_tensor_slices(validation_data).batch(batch_size)\n",
        "\n",
        "        history = {\n",
        "            'loss': list(),\n",
        "            'accuracy': list(),\n",
        "            'val_loss': list(),\n",
        "            'val_accuracy': list(),\n",
        "        }\n",
        "\n",
        "        best_val_loss = np.inf\n",
        "        training_stalled = False\n",
        "        for epoch in range(epochs):\n",
        "            summed_loss = 0\n",
        "            summed_accuracy = 0\n",
        "\n",
        "            if verbose:\n",
        "                print(\"##########################################################\")\n",
        "                print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "\n",
        "            if epoch < self_scaling_epochs:\n",
        "                if verbose:\n",
        "                    print(\"Before growing:\")\n",
        "                    self.print_epoch_statistics(x, y, None, None, val_dataset, print_neurons)\n",
        "\n",
        "                loss, accuracy, val_loss, val_accuracy = self.evaluate(x, y, summed_loss, summed_accuracy, val_dataset)\n",
        "                if regularization_penalty_multiplier != 1. and val_loss >= best_val_loss * stall_coefficient:\n",
        "                    if not training_stalled:\n",
        "                        penalty = self.get_regularization_penalty() * regularization_penalty_multiplier\n",
        "                        print(\"Changing penalty...\")\n",
        "                        # TODO this must be modified, penalty can differ for each layer\n",
        "                        self.set_regularization_penalty(penalty)\n",
        "                        training_stalled = True\n",
        "                else:\n",
        "                    best_val_loss = val_loss\n",
        "                    training_stalled = False\n",
        "\n",
        "                self.grow(percentage=growth_percentage, min_new_neurons=min_new_neurons, scaling_factor=pruning_threshold)\n",
        "                if verbose:\n",
        "                    print(\"After growing:\")\n",
        "                    self.print_epoch_statistics(x, y, None, None, val_dataset, print_neurons)\n",
        "            \n",
        "            if epoch == self_scaling_epochs:\n",
        "                self.remove_regularization()\n",
        "\n",
        "            for mini_epoch in range(mini_epochs_per_epoch):\n",
        "                for step, (x_batch, y_batch) in enumerate(train_dataset):\n",
        "                    with tf.GradientTape() as tape:\n",
        "                        y_pred = self(x_batch, training=True)\n",
        "                        raw_loss = tf.keras.losses.sparse_categorical_crossentropy(y_batch, y_pred)\n",
        "                        loss_value = tf.reduce_mean(raw_loss)\n",
        "                        loss_value += sum(self.losses)  # Add losses registered by model.add_loss\n",
        "\n",
        "                        summed_loss += tf.reduce_sum(raw_loss)\n",
        "                        summed_accuracy += float(tf.reduce_sum(tf.keras.metrics.sparse_categorical_accuracy(y_batch, y_pred)))\n",
        "\n",
        "                    grads = tape.gradient(loss_value, self.trainable_variables)\n",
        "                    optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
        "            \n",
        "            if epoch < self_scaling_epochs:\n",
        "                if verbose:\n",
        "                    print(\"Before pruning:\")\n",
        "                    self.print_epoch_statistics(x, y, summed_loss, summed_accuracy, val_dataset, print_neurons)\n",
        "                self.prune(threshold=pruning_threshold)\n",
        "                if verbose:\n",
        "                    print(\"After pruning:\")\n",
        "                    self.print_epoch_statistics(x, y, None, None, val_dataset, print_neurons)\n",
        "            else:\n",
        "                if verbose:\n",
        "                    self.print_epoch_statistics(x, y, summed_loss, summed_accuracy, val_dataset, print_neurons)\n",
        "            \n",
        "            self.update_history(x, y, summed_loss, summed_accuracy, val_dataset, history)\n",
        "\n",
        "        return history"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1MrQXUTFwOe"
      },
      "source": [
        "# Accuracy benchmark - FF and convolutions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_BvNnozaO0-"
      },
      "source": [
        "epochs = 50\n",
        "self_scaling_epochs = 25\n",
        "batch_size = 256\n",
        "min_new_neurons = 20"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JozLW7ishUT0",
        "outputId": "2321ff2a-3cd3-4f1d-faae-89155d8075aa"
      },
      "source": [
        "%%time\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(100, filter_size=(3, 3), activation='selu', strides=(1, 1), padding='SAME', \n",
        "        regularization_penalty=0.00002, regularization_method='weighted_l1', \n",
        "        kernel_initializer='lecun_normal', input_shape=X_train_norm[0,:,:,:].shape),\n",
        "    Conv2D(100, filter_size=(3, 3), activation='selu', strides=(2, 2), padding='SAME', \n",
        "        regularization_penalty=0.00002, regularization_method='weighted_l1', \n",
        "        kernel_initializer='lecun_normal'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    Conv2D(100, filter_size=(3, 3), activation='selu', strides=(1, 1), padding='SAME', \n",
        "        regularization_penalty=0.00002, regularization_method='weighted_l1', \n",
        "        kernel_initializer='lecun_normal'),\n",
        "    Conv2D(100, filter_size=(3, 3), activation='selu', strides=(2, 2), padding='SAME', \n",
        "        regularization_penalty=0.00002, regularization_method='weighted_l1', \n",
        "        kernel_initializer='lecun_normal'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    Flatten(),\n",
        "    Dense(100, activation='selu', regularization_penalty=0.00002, \n",
        "        regularization_method='weighted_l1', kernel_initializer='lecun_normal'),\n",
        "    Dense(10, activation='softmax', regularization_penalty=0., \n",
        "        regularization_method=None, fixed_size=True),\n",
        "])\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002)\n",
        "\n",
        "model.fit(X_train_norm, y_train, optimizer, epochs, self_scaling_epochs, batch_size, \n",
        "          min_new_neurons, validation_data=(X_test_norm, y_test))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##########################################################\n",
            "Epoch 1/50\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 2.9377074241638184 - val_accuracy: 0.1071 - penalty: 2e-05\n",
            "hidden layer sizes: [100, 100, 100, 100, 100], total units: 500\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 2.93770694732666 - val_accuracy: 0.1071 - penalty: 2e-05\n",
            "hidden layer sizes: [120, 120, 120, 120, 120], total units: 600\n",
            "Before pruning:\n",
            "loss: 1.9169420003890991 - accuracy: 0.36044 - val_loss: 1.5022259950637817 - val_accuracy: 0.4685 - penalty: 2e-05\n",
            "hidden layer sizes: [120, 120, 120, 120, 120], total units: 600\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 1.5022430419921875 - val_accuracy: 0.4686 - penalty: 2e-05\n",
            "hidden layer sizes: [100, 100, 100, 100, 100], total units: 500\n",
            "##########################################################\n",
            "Epoch 2/50\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 1.5022430419921875 - val_accuracy: 0.4686 - penalty: 2e-05\n",
            "hidden layer sizes: [100, 100, 100, 100, 100], total units: 500\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 1.502243161201477 - val_accuracy: 0.4686 - penalty: 2e-05\n",
            "hidden layer sizes: [120, 120, 120, 120, 120], total units: 600\n",
            "Before pruning:\n",
            "loss: 1.541831135749817 - accuracy: 0.44876 - val_loss: 1.388891339302063 - val_accuracy: 0.4989 - penalty: 2e-05\n",
            "hidden layer sizes: [120, 120, 120, 120, 120], total units: 600\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 1.3889179229736328 - val_accuracy: 0.4992 - penalty: 2e-05\n",
            "hidden layer sizes: [100, 100, 100, 100, 100], total units: 500\n",
            "##########################################################\n",
            "Epoch 3/50\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 1.3889179229736328 - val_accuracy: 0.4992 - penalty: 2e-05\n",
            "hidden layer sizes: [100, 100, 100, 100, 100], total units: 500\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 1.3889179229736328 - val_accuracy: 0.4992 - penalty: 2e-05\n",
            "hidden layer sizes: [120, 120, 120, 120, 120], total units: 600\n",
            "Before pruning:\n",
            "loss: 1.4720871448516846 - accuracy: 0.47392 - val_loss: 1.3998961448669434 - val_accuracy: 0.4926 - penalty: 2e-05\n",
            "hidden layer sizes: [120, 120, 120, 120, 120], total units: 600\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 1.3997477293014526 - val_accuracy: 0.4925 - penalty: 2e-05\n",
            "hidden layer sizes: [100, 100, 100, 100, 100], total units: 500\n",
            "##########################################################\n",
            "Epoch 4/50\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 1.3997477293014526 - val_accuracy: 0.4925 - penalty: 2e-05\n",
            "hidden layer sizes: [100, 100, 100, 100, 100], total units: 500\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 1.399747610092163 - val_accuracy: 0.4925 - penalty: 2e-05\n",
            "hidden layer sizes: [120, 120, 120, 120, 120], total units: 600\n",
            "Before pruning:\n",
            "loss: 1.4289613962173462 - accuracy: 0.48476 - val_loss: 1.321987509727478 - val_accuracy: 0.5226 - penalty: 2e-05\n",
            "hidden layer sizes: [120, 120, 120, 120, 120], total units: 600\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 1.322021484375 - val_accuracy: 0.5227 - penalty: 2e-05\n",
            "hidden layer sizes: [100, 100, 96, 88, 100], total units: 484\n",
            "##########################################################\n",
            "Epoch 5/50\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 1.322021484375 - val_accuracy: 0.5227 - penalty: 2e-05\n",
            "hidden layer sizes: [100, 100, 96, 88, 100], total units: 484\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 1.3220213651657104 - val_accuracy: 0.5227 - penalty: 2e-05\n",
            "hidden layer sizes: [120, 120, 116, 108, 120], total units: 584\n",
            "Before pruning:\n",
            "loss: 1.3631298542022705 - accuracy: 0.5083 - val_loss: 1.2420017719268799 - val_accuracy: 0.552 - penalty: 2e-05\n",
            "hidden layer sizes: [120, 120, 116, 108, 120], total units: 584\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 1.241827368736267 - val_accuracy: 0.5523 - penalty: 2e-05\n",
            "hidden layer sizes: [100, 75, 84, 74, 100], total units: 433\n",
            "##########################################################\n",
            "Epoch 6/50\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 1.241827368736267 - val_accuracy: 0.5523 - penalty: 2e-05\n",
            "hidden layer sizes: [100, 75, 84, 74, 100], total units: 433\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 1.241827368736267 - val_accuracy: 0.5523 - penalty: 2e-05\n",
            "hidden layer sizes: [120, 95, 104, 94, 120], total units: 533\n",
            "Before pruning:\n",
            "loss: 1.2963899374008179 - accuracy: 0.5356 - val_loss: 1.19081449508667 - val_accuracy: 0.5788 - penalty: 2e-05\n",
            "hidden layer sizes: [120, 95, 104, 94, 120], total units: 533\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 1.190641164779663 - val_accuracy: 0.5788 - penalty: 2e-05\n",
            "hidden layer sizes: [100, 61, 80, 67, 102], total units: 410\n",
            "##########################################################\n",
            "Epoch 7/50\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 1.190641164779663 - val_accuracy: 0.5788 - penalty: 2e-05\n",
            "hidden layer sizes: [100, 61, 80, 67, 102], total units: 410\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 1.190641164779663 - val_accuracy: 0.5788 - penalty: 2e-05\n",
            "hidden layer sizes: [120, 81, 100, 87, 122], total units: 510\n",
            "Before pruning:\n",
            "loss: 1.2556546926498413 - accuracy: 0.551 - val_loss: 1.165831208229065 - val_accuracy: 0.5838 - penalty: 2e-05\n",
            "hidden layer sizes: [120, 81, 100, 87, 122], total units: 510\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 1.1658880710601807 - val_accuracy: 0.5838 - penalty: 2e-05\n",
            "hidden layer sizes: [100, 46, 65, 61, 96], total units: 368\n",
            "##########################################################\n",
            "Epoch 8/50\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 1.1658880710601807 - val_accuracy: 0.5838 - penalty: 2e-05\n",
            "hidden layer sizes: [100, 46, 65, 61, 96], total units: 368\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 1.1658880710601807 - val_accuracy: 0.5838 - penalty: 2e-05\n",
            "hidden layer sizes: [120, 66, 85, 81, 116], total units: 468\n",
            "Before pruning:\n",
            "loss: 1.2204643487930298 - accuracy: 0.56374 - val_loss: 1.13169527053833 - val_accuracy: 0.5959 - penalty: 2e-05\n",
            "hidden layer sizes: [120, 66, 85, 81, 116], total units: 468\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 1.1317449808120728 - val_accuracy: 0.5957 - penalty: 2e-05\n",
            "hidden layer sizes: [100, 43, 53, 60, 85], total units: 341\n",
            "##########################################################\n",
            "Epoch 9/50\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 1.1317449808120728 - val_accuracy: 0.5957 - penalty: 2e-05\n",
            "hidden layer sizes: [100, 43, 53, 60, 85], total units: 341\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 1.1317452192306519 - val_accuracy: 0.5957 - penalty: 2e-05\n",
            "hidden layer sizes: [120, 63, 73, 80, 105], total units: 441\n",
            "Before pruning:\n",
            "loss: 1.1869571208953857 - accuracy: 0.57802 - val_loss: 1.0981090068817139 - val_accuracy: 0.6063 - penalty: 2e-05\n",
            "hidden layer sizes: [120, 63, 73, 80, 105], total units: 441\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 1.0982930660247803 - val_accuracy: 0.6059 - penalty: 2e-05\n",
            "hidden layer sizes: [99, 42, 48, 58, 81], total units: 328\n",
            "##########################################################\n",
            "Epoch 10/50\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 1.0982930660247803 - val_accuracy: 0.6059 - penalty: 2e-05\n",
            "hidden layer sizes: [99, 42, 48, 58, 81], total units: 328\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 1.0982930660247803 - val_accuracy: 0.6059 - penalty: 2e-05\n",
            "hidden layer sizes: [119, 62, 68, 78, 101], total units: 428\n",
            "Before pruning:\n",
            "loss: 1.1519088745117188 - accuracy: 0.5893 - val_loss: 1.060515284538269 - val_accuracy: 0.6178 - penalty: 2e-05\n",
            "hidden layer sizes: [119, 62, 68, 78, 101], total units: 428\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 1.06063973903656 - val_accuracy: 0.6173 - penalty: 2e-05\n",
            "hidden layer sizes: [96, 39, 46, 57, 89], total units: 327\n",
            "##########################################################\n",
            "Epoch 11/50\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 1.06063973903656 - val_accuracy: 0.6173 - penalty: 2e-05\n",
            "hidden layer sizes: [96, 39, 46, 57, 89], total units: 327\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 1.0606396198272705 - val_accuracy: 0.6173 - penalty: 2e-05\n",
            "hidden layer sizes: [116, 59, 66, 77, 109], total units: 427\n",
            "Before pruning:\n",
            "loss: 1.117120385169983 - accuracy: 0.60208 - val_loss: 1.036548137664795 - val_accuracy: 0.6306 - penalty: 2e-05\n",
            "hidden layer sizes: [116, 59, 66, 77, 109], total units: 427\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 1.0365201234817505 - val_accuracy: 0.6307 - penalty: 2e-05\n",
            "hidden layer sizes: [92, 34, 44, 58, 84], total units: 312\n",
            "##########################################################\n",
            "Epoch 12/50\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 1.0365201234817505 - val_accuracy: 0.6307 - penalty: 2e-05\n",
            "hidden layer sizes: [92, 34, 44, 58, 84], total units: 312\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 1.0365198850631714 - val_accuracy: 0.6307 - penalty: 2e-05\n",
            "hidden layer sizes: [112, 54, 64, 78, 104], total units: 412\n",
            "Before pruning:\n",
            "loss: 1.09111487865448 - accuracy: 0.61388 - val_loss: 1.0179893970489502 - val_accuracy: 0.6352 - penalty: 2e-05\n",
            "hidden layer sizes: [112, 54, 64, 78, 104], total units: 412\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 1.018080711364746 - val_accuracy: 0.6355 - penalty: 2e-05\n",
            "hidden layer sizes: [87, 33, 40, 62, 82], total units: 304\n",
            "##########################################################\n",
            "Epoch 13/50\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 1.018080711364746 - val_accuracy: 0.6355 - penalty: 2e-05\n",
            "hidden layer sizes: [87, 33, 40, 62, 82], total units: 304\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 1.018080711364746 - val_accuracy: 0.6355 - penalty: 2e-05\n",
            "hidden layer sizes: [107, 53, 60, 82, 102], total units: 404\n",
            "Before pruning:\n",
            "loss: 1.0647658109664917 - accuracy: 0.62124 - val_loss: 0.9903009533882141 - val_accuracy: 0.6476 - penalty: 2e-05\n",
            "hidden layer sizes: [107, 53, 60, 82, 102], total units: 404\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 0.9904322028160095 - val_accuracy: 0.6472 - penalty: 2e-05\n",
            "hidden layer sizes: [84, 30, 35, 62, 87], total units: 298\n",
            "##########################################################\n",
            "Epoch 14/50\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 0.9904322028160095 - val_accuracy: 0.6472 - penalty: 2e-05\n",
            "hidden layer sizes: [84, 30, 35, 62, 87], total units: 298\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 0.9904322028160095 - val_accuracy: 0.6472 - penalty: 2e-05\n",
            "hidden layer sizes: [104, 50, 55, 82, 107], total units: 398\n",
            "Before pruning:\n",
            "loss: 1.0384883880615234 - accuracy: 0.63102 - val_loss: 0.9713183641433716 - val_accuracy: 0.6575 - penalty: 2e-05\n",
            "hidden layer sizes: [104, 50, 55, 82, 107], total units: 398\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 0.9712674617767334 - val_accuracy: 0.6572 - penalty: 2e-05\n",
            "hidden layer sizes: [83, 28, 35, 66, 88], total units: 300\n",
            "##########################################################\n",
            "Epoch 15/50\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 0.9712674617767334 - val_accuracy: 0.6572 - penalty: 2e-05\n",
            "hidden layer sizes: [83, 28, 35, 66, 88], total units: 300\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 0.971267580986023 - val_accuracy: 0.6572 - penalty: 2e-05\n",
            "hidden layer sizes: [103, 48, 55, 86, 108], total units: 400\n",
            "Before pruning:\n",
            "loss: 1.019011378288269 - accuracy: 0.63698 - val_loss: 0.9489160180091858 - val_accuracy: 0.6625 - penalty: 2e-05\n",
            "hidden layer sizes: [103, 48, 55, 86, 108], total units: 400\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 0.9490091800689697 - val_accuracy: 0.6629 - penalty: 2e-05\n",
            "hidden layer sizes: [84, 28, 32, 65, 91], total units: 300\n",
            "##########################################################\n",
            "Epoch 16/50\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 0.9490091800689697 - val_accuracy: 0.6629 - penalty: 2e-05\n",
            "hidden layer sizes: [84, 28, 32, 65, 91], total units: 300\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 0.9490091800689697 - val_accuracy: 0.6629 - penalty: 2e-05\n",
            "hidden layer sizes: [104, 48, 52, 85, 111], total units: 400\n",
            "Before pruning:\n",
            "loss: 1.0025696754455566 - accuracy: 0.64574 - val_loss: 0.9342312216758728 - val_accuracy: 0.6717 - penalty: 2e-05\n",
            "hidden layer sizes: [104, 48, 52, 85, 111], total units: 400\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 0.9341097474098206 - val_accuracy: 0.672 - penalty: 2e-05\n",
            "hidden layer sizes: [79, 26, 31, 67, 93], total units: 296\n",
            "##########################################################\n",
            "Epoch 17/50\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 0.9341097474098206 - val_accuracy: 0.672 - penalty: 2e-05\n",
            "hidden layer sizes: [79, 26, 31, 67, 93], total units: 296\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 0.9341097474098206 - val_accuracy: 0.672 - penalty: 2e-05\n",
            "hidden layer sizes: [99, 46, 51, 87, 113], total units: 396\n",
            "Before pruning:\n",
            "loss: 0.9888683557510376 - accuracy: 0.64982 - val_loss: 0.9210772514343262 - val_accuracy: 0.6782 - penalty: 2e-05\n",
            "hidden layer sizes: [99, 46, 51, 87, 113], total units: 396\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 0.9211081266403198 - val_accuracy: 0.6784 - penalty: 2e-05\n",
            "hidden layer sizes: [77, 25, 29, 62, 76], total units: 269\n",
            "##########################################################\n",
            "Epoch 18/50\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 0.9211081266403198 - val_accuracy: 0.6784 - penalty: 2e-05\n",
            "hidden layer sizes: [77, 25, 29, 62, 76], total units: 269\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 0.9211081266403198 - val_accuracy: 0.6784 - penalty: 2e-05\n",
            "hidden layer sizes: [97, 45, 49, 82, 96], total units: 369\n",
            "Before pruning:\n",
            "loss: 0.9715972542762756 - accuracy: 0.65734 - val_loss: 0.9039890766143799 - val_accuracy: 0.6865 - penalty: 2e-05\n",
            "hidden layer sizes: [97, 45, 49, 82, 96], total units: 369\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 0.9039579033851624 - val_accuracy: 0.6866 - penalty: 2e-05\n",
            "hidden layer sizes: [76, 25, 32, 64, 85], total units: 282\n",
            "##########################################################\n",
            "Epoch 19/50\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 0.9039579033851624 - val_accuracy: 0.6866 - penalty: 2e-05\n",
            "hidden layer sizes: [76, 25, 32, 64, 85], total units: 282\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 0.9039577841758728 - val_accuracy: 0.6866 - penalty: 2e-05\n",
            "hidden layer sizes: [96, 45, 52, 84, 105], total units: 382\n",
            "Before pruning:\n",
            "loss: 0.9636085033416748 - accuracy: 0.65988 - val_loss: 0.8938054442405701 - val_accuracy: 0.6863 - penalty: 2e-05\n",
            "hidden layer sizes: [96, 45, 52, 84, 105], total units: 382\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 0.8937602639198303 - val_accuracy: 0.6864 - penalty: 2e-05\n",
            "hidden layer sizes: [67, 23, 28, 63, 96], total units: 277\n",
            "##########################################################\n",
            "Epoch 20/50\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 0.8937602639198303 - val_accuracy: 0.6864 - penalty: 2e-05\n",
            "hidden layer sizes: [67, 23, 28, 63, 96], total units: 277\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 0.8937602639198303 - val_accuracy: 0.6864 - penalty: 2e-05\n",
            "hidden layer sizes: [87, 43, 48, 83, 116], total units: 377\n",
            "Before pruning:\n",
            "loss: 0.9495522379875183 - accuracy: 0.66404 - val_loss: 0.8877635598182678 - val_accuracy: 0.6911 - penalty: 2e-05\n",
            "hidden layer sizes: [87, 43, 48, 83, 116], total units: 377\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 0.8879519701004028 - val_accuracy: 0.6907 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 27, 64, 94], total units: 273\n",
            "##########################################################\n",
            "Epoch 21/50\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 0.8879519701004028 - val_accuracy: 0.6907 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 27, 64, 94], total units: 273\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 0.8879519701004028 - val_accuracy: 0.6907 - penalty: 2e-05\n",
            "hidden layer sizes: [85, 43, 47, 84, 114], total units: 373\n",
            "Before pruning:\n",
            "loss: 0.9422478079795837 - accuracy: 0.6649 - val_loss: 0.8795605301856995 - val_accuracy: 0.6899 - penalty: 2e-05\n",
            "hidden layer sizes: [85, 43, 47, 84, 114], total units: 373\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 0.8795232176780701 - val_accuracy: 0.6894 - penalty: 2e-05\n",
            "hidden layer sizes: [63, 23, 26, 64, 87], total units: 263\n",
            "##########################################################\n",
            "Epoch 22/50\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 0.8795232176780701 - val_accuracy: 0.6894 - penalty: 2e-05\n",
            "hidden layer sizes: [63, 23, 26, 64, 87], total units: 263\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 0.8795232176780701 - val_accuracy: 0.6894 - penalty: 2e-05\n",
            "hidden layer sizes: [83, 43, 46, 84, 107], total units: 363\n",
            "Before pruning:\n",
            "loss: 0.9319015741348267 - accuracy: 0.66992 - val_loss: 0.8760385513305664 - val_accuracy: 0.6979 - penalty: 2e-05\n",
            "hidden layer sizes: [83, 43, 46, 84, 107], total units: 363\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 0.876153826713562 - val_accuracy: 0.6974 - penalty: 2e-05\n",
            "hidden layer sizes: [62, 23, 25, 60, 87], total units: 257\n",
            "##########################################################\n",
            "Epoch 23/50\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 0.876153826713562 - val_accuracy: 0.6974 - penalty: 2e-05\n",
            "hidden layer sizes: [62, 23, 25, 60, 87], total units: 257\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 0.8761537075042725 - val_accuracy: 0.6974 - penalty: 2e-05\n",
            "hidden layer sizes: [82, 43, 45, 80, 107], total units: 357\n",
            "Before pruning:\n",
            "loss: 0.9224852323532104 - accuracy: 0.67466 - val_loss: 0.8709323406219482 - val_accuracy: 0.6982 - penalty: 2e-05\n",
            "hidden layer sizes: [82, 43, 45, 80, 107], total units: 357\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 0.8708728551864624 - val_accuracy: 0.6982 - penalty: 2e-05\n",
            "hidden layer sizes: [60, 23, 25, 61, 100], total units: 269\n",
            "##########################################################\n",
            "Epoch 24/50\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 0.8708728551864624 - val_accuracy: 0.6982 - penalty: 2e-05\n",
            "hidden layer sizes: [60, 23, 25, 61, 100], total units: 269\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 0.8708728551864624 - val_accuracy: 0.6982 - penalty: 2e-05\n",
            "hidden layer sizes: [80, 43, 45, 81, 120], total units: 369\n",
            "Before pruning:\n",
            "loss: 0.919285237789154 - accuracy: 0.67366 - val_loss: 0.8537863492965698 - val_accuracy: 0.7048 - penalty: 2e-05\n",
            "hidden layer sizes: [80, 43, 45, 81, 120], total units: 369\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 0.8538374304771423 - val_accuracy: 0.7046 - penalty: 2e-05\n",
            "hidden layer sizes: [58, 22, 23, 63, 104], total units: 270\n",
            "##########################################################\n",
            "Epoch 25/50\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 0.8538374304771423 - val_accuracy: 0.7046 - penalty: 2e-05\n",
            "hidden layer sizes: [58, 22, 23, 63, 104], total units: 270\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 0.8538374304771423 - val_accuracy: 0.7046 - penalty: 2e-05\n",
            "hidden layer sizes: [78, 42, 43, 83, 124], total units: 370\n",
            "Before pruning:\n",
            "loss: 0.9092385172843933 - accuracy: 0.67892 - val_loss: 0.8444966673851013 - val_accuracy: 0.7069 - penalty: 2e-05\n",
            "hidden layer sizes: [78, 42, 43, 83, 124], total units: 370\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 0.8445139527320862 - val_accuracy: 0.7073 - penalty: 2e-05\n",
            "hidden layer sizes: [62, 22, 25, 63, 94], total units: 266\n",
            "##########################################################\n",
            "Epoch 26/50\n",
            "loss: 0.9080063104629517 - accuracy: 0.67916 - val_loss: 0.8365342020988464 - val_accuracy: 0.7073 - penalty: 0.0\n",
            "hidden layer sizes: [62, 22, 25, 63, 94], total units: 266\n",
            "##########################################################\n",
            "Epoch 27/50\n",
            "loss: 0.828000009059906 - accuracy: 0.70588 - val_loss: 0.8190657496452332 - val_accuracy: 0.7116 - penalty: 0.0\n",
            "hidden layer sizes: [62, 22, 25, 63, 94], total units: 266\n",
            "##########################################################\n",
            "Epoch 28/50\n",
            "loss: 0.7950676679611206 - accuracy: 0.71846 - val_loss: 0.8015548586845398 - val_accuracy: 0.7225 - penalty: 0.0\n",
            "hidden layer sizes: [62, 22, 25, 63, 94], total units: 266\n",
            "##########################################################\n",
            "Epoch 29/50\n",
            "loss: 0.7660391926765442 - accuracy: 0.72876 - val_loss: 0.7933292984962463 - val_accuracy: 0.7249 - penalty: 0.0\n",
            "hidden layer sizes: [62, 22, 25, 63, 94], total units: 266\n",
            "##########################################################\n",
            "Epoch 30/50\n",
            "loss: 0.7464088797569275 - accuracy: 0.7346 - val_loss: 0.7815882563591003 - val_accuracy: 0.7283 - penalty: 0.0\n",
            "hidden layer sizes: [62, 22, 25, 63, 94], total units: 266\n",
            "##########################################################\n",
            "Epoch 31/50\n",
            "loss: 0.7159618735313416 - accuracy: 0.74518 - val_loss: 0.7760067582130432 - val_accuracy: 0.7291 - penalty: 0.0\n",
            "hidden layer sizes: [62, 22, 25, 63, 94], total units: 266\n",
            "##########################################################\n",
            "Epoch 32/50\n",
            "loss: 0.6948229074478149 - accuracy: 0.7534 - val_loss: 0.7641218900680542 - val_accuracy: 0.7357 - penalty: 0.0\n",
            "hidden layer sizes: [62, 22, 25, 63, 94], total units: 266\n",
            "##########################################################\n",
            "Epoch 33/50\n",
            "loss: 0.6671339869499207 - accuracy: 0.76416 - val_loss: 0.7657857537269592 - val_accuracy: 0.7354 - penalty: 0.0\n",
            "hidden layer sizes: [62, 22, 25, 63, 94], total units: 266\n",
            "##########################################################\n",
            "Epoch 34/50\n",
            "loss: 0.6452786326408386 - accuracy: 0.77196 - val_loss: 0.7514198422431946 - val_accuracy: 0.7405 - penalty: 0.0\n",
            "hidden layer sizes: [62, 22, 25, 63, 94], total units: 266\n",
            "##########################################################\n",
            "Epoch 35/50\n",
            "loss: 0.6199966669082642 - accuracy: 0.77948 - val_loss: 0.7528852224349976 - val_accuracy: 0.7449 - penalty: 0.0\n",
            "hidden layer sizes: [62, 22, 25, 63, 94], total units: 266\n",
            "##########################################################\n",
            "Epoch 36/50\n",
            "loss: 0.6039485931396484 - accuracy: 0.78654 - val_loss: 0.74161696434021 - val_accuracy: 0.7486 - penalty: 0.0\n",
            "hidden layer sizes: [62, 22, 25, 63, 94], total units: 266\n",
            "##########################################################\n",
            "Epoch 37/50\n",
            "loss: 0.5792882442474365 - accuracy: 0.795 - val_loss: 0.74861079454422 - val_accuracy: 0.7479 - penalty: 0.0\n",
            "hidden layer sizes: [62, 22, 25, 63, 94], total units: 266\n",
            "##########################################################\n",
            "Epoch 38/50\n",
            "loss: 0.5582988262176514 - accuracy: 0.80156 - val_loss: 0.7483891248703003 - val_accuracy: 0.7467 - penalty: 0.0\n",
            "hidden layer sizes: [62, 22, 25, 63, 94], total units: 266\n",
            "##########################################################\n",
            "Epoch 39/50\n",
            "loss: 0.5382571220397949 - accuracy: 0.8096 - val_loss: 0.7504693269729614 - val_accuracy: 0.7486 - penalty: 0.0\n",
            "hidden layer sizes: [62, 22, 25, 63, 94], total units: 266\n",
            "##########################################################\n",
            "Epoch 40/50\n",
            "loss: 0.524891197681427 - accuracy: 0.81352 - val_loss: 0.7454087138175964 - val_accuracy: 0.7495 - penalty: 0.0\n",
            "hidden layer sizes: [62, 22, 25, 63, 94], total units: 266\n",
            "##########################################################\n",
            "Epoch 41/50\n",
            "loss: 0.5065606832504272 - accuracy: 0.819 - val_loss: 0.7508408427238464 - val_accuracy: 0.7495 - penalty: 0.0\n",
            "hidden layer sizes: [62, 22, 25, 63, 94], total units: 266\n",
            "##########################################################\n",
            "Epoch 42/50\n",
            "loss: 0.49009305238723755 - accuracy: 0.8267 - val_loss: 0.7523502707481384 - val_accuracy: 0.7525 - penalty: 0.0\n",
            "hidden layer sizes: [62, 22, 25, 63, 94], total units: 266\n",
            "##########################################################\n",
            "Epoch 43/50\n",
            "loss: 0.48025089502334595 - accuracy: 0.82896 - val_loss: 0.7577595710754395 - val_accuracy: 0.7494 - penalty: 0.0\n",
            "hidden layer sizes: [62, 22, 25, 63, 94], total units: 266\n",
            "##########################################################\n",
            "Epoch 44/50\n",
            "loss: 0.4649268388748169 - accuracy: 0.83516 - val_loss: 0.761668860912323 - val_accuracy: 0.7504 - penalty: 0.0\n",
            "hidden layer sizes: [62, 22, 25, 63, 94], total units: 266\n",
            "##########################################################\n",
            "Epoch 45/50\n",
            "loss: 0.4493297338485718 - accuracy: 0.83986 - val_loss: 0.7704628705978394 - val_accuracy: 0.7494 - penalty: 0.0\n",
            "hidden layer sizes: [62, 22, 25, 63, 94], total units: 266\n",
            "##########################################################\n",
            "Epoch 46/50\n",
            "loss: 0.44488030672073364 - accuracy: 0.84224 - val_loss: 0.7666493058204651 - val_accuracy: 0.7567 - penalty: 0.0\n",
            "hidden layer sizes: [62, 22, 25, 63, 94], total units: 266\n",
            "##########################################################\n",
            "Epoch 47/50\n",
            "loss: 0.4336094260215759 - accuracy: 0.84548 - val_loss: 0.7782170176506042 - val_accuracy: 0.7521 - penalty: 0.0\n",
            "hidden layer sizes: [62, 22, 25, 63, 94], total units: 266\n",
            "##########################################################\n",
            "Epoch 48/50\n",
            "loss: 0.41823235154151917 - accuracy: 0.85104 - val_loss: 0.7773575186729431 - val_accuracy: 0.7532 - penalty: 0.0\n",
            "hidden layer sizes: [62, 22, 25, 63, 94], total units: 266\n",
            "##########################################################\n",
            "Epoch 49/50\n",
            "loss: 0.40995901823043823 - accuracy: 0.8545 - val_loss: 0.785649299621582 - val_accuracy: 0.7549 - penalty: 0.0\n",
            "hidden layer sizes: [62, 22, 25, 63, 94], total units: 266\n",
            "##########################################################\n",
            "Epoch 50/50\n",
            "loss: 0.395946204662323 - accuracy: 0.85818 - val_loss: 0.7888185977935791 - val_accuracy: 0.7521 - penalty: 0.0\n",
            "hidden layer sizes: [62, 22, 25, 63, 94], total units: 266\n",
            "CPU times: user 6min 10s, sys: 7.53 s, total: 6min 17s\n",
            "Wall time: 6min 47s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfrwJeFXzOVL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}