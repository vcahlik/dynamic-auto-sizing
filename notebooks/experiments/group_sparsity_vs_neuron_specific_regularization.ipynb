{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "tf_multi_layer_ssnet_inverse.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_deAUKlniFk",
        "outputId": "fd1bbcc1-b80f-42a6-d49b-e27f8c65df49"
      },
      "source": [
        "# In the output, check that you have Tesla V100 (best), Tesla P100, or something similar\n",
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Jul 21 13:31:20 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    32W / 250W |   2475MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKwUwV_NneIo"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOoXBq05neIt"
      },
      "source": [
        "dtype = 'float32'\n",
        "tf.keras.backend.set_floatx(dtype)"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfMGB9KZneIu"
      },
      "source": [
        "# fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "# (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# X_train = X_train.astype(dtype) / 255.0\n",
        "# y_train = y_train.astype(dtype)\n",
        "# X_test = X_test.astype(dtype)  / 255.0\n",
        "# y_test = y_test.astype(dtype)\n",
        "\n",
        "# X_train = np.reshape(X_train, (-1, 784))\n",
        "# X_test = np.reshape(X_test, (-1, 784))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BrJPdkBneIv"
      },
      "source": [
        "cifar10 = tf.keras.datasets.cifar10\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "X_train = X_train.astype(dtype) / 255.0\n",
        "y_train = y_train.astype(dtype)\n",
        "X_test = X_test.astype(dtype)  / 255.0\n",
        "y_test = y_test.astype(dtype)\n",
        "\n",
        "X_train = np.reshape(X_train, (-1, 3072))\n",
        "X_test = np.reshape(X_test, (-1, 3072))"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5uTvu5kxF-b"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "\n",
        "X_train_norm = scaler.transform(X_train)\n",
        "X_test_norm = scaler.transform(X_test)"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tg4vCToybBnx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba4a2b2b-508f-47b6-f263-3187a6e9c68d"
      },
      "source": [
        "input_units = 4\n",
        "units = 10\n",
        "\n",
        "W = tf.constant(\n",
        "    [[1, 1, 1, 1, 1],\n",
        "     [1, 2, 2, 2, 2],\n",
        "     [2, 2, 3, 1, 3]],\n",
        "    dtype=dtype\n",
        ")\n",
        "b = tf.constant(\n",
        "    [0, 0, 0, 1, 1],\n",
        "    dtype=dtype\n",
        ")\n",
        "W"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 5), dtype=float32, numpy=\n",
              "array([[1., 1., 1., 1., 1.],\n",
              "       [1., 2., 2., 2., 2.],\n",
              "       [2., 2., 3., 1., 3.]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzUXg-rvWRne",
        "outputId": "ade4f779-472e-40da-f085-52483216a354",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "b"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5,), dtype=float32, numpy=array([0., 0., 0., 1., 1.], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRQnxag4WPnh",
        "outputId": "555f06ad-bc78-4bc7-ae68-7608b94f8c29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tf.concat([W, tf.reshape(b, (1, -1))], axis=0)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(4, 5), dtype=float32, numpy=\n",
              "array([[1., 1., 1., 1., 1.],\n",
              "       [1., 2., 2., 2., 2.],\n",
              "       [2., 2., 3., 1., 3.],\n",
              "       [0., 0., 0., 1., 1.]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxrhHFyn3r5H"
      },
      "source": [
        "if len(W.shape) == 1:\n",
        "    W = tf.reshape(W, (-1, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFVqco2K31l6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "941f9cb8-345b-4521-b4ee-26d9062402b8"
      },
      "source": [
        "W"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
              "array([[1.],\n",
              "       [2.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSnl9Bwayg1N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34a0a240-a2e3-41b5-8c3e-c3c3be22a315"
      },
      "source": [
        "tf.norm(\n",
        "    W, ord=np.inf, axis=-1\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5,), dtype=float32, numpy=array([1., 2., 1., 1., 1.], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UzMURXobYHh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af7b8d83-34b8-41e3-e582-16a01b71fe49"
      },
      "source": [
        "regularization_penalty = 1\n",
        "scaling_vector = tf.cumsum(tf.constant(regularization_penalty, shape=(W.shape[0],), dtype=dtype), axis=0) - regularization_penalty\n",
        "scaling_vector"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 1., 2., 3.], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOcWpN9kEzxY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d876ac33-d5ac-4949-9f03-a5d171ce4ff4"
      },
      "source": [
        "tf.reshape(scaling_vector, (-1, 1)) * tf.abs(W)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(4, 10), dtype=float32, numpy=\n",
              "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "       [2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],\n",
              "       [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTZq4KMpneIv"
      },
      "source": [
        "class SSRegularizer(tf.keras.regularizers.Regularizer):\n",
        "    def __init__(self, regularization_penalty, regularization_method):\n",
        "        self.regularization_penalty = regularization_penalty\n",
        "        self.regularization_method = regularization_method\n",
        "\n",
        "    def __call__(self, x):\n",
        "        if self.regularization_method == 'weighted_l1':\n",
        "            return self.weighted_l1(x)\n",
        "        elif self.regularization_method == 'group_sparsity':\n",
        "            return self.group_sparsity(x)\n",
        "        else:\n",
        "            raise NotImplementedError(f\"Unknown regularization method {self.regularization_method}\")\n",
        "    \n",
        "    def weighted_l1(self, x):\n",
        "        # TODO this docstring was written for the inverted version, FIX\n",
        "        #\n",
        "        # I.e. for a parameter matrix of 4 input and 10 output neurons:\n",
        "        #\n",
        "        # [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
        "        #  [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
        "        #  [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
        "        #  [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]\n",
        "        #\n",
        "        # The scaling vector could be [0., 1., 2., 3.], and the resulting weighted values could be\n",
        "        #\n",
        "        # [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
        "        #  [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
        "        #  [2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],\n",
        "        #  [3., 3., 3., 3., 3., 3., 3., 3., 3., 3.]]\n",
        "        #\n",
        "        # Therefore every additional input neuron is regularized more.\n",
        "\n",
        "        scaling_vector = tf.cumsum(tf.constant(self.regularization_penalty, shape=(x.shape[-1],), dtype=dtype), axis=0) - self.regularization_penalty\n",
        "        weighted_values = scaling_vector * tf.abs(x)\n",
        "        return tf.reduce_sum(weighted_values)\n",
        "    \n",
        "    def group_sparsity(self, x):\n",
        "        # TODO this docstring was written for the inverted version, FIX\n",
        "        #\n",
        "        # I.e. for a parameter matrix of 3 input and 5 output neurons:\n",
        "        #\n",
        "        # [[1., 1., 1., 1., 1.],\n",
        "        #  [1., 2., 2., 2., 2.],\n",
        "        #  [2., 2., 3., 1., 3.]]\n",
        "        #\n",
        "        # The resulting vector of group norms is [1., 2., 3.], therefore for\n",
        "        # every input neuron, its outgoing connections form a group.\n",
        "\n",
        "        print(x.shape)\n",
        "        group_norms = tf.norm(x, ord=np.inf, axis=1)\n",
        "        return self.regularization_penalty * tf.reduce_sum(group_norms)\n",
        "\n",
        "    def get_config(self):\n",
        "        return {'regularization_penalty': float(self.regularization_penalty)}\n",
        "\n",
        "\n",
        "class SSLayer(tf.keras.Model):\n",
        "    def __init__(self, input_units, units, activation, regularization_penalty, regularization_method, kernel_initializer, bias_initializer, regularize=True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_units = input_units\n",
        "        self.units = units\n",
        "        self.activation = activation\n",
        "        self.regularization_penalty = regularization_penalty\n",
        "        self.regularization_method = regularization_method\n",
        "        self.kernel_initializer = kernel_initializer\n",
        "        self.bias_initializer = bias_initializer\n",
        "        \n",
        "        self.A = tf.keras.activations.get(activation)\n",
        "        self.W_init = tf.keras.initializers.get(kernel_initializer)\n",
        "        self.b_init = tf.keras.initializers.get(bias_initializer)\n",
        "        self.regularizer = SSRegularizer(self.regularization_penalty, self.regularization_method)\n",
        "        \n",
        "        self.W = tf.Variable(\n",
        "            name='W',\n",
        "            initial_value=self.W_init(shape=(input_units, units), dtype=dtype),\n",
        "            trainable=True)\n",
        "        \n",
        "        self.b = tf.Variable(\n",
        "            name='b',\n",
        "            initial_value=self.b_init(shape=(units,), dtype=dtype),\n",
        "            trainable=True)\n",
        "        \n",
        "        if self.regularization_method is not None:\n",
        "            self.add_loss(lambda: self.regularizer(tf.concat([self.W, tf.reshape(self.b, (1, -1))], axis=0)))\n",
        "    \n",
        "    def call(self, inputs):\n",
        "        return self.A(tf.matmul(inputs, self.W) + self.b)\n",
        "    \n",
        "    def copy_without_regularization(self):\n",
        "        copy = SSLayer(\n",
        "            self.input_units, \n",
        "            self.units, \n",
        "            self.activation, \n",
        "            regularization_penalty=self.regularization_penalty, \n",
        "            regularization_method=None, \n",
        "            kernel_initializer=self.kernel_initializer, \n",
        "            bias_initializer=self.bias_initializer\n",
        "        )\n",
        "        copy.W = self.W\n",
        "        copy.b = self.b\n",
        "        return copy\n",
        "\n",
        "\n",
        "class SSModel(tf.keras.Model):\n",
        "    def __init__(self, layer_sizes, activation=None, regularization_penalty=0.01, regularization_method='weighted_l1', kernel_initializer='glorot_uniform', bias_initializer='zeros'):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.sslayers = list()\n",
        "        for l in range(len(layer_sizes) - 1):\n",
        "            input_units = layer_sizes[l]\n",
        "            units = layer_sizes[l + 1]\n",
        "            if l < len(layer_sizes) - 2:\n",
        "                layer = SSLayer(input_units, units, activation, regularization_penalty, regularization_method, kernel_initializer, bias_initializer)\n",
        "            else:  # Last layer\n",
        "                layer = SSLayer(input_units, units, 'softmax', 0., regularization_method, kernel_initializer, bias_initializer)\n",
        "            self.sslayers.append(layer)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = inputs\n",
        "        for layer in self.sslayers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "    \n",
        "    def get_layer_sizes(self):\n",
        "        layer_sizes = list()\n",
        "        for l in range(len(self.sslayers)):\n",
        "            layer = self.sslayers[l]\n",
        "            layer_sizes.append(layer.W.shape[0])\n",
        "            if l == len(self.sslayers) - 1:  # Last layer\n",
        "                layer_sizes.append(layer.W.shape[1])\n",
        "        return layer_sizes\n",
        "    \n",
        "    def print_neurons(self):\n",
        "        for layer in self.sslayers[:-1]:\n",
        "            print(get_param_string(layer.W, layer.b))\n",
        "    \n",
        "    def remove_regularization(self):\n",
        "        for l in range(len(self.sslayers)):\n",
        "            self.sslayers[l] = self.sslayers[l].copy_without_regularization()\n",
        "    \n",
        "    def set_regularization_penalty(self, regularization_penalty):\n",
        "        for l in range(0, len(self.sslayers) - 1):  # Every layer except of the last is regularized\n",
        "            self.sslayers[l].regularizer.regularization_penalty = regularization_penalty\n",
        "    \n",
        "    def prune(self, threshold=0.001):\n",
        "        for l in range(len(self.sslayers) - 1):\n",
        "            layer1 = self.sslayers[l]\n",
        "            layer2 = self.sslayers[l + 1]\n",
        "            \n",
        "            W1 = layer1.W.value()\n",
        "            b1 = layer1.b.value()\n",
        "            W2 = layer2.W.value()\n",
        "\n",
        "            weights_with_biases = tf.concat([W1, tf.reshape(b1, (1, -1))], axis=0)\n",
        "            neurons_are_active = tf.math.reduce_max(tf.abs(weights_with_biases), axis=0) >= threshold\n",
        "            active_neurons_indices = tf.reshape(tf.where(neurons_are_active), (-1,))\n",
        "            \n",
        "            new_W1 = tf.gather(W1, active_neurons_indices, axis=1)\n",
        "            new_b1 = tf.gather(b1, active_neurons_indices, axis=0)\n",
        "            new_W2 = tf.gather(W2, active_neurons_indices, axis=0)\n",
        "\n",
        "            layer1.W = tf.Variable(name='W', initial_value=new_W1, trainable=True)\n",
        "            layer1.b = tf.Variable(name='b', initial_value=new_b1, trainable=True)\n",
        "            layer2.W = tf.Variable(name='W', initial_value=new_W2, trainable=True)\n",
        "    \n",
        "    def grow(self, min_new_neurons=5, scaling_factor=0.001):   \n",
        "        for l in range(len(self.sslayers) - 1):\n",
        "            layer1 = self.sslayers[l]\n",
        "            layer2 = self.sslayers[l + 1]\n",
        "       \n",
        "            W1 = layer1.W.value()\n",
        "            b1 = layer1.b.value()\n",
        "            W2 = layer2.W.value()\n",
        "\n",
        "            n_new_neurons = max(min_new_neurons, int(W1.shape[1] * 0.2))\n",
        "\n",
        "            W1_growth = layer1.W_init(shape=(W1.shape[0], W1.shape[1] + n_new_neurons), dtype=dtype)[:, -n_new_neurons:] * scaling_factor\n",
        "            b1_growth = layer1.b_init(shape=(n_new_neurons,), dtype=dtype)\n",
        "            W2_growth = layer2.W_init(shape=(W2.shape[0] + n_new_neurons, W2.shape[1]), dtype=dtype)[-n_new_neurons:, :] * scaling_factor  # TODO try also multiplying by scaling_factor\n",
        "\n",
        "            new_W1 = tf.concat([W1, W1_growth], axis=1)\n",
        "            new_b1 = tf.concat([b1, b1_growth], axis=0)\n",
        "            new_W2 = tf.concat([W2, W2_growth], axis=0)\n",
        "\n",
        "            layer1.W = tf.Variable(name='W1', initial_value=new_W1, trainable=True)\n",
        "            layer1.b = tf.Variable(name='b1', initial_value=new_b1, trainable=True)\n",
        "            layer2.W = tf.Variable(name='W2', initial_value=new_W2, trainable=True)"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WPKvoQQneIx"
      },
      "source": [
        "def get_param_string(weights, bias):\n",
        "    param_string = \"\"\n",
        "    weights_with_bias = tf.concat([weights, tf.reshape(bias, (1, -1))], axis=0)\n",
        "    max_parameters = tf.math.reduce_max(tf.abs(weights_with_bias), axis=0).numpy()\n",
        "    magnitudes = np.floor(np.log10(max_parameters))\n",
        "    for m in magnitudes:\n",
        "        if m > 0:\n",
        "            m = 0\n",
        "        param_string += str(int(-m))\n",
        "    return param_string\n",
        "\n",
        "\n",
        "def print_epoch_statistics(model, x, y, validation_data):\n",
        "    x_val = validation_data[0]\n",
        "    y_val = validation_data[1]\n",
        "\n",
        "    y_pred = model(x)\n",
        "    loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(y, y_pred))\n",
        "    accuracy = tf.reduce_mean(tf.keras.metrics.sparse_categorical_accuracy(y, y_pred))\n",
        "    \n",
        "    y_val_pred = model(x_val)\n",
        "    val_loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(y_val, y_val_pred))\n",
        "    val_accuracy = tf.reduce_mean(tf.keras.metrics.sparse_categorical_accuracy(y_val, y_val_pred))\n",
        "    print(f\"loss: {loss} - accuracy: {accuracy} - val_loss: {val_loss} - val_accuracy: {val_accuracy}\")\n",
        "    print(f\"layer sizes: {model.get_layer_sizes()}\")\n",
        "    model.print_neurons()\n",
        "\n",
        "\n",
        "def train_model(model, x, y, optimizer, epochs, self_scaling_epochs, batch_size, min_new_neurons, validation_data):\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
        "    train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "\n",
        "        if epoch < self_scaling_epochs:\n",
        "            print(\"Before growing:\")\n",
        "            print_epoch_statistics(model, x, y, validation_data)\n",
        "            model.grow(min_new_neurons=min_new_neurons, scaling_factor=0.001)\n",
        "            print(\"After growing:\")\n",
        "            print_epoch_statistics(model, x, y, validation_data)\n",
        "        \n",
        "        if epoch == self_scaling_epochs:\n",
        "            model.remove_regularization()\n",
        "\n",
        "        for step, (x_batch, y_batch) in enumerate(train_dataset):\n",
        "            with tf.GradientTape() as tape:\n",
        "                y_pred = model(x_batch, training=True)\n",
        "                loss_value = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(y_batch, y_pred))\n",
        "                loss_value += sum(model.losses)\n",
        "\n",
        "            grads = tape.gradient(loss_value, model.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "        \n",
        "        if epoch < self_scaling_epochs:\n",
        "            print(\"Before pruning:\")\n",
        "            print_epoch_statistics(model, x, y, validation_data)\n",
        "            model.prune(threshold=0.001)\n",
        "            print(\"After pruning:\")\n",
        "            print_epoch_statistics(model, x, y, validation_data)\n",
        "        else:\n",
        "            print_epoch_statistics(model, x, y, validation_data)"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1MrQXUTFwOe"
      },
      "source": [
        "# Neuron specific regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOgW8Vb5Ghcg"
      },
      "source": [
        "epochs = 20\n",
        "self_scaling_epochs = 10\n",
        "batch_size = 32\n",
        "min_new_neurons = 100"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RdrNMmzqGBMV",
        "outputId": "d20ea60e-108a-42cb-eb87-4db15f7e7592"
      },
      "source": [
        "model = SSModel(layer_sizes=[3072, 100, 100, 100, 100, 10], activation='selu', regularization_penalty=0.0001, regularization_method='weighted_l1', kernel_initializer='lecun_normal')\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "train_model(model, X_train_norm, y_train, optimizer, epochs, self_scaling_epochs, batch_size, min_new_neurons, validation_data=(X_test_norm, y_test))"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "Before growing:\n",
            "loss: 2.7119767665863037 - accuracy: 0.10016000270843506 - val_loss: 2.7168233394622803 - val_accuracy: 0.09939999878406525\n",
            "layer sizes: [3072, 100, 100, 100, 100, 10]\n",
            "2222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "After growing:\n",
            "loss: 2.7119767665863037 - accuracy: 0.10016000270843506 - val_loss: 2.7168233394622803 - val_accuracy: 0.09939999878406525\n",
            "layer sizes: [3072, 200, 200, 200, 200, 10]\n",
            "22222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222225555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555\n",
            "11111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111114444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444\n",
            "11111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111114444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444\n",
            "11111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111114444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444\n",
            "Before pruning:\n",
            "loss: 1.7060856819152832 - accuracy: 0.38471999764442444 - val_loss: 1.707443356513977 - val_accuracy: 0.3837999999523163\n",
            "layer sizes: [3072, 200, 200, 200, 200, 10]\n",
            "11111121113333233333333333333333343443434344343344434443444434444444444444444344444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444\n",
            "11111111111212211111111111111121221113133131323133233333333333333333444342333333434334333323333333333334433344333433434334433433333333443443344333344344333334443434334434343334344433434443434343444433\n",
            "11111111111111111211112322222333232333333324143333433333333333344433343343344333344444343444443334444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444\n",
            "11111111111111111121121112122121131123321223233322332233223333333333333333333333333333333433333333334344434344333344344344343434333333333333444334444433333344344433344443334433334344434344444444444434\n",
            "After pruning:\n",
            "loss: 1.7056148052215576 - accuracy: 0.385019987821579 - val_loss: 1.7069681882858276 - val_accuracy: 0.3833000063896179\n",
            "layer sizes: [3072, 44, 149, 76, 146, 10]\n",
            "11111121113333233333333333333333333333333333\n",
            "11111111111212211111111111111121221113133131323133233333333333333333323333333333333233333333333333333333333333333333333333333333333333333333333333333\n",
            "1111111111111111121111232222233323233333332133333333333333333333333333333333\n",
            "11111111111111111121121112122121131123321223233322332233223333333333333333333333333333333333333333333333333333333333333333333333333333333333333333\n",
            "Epoch 2/20\n",
            "Before growing:\n",
            "loss: 1.7056148052215576 - accuracy: 0.385019987821579 - val_loss: 1.7069681882858276 - val_accuracy: 0.3833000063896179\n",
            "layer sizes: [3072, 44, 149, 76, 146, 10]\n",
            "11111121113333233333333333333333333333333333\n",
            "11111111111212211111111111111121221113133131323133233333333333333333323333333333333233333333333333333333333333333333333333333333333333333333333333333\n",
            "1111111111111111121111232222233323233333332133333333333333333333333333333333\n",
            "11111111111111111121121112122121131123321223233322332233223333333333333333333333333333333333333333333333333333333333333333333333333333333333333333\n",
            "After growing:\n",
            "loss: 1.7056150436401367 - accuracy: 0.385019987821579 - val_loss: 1.7069681882858276 - val_accuracy: 0.3833000063896179\n",
            "layer sizes: [3072, 144, 249, 176, 246, 10]\n",
            "111111211133332333333333333333333333333333335555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555\n",
            "111111111112122111111111111111212211131331313231332333333333333333333233333333333332333333333333333333333333333333333333333333333333333333333333333334444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444\n",
            "11111111111111111211112322222333232333333321333333333333333333333333333333334444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444\n",
            "111111111111111111211211121221211311233212232333223322332233333333333333333333333333333333333333333333333333333333333333333333333333333333333333334444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-103-7057285a28cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself_scaling_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_new_neurons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-101-1be19bcf3920>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, x, y, optimizer, epochs, self_scaling_epochs, batch_size, min_new_neurons, validation_data)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mloss_value\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1078\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1079\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1080\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    157\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_grad.py\u001b[0m in \u001b[0;36m_MulGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m   1383\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m     gy = array_ops.reshape(\n\u001b[0;32m-> 1385\u001b[0;31m         math_ops.reduce_sum(gen_math_ops.mul(x, grad), ry), sy)\n\u001b[0m\u001b[1;32m   1386\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mHas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m   \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m   \u001b[0mtensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_set_static_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m   8383\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8384\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0;32m-> 8385\u001b[0;31m         _ctx, \"Reshape\", name, tensor, shape)\n\u001b[0m\u001b[1;32m   8386\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8387\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GykeuGs15xwg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}