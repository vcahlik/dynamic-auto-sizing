{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf_multi_layer_ssnet_inverse.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_deAUKlniFk",
        "outputId": "8e30c4fc-7afb-4391-f7bb-a57b0d50021e"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Sep 24 13:16:53 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.63.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   49C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKwUwV_NneIo"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOoXBq05neIt"
      },
      "source": [
        "dtype = 'float32'\n",
        "tf.keras.backend.set_floatx(dtype)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BrJPdkBneIv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12fb3720-1b9e-4b1d-d7d0-0b3013e75380"
      },
      "source": [
        "cifar10 = tf.keras.datasets.cifar10\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "X_train = X_train.astype(dtype) / 255.0\n",
        "y_train = y_train.astype(dtype)\n",
        "X_test = X_test.astype(dtype)  / 255.0\n",
        "y_test = y_test.astype(dtype)\n",
        "\n",
        "X_train = np.reshape(X_train, (-1, 3072))\n",
        "X_test = np.reshape(X_test, (-1, 3072))\n",
        "\n",
        "X = np.concatenate((X_train, X_test), axis=0)\n",
        "y = np.concatenate((y_train, y_test), axis=0)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 4s 0us/step\n",
            "170508288/170498071 [==============================] - 4s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5uTvu5kxF-b"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "\n",
        "X_norm = scaler.transform(X)\n",
        "X_train_norm = scaler.transform(X_train)\n",
        "X_test_norm = scaler.transform(X_test)\n",
        "\n",
        "X_train_norm = np.reshape(X_train_norm, (-1, 32, 32, 3))\n",
        "X_test_norm = np.reshape(X_test_norm, (-1, 32, 32, 3))"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTZq4KMpneIv"
      },
      "source": [
        "class Regularizer(tf.keras.regularizers.Regularizer):\n",
        "    def __init__(self, regularization_penalty, regularization_method):\n",
        "        self.regularization_penalty = regularization_penalty\n",
        "        self.regularization_method = regularization_method\n",
        "\n",
        "    def __call__(self, x):\n",
        "        if self.regularization_method == 'weighted_l1':\n",
        "            return self.weighted_l1(x)\n",
        "        elif self.regularization_method == 'group_sparsity':\n",
        "            return self.group_sparsity(x)\n",
        "        else:\n",
        "            raise NotImplementedError(f\"Unknown regularization method {self.regularization_method}\")\n",
        "    \n",
        "    def weighted_l1(self, x):\n",
        "        # I.e. for a parameter matrix of 4 input and 10 output neurons:\n",
        "        #\n",
        "        # [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
        "        #  [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
        "        #  [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
        "        #  [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]\n",
        "        #\n",
        "        # the scaling tensor, as well as the resulting weighted values, could be:\n",
        "        #\n",
        "        # [[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n",
        "        #  [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n",
        "        #  [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n",
        "        #  [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]]\n",
        "        #\n",
        "        # Therefore every additional output neuron is regularized more.\n",
        "\n",
        "        scaling_tensor = tf.cumsum(tf.constant(self.regularization_penalty, shape=x.shape, dtype=dtype), axis=-1)\n",
        "        weighted_values = scaling_tensor * tf.abs(x)\n",
        "        return tf.reduce_sum(weighted_values)\n",
        "    \n",
        "    def group_sparsity(self, x):\n",
        "        # I.e. for a parameter matrix of 3 input and 5 output neurons:\n",
        "        #\n",
        "        # [[1., 1., 1., 1., 1.],\n",
        "        #  [1., 2., 2., 1., 2.],\n",
        "        #  [2., 2., 3., 1., 3.]]\n",
        "        #\n",
        "        # The resulting vector of group norms is [2., 2., 3., 1., 3.], therefore for\n",
        "        # every output neuron, its incoming connections form a group.\n",
        "\n",
        "        # TODO implement for Conv2D layers\n",
        "        group_norms = tf.norm(x, ord=2, axis=0)\n",
        "        # assert group_norms.shape[0] == x.shape[1]\n",
        "        return self.regularization_penalty * tf.reduce_sum(group_norms)\n",
        "\n",
        "    def get_config(self):\n",
        "        return {'regularization_penalty': float(self.regularization_penalty)}\n",
        "\n",
        "\n",
        "class ModelReference:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "\n",
        "\n",
        "class CustomLayer(tf.keras.Model):\n",
        "    def __init__(self, input_shape):\n",
        "        super().__init__()\n",
        "\n",
        "        self.inpt_shp = input_shape\n",
        "    \n",
        "    def configure(self, model):\n",
        "        self.mr = ModelReference(model)\n",
        "    \n",
        "    def get_input_shape(self):\n",
        "        if self.inpt_shp is not None:\n",
        "            return self.inpt_shp\n",
        "        \n",
        "        return self.mr.model.get_layer_input_shape(self)\n",
        "    \n",
        "    def get_output_shape(self):\n",
        "        return self.mr.model.get_layer_output_shape(self)\n",
        "\n",
        "\n",
        "class Dense(CustomLayer):\n",
        "    def __init__(self, units, activation, regularization_penalty=0.01, \n",
        "                 regularization_method='weighted_l1', kernel_initializer='glorot_uniform', \n",
        "                 bias_initializer='zeros', input_shape=None, fixed_size=False):\n",
        "        super().__init__(input_shape)\n",
        "\n",
        "        self.units = units\n",
        "        self.activation = activation\n",
        "        self.regularization_penalty = regularization_penalty\n",
        "        self.regularization_method = regularization_method\n",
        "        self.kernel_initializer = kernel_initializer\n",
        "        self.bias_initializer = bias_initializer\n",
        "        self.fixed_size = fixed_size\n",
        "        \n",
        "        self.A = tf.keras.activations.get(activation)\n",
        "        self.W_init = tf.keras.initializers.get(kernel_initializer)\n",
        "        self.b_init = tf.keras.initializers.get(bias_initializer)\n",
        "        self.regularizer = Regularizer(self.regularization_penalty, self.regularization_method)\n",
        "    \n",
        "    def configure(self, model):\n",
        "        super().configure(model)\n",
        "\n",
        "        input_units = self.get_input_units_count()\n",
        "\n",
        "        self.W = tf.Variable(\n",
        "            name='W',\n",
        "            initial_value=self.W_init(shape=(input_units, self.units), dtype=dtype),\n",
        "            trainable=True)\n",
        "        \n",
        "        self.b = tf.Variable(\n",
        "            name='b',\n",
        "            initial_value=self.b_init(shape=(self.units,), dtype=dtype),\n",
        "            trainable=True)\n",
        "        \n",
        "        if self.regularization_method is not None:\n",
        "            self.add_loss(lambda: self.regularizer(tf.concat([self.W, tf.reshape(self.b, (1, -1))], axis=0)))\n",
        "    \n",
        "    def call(self, inputs):\n",
        "        return self.A(tf.matmul(inputs, self.W) + self.b)\n",
        "    \n",
        "    def copy_without_regularization(self):\n",
        "        # TODO fix\n",
        "        copy = Dense(\n",
        "            self.input_units, \n",
        "            self.units, \n",
        "            self.activation, \n",
        "            regularization_penalty=self.regularization_penalty, \n",
        "            regularization_method=None, \n",
        "            kernel_initializer=self.kernel_initializer, \n",
        "            bias_initializer=self.bias_initializer\n",
        "        )\n",
        "        copy.W = self.W\n",
        "        copy.b = self.b\n",
        "        return copy\n",
        "    \n",
        "    def get_input_units_count(self):\n",
        "        input_shape = self.get_input_shape()\n",
        "        if len(input_shape) != 1:\n",
        "            raise Exception(f\"Invalid input shape {input_shape}.\")\n",
        "        return input_shape[0]\n",
        "    \n",
        "    def get_size(self):\n",
        "        return self.get_input_units_count(), self.W.shape[1]\n",
        "    \n",
        "    def prune(self, threshold, active_input_units_indices):\n",
        "        # Remove connections from pruned units in previous layer\n",
        "        new_W = tf.gather(self.W.value(), active_input_units_indices, axis=0)\n",
        "\n",
        "        if self.fixed_size:\n",
        "            active_output_neurons_indices = list(range(new_W.shape[1]))\n",
        "        else:\n",
        "            # Prune units in this layer\n",
        "            weights_with_biases = tf.concat([new_W, tf.reshape(self.b.value(), (1, -1))], axis=0)\n",
        "            neurons_are_active = tf.math.reduce_max(tf.abs(weights_with_biases), axis=0) >= threshold\n",
        "            active_output_neurons_indices = tf.reshape(tf.where(neurons_are_active), (-1,))\n",
        "            \n",
        "            new_W = tf.gather(new_W, active_output_neurons_indices, axis=1)\n",
        "            new_b = tf.gather(self.b.value(), active_output_neurons_indices, axis=0)\n",
        "\n",
        "            self.b = tf.Variable(name='b', initial_value=new_b, trainable=True)\n",
        "\n",
        "        self.W = tf.Variable(name='W', initial_value=new_W, trainable=True)\n",
        "\n",
        "        return active_output_neurons_indices\n",
        "    \n",
        "    def grow(self, n_new_input_units, percentage, min_new_units, scaling_factor):\n",
        "        if n_new_input_units > 0:\n",
        "            # Add connections to grown units in previous layer\n",
        "            W_growth = self.W_init(shape=(self.W.shape[0] + n_new_input_units, self.W.shape[1]), dtype=dtype)[-n_new_input_units:, :] * scaling_factor  # TODO is it better to be multiplying here by scaling_factor? It does help with not increasing the max weights of existing neurons when new neurons are added.\n",
        "            new_W = tf.concat([self.W.value(), W_growth], axis=0)\n",
        "        else:\n",
        "            new_W = self.W.value()\n",
        "\n",
        "        if self.fixed_size:\n",
        "            n_new_output_units = 0\n",
        "        else:\n",
        "            # Grow new units in this layer\n",
        "            n_new_output_units = max(min_new_units, int(new_W.shape[1] * percentage))\n",
        "            W_growth = self.W_init(shape=(new_W.shape[0], new_W.shape[1] + n_new_output_units), dtype=dtype)[:, -n_new_output_units:] * scaling_factor\n",
        "            b_growth = self.b_init(shape=(n_new_output_units,), dtype=dtype)  # TODO for all possible bias initializers to work properly, the whole bias vector should be initialized at once\n",
        "            new_W = tf.concat([new_W, W_growth], axis=1)\n",
        "            new_b = tf.concat([self.b.value(), b_growth], axis=0)\n",
        "\n",
        "            self.b = tf.Variable(name='b', initial_value=new_b, trainable=True)\n",
        "\n",
        "        self.W = tf.Variable(name='W', initial_value=new_W, trainable=True)\n",
        "\n",
        "        return n_new_output_units\n",
        "    \n",
        "    def get_param_string():\n",
        "        param_string = \"\"\n",
        "        weights_with_bias = tf.concat([self.W, tf.reshape(self.b, (1, -1))], axis=0)\n",
        "        max_parameters = tf.math.reduce_max(tf.abs(weights_with_bias), axis=0).numpy()\n",
        "        magnitudes = np.floor(np.log10(max_parameters))\n",
        "        for m in magnitudes:\n",
        "            if m > 0:\n",
        "                m = 0\n",
        "            param_string += str(int(-m))\n",
        "        return param_string\n",
        "\n",
        "\n",
        "class Conv2D(CustomLayer):\n",
        "    def __init__(self, filters, filter_size, activation, strides=(1, 1), \n",
        "                 padding='SAME', regularization_penalty=0.01, \n",
        "                 regularization_method='weighted_l1', kernel_initializer='glorot_uniform',\n",
        "                 bias_initializer='zeros', input_shape=None, fixed_size=False):\n",
        "        super().__init__(input_shape)\n",
        "    \n",
        "        self.filters = filters\n",
        "        self.filter_size = filter_size\n",
        "        self.activation = activation\n",
        "        self.strides = strides\n",
        "        self.padding = padding\n",
        "        self.regularization_penalty = regularization_penalty\n",
        "        self.regularization_method = regularization_method\n",
        "        self.kernel_initializer = kernel_initializer\n",
        "        self.bias_initializer = bias_initializer\n",
        "        self.fixed_size = fixed_size\n",
        "        \n",
        "        self.A = tf.keras.activations.get(activation)\n",
        "        self.F_init = tf.keras.initializers.get(kernel_initializer)\n",
        "        self.b_init = tf.keras.initializers.get(bias_initializer)\n",
        "        self.regularizer = Regularizer(self.regularization_penalty, self.regularization_method)\n",
        "    \n",
        "    def configure(self, model):\n",
        "        super().configure(model)\n",
        "\n",
        "        input_filters = self.get_input_filters_count()\n",
        "\n",
        "        self.F = tf.Variable(\n",
        "            name='F',\n",
        "            initial_value=self.F_init(\n",
        "                shape=(self.filter_size[0], self.filter_size[1], input_filters, self.filters), dtype=dtype\n",
        "            ),\n",
        "            trainable=True)\n",
        "        \n",
        "        self.b = tf.Variable(\n",
        "            name='b',\n",
        "            initial_value=self.b_init(shape=(self.filters,), dtype=dtype),\n",
        "            trainable=True)\n",
        "\n",
        "        if self.regularization_method is not None:\n",
        "            self.add_loss(lambda: self.regularizer(tf.concat([tf.reshape(self.F, (-1, self.F.shape[-1])), tf.reshape(self.b, (1, -1))], axis=0)))\n",
        "    \n",
        "    def call(self, inputs):\n",
        "        y = tf.nn.conv2d(inputs, self.F, strides=self.strides, padding=self.padding)\n",
        "        y = tf.nn.bias_add(y, self.b)\n",
        "        y = self.A(y)\n",
        "        return y\n",
        "    \n",
        "    def copy_without_regularization(self):\n",
        "        # TODO fix\n",
        "        copy = Conv2D(\n",
        "            self.input_filters,\n",
        "            self.filters,\n",
        "            self.filter_size,\n",
        "            self.activation, \n",
        "            strides=self.strides, \n",
        "            padding=self.padding, \n",
        "            kernel_initializer=self.kernel_initializer, \n",
        "            bias_initializer=self.bias_initializer \n",
        "        )\n",
        "        copy.F = self.F\n",
        "        copy.b = self.b\n",
        "        return copy\n",
        "    \n",
        "    def get_input_filters_count(self):\n",
        "        input_shape = self.get_input_shape()\n",
        "        return input_shape[-1]\n",
        "    \n",
        "    def get_size(self):\n",
        "        return self.get_input_filters_count(), self.F.shape[-1]\n",
        "    \n",
        "    def prune(self, threshold, active_input_units_indices):\n",
        "        # Remove connections from pruned units in previous layer\n",
        "        new_F = tf.gather(self.F.value(), active_input_units_indices, axis=-2)\n",
        "\n",
        "        if self.fixed_size:\n",
        "            active_output_filters_indices = list(range(new_F.shape[-1]))\n",
        "        else:\n",
        "            # Prune units in this layer\n",
        "            F_reduced_max = tf.reshape(tf.math.reduce_max(tf.abs(new_F), axis=(0, 1, 2)), (1, -1))\n",
        "            F_reduced_max_with_biases = tf.concat([F_reduced_max, tf.reshape(self.b.value(), (1, -1))], axis=0)\n",
        "            filters_are_active = tf.math.reduce_max(tf.abs(F_reduced_max_with_biases), axis=0) >= threshold\n",
        "            active_output_filters_indices = tf.reshape(tf.where(filters_are_active), (-1,))\n",
        "            \n",
        "            new_F = tf.gather(new_F, active_output_filters_indices, axis=-1)\n",
        "            new_b = tf.gather(self.b.value(), active_output_filters_indices, axis=0)\n",
        "\n",
        "            self.b = tf.Variable(name='b', initial_value=new_b, trainable=True)\n",
        "\n",
        "        self.F = tf.Variable(name='F', initial_value=new_F, trainable=True)\n",
        "\n",
        "        return active_output_filters_indices\n",
        "\n",
        "    def grow(self, n_new_input_units, percentage, min_new_units, scaling_factor):\n",
        "        if n_new_input_units > 0:\n",
        "            # Add connections to grown units in previous layer\n",
        "            F_growth = self.F_init(shape=(self.F.shape[0], self.F.shape[1], self.F.shape[2] + n_new_input_units, self.F.shape[3]), dtype=dtype)[:, :, -n_new_input_units:, :] * scaling_factor  # TODO is it better to be multiplying here by scaling_factor? It does help with not increasing the max weights of existing neurons when new neurons are added.\n",
        "            new_F = tf.concat([self.F.value(), F_growth], axis=-2)\n",
        "        else:\n",
        "            new_F = self.F.value()\n",
        "\n",
        "        if self.fixed_size:\n",
        "            n_new_output_units = 0\n",
        "        else:\n",
        "            # Grow new units in this layer\n",
        "            n_new_output_units = max(min_new_units, int(new_F.shape[-1] * percentage))\n",
        "            F_growth = self.F_init(shape=(new_F.shape[0], new_F.shape[1], new_F.shape[2], new_F.shape[3] + n_new_output_units), dtype=dtype)[:, :, :, -n_new_output_units:] * scaling_factor\n",
        "            b_growth = self.b_init(shape=(n_new_output_units,), dtype=dtype)  # TODO for all possible bias initializers to work properly, the whole bias vector should be initialized at once\n",
        "            new_F = tf.concat([new_F, F_growth], axis=-1)\n",
        "            new_b = tf.concat([self.b.value(), b_growth], axis=0)\n",
        "\n",
        "            self.b = tf.Variable(name='b', initial_value=new_b, trainable=True)\n",
        "\n",
        "        self.F = tf.Variable(name='F', initial_value=new_F, trainable=True)\n",
        "\n",
        "        return n_new_output_units\n",
        "\n",
        "    def get_param_string():\n",
        "        param_string = \"\"\n",
        "        # TODO\n",
        "        return param_string\n",
        "\n",
        "\n",
        "class Flatten(tf.keras.Model):\n",
        "    def call(self, inputs):\n",
        "        return tf.reshape(tf.transpose(inputs, perm=[0, 3, 1, 2]), (inputs.shape[0], -1))\n",
        "\n",
        "\n",
        "class Sequential(tf.keras.Model):\n",
        "    def __init__(self, layers, activation=None):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.lrs = list()\n",
        "        for layer in layers:\n",
        "            self.lrs.append(layer)\n",
        "            if isinstance(layer, CustomLayer):\n",
        "                layer.configure(self)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = inputs\n",
        "        for layer in self.lrs:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "    \n",
        "    def get_layer_input_shape(self, target_layer):\n",
        "        input = np.random.normal(size=(1,) + self.lrs[0].inpt_shp)\n",
        "        for layer in self.lrs:\n",
        "            if layer is target_layer:\n",
        "                return tuple(input.shape[1:])\n",
        "            input = layer(input)\n",
        "        raise Exception(\"Layer not found in the model.\")\n",
        "\n",
        "    def get_layer_output_shape(self, target_layer):\n",
        "        input = np.random.normal(size=(1,) + self.lrs[0].inpt_shp)\n",
        "        for layer in self.lrs:\n",
        "            output = layer(input)\n",
        "            if layer is target_layer:\n",
        "                return tuple(output.shape[1:])\n",
        "            input = output\n",
        "        raise Exception(\"Layer not found in the model.\")\n",
        "    \n",
        "    def get_layer_sizes(self):\n",
        "        \"\"\"\n",
        "        Returns the sizes of all layers in the model, including the input and output layer.\n",
        "        \"\"\"\n",
        "        layer_sizes = list()\n",
        "        first_layer = True\n",
        "        for l in range(len(self.lrs)):\n",
        "            layer = self.lrs[l]\n",
        "            if isinstance(layer, CustomLayer):\n",
        "                layer_size = layer.get_size()\n",
        "                if first_layer:\n",
        "                    layer_sizes.append(layer_size[0])\n",
        "                    first_layer = False\n",
        "                layer_sizes.append(layer_size[1])\n",
        "        return layer_sizes\n",
        "    \n",
        "    def get_hidden_layer_sizes(self):\n",
        "        return self.get_layer_sizes()[1:-1]\n",
        "    \n",
        "    def remove_regularization(self):\n",
        "        for l in range(len(self.lrs)):\n",
        "            layer = self.lrs[l]\n",
        "            if isinstance(layer, CustomLayer):\n",
        "                self.lrs[l] = layer.copy_without_regularization()\n",
        "    \n",
        "    def get_regularization_penalty(self):\n",
        "        return self.lrs[-2].regularizer.regularization_penalty\n",
        "    \n",
        "    def set_regularization_penalty(self, regularization_penalty):\n",
        "        for layer in self.lrs:\n",
        "            if isinstance(layer, CustomLayer) and not layer.fixed_size:\n",
        "                layer.regularizer.regularization_penalty = regularization_penalty\n",
        "    \n",
        "    def prune(self, threshold=0.001):\n",
        "        # for l in range(len(self.lrs) - 1):\n",
        "        #     layer1 = self.lrs[l]\n",
        "        #     layer2 = self.lrs[l + 1]\n",
        "            \n",
        "        #     W1 = layer1.W.value()\n",
        "        #     b1 = layer1.b.value()\n",
        "        #     W2 = layer2.W.value()\n",
        "\n",
        "        #     weights_with_biases = tf.concat([W1, tf.reshape(b1, (1, -1))], axis=0)\n",
        "        #     neurons_are_active = tf.math.reduce_max(tf.abs(weights_with_biases), axis=0) >= threshold\n",
        "        #     active_neurons_indices = tf.reshape(tf.where(neurons_are_active), (-1,))\n",
        "            \n",
        "        #     new_W1 = tf.gather(W1, active_neurons_indices, axis=1)\n",
        "        #     new_b1 = tf.gather(b1, active_neurons_indices, axis=0)\n",
        "        #     new_W2 = tf.gather(W2, active_neurons_indices, axis=0)\n",
        "\n",
        "        #     layer1.W = tf.Variable(name='W', initial_value=new_W1, trainable=True)\n",
        "        #     layer1.b = tf.Variable(name='b', initial_value=new_b1, trainable=True)\n",
        "        #     layer2.W = tf.Variable(name='W', initial_value=new_W2, trainable=True)\n",
        "        input_shape = self.lrs[0].get_input_shape()\n",
        "        n_input_units = input_shape[-1]\n",
        "        active_units_indices = list(range(n_input_units))\n",
        "\n",
        "        last_custom_layer = None\n",
        "        for layer in self.lrs:\n",
        "            if isinstance(layer, CustomLayer):\n",
        "                if last_custom_layer is not None and type(last_custom_layer) != type(layer):\n",
        "                    if type(last_custom_layer) == Conv2D and type(layer) == Dense:\n",
        "                        convolutional_shape = last_custom_layer.get_output_shape()\n",
        "                        active_units_indices = self.convert_channel_indices_to_flattened_indices(active_units_indices, convolutional_shape)\n",
        "                    else:\n",
        "                        raise Exception(\"Incorrect order of custom layer types.\")\n",
        "                active_units_indices = layer.prune(threshold, active_units_indices)\n",
        "                last_custom_layer = layer\n",
        "    \n",
        "    def grow(self, percentage, min_new_neurons=5, scaling_factor=0.001):   \n",
        "        # for l in range(len(self.lrs) - 1):\n",
        "        #     layer1 = self.lrs[l]\n",
        "        #     layer2 = self.lrs[l + 1]\n",
        "       \n",
        "        #     W1 = layer1.W.value()\n",
        "        #     b1 = layer1.b.value()\n",
        "        #     W2 = layer2.W.value()\n",
        "\n",
        "        #     n_new_neurons = max(min_new_neurons, int(W1.shape[1] * percentage))\n",
        "\n",
        "        #     W1_growth = layer1.W_init(shape=(W1.shape[0], W1.shape[1] + n_new_neurons), dtype=dtype)[:, -n_new_neurons:] * scaling_factor\n",
        "        #     b1_growth = layer1.b_init(shape=(n_new_neurons,), dtype=dtype)\n",
        "        #     W2_growth = layer2.W_init(shape=(W2.shape[0] + n_new_neurons, W2.shape[1]), dtype=dtype)[-n_new_neurons:, :] * scaling_factor  # TODO is it better to be multiplying here by scaling_factor? It does help with not increasing the max weights of existing neurons when new neurons are added.\n",
        "\n",
        "        #     new_W1 = tf.concat([W1, W1_growth], axis=1)\n",
        "        #     new_b1 = tf.concat([b1, b1_growth], axis=0)\n",
        "        #     new_W2 = tf.concat([W2, W2_growth], axis=0)\n",
        "\n",
        "        #     layer1.W = tf.Variable(name='W1', initial_value=new_W1, trainable=True)\n",
        "        #     layer1.b = tf.Variable(name='b1', initial_value=new_b1, trainable=True)\n",
        "        #     layer2.W = tf.Variable(name='W2', initial_value=new_W2, trainable=True)\n",
        "        n_new_units = 0\n",
        "\n",
        "        last_custom_layer = None\n",
        "        for layer in self.lrs:\n",
        "            if isinstance(layer, CustomLayer):\n",
        "                if last_custom_layer is not None and type(last_custom_layer) != type(layer):\n",
        "                    if type(last_custom_layer) == Conv2D and type(layer) == Dense:\n",
        "                        convolutional_shape = last_custom_layer.get_output_shape()\n",
        "                        n_new_units = n_new_units * convolutional_shape[0] * convolutional_shape[1]\n",
        "                    else:\n",
        "                        raise Exception(\"Incorrect order of custom layer types.\")\n",
        "                n_new_units = layer.grow(n_new_units, percentage, min_new_units=min_new_neurons, scaling_factor=scaling_factor)\n",
        "                last_custom_layer = layer\n",
        "    \n",
        "    @staticmethod\n",
        "    def convert_channel_indices_to_flattened_indices(channel_indices, convolutional_shape):\n",
        "        dense_indices = list()\n",
        "        units_per_channel = convolutional_shape[0] * convolutional_shape[1]\n",
        "        for channel_index in channel_indices:\n",
        "            for iter in range(units_per_channel):\n",
        "                dense_indices.append(channel_index * units_per_channel + iter)\n",
        "        return dense_indices\n",
        "    \n",
        "    def print_neurons(self):\n",
        "        for layer in self.lrs[:-1]:\n",
        "            print(layer.get_param_string())\n",
        "    \n",
        "    def evaluate(self, x, y, summed_training_loss, summed_training_accuracy, val_dataset):\n",
        "        # Calculate training loss and accuracy\n",
        "        if summed_training_loss is not None:\n",
        "            loss = summed_training_loss / x.shape[0]\n",
        "        else:\n",
        "            loss = None\n",
        "        \n",
        "        if summed_training_accuracy is not None:\n",
        "            accuracy = summed_training_accuracy / x.shape[0]\n",
        "        else:\n",
        "            accuracy = None\n",
        "        \n",
        "        # Calculate val loss and accuracy\n",
        "        summed_val_loss = 0\n",
        "        summed_val_accuracy = 0\n",
        "        n_val_instances = 0\n",
        "        \n",
        "        for step, (x_batch, y_batch) in enumerate(val_dataset):\n",
        "            y_pred = self(x_batch)\n",
        "            summed_val_loss += tf.reduce_sum(tf.keras.losses.sparse_categorical_crossentropy(y_batch, y_pred))\n",
        "            summed_val_accuracy += float(tf.reduce_sum(tf.keras.metrics.sparse_categorical_accuracy(y_batch, y_pred)))\n",
        "            n_val_instances += x_batch.shape[0]\n",
        "        \n",
        "        val_loss = summed_val_loss / n_val_instances\n",
        "        val_accuracy = summed_val_accuracy / n_val_instances\n",
        "\n",
        "        return loss, accuracy, val_loss, val_accuracy\n",
        "    \n",
        "    def print_epoch_statistics(self, x, y, summed_training_loss, summed_training_accuracy, val_dataset, print_neurons):\n",
        "        loss, accuracy, val_loss, val_accuracy = self.evaluate(x, y, summed_training_loss, summed_training_accuracy, val_dataset)\n",
        "        print(f\"loss: {loss} - accuracy: {accuracy} - val_loss: {val_loss} - val_accuracy: {val_accuracy} - penalty: {model.get_regularization_penalty()}\")\n",
        "        hidden_layer_sizes = self.get_hidden_layer_sizes()\n",
        "        print(f\"hidden layer sizes: {hidden_layer_sizes}, total units: {sum(hidden_layer_sizes)}\")\n",
        "        if print_neurons:\n",
        "            self.print_neurons()\n",
        "    \n",
        "    def update_history(self, x, y, summed_loss, summed_accuracy, val_dataset, history):\n",
        "        loss, accuracy, val_loss, val_accuracy = self.evaluate(x, y, summed_loss, summed_accuracy, val_dataset)\n",
        "        history['loss'].append(loss)\n",
        "        history['accuracy'].append(accuracy)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_accuracy'].append(val_accuracy)\n",
        "\n",
        "    def fit(self, x, y, optimizer, epochs, self_scaling_epochs, batch_size, min_new_neurons, validation_data, pruning_threshold=0.001, \n",
        "            regularization_penalty_multiplier=1., stall_coefficient=1, growth_percentage=0.2, mini_epochs_per_epoch=1, verbose=True, print_neurons=False):\n",
        "        train_dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
        "        train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "\n",
        "        val_dataset = tf.data.Dataset.from_tensor_slices(validation_data).batch(batch_size)\n",
        "\n",
        "        history = {\n",
        "            'loss': list(),\n",
        "            'accuracy': list(),\n",
        "            'val_loss': list(),\n",
        "            'val_accuracy': list(),\n",
        "        }\n",
        "\n",
        "        best_val_loss = np.inf\n",
        "        training_stalled = False\n",
        "        for epoch in range(epochs):\n",
        "            summed_loss = 0\n",
        "            summed_accuracy = 0\n",
        "\n",
        "            if verbose:\n",
        "                print(\"##########################################################\")\n",
        "                print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "\n",
        "            if epoch < self_scaling_epochs:\n",
        "                if verbose:\n",
        "                    print(\"Before growing:\")\n",
        "                    self.print_epoch_statistics(x, y, None, None, val_dataset, print_neurons)\n",
        "\n",
        "                loss, accuracy, val_loss, val_accuracy = self.evaluate(x, y, summed_loss, summed_accuracy, val_dataset)\n",
        "                if regularization_penalty_multiplier != 1. and val_loss >= best_val_loss * stall_coefficient:\n",
        "                    if not training_stalled:\n",
        "                        penalty = self.get_regularization_penalty() * regularization_penalty_multiplier\n",
        "                        print(\"Changing penalty...\")\n",
        "                        # TODO this must be modified, penalty can differ for each layer\n",
        "                        self.set_regularization_penalty(penalty)\n",
        "                        training_stalled = True\n",
        "                else:\n",
        "                    best_val_loss = val_loss\n",
        "                    training_stalled = False\n",
        "\n",
        "                self.grow(percentage=growth_percentage, min_new_neurons=min_new_neurons, scaling_factor=pruning_threshold)\n",
        "                if verbose:\n",
        "                    print(\"After growing:\")\n",
        "                    self.print_epoch_statistics(x, y, None, None, val_dataset, print_neurons)\n",
        "            \n",
        "            if epoch == self_scaling_epochs and self_scaling_epochs > 0:\n",
        "                self.remove_regularization()\n",
        "\n",
        "            for mini_epoch in range(mini_epochs_per_epoch):\n",
        "                for step, (x_batch, y_batch) in enumerate(train_dataset):\n",
        "                    with tf.GradientTape() as tape:\n",
        "                        y_pred = self(x_batch, training=True)\n",
        "                        raw_loss = tf.keras.losses.sparse_categorical_crossentropy(y_batch, y_pred)\n",
        "                        loss_value = tf.reduce_mean(raw_loss)\n",
        "                        loss_value += sum(self.losses)  # Add losses registered by model.add_loss\n",
        "\n",
        "                        summed_loss += tf.reduce_sum(raw_loss)\n",
        "                        summed_accuracy += float(tf.reduce_sum(tf.keras.metrics.sparse_categorical_accuracy(y_batch, y_pred)))\n",
        "\n",
        "                    grads = tape.gradient(loss_value, self.trainable_variables)\n",
        "                    optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
        "            \n",
        "            if epoch < self_scaling_epochs:\n",
        "                if verbose:\n",
        "                    print(\"Before pruning:\")\n",
        "                    self.print_epoch_statistics(x, y, summed_loss, summed_accuracy, val_dataset, print_neurons)\n",
        "                self.prune(threshold=pruning_threshold)\n",
        "                if verbose:\n",
        "                    print(\"After pruning:\")\n",
        "                    self.print_epoch_statistics(x, y, None, None, val_dataset, print_neurons)\n",
        "            else:\n",
        "                if verbose:\n",
        "                    self.print_epoch_statistics(x, y, summed_loss, summed_accuracy, val_dataset, print_neurons)\n",
        "            \n",
        "            self.update_history(x, y, summed_loss, summed_accuracy, val_dataset, history)\n",
        "\n",
        "        return history"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1MrQXUTFwOe"
      },
      "source": [
        "# Convolutions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gdiJWU-PmHa"
      },
      "source": [
        "F = tf.cumsum(tf.cumsum(tf.ones(shape=(2, 3, 3, 5), dtype=dtype), axis=-1), axis=0)\n",
        "b = tf.constant(0.1, shape=(5,), dtype=dtype)\n",
        "new_F = F"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOQy8PBvhB4R",
        "outputId": "e662d992-bb5f-4101-e3d1-c8f324c68ede"
      },
      "source": [
        "F"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 3, 3, 5), dtype=float32, numpy=\n",
              "array([[[[ 1.,  2.,  3.,  4.,  5.],\n",
              "         [ 1.,  2.,  3.,  4.,  5.],\n",
              "         [ 1.,  2.,  3.,  4.,  5.]],\n",
              "\n",
              "        [[ 1.,  2.,  3.,  4.,  5.],\n",
              "         [ 1.,  2.,  3.,  4.,  5.],\n",
              "         [ 1.,  2.,  3.,  4.,  5.]],\n",
              "\n",
              "        [[ 1.,  2.,  3.,  4.,  5.],\n",
              "         [ 1.,  2.,  3.,  4.,  5.],\n",
              "         [ 1.,  2.,  3.,  4.,  5.]]],\n",
              "\n",
              "\n",
              "       [[[ 2.,  4.,  6.,  8., 10.],\n",
              "         [ 2.,  4.,  6.,  8., 10.],\n",
              "         [ 2.,  4.,  6.,  8., 10.]],\n",
              "\n",
              "        [[ 2.,  4.,  6.,  8., 10.],\n",
              "         [ 2.,  4.,  6.,  8., 10.],\n",
              "         [ 2.,  4.,  6.,  8., 10.]],\n",
              "\n",
              "        [[ 2.,  4.,  6.,  8., 10.],\n",
              "         [ 2.,  4.,  6.,  8., 10.],\n",
              "         [ 2.,  4.,  6.,  8., 10.]]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xo4Un7wgfft2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee42006a-490c-49fc-ec0f-bbefd63c0b9b"
      },
      "source": [
        "tf.reshape(F, (-1, 5))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(18, 5), dtype=float32, numpy=\n",
              "array([[1., 2., 3., 4., 5.],\n",
              "       [1., 2., 3., 4., 5.],\n",
              "       [1., 2., 3., 4., 5.],\n",
              "       [1., 2., 3., 4., 5.],\n",
              "       [1., 2., 3., 4., 5.],\n",
              "       [1., 2., 3., 4., 5.],\n",
              "       [1., 2., 3., 4., 5.],\n",
              "       [1., 2., 3., 4., 5.],\n",
              "       [1., 2., 3., 4., 5.],\n",
              "       [1., 2., 3., 4., 5.],\n",
              "       [1., 2., 3., 4., 5.],\n",
              "       [1., 2., 3., 4., 5.],\n",
              "       [1., 2., 3., 4., 5.],\n",
              "       [1., 2., 3., 4., 5.],\n",
              "       [1., 2., 3., 4., 5.],\n",
              "       [1., 2., 3., 4., 5.],\n",
              "       [1., 2., 3., 4., 5.],\n",
              "       [1., 2., 3., 4., 5.]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqOPki7AM2IB",
        "outputId": "d0d2f590-6213-4b7f-82e5-709a503f700f"
      },
      "source": [
        "tf.cumsum(tf.constant(0.1, shape=F.shape, dtype=dtype), axis=-1)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 3, 3, 5), dtype=float32, numpy=\n",
              "array([[[[0.1, 0.2, 0.3, 0.4, 0.5],\n",
              "         [0.1, 0.2, 0.3, 0.4, 0.5],\n",
              "         [0.1, 0.2, 0.3, 0.4, 0.5]],\n",
              "\n",
              "        [[0.1, 0.2, 0.3, 0.4, 0.5],\n",
              "         [0.1, 0.2, 0.3, 0.4, 0.5],\n",
              "         [0.1, 0.2, 0.3, 0.4, 0.5]],\n",
              "\n",
              "        [[0.1, 0.2, 0.3, 0.4, 0.5],\n",
              "         [0.1, 0.2, 0.3, 0.4, 0.5],\n",
              "         [0.1, 0.2, 0.3, 0.4, 0.5]]],\n",
              "\n",
              "\n",
              "       [[[0.1, 0.2, 0.3, 0.4, 0.5],\n",
              "         [0.1, 0.2, 0.3, 0.4, 0.5],\n",
              "         [0.1, 0.2, 0.3, 0.4, 0.5]],\n",
              "\n",
              "        [[0.1, 0.2, 0.3, 0.4, 0.5],\n",
              "         [0.1, 0.2, 0.3, 0.4, 0.5],\n",
              "         [0.1, 0.2, 0.3, 0.4, 0.5]],\n",
              "\n",
              "        [[0.1, 0.2, 0.3, 0.4, 0.5],\n",
              "         [0.1, 0.2, 0.3, 0.4, 0.5],\n",
              "         [0.1, 0.2, 0.3, 0.4, 0.5]]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EclKzR2Eg9uo",
        "outputId": "110a26c8-8de6-4633-868a-66f64b35b94b"
      },
      "source": [
        "flattened = tf.reshape(tf.transpose(F, perm=[0, 3, 1, 2]), (F.shape[0], -1))\n",
        "flattened"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 45), dtype=float32, numpy=\n",
              "array([[ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  2.,  2.,  2.,  2.,\n",
              "         2.,  2.,  2.,  2.,  2.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,\n",
              "         3.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  5.,  5.,  5.,\n",
              "         5.,  5.,  5.,  5.,  5.,  5.],\n",
              "       [ 2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  4.,  4.,  4.,  4.,\n",
              "         4.,  4.,  4.,  4.,  4.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,\n",
              "         6.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8., 10., 10., 10.,\n",
              "        10., 10., 10., 10., 10., 10.]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kq9Lm8ukzs-",
        "outputId": "77b656bf-60d4-4305-d0c3-3252097fb4eb"
      },
      "source": [
        "input = [1, 2, 4]\n",
        "output = list()\n",
        "for inp in input:\n",
        "    for iter in range(3*3):\n",
        "        output.append(inp * 3*3 + iter)\n",
        "output"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[9,\n",
              " 10,\n",
              " 11,\n",
              " 12,\n",
              " 13,\n",
              " 14,\n",
              " 15,\n",
              " 16,\n",
              " 17,\n",
              " 18,\n",
              " 19,\n",
              " 20,\n",
              " 21,\n",
              " 22,\n",
              " 23,\n",
              " 24,\n",
              " 25,\n",
              " 26,\n",
              " 36,\n",
              " 37,\n",
              " 38,\n",
              " 39,\n",
              " 40,\n",
              " 41,\n",
              " 42,\n",
              " 43,\n",
              " 44]"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfznA0IvmHjm",
        "outputId": "afe719da-3e83-4495-9779-5c551f0dbf5b"
      },
      "source": [
        "tf.gather(flattened, output, axis=-1)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 27), dtype=float32, numpy=\n",
              "array([[ 2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  3.,  3.,  3.,  3.,\n",
              "         3.,  3.,  3.,  3.,  3.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
              "         5.],\n",
              "       [ 4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  6.,  6.,  6.,  6.,\n",
              "         6.,  6.,  6.,  6.,  6., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
              "        10.]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mt42xdvpgnVW",
        "outputId": "2cddab19-5d94-46aa-e3b0-dd611e0d2fbc"
      },
      "source": [
        "tf.reshape(F, (-1,))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(90,), dtype=float32, numpy=\n",
              "array([1., 2., 3., 4., 5., 1., 2., 3., 4., 5., 1., 2., 3., 4., 5., 1., 2.,\n",
              "       3., 4., 5., 1., 2., 3., 4., 5., 1., 2., 3., 4., 5., 1., 2., 3., 4.,\n",
              "       5., 1., 2., 3., 4., 5., 1., 2., 3., 4., 5., 1., 2., 3., 4., 5., 1.,\n",
              "       2., 3., 4., 5., 1., 2., 3., 4., 5., 1., 2., 3., 4., 5., 1., 2., 3.,\n",
              "       4., 5., 1., 2., 3., 4., 5., 1., 2., 3., 4., 5., 1., 2., 3., 4., 5.,\n",
              "       1., 2., 3., 4., 5.], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQyV7ZghcD4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fc65fb4-9c7b-4c83-a1b8-f9fe6b6ec315"
      },
      "source": [
        "flatten = tf.keras.layers.Flatten(data_format='channels_last')\n",
        "flattened = flatten(F)\n",
        "flattened"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 45), dtype=float32, numpy=\n",
              "array([[1., 2., 3., 4., 5., 1., 2., 3., 4., 5., 1., 2., 3., 4., 5., 1.,\n",
              "        2., 3., 4., 5., 1., 2., 3., 4., 5., 1., 2., 3., 4., 5., 1., 2.,\n",
              "        3., 4., 5., 1., 2., 3., 4., 5., 1., 2., 3., 4., 5.],\n",
              "       [1., 2., 3., 4., 5., 1., 2., 3., 4., 5., 1., 2., 3., 4., 5., 1.,\n",
              "        2., 3., 4., 5., 1., 2., 3., 4., 5., 1., 2., 3., 4., 5., 1., 2.,\n",
              "        3., 4., 5., 1., 2., 3., 4., 5., 1., 2., 3., 4., 5.]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9fiKg6NcCUT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f343016-6120-4085-d662-0ee1809d9c67"
      },
      "source": [
        "flattened.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([2, 45])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKR4-TKF_IaF",
        "outputId": "c7413317-c408-41fb-e670-a5578b2b71af"
      },
      "source": [
        "3*3*5"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "45"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbSSyww__QGm",
        "outputId": "ced00c63-a9d6-474e-9726-511d85254d81"
      },
      "source": [
        "input = [1, 2, 4]\n",
        "output = list()\n",
        "for iter in range(3*3):\n",
        "    for inp in input:\n",
        "        output.append(inp + iter * 5)\n",
        "output"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 2,\n",
              " 4,\n",
              " 6,\n",
              " 7,\n",
              " 9,\n",
              " 11,\n",
              " 12,\n",
              " 14,\n",
              " 16,\n",
              " 17,\n",
              " 19,\n",
              " 21,\n",
              " 22,\n",
              " 24,\n",
              " 26,\n",
              " 27,\n",
              " 29,\n",
              " 31,\n",
              " 32,\n",
              " 34,\n",
              " 36,\n",
              " 37,\n",
              " 39,\n",
              " 41,\n",
              " 42,\n",
              " 44]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHtSPz4nABhf",
        "outputId": "800a9159-5a62-425a-9569-c2fc61fcc152"
      },
      "source": [
        "flattened[0]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(45,), dtype=float32, numpy=\n",
              "array([1., 2., 3., 4., 5., 1., 2., 3., 4., 5., 1., 2., 3., 4., 5., 1., 2.,\n",
              "       3., 4., 5., 1., 2., 3., 4., 5., 1., 2., 3., 4., 5., 1., 2., 3., 4.,\n",
              "       5., 1., 2., 3., 4., 5., 1., 2., 3., 4., 5.], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-YABTMYABmu",
        "outputId": "e96eabc1-a1e3-4d27-9d2f-4f21b0598efa"
      },
      "source": [
        "tf.gather(flattened, output, axis=-1)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 27), dtype=float32, numpy=\n",
              "array([[2., 3., 5., 2., 3., 5., 2., 3., 5., 2., 3., 5., 2., 3., 5., 2.,\n",
              "        3., 5., 2., 3., 5., 2., 3., 5., 2., 3., 5.],\n",
              "       [2., 3., 5., 2., 3., 5., 2., 3., 5., 2., 3., 5., 2., 3., 5., 2.,\n",
              "        3., 5., 2., 3., 5., 2., 3., 5., 2., 3., 5.]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PVjvRB4cFsN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5ae9226-63d9-485c-ccd1-4ec9acbe5b22"
      },
      "source": [
        "F_reduced_max = tf.reshape(tf.math.reduce_max(tf.abs(F), axis=(0, 1, 2)), (1, -1))\n",
        "F_reduced_max_with_biases = tf.concat([F_reduced_max, tf.reshape(b, (1, -1))], axis=0)\n",
        "filters_are_active = tf.math.reduce_max(tf.abs(F_reduced_max_with_biases), axis=0) >= 3\n",
        "filters_are_active"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5,), dtype=bool, numpy=array([False, False,  True,  True,  True])>"
            ]
          },
          "metadata": {},
          "execution_count": 188
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sr0gAduGbC-m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb6b389f-72d0-4eee-efc3-e576f73b9232"
      },
      "source": [
        "b"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 1, 2, 5), dtype=float32, numpy=\n",
              "array([[[[0.1, 0.2, 0.3, 0.4, 0.5],\n",
              "         [0.1, 0.2, 0.3, 0.4, 0.5]]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0c9-0n1aCLf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "outputId": "c01ef6a5-a283-4247-e215-0e18b794fd74"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-160-aa23eea9d44a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   1767\u001b[0m           dtype=dtypes.int32).get_shape().assert_has_rank(0)\n\u001b[1;32m   1768\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1769\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mconcat_v2\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   1211\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6939\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6940\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6941\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6942\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: ConcatOp : Dimensions of inputs should match: shape[0] = [3,3,2,5] vs. shape[1] = [1,1,2,5] [Op:ConcatV2] name: concat"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htoGZYIsRubZ"
      },
      "source": [
        "weights_with_biases = F\n",
        "filters_are_active = tf.math.reduce_max(tf.abs(weights_with_biases), axis=(0, 1, 2)) >= 3\n",
        "active_output_filters_indices = tf.reshape(tf.where(filters_are_active), (-1,))\n",
        "new_F = tf.gather(F, active_output_neurons_indices, axis=-1)\n",
        "new_b = tf.gather(b, active_output_neurons_indices, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJVCxZqYYn1v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3507b203-380c-4a35-bb3b-4db77db6e124"
      },
      "source": [
        "new_F"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 3, 2, 3), dtype=float32, numpy=\n",
              "array([[[[3., 4., 5.],\n",
              "         [3., 4., 5.]],\n",
              "\n",
              "        [[3., 4., 5.],\n",
              "         [3., 4., 5.]],\n",
              "\n",
              "        [[3., 4., 5.],\n",
              "         [3., 4., 5.]]],\n",
              "\n",
              "\n",
              "       [[[3., 4., 5.],\n",
              "         [3., 4., 5.]],\n",
              "\n",
              "        [[3., 4., 5.],\n",
              "         [3., 4., 5.]],\n",
              "\n",
              "        [[3., 4., 5.],\n",
              "         [3., 4., 5.]]],\n",
              "\n",
              "\n",
              "       [[[3., 4., 5.],\n",
              "         [3., 4., 5.]],\n",
              "\n",
              "        [[3., 4., 5.],\n",
              "         [3., 4., 5.]],\n",
              "\n",
              "        [[3., 4., 5.],\n",
              "         [3., 4., 5.]]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLNZ7Rm5YS-j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6f254a8-975d-40ee-977b-578a9b77cc4d"
      },
      "source": [
        "new_b"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([0.1, 0.1, 0.1], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kkMjOFSPYy4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66fe0488-7adc-47b8-df9a-02f3709b9a2d"
      },
      "source": [
        "w = tf.cumsum(x, axis=-2)\n",
        "w"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 3, 4, 2), dtype=float32, numpy=\n",
              "array([[[[1., 1.],\n",
              "         [2., 2.],\n",
              "         [3., 3.],\n",
              "         [4., 4.]],\n",
              "\n",
              "        [[1., 1.],\n",
              "         [2., 2.],\n",
              "         [3., 3.],\n",
              "         [4., 4.]],\n",
              "\n",
              "        [[1., 1.],\n",
              "         [2., 2.],\n",
              "         [3., 3.],\n",
              "         [4., 4.]]],\n",
              "\n",
              "\n",
              "       [[[1., 1.],\n",
              "         [2., 2.],\n",
              "         [3., 3.],\n",
              "         [4., 4.]],\n",
              "\n",
              "        [[1., 1.],\n",
              "         [2., 2.],\n",
              "         [3., 3.],\n",
              "         [4., 4.]],\n",
              "\n",
              "        [[1., 1.],\n",
              "         [2., 2.],\n",
              "         [3., 3.],\n",
              "         [4., 4.]]],\n",
              "\n",
              "\n",
              "       [[[1., 1.],\n",
              "         [2., 2.],\n",
              "         [3., 3.],\n",
              "         [4., 4.]],\n",
              "\n",
              "        [[1., 1.],\n",
              "         [2., 2.],\n",
              "         [3., 3.],\n",
              "         [4., 4.]],\n",
              "\n",
              "        [[1., 1.],\n",
              "         [2., 2.],\n",
              "         [3., 3.],\n",
              "         [4., 4.]]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fm0nVspvP8uI"
      },
      "source": [
        "active_input_neurons_indices = (1, 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wlszDSCP78D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d982fa2-c54d-4458-fe71-fc1a04832603"
      },
      "source": [
        "tf.gather(w, active_input_neurons_indices, axis=-2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 3, 2, 2), dtype=float32, numpy=\n",
              "array([[[[2., 2.],\n",
              "         [4., 4.]],\n",
              "\n",
              "        [[2., 2.],\n",
              "         [4., 4.]],\n",
              "\n",
              "        [[2., 2.],\n",
              "         [4., 4.]]],\n",
              "\n",
              "\n",
              "       [[[2., 2.],\n",
              "         [4., 4.]],\n",
              "\n",
              "        [[2., 2.],\n",
              "         [4., 4.]],\n",
              "\n",
              "        [[2., 2.],\n",
              "         [4., 4.]]],\n",
              "\n",
              "\n",
              "       [[[2., 2.],\n",
              "         [4., 4.]],\n",
              "\n",
              "        [[2., 2.],\n",
              "         [4., 4.]],\n",
              "\n",
              "        [[2., 2.],\n",
              "         [4., 4.]]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1MZm6Dpe6R0"
      },
      "source": [
        "x = X_train_norm[:2, :, :, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fEvUA5ufRGd"
      },
      "source": [
        "x = np.array([[\n",
        "      [[0, 1, 2], [0, 1, 2]],\n",
        "      [[0, 1, 2], [0, 1, 2]],\n",
        "]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cO63lErYfnox",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53d7f33d-aebd-4334-9bfe-060da926e54b"
      },
      "source": [
        "x.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 2, 2, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiDSGOEMeZXM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8e64103-7ca0-4565-f49e-3e56b5179d5c"
      },
      "source": [
        "flatten = tf.keras.layers.Flatten(data_format='channels_last')\n",
        "y = flatten(x)\n",
        "y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 12), dtype=int64, numpy=array([[0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2]])>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuydlFO4fMLL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08c5f359-3e3b-4adf-9363-96c592103c74"
      },
      "source": [
        "y.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([2, 3072])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByLRkOAPoGcc"
      },
      "source": [
        "epochs = 20\n",
        "self_scaling_epochs = 20\n",
        "batch_size = 256\n",
        "min_new_neurons = 20"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "382QHGlZvyl9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "337ec911-97e2-447f-867d-a77870803f61"
      },
      "source": [
        "model = Sequential([\n",
        "        Dense(20, activation='selu', regularization_penalty=0.000001, \n",
        "            regularization_method='weighted_l1', kernel_initializer='lecun_normal', input_shape=X_train_norm[0, :].shape),\n",
        "        Dense(20, activation='selu', regularization_penalty=0.000001, \n",
        "            regularization_method='weighted_l1', kernel_initializer='lecun_normal'),\n",
        "        Dense(20, activation='selu', regularization_penalty=0.000001, \n",
        "            regularization_method='weighted_l1', kernel_initializer='lecun_normal'),\n",
        "        Dense(20, activation='selu', regularization_penalty=0.000001, \n",
        "            regularization_method='weighted_l1', kernel_initializer='lecun_normal'),\n",
        "        Dense(10, activation='softmax', regularization_penalty=0., \n",
        "            regularization_method=None, fixed_size=True),\n",
        "    ])\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002)\n",
        "\n",
        "model.fit(X_train_norm, y_train, optimizer, epochs, self_scaling_epochs, batch_size, \n",
        "          min_new_neurons, validation_data=(X_test_norm, y_test))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##########################################################\n",
            "Epoch 1/20\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 2.847022533416748 - val_accuracy: 0.0938 - penalty: 1e-06\n",
            "hidden layer sizes: [20, 20, 20, 20], total units: 80\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 2.84702205657959 - val_accuracy: 0.0938 - penalty: 1e-06\n",
            "hidden layer sizes: [40, 40, 40, 40], total units: 160\n",
            "Before pruning:\n",
            "loss: 1.9452203512191772 - accuracy: 0.32256 - val_loss: 1.7487735748291016 - val_accuracy: 0.3872 - penalty: 1e-06\n",
            "hidden layer sizes: [40, 40, 40, 40], total units: 160\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 1.7487735748291016 - val_accuracy: 0.3872 - penalty: 1e-06\n",
            "hidden layer sizes: [40, 40, 40, 40], total units: 160\n",
            "##########################################################\n",
            "Epoch 2/20\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 1.7487735748291016 - val_accuracy: 0.3872 - penalty: 1e-06\n",
            "hidden layer sizes: [40, 40, 40, 40], total units: 160\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 1.7487735748291016 - val_accuracy: 0.3872 - penalty: 1e-06\n",
            "hidden layer sizes: [60, 60, 60, 60], total units: 240\n",
            "Before pruning:\n",
            "loss: 1.6932038068771362 - accuracy: 0.40406 - val_loss: 1.617717981338501 - val_accuracy: 0.432 - penalty: 1e-06\n",
            "hidden layer sizes: [60, 60, 60, 60], total units: 240\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 1.617717981338501 - val_accuracy: 0.432 - penalty: 1e-06\n",
            "hidden layer sizes: [60, 60, 60, 60], total units: 240\n",
            "##########################################################\n",
            "Epoch 3/20\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 1.617717981338501 - val_accuracy: 0.432 - penalty: 1e-06\n",
            "hidden layer sizes: [60, 60, 60, 60], total units: 240\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 1.617717981338501 - val_accuracy: 0.432 - penalty: 1e-06\n",
            "hidden layer sizes: [80, 80, 80, 80], total units: 320\n",
            "Before pruning:\n",
            "loss: 1.5950701236724854 - accuracy: 0.43732 - val_loss: 1.5567930936813354 - val_accuracy: 0.4541 - penalty: 1e-06\n",
            "hidden layer sizes: [80, 80, 80, 80], total units: 320\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 1.5567940473556519 - val_accuracy: 0.4542 - penalty: 1e-06\n",
            "hidden layer sizes: [78, 80, 80, 80], total units: 318\n",
            "##########################################################\n",
            "Epoch 4/20\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 1.5567940473556519 - val_accuracy: 0.4542 - penalty: 1e-06\n",
            "hidden layer sizes: [78, 80, 80, 80], total units: 318\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 1.5567940473556519 - val_accuracy: 0.4542 - penalty: 1e-06\n",
            "hidden layer sizes: [98, 100, 100, 100], total units: 398\n",
            "Before pruning:\n",
            "loss: 1.531063437461853 - accuracy: 0.46044 - val_loss: 1.5098843574523926 - val_accuracy: 0.4652 - penalty: 1e-06\n",
            "hidden layer sizes: [98, 100, 100, 100], total units: 398\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 1.509883165359497 - val_accuracy: 0.4651 - penalty: 1e-06\n",
            "hidden layer sizes: [98, 100, 92, 96], total units: 386\n",
            "##########################################################\n",
            "Epoch 5/20\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 1.509883165359497 - val_accuracy: 0.4651 - penalty: 1e-06\n",
            "hidden layer sizes: [98, 100, 92, 96], total units: 386\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 1.5098830461502075 - val_accuracy: 0.4651 - penalty: 1e-06\n",
            "hidden layer sizes: [118, 120, 112, 116], total units: 466\n",
            "Before pruning:\n",
            "loss: 1.486151933670044 - accuracy: 0.47494 - val_loss: 1.4772998094558716 - val_accuracy: 0.48 - penalty: 1e-06\n",
            "hidden layer sizes: [118, 120, 112, 116], total units: 466\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 1.4772988557815552 - val_accuracy: 0.48 - penalty: 1e-06\n",
            "hidden layer sizes: [114, 120, 103, 103], total units: 440\n",
            "##########################################################\n",
            "Epoch 6/20\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 1.4772988557815552 - val_accuracy: 0.48 - penalty: 1e-06\n",
            "hidden layer sizes: [114, 120, 103, 103], total units: 440\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 1.4772988557815552 - val_accuracy: 0.48 - penalty: 1e-06\n",
            "hidden layer sizes: [136, 144, 123, 123], total units: 526\n",
            "Before pruning:\n",
            "loss: 1.4510244131088257 - accuracy: 0.48604 - val_loss: 1.4682587385177612 - val_accuracy: 0.4808 - penalty: 1e-06\n",
            "hidden layer sizes: [136, 144, 123, 123], total units: 526\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 1.4682543277740479 - val_accuracy: 0.4807 - penalty: 1e-06\n",
            "hidden layer sizes: [132, 144, 114, 111], total units: 501\n",
            "##########################################################\n",
            "Epoch 7/20\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 1.4682543277740479 - val_accuracy: 0.4807 - penalty: 1e-06\n",
            "hidden layer sizes: [132, 144, 114, 111], total units: 501\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 1.4682543277740479 - val_accuracy: 0.4807 - penalty: 1e-06\n",
            "hidden layer sizes: [158, 172, 136, 133], total units: 599\n",
            "Before pruning:\n",
            "loss: 1.4220269918441772 - accuracy: 0.49738 - val_loss: 1.4586914777755737 - val_accuracy: 0.4828 - penalty: 1e-06\n",
            "hidden layer sizes: [158, 172, 136, 133], total units: 599\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 1.4587644338607788 - val_accuracy: 0.4828 - penalty: 1e-06\n",
            "hidden layer sizes: [148, 168, 122, 116], total units: 554\n",
            "##########################################################\n",
            "Epoch 8/20\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 1.4587644338607788 - val_accuracy: 0.4828 - penalty: 1e-06\n",
            "hidden layer sizes: [148, 168, 122, 116], total units: 554\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 1.4587644338607788 - val_accuracy: 0.4828 - penalty: 1e-06\n",
            "hidden layer sizes: [177, 201, 146, 139], total units: 663\n",
            "Before pruning:\n",
            "loss: 1.3944101333618164 - accuracy: 0.50546 - val_loss: 1.4363137483596802 - val_accuracy: 0.4952 - penalty: 1e-06\n",
            "hidden layer sizes: [177, 201, 146, 139], total units: 663\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 1.4362996816635132 - val_accuracy: 0.495 - penalty: 1e-06\n",
            "hidden layer sizes: [172, 196, 134, 110], total units: 612\n",
            "##########################################################\n",
            "Epoch 9/20\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 1.4362996816635132 - val_accuracy: 0.495 - penalty: 1e-06\n",
            "hidden layer sizes: [172, 196, 134, 110], total units: 612\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 1.4362998008728027 - val_accuracy: 0.495 - penalty: 1e-06\n",
            "hidden layer sizes: [206, 235, 160, 132], total units: 733\n",
            "Before pruning:\n",
            "loss: 1.3739022016525269 - accuracy: 0.51368 - val_loss: 1.4447834491729736 - val_accuracy: 0.4908 - penalty: 1e-06\n",
            "hidden layer sizes: [206, 235, 160, 132], total units: 733\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 1.4447710514068604 - val_accuracy: 0.4908 - penalty: 1e-06\n",
            "hidden layer sizes: [194, 218, 131, 118], total units: 661\n",
            "##########################################################\n",
            "Epoch 10/20\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 1.4447710514068604 - val_accuracy: 0.4908 - penalty: 1e-06\n",
            "hidden layer sizes: [194, 218, 131, 118], total units: 661\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 1.44477117061615 - val_accuracy: 0.4908 - penalty: 1e-06\n",
            "hidden layer sizes: [232, 261, 157, 141], total units: 791\n",
            "Before pruning:\n",
            "loss: 1.3534380197525024 - accuracy: 0.51908 - val_loss: 1.426801085472107 - val_accuracy: 0.4967 - penalty: 1e-06\n",
            "hidden layer sizes: [232, 261, 157, 141], total units: 791\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 1.4268704652786255 - val_accuracy: 0.4968 - penalty: 1e-06\n",
            "hidden layer sizes: [203, 217, 125, 119], total units: 664\n",
            "##########################################################\n",
            "Epoch 11/20\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 1.4268704652786255 - val_accuracy: 0.4968 - penalty: 1e-06\n",
            "hidden layer sizes: [203, 217, 125, 119], total units: 664\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 1.426870584487915 - val_accuracy: 0.4968 - penalty: 1e-06\n",
            "hidden layer sizes: [243, 260, 150, 142], total units: 795\n",
            "Before pruning:\n",
            "loss: 1.3362910747528076 - accuracy: 0.52698 - val_loss: 1.4262040853500366 - val_accuracy: 0.4949 - penalty: 1e-06\n",
            "hidden layer sizes: [243, 260, 150, 142], total units: 795\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 1.4262185096740723 - val_accuracy: 0.4946 - penalty: 1e-06\n",
            "hidden layer sizes: [228, 210, 132, 128], total units: 698\n",
            "##########################################################\n",
            "Epoch 12/20\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 1.4262185096740723 - val_accuracy: 0.4946 - penalty: 1e-06\n",
            "hidden layer sizes: [228, 210, 132, 128], total units: 698\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 1.4262185096740723 - val_accuracy: 0.4946 - penalty: 1e-06\n",
            "hidden layer sizes: [273, 252, 158, 153], total units: 836\n",
            "Before pruning:\n",
            "loss: 1.3206266164779663 - accuracy: 0.53322 - val_loss: 1.4255939722061157 - val_accuracy: 0.4931 - penalty: 1e-06\n",
            "hidden layer sizes: [273, 252, 158, 153], total units: 836\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 1.4254165887832642 - val_accuracy: 0.4928 - penalty: 1e-06\n",
            "hidden layer sizes: [212, 229, 120, 130], total units: 691\n",
            "##########################################################\n",
            "Epoch 13/20\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 1.4254165887832642 - val_accuracy: 0.4928 - penalty: 1e-06\n",
            "hidden layer sizes: [212, 229, 120, 130], total units: 691\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 1.4254164695739746 - val_accuracy: 0.4928 - penalty: 1e-06\n",
            "hidden layer sizes: [254, 274, 144, 156], total units: 828\n",
            "Before pruning:\n",
            "loss: 1.305312156677246 - accuracy: 0.53872 - val_loss: 1.4191508293151855 - val_accuracy: 0.5 - penalty: 1e-06\n",
            "hidden layer sizes: [254, 274, 144, 156], total units: 828\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 1.4190995693206787 - val_accuracy: 0.4999 - penalty: 1e-06\n",
            "hidden layer sizes: [238, 263, 121, 129], total units: 751\n",
            "##########################################################\n",
            "Epoch 14/20\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 1.4190995693206787 - val_accuracy: 0.4999 - penalty: 1e-06\n",
            "hidden layer sizes: [238, 263, 121, 129], total units: 751\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 1.4190994501113892 - val_accuracy: 0.4999 - penalty: 1e-06\n",
            "hidden layer sizes: [285, 315, 145, 154], total units: 899\n",
            "Before pruning:\n",
            "loss: 1.2916194200515747 - accuracy: 0.54382 - val_loss: 1.417806625366211 - val_accuracy: 0.4987 - penalty: 1e-06\n",
            "hidden layer sizes: [285, 315, 145, 154], total units: 899\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 1.417663335800171 - val_accuracy: 0.4989 - penalty: 1e-06\n",
            "hidden layer sizes: [241, 293, 121, 140], total units: 795\n",
            "##########################################################\n",
            "Epoch 15/20\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 1.417663335800171 - val_accuracy: 0.4989 - penalty: 1e-06\n",
            "hidden layer sizes: [241, 293, 121, 140], total units: 795\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 1.4176632165908813 - val_accuracy: 0.4989 - penalty: 1e-06\n",
            "hidden layer sizes: [289, 351, 145, 168], total units: 953\n",
            "Before pruning:\n",
            "loss: 1.27534019947052 - accuracy: 0.55034 - val_loss: 1.413841962814331 - val_accuracy: 0.4989 - penalty: 1e-06\n",
            "hidden layer sizes: [289, 351, 145, 168], total units: 953\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 1.4138699769973755 - val_accuracy: 0.4987 - penalty: 1e-06\n",
            "hidden layer sizes: [267, 288, 115, 139], total units: 809\n",
            "##########################################################\n",
            "Epoch 16/20\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 1.4138699769973755 - val_accuracy: 0.4987 - penalty: 1e-06\n",
            "hidden layer sizes: [267, 288, 115, 139], total units: 809\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 1.4138699769973755 - val_accuracy: 0.4987 - penalty: 1e-06\n",
            "hidden layer sizes: [320, 345, 138, 166], total units: 969\n",
            "Before pruning:\n",
            "loss: 1.260942816734314 - accuracy: 0.55546 - val_loss: 1.4225391149520874 - val_accuracy: 0.4966 - penalty: 1e-06\n",
            "hidden layer sizes: [320, 345, 138, 166], total units: 969\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 1.4219051599502563 - val_accuracy: 0.4971 - penalty: 1e-06\n",
            "hidden layer sizes: [268, 299, 131, 158], total units: 856\n",
            "##########################################################\n",
            "Epoch 17/20\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 1.4219051599502563 - val_accuracy: 0.4971 - penalty: 1e-06\n",
            "hidden layer sizes: [268, 299, 131, 158], total units: 856\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 1.4219053983688354 - val_accuracy: 0.4971 - penalty: 1e-06\n",
            "hidden layer sizes: [321, 358, 157, 189], total units: 1025\n",
            "Before pruning:\n",
            "loss: 1.2511773109436035 - accuracy: 0.55652 - val_loss: 1.413221836090088 - val_accuracy: 0.4987 - penalty: 1e-06\n",
            "hidden layer sizes: [321, 358, 157, 189], total units: 1025\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 1.4132758378982544 - val_accuracy: 0.4986 - penalty: 1e-06\n",
            "hidden layer sizes: [298, 316, 143, 161], total units: 918\n",
            "##########################################################\n",
            "Epoch 18/20\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 1.4132758378982544 - val_accuracy: 0.4986 - penalty: 1e-06\n",
            "hidden layer sizes: [298, 316, 143, 161], total units: 918\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 1.4132758378982544 - val_accuracy: 0.4986 - penalty: 1e-06\n",
            "hidden layer sizes: [357, 379, 171, 193], total units: 1100\n",
            "Before pruning:\n",
            "loss: 1.2374706268310547 - accuracy: 0.5622 - val_loss: 1.4080685377120972 - val_accuracy: 0.4979 - penalty: 1e-06\n",
            "hidden layer sizes: [357, 379, 171, 193], total units: 1100\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 1.4082773923873901 - val_accuracy: 0.4976 - penalty: 1e-06\n",
            "hidden layer sizes: [250, 323, 141, 174], total units: 888\n",
            "##########################################################\n",
            "Epoch 19/20\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 1.4082773923873901 - val_accuracy: 0.4976 - penalty: 1e-06\n",
            "hidden layer sizes: [250, 323, 141, 174], total units: 888\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 1.4082773923873901 - val_accuracy: 0.4976 - penalty: 1e-06\n",
            "hidden layer sizes: [300, 387, 169, 208], total units: 1064\n",
            "Before pruning:\n",
            "loss: 1.2334518432617188 - accuracy: 0.56502 - val_loss: 1.4120644330978394 - val_accuracy: 0.4982 - penalty: 1e-06\n",
            "hidden layer sizes: [300, 387, 169, 208], total units: 1064\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 1.4121218919754028 - val_accuracy: 0.498 - penalty: 1e-06\n",
            "hidden layer sizes: [262, 311, 146, 156], total units: 875\n",
            "##########################################################\n",
            "Epoch 20/20\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 1.4121218919754028 - val_accuracy: 0.498 - penalty: 1e-06\n",
            "hidden layer sizes: [262, 311, 146, 156], total units: 875\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 1.4121216535568237 - val_accuracy: 0.498 - penalty: 1e-06\n",
            "hidden layer sizes: [314, 373, 175, 187], total units: 1049\n",
            "Before pruning:\n",
            "loss: 1.2193961143493652 - accuracy: 0.56912 - val_loss: 1.4202760457992554 - val_accuracy: 0.4993 - penalty: 1e-06\n",
            "hidden layer sizes: [314, 373, 175, 187], total units: 1049\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 1.4200063943862915 - val_accuracy: 0.4996 - penalty: 1e-06\n",
            "hidden layer sizes: [274, 319, 153, 152], total units: 898\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': [0.32256,\n",
              "  0.40406,\n",
              "  0.43732,\n",
              "  0.46044,\n",
              "  0.47494,\n",
              "  0.48604,\n",
              "  0.49738,\n",
              "  0.50546,\n",
              "  0.51368,\n",
              "  0.51908,\n",
              "  0.52698,\n",
              "  0.53322,\n",
              "  0.53872,\n",
              "  0.54382,\n",
              "  0.55034,\n",
              "  0.55546,\n",
              "  0.55652,\n",
              "  0.5622,\n",
              "  0.56502,\n",
              "  0.56912],\n",
              " 'loss': [<tf.Tensor: shape=(), dtype=float32, numpy=1.9452204>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=1.6932038>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=1.5950701>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=1.5310634>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=1.4861519>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=1.4510244>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=1.422027>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=1.3944101>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=1.3739022>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=1.353438>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=1.3362911>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=1.3206266>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=1.3053122>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=1.2916194>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=1.2753402>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=1.2609428>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=1.2511773>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=1.2374706>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=1.2334518>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=1.2193961>],\n",
              " 'val_accuracy': [0.3872,\n",
              "  0.432,\n",
              "  0.4542,\n",
              "  0.4651,\n",
              "  0.48,\n",
              "  0.4807,\n",
              "  0.4828,\n",
              "  0.495,\n",
              "  0.4908,\n",
              "  0.4968,\n",
              "  0.4946,\n",
              "  0.4928,\n",
              "  0.4999,\n",
              "  0.4989,\n",
              "  0.4987,\n",
              "  0.4971,\n",
              "  0.4986,\n",
              "  0.4976,\n",
              "  0.498,\n",
              "  0.4996],\n",
              " 'val_loss': [<tf.Tensor: shape=(), dtype=float32, numpy=1.7487736>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=1.617718>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=1.556794>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=1.5098832>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=1.4772989>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=1.4682543>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=1.4587644>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=1.4362997>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=1.444771>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=1.4268705>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=1.4262185>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=1.4254166>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=1.4190996>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=1.4176633>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=1.41387>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=1.4219052>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=1.4132758>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=1.4082774>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=1.4121219>,\n",
              "  <tf.Tensor: shape=(), dtype=float32, numpy=1.4200064>]}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0C3tMuzb7m2s"
      },
      "source": [
        "model = Sequential([\n",
        "        Conv2D(192, filter_size=(3, 3), activation='selu', strides=(1, 1), padding='SAME', \n",
        "            regularization_penalty=0.000005, regularization_method='weighted_l1', \n",
        "            kernel_initializer='lecun_normal', input_shape=X_train_norm[0,:,:,:].shape),\n",
        "        Conv2D(192, filter_size=(3, 3), activation='selu', strides=(2, 2), padding='SAME', \n",
        "            regularization_penalty=0.000005, regularization_method='weighted_l1', \n",
        "            kernel_initializer='lecun_normal'),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        Conv2D(192, filter_size=(3, 3), activation='selu', strides=(1, 1), padding='SAME', \n",
        "            regularization_penalty=0.000005, regularization_method='weighted_l1', \n",
        "            kernel_initializer='lecun_normal'),\n",
        "        Conv2D(192, filter_size=(3, 3), activation='selu', strides=(2, 2), padding='SAME', \n",
        "            regularization_penalty=0.000005, regularization_method='weighted_l1', \n",
        "            kernel_initializer='lecun_normal'),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        Flatten(),\n",
        "        # tf.keras.layers.Flatten(),\n",
        "        Dense(256, activation='selu', regularization_penalty=0.000001, \n",
        "            regularization_method='weighted_l1', kernel_initializer='lecun_normal'),\n",
        "        Dense(10, activation='softmax', regularization_penalty=0., \n",
        "            regularization_method=None, fixed_size=True),\n",
        "    ])\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002)\n",
        "\n",
        "model.fit(X_train_norm, y_train, optimizer, epochs, self_scaling_epochs, batch_size, \n",
        "          min_new_neurons, validation_data=(X_test_norm, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-suYTV264gm"
      },
      "source": [
        "# 74.5 % vs 71.5 % accuracy in the following two models - 3 % boost! (output deleted :P)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQnJaU_Ray-V"
      },
      "source": [
        "model = Sequential([\n",
        "        Conv2D(192, filter_size=(3, 3), activation='selu', strides=(1, 1), padding='SAME', \n",
        "            regularization_penalty=0.000005, regularization_method='weighted_l1', \n",
        "            kernel_initializer='lecun_normal', input_shape=X_train_norm[0,:,:,:].shape),\n",
        "        Conv2D(192, filter_size=(3, 3), activation='selu', strides=(2, 2), padding='SAME', \n",
        "            regularization_penalty=0.000005, regularization_method='weighted_l1', \n",
        "            kernel_initializer='lecun_normal'),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        Conv2D(192, filter_size=(3, 3), activation='selu', strides=(1, 1), padding='SAME', \n",
        "            regularization_penalty=0.000005, regularization_method='weighted_l1', \n",
        "            kernel_initializer='lecun_normal'),\n",
        "        Conv2D(192, filter_size=(3, 3), activation='selu', strides=(2, 2), padding='SAME', \n",
        "            regularization_penalty=0.000005, regularization_method='weighted_l1', \n",
        "            kernel_initializer='lecun_normal'),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        Flatten(),\n",
        "        Dense(256, activation='selu', regularization_penalty=0.000001, \n",
        "            regularization_method='weighted_l1', kernel_initializer='lecun_normal'),\n",
        "        Dense(10, activation='softmax', regularization_penalty=0., \n",
        "            regularization_method=None, fixed_size=True),\n",
        "    ])\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002)\n",
        "\n",
        "model.fit(X_train_norm, y_train, optimizer, epochs, self_scaling_epochs, batch_size, \n",
        "          min_new_neurons, validation_data=(X_test_norm, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IoJydQX7bsmq",
        "scrolled": false,
        "outputId": "c83f7f79-7bf7-4370-e9b4-33dcfad8d311"
      },
      "source": [
        "%%time\n",
        "\n",
        "model = Sequential([\n",
        "        Conv2D(3, 96, filter_size=(3, 3), activation='selu', \n",
        "            strides=(1, 1), padding='SAME', kernel_initializer='lecun_normal', input_shape=X_train_norm[0,:,:,:].shape),\n",
        "        Conv2D(96, 96, filter_size=(3, 3), activation='selu', \n",
        "            strides=(2, 2), padding='SAME', kernel_initializer='lecun_normal'),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        Conv2D(96, 192, filter_size=(3, 3), activation='selu', \n",
        "            strides=(1, 1), padding='SAME', kernel_initializer='lecun_normal'),\n",
        "        Conv2D(96, 192, filter_size=(3, 3), activation='selu', \n",
        "            strides=(2, 2), padding='SAME', kernel_initializer='lecun_normal'),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        Dense(12288, 256, activation='selu', regularization_penalty=0., \n",
        "            regularization_method=None, kernel_initializer='lecun_normal'),\n",
        "        Dense(256, 10, activation='softmax', regularization_penalty=0., \n",
        "            regularization_method=None),\n",
        "    ])\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "model.fit(X_train_norm, y_train, optimizer, epochs, self_scaling_epochs, batch_size, \n",
        "          min_new_neurons, validation_data=(X_test_norm, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##########################################################\n",
            "Epoch 1/25\n",
            "loss: 2.1398861408233643 - accuracy: 0.39144 - val_loss: 1.3741620779037476 - val_accuracy: 0.5007 - penalty: 0.0\n",
            "hidden layer sizes: [96, 96, 192, 192, 256], total units: 832\n",
            "##########################################################\n",
            "Epoch 2/25\n",
            "loss: 1.3535252809524536 - accuracy: 0.51546 - val_loss: 1.268190860748291 - val_accuracy: 0.5486 - penalty: 0.0\n",
            "hidden layer sizes: [96, 96, 192, 192, 256], total units: 832\n",
            "##########################################################\n",
            "Epoch 3/25\n",
            "loss: 1.1914558410644531 - accuracy: 0.5725 - val_loss: 1.166611671447754 - val_accuracy: 0.5835 - penalty: 0.0\n",
            "hidden layer sizes: [96, 96, 192, 192, 256], total units: 832\n",
            "##########################################################\n",
            "Epoch 4/25\n",
            "loss: 1.0351282358169556 - accuracy: 0.63084 - val_loss: 1.087954044342041 - val_accuracy: 0.623 - penalty: 0.0\n",
            "hidden layer sizes: [96, 96, 192, 192, 256], total units: 832\n",
            "##########################################################\n",
            "Epoch 5/25\n",
            "loss: 0.9159864783287048 - accuracy: 0.67614 - val_loss: 0.9969214797019958 - val_accuracy: 0.6613 - penalty: 0.0\n",
            "hidden layer sizes: [96, 96, 192, 192, 256], total units: 832\n",
            "##########################################################\n",
            "Epoch 6/25\n",
            "loss: 0.8134353160858154 - accuracy: 0.7108 - val_loss: 1.0081638097763062 - val_accuracy: 0.6658 - penalty: 0.0\n",
            "hidden layer sizes: [96, 96, 192, 192, 256], total units: 832\n",
            "##########################################################\n",
            "Epoch 7/25\n",
            "loss: 0.7176882028579712 - accuracy: 0.74724 - val_loss: 0.9864904284477234 - val_accuracy: 0.675 - penalty: 0.0\n",
            "hidden layer sizes: [96, 96, 192, 192, 256], total units: 832\n",
            "##########################################################\n",
            "Epoch 8/25\n",
            "loss: 0.6499534249305725 - accuracy: 0.76928 - val_loss: 0.973360538482666 - val_accuracy: 0.6957 - penalty: 0.0\n",
            "hidden layer sizes: [96, 96, 192, 192, 256], total units: 832\n",
            "##########################################################\n",
            "Epoch 9/25\n",
            "loss: 0.5757144093513489 - accuracy: 0.79518 - val_loss: 1.0311030149459839 - val_accuracy: 0.6877 - penalty: 0.0\n",
            "hidden layer sizes: [96, 96, 192, 192, 256], total units: 832\n",
            "##########################################################\n",
            "Epoch 10/25\n",
            "loss: 0.5312620997428894 - accuracy: 0.8151 - val_loss: 1.0731127262115479 - val_accuracy: 0.6944 - penalty: 0.0\n",
            "hidden layer sizes: [96, 96, 192, 192, 256], total units: 832\n",
            "##########################################################\n",
            "Epoch 11/25\n",
            "loss: 0.49278587102890015 - accuracy: 0.82826 - val_loss: 1.1032124757766724 - val_accuracy: 0.6912 - penalty: 0.0\n",
            "hidden layer sizes: [96, 96, 192, 192, 256], total units: 832\n",
            "##########################################################\n",
            "Epoch 12/25\n",
            "loss: 0.45533832907676697 - accuracy: 0.83932 - val_loss: 1.1450512409210205 - val_accuracy: 0.6946 - penalty: 0.0\n",
            "hidden layer sizes: [96, 96, 192, 192, 256], total units: 832\n",
            "##########################################################\n",
            "Epoch 13/25\n",
            "loss: 0.417134165763855 - accuracy: 0.8523 - val_loss: 1.137213945388794 - val_accuracy: 0.6986 - penalty: 0.0\n",
            "hidden layer sizes: [96, 96, 192, 192, 256], total units: 832\n",
            "##########################################################\n",
            "Epoch 14/25\n",
            "loss: 0.39540520310401917 - accuracy: 0.8612 - val_loss: 1.1061996221542358 - val_accuracy: 0.7029 - penalty: 0.0\n",
            "hidden layer sizes: [96, 96, 192, 192, 256], total units: 832\n",
            "##########################################################\n",
            "Epoch 15/25\n",
            "loss: 0.37588027119636536 - accuracy: 0.87056 - val_loss: 1.133886694908142 - val_accuracy: 0.7044 - penalty: 0.0\n",
            "hidden layer sizes: [96, 96, 192, 192, 256], total units: 832\n",
            "##########################################################\n",
            "Epoch 16/25\n",
            "loss: 0.34373739361763 - accuracy: 0.88308 - val_loss: 1.1591758728027344 - val_accuracy: 0.7147 - penalty: 0.0\n",
            "hidden layer sizes: [96, 96, 192, 192, 256], total units: 832\n",
            "##########################################################\n",
            "Epoch 17/25\n",
            "loss: 0.3225548565387726 - accuracy: 0.88992 - val_loss: 1.190388560295105 - val_accuracy: 0.711 - penalty: 0.0\n",
            "hidden layer sizes: [96, 96, 192, 192, 256], total units: 832\n",
            "##########################################################\n",
            "Epoch 18/25\n",
            "loss: 0.3176613748073578 - accuracy: 0.89348 - val_loss: 1.1829746961593628 - val_accuracy: 0.7074 - penalty: 0.0\n",
            "hidden layer sizes: [96, 96, 192, 192, 256], total units: 832\n",
            "##########################################################\n",
            "Epoch 19/25\n",
            "loss: 0.28977060317993164 - accuracy: 0.90038 - val_loss: 1.396437168121338 - val_accuracy: 0.7148 - penalty: 0.0\n",
            "hidden layer sizes: [96, 96, 192, 192, 256], total units: 832\n",
            "##########################################################\n",
            "Epoch 20/25\n",
            "loss: 0.28722086548805237 - accuracy: 0.90372 - val_loss: 1.291654348373413 - val_accuracy: 0.7115 - penalty: 0.0\n",
            "hidden layer sizes: [96, 96, 192, 192, 256], total units: 832\n",
            "##########################################################\n",
            "Epoch 21/25\n",
            "loss: 0.26941731572151184 - accuracy: 0.90884 - val_loss: 1.4155263900756836 - val_accuracy: 0.7053 - penalty: 0.0\n",
            "hidden layer sizes: [96, 96, 192, 192, 256], total units: 832\n",
            "##########################################################\n",
            "Epoch 22/25\n",
            "loss: 0.26944756507873535 - accuracy: 0.90862 - val_loss: 1.2642757892608643 - val_accuracy: 0.7172 - penalty: 0.0\n",
            "hidden layer sizes: [96, 96, 192, 192, 256], total units: 832\n",
            "##########################################################\n",
            "Epoch 23/25\n",
            "loss: 0.2647763788700104 - accuracy: 0.91274 - val_loss: 1.3459445238113403 - val_accuracy: 0.6958 - penalty: 0.0\n",
            "hidden layer sizes: [96, 96, 192, 192, 256], total units: 832\n",
            "##########################################################\n",
            "Epoch 24/25\n",
            "loss: 0.2519494593143463 - accuracy: 0.91652 - val_loss: 1.2954438924789429 - val_accuracy: 0.7191 - penalty: 0.0\n",
            "hidden layer sizes: [96, 96, 192, 192, 256], total units: 832\n",
            "##########################################################\n",
            "Epoch 25/25\n",
            "loss: 0.23448781669139862 - accuracy: 0.92198 - val_loss: 1.3476343154907227 - val_accuracy: 0.7157 - penalty: 0.0\n",
            "hidden layer sizes: [96, 96, 192, 192, 256], total units: 832\n",
            "CPU times: user 4min 53s, sys: 4 s, total: 4min 57s\n",
            "Wall time: 6min 27s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWOQuGEVq6Gj"
      },
      "source": [
        "# Temto model je na Cifar10 jiz pomerne vyladeny\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Conv2D(input_shape=X_train_norm[0,:,:,:].shape, filters=96, kernel_size=(3,3), activation='selu', kernel_initializer='lecun_normal'))\n",
        "model.add(tf.keras.layers.Conv2D(filters=96, kernel_size=(3,3), strides=2, activation='selu', kernel_initializer='lecun_normal'))\n",
        "model.add(tf.keras.layers.Dropout(0.2))\n",
        "model.add(tf.keras.layers.Conv2D(filters=192, kernel_size=(3,3), activation='selu', kernel_initializer='lecun_normal'))\n",
        "model.add(tf.keras.layers.Conv2D(filters=192, kernel_size=(3,3), strides=2, activation='selu', kernel_initializer='lecun_normal'))\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(256, activation='selu', kernel_initializer='lecun_normal'))\n",
        "model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDMBpQB1zkdk"
      },
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4l2I_etAZ4is",
        "outputId": "7ea71652-9e59-4a95-838d-fb4d06ccab0b"
      },
      "source": [
        "%%time\n",
        "\n",
        "model.fit(X_train_norm, y_train, epochs=25, batch_size=256, validation_data=(X_test_norm, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "196/196 [==============================] - 9s 46ms/step - loss: 2.0276 - accuracy: 0.3617 - val_loss: 1.4487 - val_accuracy: 0.4768\n",
            "Epoch 2/25\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 1.4076 - accuracy: 0.4938 - val_loss: 1.3187 - val_accuracy: 0.5150\n",
            "Epoch 3/25\n",
            "196/196 [==============================] - 9s 46ms/step - loss: 1.2646 - accuracy: 0.5483 - val_loss: 1.1942 - val_accuracy: 0.5702\n",
            "Epoch 4/25\n",
            "196/196 [==============================] - 9s 46ms/step - loss: 1.1380 - accuracy: 0.5945 - val_loss: 1.1212 - val_accuracy: 0.5984\n",
            "Epoch 5/25\n",
            "196/196 [==============================] - 9s 46ms/step - loss: 1.0255 - accuracy: 0.6347 - val_loss: 1.0245 - val_accuracy: 0.6363\n",
            "Epoch 6/25\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 0.9390 - accuracy: 0.6672 - val_loss: 0.9999 - val_accuracy: 0.6479\n",
            "Epoch 7/25\n",
            "196/196 [==============================] - 9s 44ms/step - loss: 0.8667 - accuracy: 0.6939 - val_loss: 0.9372 - val_accuracy: 0.6731\n",
            "Epoch 8/25\n",
            "196/196 [==============================] - 9s 44ms/step - loss: 0.7990 - accuracy: 0.7169 - val_loss: 0.9421 - val_accuracy: 0.6771\n",
            "Epoch 9/25\n",
            "196/196 [==============================] - 9s 44ms/step - loss: 0.7508 - accuracy: 0.7351 - val_loss: 0.9119 - val_accuracy: 0.6902\n",
            "Epoch 10/25\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 0.6957 - accuracy: 0.7550 - val_loss: 0.8888 - val_accuracy: 0.7011\n",
            "Epoch 11/25\n",
            "196/196 [==============================] - 9s 44ms/step - loss: 0.6530 - accuracy: 0.7692 - val_loss: 0.8857 - val_accuracy: 0.7033\n",
            "Epoch 12/25\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 0.6118 - accuracy: 0.7842 - val_loss: 0.8710 - val_accuracy: 0.7076\n",
            "Epoch 13/25\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 0.5877 - accuracy: 0.7917 - val_loss: 0.9078 - val_accuracy: 0.7080\n",
            "Epoch 14/25\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 0.5419 - accuracy: 0.8086 - val_loss: 0.8814 - val_accuracy: 0.7109\n",
            "Epoch 15/25\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 0.5262 - accuracy: 0.8135 - val_loss: 0.9352 - val_accuracy: 0.7146\n",
            "Epoch 16/25\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 0.4963 - accuracy: 0.8243 - val_loss: 0.9018 - val_accuracy: 0.7223\n",
            "Epoch 17/25\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 0.4799 - accuracy: 0.8313 - val_loss: 0.9216 - val_accuracy: 0.7140\n",
            "Epoch 18/25\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 0.4564 - accuracy: 0.8392 - val_loss: 0.9259 - val_accuracy: 0.7286\n",
            "Epoch 19/25\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 0.4373 - accuracy: 0.8470 - val_loss: 0.9008 - val_accuracy: 0.7209\n",
            "Epoch 20/25\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 0.4227 - accuracy: 0.8521 - val_loss: 0.9384 - val_accuracy: 0.7263\n",
            "Epoch 21/25\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 0.3927 - accuracy: 0.8639 - val_loss: 0.9229 - val_accuracy: 0.7233\n",
            "Epoch 22/25\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 0.3971 - accuracy: 0.8618 - val_loss: 0.9479 - val_accuracy: 0.7231\n",
            "Epoch 23/25\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 0.3730 - accuracy: 0.8714 - val_loss: 0.9526 - val_accuracy: 0.7309\n",
            "Epoch 24/25\n",
            "196/196 [==============================] - 9s 45ms/step - loss: 0.3650 - accuracy: 0.8750 - val_loss: 0.9737 - val_accuracy: 0.7286\n",
            "Epoch 25/25\n",
            "196/196 [==============================] - 9s 46ms/step - loss: 0.3604 - accuracy: 0.8747 - val_loss: 1.0245 - val_accuracy: 0.7315\n",
            "CPU times: user 3min 3s, sys: 6.67 s, total: 3min 9s\n",
            "Wall time: 3min 42s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc8d011d890>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_BvNnozaO0-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}