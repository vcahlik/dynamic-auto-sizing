{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f_deAUKlniFk",
    "outputId": "be625b4c-7f4d-47b1-afa4-c2302557720d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: nvidia-smi\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
    "  print('and then re-execute this cell.')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yKwUwV_NneIo"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-17 10:22:06.824078: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-09-17 10:22:06.824103: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "BOoXBq05neIt"
   },
   "outputs": [],
   "source": [
    "dtype = 'float32'\n",
    "tf.keras.backend.set_floatx(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_BrJPdkBneIv"
   },
   "outputs": [],
   "source": [
    "cifar10 = tf.keras.datasets.cifar10\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "X_train = X_train.astype(dtype) / 255.0\n",
    "y_train = y_train.astype(dtype)\n",
    "X_test = X_test.astype(dtype)  / 255.0\n",
    "y_test = y_test.astype(dtype)\n",
    "\n",
    "X_train = np.reshape(X_train, (-1, 3072))\n",
    "X_test = np.reshape(X_test, (-1, 3072))\n",
    "\n",
    "X = np.concatenate((X_train, X_test), axis=0)\n",
    "y = np.concatenate((y_train, y_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "h5uTvu5kxF-b"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_norm = scaler.transform(X)\n",
    "X_train_norm = scaler.transform(X_train)\n",
    "X_test_norm = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "UTZq4KMpneIv"
   },
   "outputs": [],
   "source": [
    "class Regularizer(tf.keras.regularizers.Regularizer):\n",
    "    def __init__(self, regularization_penalty, regularization_method):\n",
    "        self.regularization_penalty = regularization_penalty\n",
    "        self.regularization_method = regularization_method\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if self.regularization_method == 'weighted_l1':\n",
    "            return self.weighted_l1(x)\n",
    "        elif self.regularization_method == 'group_sparsity':\n",
    "            return self.group_sparsity(x)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Unknown regularization method {self.regularization_method}\")\n",
    "    \n",
    "    def weighted_l1(self, x):\n",
    "        # I.e. for a parameter matrix of 4 input and 10 output neurons:\n",
    "        #\n",
    "        # [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
    "        #  [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
    "        #  [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
    "        #  [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]\n",
    "        #\n",
    "        # The scaling vector could be [0., 1., 2., 3., 4., 5., 6., 7., 8., 9.],\n",
    "        # and the resulting weighted values could be\n",
    "        #\n",
    "        # [[0., 1., 2., 3., 4., 5., 6., 7., 8., 9.],\n",
    "        #  [0., 1., 2., 3., 4., 5., 6., 7., 8., 9.],\n",
    "        #  [0., 1., 2., 3., 4., 5., 6., 7., 8., 9.],\n",
    "        #  [0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]]\n",
    "        #\n",
    "        # Therefore every additional output neuron is regularized more.\n",
    "\n",
    "        scaling_vector = tf.cumsum(tf.constant(self.regularization_penalty, shape=(x.shape[-1],), dtype=dtype), axis=0)\n",
    "        weighted_values = scaling_vector * tf.abs(x)\n",
    "        return tf.reduce_sum(weighted_values)\n",
    "    \n",
    "    def group_sparsity(self, x):\n",
    "        # I.e. for a parameter matrix of 3 input and 5 output neurons:\n",
    "        #\n",
    "        # [[1., 1., 1., 1., 1.],\n",
    "        #  [1., 2., 2., 1., 2.],\n",
    "        #  [2., 2., 3., 1., 3.]]\n",
    "        #\n",
    "        # The resulting vector of group norms is [2., 2., 3., 1., 3.], therefore for\n",
    "        # every output neuron, its incoming connections form a group.\n",
    "\n",
    "        group_norms = tf.norm(x, ord=2, axis=0)\n",
    "        # assert group_norms.shape[0] == x.shape[1]\n",
    "        return self.regularization_penalty * tf.reduce_sum(group_norms)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'regularization_penalty': float(self.regularization_penalty)}\n",
    "\n",
    "\n",
    "class Dense(tf.keras.Model):\n",
    "    def __init__(self, input_units, units, activation, regularization_penalty=0.01, \n",
    "                 regularization_method='weighted_l1', kernel_initializer='glorot_uniform', \n",
    "                 bias_initializer='zeros', regularize=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_units = input_units\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "        self.regularization_penalty = regularization_penalty\n",
    "        self.regularization_method = regularization_method\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.bias_initializer = bias_initializer\n",
    "        \n",
    "        self.A = tf.keras.activations.get(activation)\n",
    "        self.W_init = tf.keras.initializers.get(kernel_initializer)\n",
    "        self.b_init = tf.keras.initializers.get(bias_initializer)\n",
    "        self.regularizer = Regularizer(self.regularization_penalty, self.regularization_method)\n",
    "        \n",
    "        self.W = tf.Variable(\n",
    "            name='W',\n",
    "            initial_value=self.W_init(shape=(input_units, units), dtype=dtype),\n",
    "            trainable=True)\n",
    "        \n",
    "        self.b = tf.Variable(\n",
    "            name='b',\n",
    "            initial_value=self.b_init(shape=(units,), dtype=dtype),\n",
    "            trainable=True)\n",
    "        \n",
    "        if self.regularization_method is not None:\n",
    "            self.add_loss(lambda: self.regularizer(tf.concat([self.W, tf.reshape(self.b, (1, -1))], axis=0)))\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return self.A(tf.matmul(inputs, self.W) + self.b)\n",
    "    \n",
    "    def copy_without_regularization(self):\n",
    "        copy = Dense(\n",
    "            self.input_units, \n",
    "            self.units, \n",
    "            self.activation, \n",
    "            regularization_penalty=self.regularization_penalty, \n",
    "            regularization_method=None, \n",
    "            kernel_initializer=self.kernel_initializer, \n",
    "            bias_initializer=self.bias_initializer\n",
    "        )\n",
    "        copy.W = self.W\n",
    "        copy.b = self.b\n",
    "        return copy\n",
    "\n",
    "\n",
    "class Sequential(tf.keras.Model):\n",
    "    def __init__(self, layers, activation=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lrs = list()\n",
    "        for layer in layers:\n",
    "            self.lrs.append(layer)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.lrs:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def get_layer_sizes(self):\n",
    "        layer_sizes = list()\n",
    "        for l in range(len(self.lrs)):\n",
    "            layer = self.lrs[l]\n",
    "            layer_sizes.append(layer.W.shape[0])\n",
    "            if l == len(self.lrs) - 1:  # Last layer\n",
    "                layer_sizes.append(layer.W.shape[1])\n",
    "        return layer_sizes\n",
    "    \n",
    "    def get_hidden_layer_sizes(self):\n",
    "        return self.get_layer_sizes()[1:-1]\n",
    "    \n",
    "    def remove_regularization(self):\n",
    "        for l in range(len(self.lrs)):\n",
    "            self.lrs[l] = self.lrs[l].copy_without_regularization()\n",
    "    \n",
    "    def get_regularization_penalty(self):\n",
    "        return self.lrs[0].regularizer.regularization_penalty\n",
    "    \n",
    "    def set_regularization_penalty(self, regularization_penalty):\n",
    "        for l in range(0, len(self.lrs) - 1):  # Every layer except of the last is regularized\n",
    "            self.lrs[l].regularizer.regularization_penalty = regularization_penalty\n",
    "    \n",
    "    def prune(self, threshold=0.001):\n",
    "        for l in range(len(self.lrs) - 1):\n",
    "            layer1 = self.lrs[l]\n",
    "            layer2 = self.lrs[l + 1]\n",
    "            \n",
    "            W1 = layer1.W.value()\n",
    "            b1 = layer1.b.value()\n",
    "            W2 = layer2.W.value()\n",
    "\n",
    "            weights_with_biases = tf.concat([W1, tf.reshape(b1, (1, -1))], axis=0)\n",
    "            neurons_are_active = tf.math.reduce_max(tf.abs(weights_with_biases), axis=0) >= threshold\n",
    "            active_neurons_indices = tf.reshape(tf.where(neurons_are_active), (-1,))\n",
    "            \n",
    "            new_W1 = tf.gather(W1, active_neurons_indices, axis=1)\n",
    "            new_b1 = tf.gather(b1, active_neurons_indices, axis=0)\n",
    "            new_W2 = tf.gather(W2, active_neurons_indices, axis=0)\n",
    "\n",
    "            layer1.W = tf.Variable(name='W', initial_value=new_W1, trainable=True)\n",
    "            layer1.b = tf.Variable(name='b', initial_value=new_b1, trainable=True)\n",
    "            layer2.W = tf.Variable(name='W', initial_value=new_W2, trainable=True)\n",
    "    \n",
    "    def grow(self, percentage, min_new_neurons=5, scaling_factor=0.001):   \n",
    "        for l in range(len(self.lrs) - 1):\n",
    "            layer1 = self.lrs[l]\n",
    "            layer2 = self.lrs[l + 1]\n",
    "       \n",
    "            W1 = layer1.W.value()\n",
    "            b1 = layer1.b.value()\n",
    "            W2 = layer2.W.value()\n",
    "\n",
    "            n_new_neurons = max(min_new_neurons, int(W1.shape[1] * percentage))\n",
    "\n",
    "            W1_growth = layer1.W_init(shape=(W1.shape[0], W1.shape[1] + n_new_neurons), dtype=dtype)[:, -n_new_neurons:] * scaling_factor\n",
    "            b1_growth = layer1.b_init(shape=(n_new_neurons,), dtype=dtype)\n",
    "            W2_growth = layer2.W_init(shape=(W2.shape[0] + n_new_neurons, W2.shape[1]), dtype=dtype)[-n_new_neurons:, :] * scaling_factor  # TODO is it better to be multiplying here by scaling_factor? It does help with not increasing the max weights of existing neurons when new neurons are added.\n",
    "\n",
    "            new_W1 = tf.concat([W1, W1_growth], axis=1)\n",
    "            new_b1 = tf.concat([b1, b1_growth], axis=0)\n",
    "            new_W2 = tf.concat([W2, W2_growth], axis=0)\n",
    "\n",
    "            layer1.W = tf.Variable(name='W1', initial_value=new_W1, trainable=True)\n",
    "            layer1.b = tf.Variable(name='b1', initial_value=new_b1, trainable=True)\n",
    "            layer2.W = tf.Variable(name='W2', initial_value=new_W2, trainable=True)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_param_string(weights, bias):\n",
    "        param_string = \"\"\n",
    "        weights_with_bias = tf.concat([weights, tf.reshape(bias, (1, -1))], axis=0)\n",
    "        max_parameters = tf.math.reduce_max(tf.abs(weights_with_bias), axis=0).numpy()\n",
    "        magnitudes = np.floor(np.log10(max_parameters))\n",
    "        for m in magnitudes:\n",
    "            if m > 0:\n",
    "                m = 0\n",
    "            param_string += str(int(-m))\n",
    "        return param_string\n",
    "    \n",
    "    def print_neurons(self):\n",
    "        for layer in self.lrs[:-1]:\n",
    "            print(self.get_param_string(layer.W, layer.b))\n",
    "    \n",
    "    def evaluate(self, x, y, validation_data):\n",
    "        x_val = validation_data[0]\n",
    "        y_val = validation_data[1]\n",
    "\n",
    "        y_pred = self(x)\n",
    "        loss = float(tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(y, y_pred)))\n",
    "        accuracy = float(tf.reduce_mean(tf.keras.metrics.sparse_categorical_accuracy(y, y_pred)))\n",
    "        \n",
    "        y_val_pred = self(x_val)\n",
    "        val_loss = float(tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(y_val, y_val_pred)))\n",
    "        val_accuracy = float(tf.reduce_mean(tf.keras.metrics.sparse_categorical_accuracy(y_val, y_val_pred)))\n",
    "\n",
    "        return loss, accuracy, val_loss, val_accuracy\n",
    "    \n",
    "    def print_epoch_statistics(self, x, y, validation_data, print_neurons):\n",
    "        loss, accuracy, val_loss, val_accuracy = self.evaluate(x, y, validation_data)\n",
    "        print(f\"loss: {loss} - accuracy: {accuracy} - val_loss: {val_loss} - val_accuracy: {val_accuracy} - penalty: {model.get_regularization_penalty()}\")\n",
    "        hidden_layer_sizes = self.get_hidden_layer_sizes()\n",
    "        print(f\"hidden layer sizes: {hidden_layer_sizes}, total neurons: {sum(hidden_layer_sizes)}\")\n",
    "        if print_neurons:\n",
    "            self.print_neurons()\n",
    "    \n",
    "    def update_history(self, x, y, validation_data, history):\n",
    "        loss, accuracy, val_loss, val_accuracy = self.evaluate(x, y, validation_data)\n",
    "        history['loss'].append(loss)\n",
    "        history['accuracy'].append(accuracy)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_accuracy'].append(val_accuracy)\n",
    "\n",
    "    def fit(self, x, y, optimizer, epochs, self_scaling_epochs, batch_size, min_new_neurons, validation_data, pruning_threshold=0.001, \n",
    "            regularization_penalty_multiplier=1., stall_coefficient=1, growth_percentage=0.2, mini_epochs_per_epoch=1, verbose=True, print_neurons=False):\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "        train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "        history = {\n",
    "            'loss': list(),\n",
    "            'accuracy': list(),\n",
    "            'val_loss': list(),\n",
    "            'val_accuracy': list(),\n",
    "        }\n",
    "\n",
    "        best_val_loss = np.inf\n",
    "        training_stalled = False\n",
    "        for epoch in range(epochs):\n",
    "            if verbose:\n",
    "                print(\"##########################################################\")\n",
    "                print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "            if epoch < self_scaling_epochs:\n",
    "                if verbose:\n",
    "                    print(\"Before growing:\")\n",
    "                    self.print_epoch_statistics(x, y, validation_data, print_neurons)\n",
    "\n",
    "                loss, accuracy, val_loss, val_accuracy = self.evaluate(x, y, validation_data)\n",
    "                if val_loss >= best_val_loss * stall_coefficient:\n",
    "                    if not training_stalled:\n",
    "                        penalty = self.get_regularization_penalty() * regularization_penalty_multiplier\n",
    "                        self.set_regularization_penalty(penalty)\n",
    "                        training_stalled = True\n",
    "                else:\n",
    "                    best_val_loss = val_loss\n",
    "                    training_stalled = False\n",
    "\n",
    "                self.grow(percentage=growth_percentage, min_new_neurons=min_new_neurons, scaling_factor=pruning_threshold)\n",
    "                if verbose:\n",
    "                    print(\"After growing:\")\n",
    "                    self.print_epoch_statistics(x, y, validation_data, print_neurons)\n",
    "            \n",
    "            if epoch == self_scaling_epochs:\n",
    "                self.remove_regularization()\n",
    "\n",
    "            for mini_epoch in range(mini_epochs_per_epoch):\n",
    "                for step, (x_batch, y_batch) in enumerate(train_dataset):\n",
    "                    with tf.GradientTape() as tape:\n",
    "                        y_pred = self(x_batch, training=True)\n",
    "                        loss_value = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(y_batch, y_pred))\n",
    "                        loss_value += sum(self.losses)\n",
    "\n",
    "                    grads = tape.gradient(loss_value, self.trainable_variables)\n",
    "                    optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "            \n",
    "            if epoch < self_scaling_epochs:\n",
    "                if verbose:\n",
    "                    print(\"Before pruning:\")\n",
    "                    self.print_epoch_statistics(x, y, validation_data, print_neurons)\n",
    "                self.prune(threshold=pruning_threshold)\n",
    "                if verbose:\n",
    "                    print(\"After pruning:\")\n",
    "                    self.print_epoch_statistics(x, y, validation_data, print_neurons)\n",
    "            else:\n",
    "                if verbose:\n",
    "                    self.print_epoch_statistics(x, y, validation_data, print_neurons)\n",
    "            \n",
    "            self.update_history(x, y, validation_data, history)\n",
    "\n",
    "        return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l1MrQXUTFwOe"
   },
   "source": [
    "# Convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ByLRkOAPoGcc"
   },
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "self_scaling_epochs = 2\n",
    "batch_size = 32\n",
    "min_new_neurons = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IoJydQX7bsmq",
    "outputId": "0095522c-339f-4492-9664-e9b8ce0bec1c",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################################################\n",
      "Epoch 1/2\n",
      "Before growing:\n",
      "loss: 2.888000965118408 - accuracy: 0.08420000225305557 - val_loss: 2.8840060234069824 - val_accuracy: 0.08309999853372574 - penalty: 0.001\n",
      "hidden layer sizes: [30, 30], total neurons: 60\n",
      "After growing:\n",
      "loss: 2.8880012035369873 - accuracy: 0.08420000225305557 - val_loss: 2.8840060234069824 - val_accuracy: 0.08309999853372574 - penalty: 0.001\n",
      "hidden layer sizes: [50, 50], total neurons: 100\n",
      "Before pruning:\n",
      "loss: 1.8788080215454102 - accuracy: 0.3310999870300293 - val_loss: 1.882436752319336 - val_accuracy: 0.32659998536109924 - penalty: 0.001\n",
      "hidden layer sizes: [50, 50], total neurons: 100\n",
      "After pruning:\n",
      "loss: 1.878679871559143 - accuracy: 0.3349800109863281 - val_loss: 1.8801465034484863 - val_accuracy: 0.33469998836517334 - penalty: 0.001\n",
      "hidden layer sizes: [9, 30], total neurons: 39\n",
      "##########################################################\n",
      "Epoch 2/2\n",
      "Before growing:\n",
      "loss: 1.878679871559143 - accuracy: 0.3349800109863281 - val_loss: 1.8801465034484863 - val_accuracy: 0.33469998836517334 - penalty: 0.001\n",
      "hidden layer sizes: [9, 30], total neurons: 39\n",
      "After growing:\n",
      "loss: 1.878679871559143 - accuracy: 0.3349800109863281 - val_loss: 1.8801465034484863 - val_accuracy: 0.33469998836517334 - penalty: 0.001\n",
      "hidden layer sizes: [29, 50], total neurons: 79\n",
      "Before pruning:\n",
      "loss: 1.8331375122070312 - accuracy: 0.34477999806404114 - val_loss: 1.838687539100647 - val_accuracy: 0.34459999203681946 - penalty: 0.001\n",
      "hidden layer sizes: [29, 50], total neurons: 79\n",
      "After pruning:\n",
      "loss: 1.8319804668426514 - accuracy: 0.34516000747680664 - val_loss: 1.8372929096221924 - val_accuracy: 0.34470000863075256 - penalty: 0.001\n",
      "hidden layer sizes: [7, 28], total neurons: 35\n",
      "CPU times: user 1min 11s, sys: 6.85 s, total: 1min 18s\n",
      "Wall time: 1min 3s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': [1.878679871559143, 1.8319804668426514],\n",
       " 'accuracy': [0.3349800109863281, 0.34516000747680664],\n",
       " 'val_loss': [1.8801465034484863, 1.8372929096221924],\n",
       " 'val_accuracy': [0.33469998836517334, 0.34470000863075256]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = Sequential([\n",
    "        Dense(3072, 30, activation='selu', regularization_penalty=0.001, regularization_method='weighted_l1', kernel_initializer='lecun_normal'),\n",
    "        Dense(30, 30, activation='selu', regularization_penalty=0.001, regularization_method='weighted_l1', kernel_initializer='lecun_normal'),\n",
    "        Dense(30, 10, activation='softmax', regularization_penalty=0., regularization_method='weighted_l1', kernel_initializer='lecun_normal'),\n",
    "    ])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002)\n",
    "\n",
    "model.fit(X_train_norm, y_train, optimizer, epochs, self_scaling_epochs, batch_size, \n",
    "          min_new_neurons, validation_data=(X_test_norm, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "IbyAxn8RZzZJ"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(3072, activation='selu', kernel_initializer='lecun_normal'),\n",
    "    tf.keras.layers.Dense(117, activation='selu', kernel_initializer='lecun_normal'),\n",
    "    tf.keras.layers.Dense(109, activation='selu', kernel_initializer='lecun_normal'),\n",
    "    tf.keras.layers.Dense(104, activation='selu', kernel_initializer='lecun_normal'),\n",
    "    tf.keras.layers.Dense(139, activation='selu', kernel_initializer='lecun_normal'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax', kernel_initializer='lecun_normal'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "3JDpPwkrZ2xx"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.0002), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4l2I_etAZ4is",
    "outputId": "de994e1b-e913-4c25-9457-55e4d725f2ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "263/263 [==============================] - 1s 4ms/step - loss: 0.4151 - accuracy: 0.8404 - val_loss: 0.3217 - val_accuracy: 0.8722\n",
      "Epoch 2/20\n",
      "263/263 [==============================] - 1s 4ms/step - loss: 0.2258 - accuracy: 0.9095 - val_loss: 0.2861 - val_accuracy: 0.8914\n",
      "Epoch 3/20\n",
      "263/263 [==============================] - 1s 4ms/step - loss: 0.1857 - accuracy: 0.9270 - val_loss: 0.2878 - val_accuracy: 0.8933\n",
      "Epoch 4/20\n",
      "263/263 [==============================] - 1s 4ms/step - loss: 0.1265 - accuracy: 0.9476 - val_loss: 0.2857 - val_accuracy: 0.8989\n",
      "Epoch 5/20\n",
      "263/263 [==============================] - 1s 4ms/step - loss: 0.0938 - accuracy: 0.9640 - val_loss: 0.2879 - val_accuracy: 0.9103\n",
      "Epoch 6/20\n",
      "263/263 [==============================] - 1s 4ms/step - loss: 0.0858 - accuracy: 0.9680 - val_loss: 0.3245 - val_accuracy: 0.9025\n",
      "Epoch 7/20\n",
      "263/263 [==============================] - 1s 4ms/step - loss: 0.0887 - accuracy: 0.9683 - val_loss: 0.3448 - val_accuracy: 0.8972\n",
      "Epoch 8/20\n",
      "263/263 [==============================] - 1s 4ms/step - loss: 0.0607 - accuracy: 0.9782 - val_loss: 0.3225 - val_accuracy: 0.9106\n",
      "Epoch 9/20\n",
      "263/263 [==============================] - 1s 4ms/step - loss: 0.0666 - accuracy: 0.9754 - val_loss: 0.3264 - val_accuracy: 0.9108\n",
      "Epoch 10/20\n",
      "263/263 [==============================] - 1s 4ms/step - loss: 0.0671 - accuracy: 0.9746 - val_loss: 0.4087 - val_accuracy: 0.8989\n",
      "Epoch 11/20\n",
      "263/263 [==============================] - 1s 4ms/step - loss: 0.0510 - accuracy: 0.9824 - val_loss: 0.4092 - val_accuracy: 0.9019\n",
      "Epoch 12/20\n",
      "263/263 [==============================] - 1s 4ms/step - loss: 0.0449 - accuracy: 0.9842 - val_loss: 0.3984 - val_accuracy: 0.9078\n",
      "Epoch 13/20\n",
      "263/263 [==============================] - 1s 4ms/step - loss: 0.0445 - accuracy: 0.9840 - val_loss: 0.3977 - val_accuracy: 0.9083\n",
      "Epoch 14/20\n",
      "263/263 [==============================] - 1s 4ms/step - loss: 0.0357 - accuracy: 0.9868 - val_loss: 0.3933 - val_accuracy: 0.9164\n",
      "Epoch 15/20\n",
      "263/263 [==============================] - 1s 4ms/step - loss: 0.0260 - accuracy: 0.9905 - val_loss: 0.4204 - val_accuracy: 0.9083\n",
      "Epoch 16/20\n",
      "263/263 [==============================] - 1s 4ms/step - loss: 0.0748 - accuracy: 0.9767 - val_loss: 0.4453 - val_accuracy: 0.8967\n",
      "Epoch 17/20\n",
      "263/263 [==============================] - 1s 4ms/step - loss: 0.0558 - accuracy: 0.9813 - val_loss: 0.3681 - val_accuracy: 0.9131\n",
      "Epoch 18/20\n",
      "263/263 [==============================] - 1s 4ms/step - loss: 0.0160 - accuracy: 0.9942 - val_loss: 0.4286 - val_accuracy: 0.9156\n",
      "Epoch 19/20\n",
      "263/263 [==============================] - 1s 4ms/step - loss: 0.0249 - accuracy: 0.9906 - val_loss: 0.4434 - val_accuracy: 0.9092\n",
      "Epoch 20/20\n",
      "263/263 [==============================] - 1s 4ms/step - loss: 0.0282 - accuracy: 0.9911 - val_loss: 0.4694 - val_accuracy: 0.9075\n",
      "CPU times: user 21.8 s, sys: 1.79 s, total: 23.6 s\n",
      "Wall time: 41.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdf5146af10>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model.fit(X_norm_automobiles_dogs_train, y_automobiles_dogs_train, epochs=20, validation_data=(X_norm_automobiles_dogs_test, y_automobiles_dogs_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x_BvNnozaO0-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "tf_multi_layer_ssnet_inverse.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
