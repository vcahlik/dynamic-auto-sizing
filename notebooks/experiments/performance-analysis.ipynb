{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf_multi_layer_ssnet_inverse.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "background_execution": "on"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_deAUKlniFk",
        "outputId": "792836e9-1202-4261-a973-dbe68c76aecd"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Jan 11 16:21:21 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.46       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    25W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKwUwV_NneIo"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "from enum import Enum\n",
        "import imageio\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "dtype = 'float32'\n",
        "tf.keras.backend.set_floatx(dtype)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the Tiny ImageNet dataset\n",
        "\n",
        "# ! git clone https://github.com/seshuad/IMagenet\n",
        "# ! ls 'IMagenet/tiny-imagenet-200/'"
      ],
      "metadata": {
        "id": "jw1XOkejw4zK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTZq4KMpneIv"
      },
      "source": [
        "################################################################################\n",
        "# DATASETS\n",
        "################################################################################\n",
        "\n",
        "\n",
        "class Dataset:\n",
        "    def __init__(self, X_train, y_train, X_test, y_test, shape, shape_flattened):\n",
        "        X_train = X_train.astype(dtype) / 255.0\n",
        "        y_train = y_train.astype(dtype)\n",
        "        X_test = X_test.astype(dtype)  / 255.0\n",
        "        y_test = y_test.astype(dtype)\n",
        "\n",
        "        X_train = np.reshape(X_train, shape_flattened)\n",
        "        X_test = np.reshape(X_test, shape_flattened)\n",
        "\n",
        "        X = np.concatenate((X_train, X_test), axis=0)\n",
        "        y = np.concatenate((y_train, y_test), axis=0)\n",
        "\n",
        "        from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(X_train)  # Scaling each feature independently\n",
        "\n",
        "        X_norm = scaler.transform(X)\n",
        "        X_train_norm = scaler.transform(X_train)\n",
        "        X_test_norm = scaler.transform(X_test)\n",
        "\n",
        "        X_norm = np.reshape(X_norm, shape)\n",
        "        X_train_norm = np.reshape(X_train_norm, shape)\n",
        "        X_test_norm = np.reshape(X_test_norm, shape)\n",
        "\n",
        "        self.X_norm = X_norm\n",
        "        self.y = y\n",
        "        self.X_train_norm = X_train_norm\n",
        "        self.y_train = y_train\n",
        "        self.X_test_norm = X_test_norm\n",
        "        self.y_test = y_test\n",
        "\n",
        "\n",
        "def get_cifar_10_dataset():\n",
        "    cifar10 = tf.keras.datasets.cifar10\n",
        "    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "    shape = (-1, 32, 32, 3)\n",
        "    shape_flattened = (-1, 3072)  # Scaling each feature independently\n",
        "    return Dataset(X_train, y_train, X_test, y_test, shape=shape, shape_flattened=shape_flattened)\n",
        "\n",
        "\n",
        "def get_cifar_100_dataset():\n",
        "    cifar100 = tf.keras.datasets.cifar100\n",
        "    (X_train, y_train), (X_test, y_test) = cifar100.load_data()\n",
        "    shape = (-1, 32, 32, 3)\n",
        "    shape_flattened = (-1, 3072)  # Scaling each feature independently\n",
        "    return Dataset(X_train, y_train, X_test, y_test, shape=shape, shape_flattened=shape_flattened)\n",
        "\n",
        "\n",
        "def get_svhn_dataset():\n",
        "    from scipy import io\n",
        "\n",
        "    X_train = io.loadmat(train_filename, variable_names='X').get('X')\n",
        "    y_train = io.loadmat(train_filename, variable_names='y').get('y')\n",
        "    X_test = io.loadmat(test_filename, variable_names='X').get('X')\n",
        "    y_test = io.loadmat(test_filename, variable_names='y').get('y')\n",
        "\n",
        "    X_train = np.moveaxis(X_train, -1, 0)\n",
        "    y_train -= 1\n",
        "    X_test = np.moveaxis(X_test, -1, 0)\n",
        "    y_test -= 1\n",
        "\n",
        "    shape = (-1, 32, 32, 3)\n",
        "    shape_flattened = (-1, 3072)  # Scaling each feature independently\n",
        "    return Dataset(X_train, y_train, X_test, y_test, shape=shape, shape_flattened=shape_flattened)\n",
        "\n",
        "\n",
        "def get_tiny_imagenet_dataset():\n",
        "    path = 'IMagenet/tiny-imagenet-200/'\n",
        "\n",
        "    id_dict = {}\n",
        "    for i, line in enumerate(open(path + 'wnids.txt', 'r')):\n",
        "        id_dict[line.replace('\\n', '')] = i\n",
        "\n",
        "    train_data = list()\n",
        "    test_data = list()\n",
        "    train_labels = list()\n",
        "    test_labels = list()\n",
        "\n",
        "    for key, value in id_dict.items():\n",
        "        train_data += [imageio.imread(path + 'train/{}/images/{}_{}.JPEG'.format(key, key, str(i)), pilmode='RGB') for i in range(500)]\n",
        "        train_labels_ = np.array([[0]*200]*500)\n",
        "        train_labels_[:, value] = 1\n",
        "        train_labels += train_labels_.tolist()\n",
        "\n",
        "    for line in open(path + 'val/val_annotations.txt'):\n",
        "        img_name, class_id = line.split('\\t')[:2]\n",
        "        test_data.append(imageio.imread(path + 'val/images/{}'.format(img_name), pilmode='RGB'))\n",
        "        test_labels_ = np.array([[0]*200])\n",
        "        test_labels_[0, id_dict[class_id]] = 1\n",
        "        test_labels += test_labels_.tolist()\n",
        "\n",
        "    X_train = np.array(train_data)\n",
        "    y_train = np.argmax(np.array(train_labels), axis=1)\n",
        "    X_test = np.array(test_data)\n",
        "    y_test = np.argmax(np.array(test_labels), axis=1)\n",
        "\n",
        "    shape = (-1, 64, 64, 3)\n",
        "    shape_flattened = (-1, 12288)  # Scaling each feature independently\n",
        "    return Dataset(X_train, y_train, X_test, y_test, shape=shape, shape_flattened=shape_flattened)\n",
        "\n",
        "\n",
        "def get_mnist_dataset():\n",
        "    mnist = tf.keras.datasets.mnist\n",
        "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "    shape = (-1, 28, 28, 1)\n",
        "    shape_flattened = (-1, 1)  # Scaling all features together\n",
        "    return Dataset(X_train, y_train, X_test, y_test, shape=shape, shape_flattened=shape_flattened)\n",
        "\n",
        "\n",
        "def get_fashion_mnist_dataset():\n",
        "    fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "    (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "    shape = (-1, 28, 28, 1)\n",
        "    shape_flattened = (-1, 1)  # Scaling all features together\n",
        "    return Dataset(X_train, y_train, X_test, y_test, shape=shape, shape_flattened=shape_flattened)\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# REGULARIZERS\n",
        "################################################################################\n",
        "\n",
        "\n",
        "class Regularizer(tf.keras.regularizers.Regularizer):\n",
        "    def __init__(self, regularization_penalty, regularization_method):\n",
        "        self.regularization_penalty = regularization_penalty\n",
        "        self.regularization_method = regularization_method\n",
        "        self.n_new_neurons = 0\n",
        "        self.scaling_tensor = None\n",
        "        if self.regularization_method == 'weighted_l1_reordered':\n",
        "            self.update_scaling_tensor = True\n",
        "        else:\n",
        "            self.update_scaling_tensor = None\n",
        "\n",
        "    def __call__(self, x):\n",
        "        if self.regularization_method == 'weighted_l1':\n",
        "            return self.weighted_l1(x)\n",
        "        elif self.regularization_method == 'weighted_l1_reordered':\n",
        "            return self.weighted_l1_reordered(x)\n",
        "        elif self.regularization_method == 'group_sparsity':\n",
        "            return self.group_sparsity(x)\n",
        "        elif self.regularization_method == 'l1':\n",
        "            return self.l1(x)\n",
        "        else:\n",
        "            raise NotImplementedError(f\"Unknown regularization method {self.regularization_method}\")\n",
        "    \n",
        "    def weighted_l1(self, x):\n",
        "        # I.e. for a parameter matrix of 4 input and 10 output neurons:\n",
        "        #\n",
        "        # [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
        "        #  [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
        "        #  [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
        "        #  [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]\n",
        "        #\n",
        "        # the scaling tensor, as well as the resulting weighted values, could be:\n",
        "        #\n",
        "        # [[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n",
        "        #  [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n",
        "        #  [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n",
        "        #  [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]]\n",
        "        #\n",
        "        # Therefore every additional output neuron is regularized more.\n",
        "\n",
        "        scaling_tensor = tf.cumsum(tf.constant(self.regularization_penalty, shape=x.shape, dtype=dtype), axis=-1)\n",
        "        weighted_values = scaling_tensor * tf.abs(x)\n",
        "        return tf.reduce_sum(weighted_values)\n",
        "    \n",
        "    def weighted_l1_reordered(self, x):\n",
        "        # I.e. for a parameter matrix of 4 input and 10 output neurons:\n",
        "        #\n",
        "        # [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
        "        #  [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
        "        #  [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
        "        #  [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]\n",
        "        #\n",
        "        # the scaling tensor, as well as the resulting weighted values, could be:\n",
        "        #\n",
        "        # [[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n",
        "        #  [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n",
        "        #  [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.],\n",
        "        #  [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]]\n",
        "        #\n",
        "        # Therefore every additional output neuron is regularized more.\n",
        "\n",
        "        if self.update_scaling_tensor:\n",
        "            scaling_tensor_raw = tf.cumsum(tf.constant(self.regularization_penalty, shape=x.shape, dtype=dtype), axis=-1)\n",
        "\n",
        "            scaling_tensor_old_neurons = scaling_tensor_raw[:, :-self.n_new_neurons]\n",
        "            scaling_tensor_new_neurons = scaling_tensor_raw[:, -self.n_new_neurons:]\n",
        "            scaling_tensor_old_neurons_shuffled = tf.transpose(tf.random.shuffle(tf.transpose(scaling_tensor_old_neurons)))\n",
        "            self.scaling_tensor = tf.concat([scaling_tensor_old_neurons_shuffled, scaling_tensor_new_neurons], axis=-1)\n",
        "            self.update_scaling_tensor = False\n",
        "\n",
        "        weighted_values = self.scaling_tensor * tf.abs(x)\n",
        "        return tf.reduce_sum(weighted_values)\n",
        "    \n",
        "    def group_sparsity(self, x):\n",
        "        # I.e. for a parameter matrix of 3 input and 5 output neurons:\n",
        "        #\n",
        "        # [[1., 1., 1., 1., 1.],\n",
        "        #  [1., 2., 2., 1., 2.],\n",
        "        #  [2., 2., 3., 1., 3.]]\n",
        "        #\n",
        "        # The resulting vector of group norms is [2., 2., 3., 1., 3.], therefore for\n",
        "        # every output neuron, its incoming connections form a group.\n",
        "\n",
        "        group_norms = tf.norm(x, ord=2, axis=0)\n",
        "        # assert group_norms.shape[0] == x.shape[1]\n",
        "        return self.regularization_penalty * tf.reduce_sum(group_norms)\n",
        "    \n",
        "    def l1(self, x):\n",
        "        weighted_values = self.regularization_penalty * tf.abs(x)\n",
        "        return tf.reduce_sum(weighted_values)\n",
        "    \n",
        "    def prune(self):\n",
        "        self.n_new_neurons = 0\n",
        "        if self.regularization_method == 'weighted_l1_reordered':\n",
        "            self.update_scaling_tensor = True\n",
        "    \n",
        "    def grow(self, n_new_neurons):\n",
        "        self.n_new_neurons = n_new_neurons\n",
        "        if self.regularization_method == 'weighted_l1_reordered':\n",
        "            self.update_scaling_tensor = True\n",
        "\n",
        "    def get_config(self):\n",
        "        return {'regularization_penalty': float(self.regularization_penalty)}\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# LAYERS\n",
        "################################################################################\n",
        "\n",
        "\n",
        "class CustomLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_shape):\n",
        "        super().__init__()\n",
        "\n",
        "        self.inpt_shp = input_shape\n",
        "\n",
        "\n",
        "class Dense(CustomLayer):\n",
        "    def __init__(self, units, activation, regularization_penalty=0.01, \n",
        "                 regularization_method='weighted_l1', kernel_initializer='glorot_uniform', \n",
        "                 bias_initializer='zeros', input_shape=None, fixed_size=False):\n",
        "        super().__init__(input_shape)\n",
        "\n",
        "        self.units = units\n",
        "        self.activation = activation\n",
        "        self.regularization_penalty = regularization_penalty\n",
        "        self.regularization_method = regularization_method\n",
        "        self.kernel_initializer = kernel_initializer\n",
        "        self.bias_initializer = bias_initializer\n",
        "        self.fixed_size = fixed_size\n",
        "        \n",
        "        self.A = tf.keras.activations.get(activation)\n",
        "        self.W_init = tf.keras.initializers.get(kernel_initializer)\n",
        "        self.b_init = tf.keras.initializers.get(bias_initializer)\n",
        "        self.regularizer = Regularizer(self.regularization_penalty, self.regularization_method)\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        input_units = input_shape[-1]\n",
        "\n",
        "        self.W = tf.Variable(\n",
        "            name='W',\n",
        "            initial_value=self.W_init(shape=(input_units, self.units), dtype=dtype),\n",
        "            trainable=True)\n",
        "        \n",
        "        self.b = tf.Variable(\n",
        "            name='b',\n",
        "            initial_value=self.b_init(shape=(self.units,), dtype=dtype),\n",
        "            trainable=True)\n",
        "        \n",
        "        if self.regularization_method is not None:\n",
        "            self.add_loss(lambda: self.regularizer(tf.concat([self.W, tf.reshape(self.b, (1, -1))], axis=0)))\n",
        "    \n",
        "    def call(self, inputs, training=None):\n",
        "        return self.A(tf.matmul(inputs, self.W) + self.b)\n",
        "\n",
        "    def get_size(self):\n",
        "        return self.W.shape[0], self.W.shape[1]\n",
        "    \n",
        "    def prune(self, threshold, active_input_units_indices):\n",
        "        # Remove connections from pruned units in previous layer\n",
        "        new_W = tf.gather(self.W.value(), active_input_units_indices, axis=0)\n",
        "\n",
        "        if self.fixed_size:\n",
        "            active_output_neurons_indices = list(range(new_W.shape[1]))\n",
        "        else:\n",
        "            # Prune units in this layer\n",
        "            weights_with_biases = tf.concat([new_W, tf.reshape(self.b.value(), (1, -1))], axis=0)\n",
        "            neurons_are_active = tf.math.reduce_max(tf.abs(weights_with_biases), axis=0) >= threshold\n",
        "            active_output_neurons_indices = tf.reshape(tf.where(neurons_are_active), (-1,))\n",
        "            \n",
        "            new_W = tf.gather(new_W, active_output_neurons_indices, axis=1)\n",
        "            new_b = tf.gather(self.b.value(), active_output_neurons_indices, axis=0)\n",
        "\n",
        "            self.b = tf.Variable(name='b', initial_value=new_b, trainable=True)\n",
        "\n",
        "        self.W = tf.Variable(name='W', initial_value=new_W, trainable=True)\n",
        "\n",
        "        self.regularizer.prune()\n",
        "        return active_output_neurons_indices\n",
        "    \n",
        "    def grow(self, n_new_input_units, percentage, min_new_units, scaling_factor):\n",
        "        if n_new_input_units > 0:\n",
        "            # Add connections to grown units in previous layer\n",
        "            W_growth = self.W_init(shape=(self.W.shape[0] + n_new_input_units, self.W.shape[1]), dtype=dtype)[-n_new_input_units:, :] * scaling_factor  # TODO is it better to be multiplying here by scaling_factor? It does help with not increasing the max weights of existing neurons when new neurons are added.\n",
        "            new_W = tf.concat([self.W.value(), W_growth], axis=0)\n",
        "        else:\n",
        "            new_W = self.W.value()\n",
        "\n",
        "        if self.fixed_size:\n",
        "            n_new_output_units = 0\n",
        "        else:\n",
        "            # Grow new units in this layer\n",
        "            n_new_output_units = max(min_new_units, int(new_W.shape[1] * percentage))\n",
        "            if n_new_output_units > 0:\n",
        "                W_growth = self.W_init(shape=(new_W.shape[0], new_W.shape[1] + n_new_output_units), dtype=dtype)[:, -n_new_output_units:] * scaling_factor\n",
        "                b_growth = self.b_init(shape=(n_new_output_units,), dtype=dtype)  # TODO for all possible bias initializers to work properly, the whole bias vector should be initialized at once\n",
        "                new_W = tf.concat([new_W, W_growth], axis=1)\n",
        "                new_b = tf.concat([self.b.value(), b_growth], axis=0)\n",
        "\n",
        "                self.b = tf.Variable(name='b', initial_value=new_b, trainable=True)\n",
        "\n",
        "        self.W = tf.Variable(name='W', initial_value=new_W, trainable=True)\n",
        "\n",
        "        self.regularizer.grow(n_new_output_units)\n",
        "        return n_new_output_units\n",
        "    \n",
        "    def get_param_string():\n",
        "        param_string = \"\"\n",
        "        weights_with_bias = tf.concat([self.W, tf.reshape(self.b, (1, -1))], axis=0)\n",
        "        max_parameters = tf.math.reduce_max(tf.abs(weights_with_bias), axis=0).numpy()\n",
        "        magnitudes = np.floor(np.log10(max_parameters))\n",
        "        for m in magnitudes:\n",
        "            if m > 0:\n",
        "                m = 0\n",
        "            param_string += str(int(-m))\n",
        "        return param_string\n",
        "\n",
        "\n",
        "class Conv2D(CustomLayer):\n",
        "    def __init__(self, filters, filter_size, activation, strides=(1, 1), \n",
        "                 padding='SAME', regularization_penalty=0.01, \n",
        "                 regularization_method='weighted_l1', kernel_initializer='glorot_uniform',\n",
        "                 bias_initializer='zeros', input_shape=None, fixed_size=False):\n",
        "        super().__init__(input_shape)\n",
        "    \n",
        "        self.filters = filters\n",
        "        self.filter_size = filter_size\n",
        "        self.activation = activation\n",
        "        self.strides = strides\n",
        "        self.padding = padding\n",
        "        self.regularization_penalty = regularization_penalty\n",
        "        self.regularization_method = regularization_method\n",
        "        self.kernel_initializer = kernel_initializer\n",
        "        self.bias_initializer = bias_initializer\n",
        "        self.fixed_size = fixed_size\n",
        "        \n",
        "        self.A = tf.keras.activations.get(activation)\n",
        "        self.F_init = tf.keras.initializers.get(kernel_initializer)\n",
        "        self.b_init = tf.keras.initializers.get(bias_initializer)\n",
        "        self.regularizer = Regularizer(self.regularization_penalty, self.regularization_method)\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        input_filters = input_shape[-1]\n",
        "\n",
        "        self.F = tf.Variable(\n",
        "            name='F',\n",
        "            initial_value=self.F_init(\n",
        "                shape=(self.filter_size[0], self.filter_size[1], input_filters, self.filters), dtype=dtype\n",
        "            ),\n",
        "            trainable=True)\n",
        "        \n",
        "        self.b = tf.Variable(\n",
        "            name='b',\n",
        "            initial_value=self.b_init(shape=(self.filters,), dtype=dtype),\n",
        "            trainable=True)\n",
        "\n",
        "        if self.regularization_method is not None:\n",
        "            self.add_loss(lambda: self.regularizer(tf.concat([tf.reshape(self.F, (-1, self.F.shape[-1])), tf.reshape(self.b, (1, -1))], axis=0)))\n",
        "    \n",
        "    def call(self, inputs, training=None):\n",
        "        y = tf.nn.conv2d(inputs, self.F, strides=self.strides, padding=self.padding)\n",
        "        y = tf.nn.bias_add(y, self.b)\n",
        "        y = self.A(y)\n",
        "        return y\n",
        "    \n",
        "    def get_size(self):\n",
        "        return self.F.shape[-2], self.F.shape[-1]\n",
        "    \n",
        "    def prune(self, threshold, active_input_units_indices):\n",
        "        # Remove connections from pruned units in previous layer\n",
        "        new_F = tf.gather(self.F.value(), active_input_units_indices, axis=-2)\n",
        "\n",
        "        if self.fixed_size:\n",
        "            active_output_filters_indices = list(range(new_F.shape[-1]))\n",
        "        else:\n",
        "            # Prune units in this layer\n",
        "            F_reduced_max = tf.reshape(tf.math.reduce_max(tf.abs(new_F), axis=(0, 1, 2)), (1, -1))\n",
        "            F_reduced_max_with_biases = tf.concat([F_reduced_max, tf.reshape(self.b.value(), (1, -1))], axis=0)\n",
        "            filters_are_active = tf.math.reduce_max(tf.abs(F_reduced_max_with_biases), axis=0) >= threshold\n",
        "            active_output_filters_indices = tf.reshape(tf.where(filters_are_active), (-1,))\n",
        "            \n",
        "            new_F = tf.gather(new_F, active_output_filters_indices, axis=-1)\n",
        "            new_b = tf.gather(self.b.value(), active_output_filters_indices, axis=0)\n",
        "\n",
        "            self.b = tf.Variable(name='b', initial_value=new_b, trainable=True)\n",
        "\n",
        "        self.F = tf.Variable(name='F', initial_value=new_F, trainable=True)\n",
        "\n",
        "        self.regularizer.prune()\n",
        "        return active_output_filters_indices\n",
        "\n",
        "    def grow(self, n_new_input_units, percentage, min_new_units, scaling_factor):\n",
        "        if n_new_input_units > 0:\n",
        "            # Add connections to grown units in previous layer\n",
        "            F_growth = self.F_init(shape=(self.F.shape[0], self.F.shape[1], self.F.shape[2] + n_new_input_units, self.F.shape[3]), dtype=dtype)[:, :, -n_new_input_units:, :] * scaling_factor  # TODO is it better to be multiplying here by scaling_factor? It does help with not increasing the max weights of existing neurons when new neurons are added.\n",
        "            new_F = tf.concat([self.F.value(), F_growth], axis=-2)\n",
        "        else:\n",
        "            new_F = self.F.value()\n",
        "\n",
        "        if self.fixed_size:\n",
        "            n_new_output_units = 0\n",
        "        else:\n",
        "            # Grow new units in this layer\n",
        "            n_new_output_units = max(min_new_units, int(new_F.shape[-1] * percentage))\n",
        "            if n_new_output_units > 0:\n",
        "                F_growth = self.F_init(shape=(new_F.shape[0], new_F.shape[1], new_F.shape[2], new_F.shape[3] + n_new_output_units), dtype=dtype)[:, :, :, -n_new_output_units:] * scaling_factor\n",
        "                b_growth = self.b_init(shape=(n_new_output_units,), dtype=dtype)  # TODO for all possible bias initializers to work properly, the whole bias vector should be initialized at once\n",
        "                new_F = tf.concat([new_F, F_growth], axis=-1)\n",
        "                new_b = tf.concat([self.b.value(), b_growth], axis=0)\n",
        "\n",
        "                self.b = tf.Variable(name='b', initial_value=new_b, trainable=True)\n",
        "\n",
        "        self.F = tf.Variable(name='F', initial_value=new_F, trainable=True)\n",
        "\n",
        "        self.regularizer.grow(n_new_output_units)\n",
        "        return n_new_output_units\n",
        "\n",
        "    def get_param_string():\n",
        "        param_string = \"\"\n",
        "        # TODO\n",
        "        return param_string\n",
        "\n",
        "\n",
        "class Flatten(tf.keras.Model):\n",
        "    def call(self, inputs, training=None):\n",
        "        return tf.reshape(tf.transpose(inputs, perm=[0, 3, 1, 2]), (inputs.shape[0], -1))\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# MODELS\n",
        "################################################################################\n",
        "\n",
        "\n",
        "class EpochType(Enum):\n",
        "    DYNAMIC = 0\n",
        "    STATIC_WITH_REGULARIZATION = 1\n",
        "    STATIC_NO_REGULARIZATION = 2\n",
        "    PRUNING_ONLY = 3\n",
        "    GROWTH_ONLY = 4\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.value)\n",
        "\n",
        "\n",
        "class Schedule:\n",
        "    def __init__(self, epoch_types):\n",
        "        self.epoch_types = epoch_types\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self.epoch_types.__iter__()\n",
        "    \n",
        "    def __str__(self):\n",
        "        return ''.join([str(epoch_type.value) for epoch_type in self.epoch_types])\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return self.__str__()\n",
        "\n",
        "\n",
        "class Sequential(tf.keras.Model):\n",
        "    def __init__(self, layers, activation=None):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.lrs = layers\n",
        "        \n",
        "    def call(self, inputs, training=None):\n",
        "        x = inputs\n",
        "        for layer in self.lrs:\n",
        "            x = layer(x, training=training)\n",
        "        return x\n",
        "    \n",
        "    def get_layer_input_shape(self, target_layer):\n",
        "        if target_layer.inpt_shp is not None:\n",
        "            return target_layer.inpt_shp\n",
        "\n",
        "        input = np.random.normal(size=(1,) + self.lrs[0].inpt_shp)\n",
        "        for layer in self.lrs:\n",
        "            if layer is target_layer:\n",
        "                return tuple(input.shape[1:])\n",
        "            input = layer(input)\n",
        "        raise Exception(\"Layer not found in the model.\")\n",
        "\n",
        "    def get_layer_output_shape(self, target_layer):\n",
        "        input = np.random.normal(size=(1,) + self.lrs[0].inpt_shp)\n",
        "        for layer in self.lrs:\n",
        "            output = layer(input)\n",
        "            if layer is target_layer:\n",
        "                return tuple(output.shape[1:])\n",
        "            input = output\n",
        "        raise Exception(\"Layer not found in the model.\")\n",
        "    \n",
        "    def get_layer_sizes(self):\n",
        "        \"\"\"\n",
        "        Returns the sizes of all layers in the model, including the input and output layer.\n",
        "        \"\"\"\n",
        "        layer_sizes = list()\n",
        "        first_layer = True\n",
        "        for l in range(len(self.lrs)):\n",
        "            layer = self.lrs[l]\n",
        "            if isinstance(layer, CustomLayer):\n",
        "                layer_size = layer.get_size()\n",
        "                if first_layer:\n",
        "                    layer_sizes.append(layer_size[0])\n",
        "                    first_layer = False\n",
        "                layer_sizes.append(layer_size[1])\n",
        "        return layer_sizes\n",
        "    \n",
        "    def get_hidden_layer_sizes(self):\n",
        "        return self.get_layer_sizes()[1:-1]\n",
        "    \n",
        "    def get_regularization_penalty(self):\n",
        "        #TODO improve\n",
        "        return self.lrs[-2].regularizer.regularization_penalty\n",
        "    \n",
        "    def set_regularization_penalty(self, regularization_penalty):\n",
        "        for layer in self.lrs:\n",
        "            if isinstance(layer, CustomLayer) and not layer.fixed_size:\n",
        "                layer.regularizer.regularization_penalty = regularization_penalty\n",
        "    \n",
        "    def prune(self, params):\n",
        "        input_shape = self.get_layer_input_shape(self.lrs[0])\n",
        "        n_input_units = input_shape[-1]\n",
        "        active_units_indices = list(range(n_input_units))\n",
        "\n",
        "        last_custom_layer = None\n",
        "        for layer in self.lrs:\n",
        "            if isinstance(layer, CustomLayer):\n",
        "                if last_custom_layer is not None and type(last_custom_layer) != type(layer):\n",
        "                    if type(last_custom_layer) == Conv2D and type(layer) == Dense:\n",
        "                        convolutional_shape = self.get_layer_output_shape(last_custom_layer)\n",
        "                        active_units_indices = self.convert_channel_indices_to_flattened_indices(active_units_indices, convolutional_shape)\n",
        "                    else:\n",
        "                        raise Exception(\"Incorrect order of custom layer types.\")\n",
        "                active_units_indices = layer.prune(params.pruning_threshold, active_units_indices)\n",
        "                last_custom_layer = layer\n",
        "    \n",
        "    def grow(self, params):   \n",
        "        n_new_units = 0\n",
        "\n",
        "        last_custom_layer = None\n",
        "        for layer in self.lrs:\n",
        "            if isinstance(layer, CustomLayer):\n",
        "                if last_custom_layer is not None and type(last_custom_layer) != type(layer):\n",
        "                    if type(last_custom_layer) == Conv2D and type(layer) == Dense:\n",
        "                        convolutional_shape = self.get_layer_output_shape(last_custom_layer)\n",
        "                        n_new_units = n_new_units * convolutional_shape[0] * convolutional_shape[1]\n",
        "                    else:\n",
        "                        raise Exception(\"Incorrect order of custom layer types.\")\n",
        "                n_new_units = layer.grow(n_new_units, params.growth_percentage, min_new_units=params.min_new_neurons, scaling_factor=params.pruning_threshold)\n",
        "                last_custom_layer = layer\n",
        "    \n",
        "    @staticmethod\n",
        "    def convert_channel_indices_to_flattened_indices(channel_indices, convolutional_shape):\n",
        "        dense_indices = list()\n",
        "        units_per_channel = convolutional_shape[0] * convolutional_shape[1]\n",
        "        for channel_index in channel_indices:\n",
        "            for iter in range(units_per_channel):\n",
        "                dense_indices.append(channel_index * units_per_channel + iter)\n",
        "        return dense_indices\n",
        "    \n",
        "    def print_neurons(self):\n",
        "        for layer in self.lrs[:-1]:\n",
        "            print(layer.get_param_string())\n",
        "    \n",
        "    def evaluate(self, params, summed_training_loss, summed_training_accuracy):\n",
        "        # Calculate training loss and accuracy\n",
        "        if summed_training_loss is not None:\n",
        "            loss = summed_training_loss / params.x.shape[0]\n",
        "        else:\n",
        "            loss = None\n",
        "        \n",
        "        if summed_training_accuracy is not None:\n",
        "            accuracy = summed_training_accuracy / params.x.shape[0]\n",
        "        else:\n",
        "            accuracy = None\n",
        "        \n",
        "        # Calculate val loss and accuracy\n",
        "        summed_val_loss = 0\n",
        "        summed_val_accuracy = 0\n",
        "        n_val_instances = 0\n",
        "        \n",
        "        for step, (x_batch, y_batch) in enumerate(params.val_dataset):\n",
        "            y_pred = self(x_batch, training=False)\n",
        "            summed_val_loss += tf.reduce_sum(tf.keras.losses.sparse_categorical_crossentropy(y_batch, y_pred))\n",
        "            summed_val_accuracy += float(tf.reduce_sum(tf.keras.metrics.sparse_categorical_accuracy(y_batch, y_pred)))\n",
        "            n_val_instances += x_batch.shape[0]\n",
        "        \n",
        "        val_loss = summed_val_loss / n_val_instances\n",
        "        val_accuracy = summed_val_accuracy / n_val_instances\n",
        "\n",
        "        return loss, accuracy, val_loss, val_accuracy\n",
        "    \n",
        "    def print_epoch_statistics(self, params, summed_training_loss, summed_training_accuracy, message=None, require_result=False):\n",
        "        if not params.verbose:\n",
        "            if require_result:\n",
        "                return self.evaluate(params, summed_training_loss, summed_training_accuracy)\n",
        "            else:\n",
        "                return\n",
        "        \n",
        "        loss, accuracy, val_loss, val_accuracy = self.evaluate(params, summed_training_loss, summed_training_accuracy)  \n",
        "\n",
        "        if message is not None:\n",
        "            print(message)\n",
        "        \n",
        "        print(f\"loss: {loss} - accuracy: {accuracy} - val_loss: {val_loss} - val_accuracy: {val_accuracy} - penalty: {self.get_regularization_penalty()}\")\n",
        "        hidden_layer_sizes = self.get_hidden_layer_sizes()\n",
        "        print(f\"hidden layer sizes: {hidden_layer_sizes}, total units: {sum(hidden_layer_sizes)}\")\n",
        "        if params.print_neurons:\n",
        "            self.print_neurons()\n",
        "        \n",
        "        if require_result:\n",
        "            return loss, accuracy, val_loss, val_accuracy\n",
        "    \n",
        "    def update_history(self, params, loss, accuracy, val_loss, val_accuracy):\n",
        "        params.history['loss'].append(loss)\n",
        "        params.history['accuracy'].append(accuracy)\n",
        "        params.history['val_loss'].append(val_loss)\n",
        "        params.history['val_accuracy'].append(val_accuracy)\n",
        "        params.history['hidden_layer_sizes'].append(self.get_hidden_layer_sizes())\n",
        "    \n",
        "    @staticmethod\n",
        "    def prepare_datasets(x, y, batch_size, validation_data):\n",
        "        train_dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
        "        train_dataset = train_dataset.shuffle(buffer_size=20000).batch(batch_size)\n",
        "        val_dataset = tf.data.Dataset.from_tensor_slices(validation_data).batch(batch_size)\n",
        "        return train_dataset.prefetch(tf.data.AUTOTUNE), val_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "    \n",
        "    def manage_dynamic_regularization(self, params, val_loss):\n",
        "        if val_loss >= params.best_conditional_val_loss * params.stall_coefficient:\n",
        "            # Training is currently in stall\n",
        "            if not params.training_stalled:\n",
        "                penalty = self.get_regularization_penalty() * params.regularization_penalty_multiplier\n",
        "                print(\"Changing penalty...\")\n",
        "                # TODO this must be modified, penalty can differ for each layer\n",
        "                self.set_regularization_penalty(penalty)\n",
        "                params.training_stalled = True\n",
        "        else:\n",
        "            params.best_conditional_val_loss = val_loss\n",
        "            params.training_stalled = False\n",
        "    \n",
        "    def grow_wrapper(self, params):\n",
        "        dynamic_reqularization_active = params.regularization_penalty_multiplier != 1.\n",
        "        if dynamic_reqularization_active:\n",
        "            loss, accuracy, val_loss, val_accuracy = self.print_epoch_statistics(params, None, None, \"Before growing:\", require_result=True)\n",
        "            self.manage_dynamic_regularization(params, val_loss)\n",
        "        else:\n",
        "            self.print_epoch_statistics(params, None, None, \"Before growing:\")\n",
        "\n",
        "        self.grow(params)\n",
        "        self.print_epoch_statistics(params, None, None, \"After growing:\")\n",
        "    \n",
        "    def prune_wrapper(self, params, summed_loss, summed_accuracy):\n",
        "        loss, accuracy, _, _ = self.print_epoch_statistics(params, summed_loss, summed_accuracy, \"Before pruning:\", require_result=True)\n",
        "        self.prune(params)\n",
        "        _, _, val_loss, val_accuracy = self.print_epoch_statistics(params, None, None, \"After pruning:\", require_result=True)\n",
        "\n",
        "        self.update_history(params, loss, accuracy, val_loss, val_accuracy)\n",
        "    \n",
        "    class ParameterContainer:\n",
        "        def __init__(self, x, y, optimizer, batch_size, min_new_neurons, validation_data, pruning_threshold, \n",
        "                regularization_penalty_multiplier, stall_coefficient, growth_percentage, mini_epochs_per_epoch, verbose, print_neurons, use_static_graph):\n",
        "            self.x = x\n",
        "            self.y = y\n",
        "            self.optimizer = optimizer\n",
        "            self.batch_size = batch_size\n",
        "            self.min_new_neurons = min_new_neurons\n",
        "            self.validation_data = validation_data\n",
        "            self.pruning_threshold = pruning_threshold\n",
        "            self.regularization_penalty_multiplier = regularization_penalty_multiplier\n",
        "            self.stall_coefficient = stall_coefficient\n",
        "            self.growth_percentage = growth_percentage\n",
        "            self.mini_epochs_per_epoch = mini_epochs_per_epoch\n",
        "            self.verbose = verbose\n",
        "            self.print_neurons = print_neurons\n",
        "            self.use_static_graph = use_static_graph\n",
        "\n",
        "            self.train_dataset, self.val_dataset = Sequential.prepare_datasets(x, y, batch_size, validation_data)\n",
        "            self.history = self.prepare_history()\n",
        "\n",
        "            self.best_conditional_val_loss = np.inf\n",
        "            self.training_stalled = False\n",
        "        \n",
        "        @staticmethod\n",
        "        def prepare_history():\n",
        "            history = {\n",
        "                'loss': list(),\n",
        "                'accuracy': list(),\n",
        "                'val_loss': list(),\n",
        "                'val_accuracy': list(),\n",
        "                'hidden_layer_sizes': list(),\n",
        "            }\n",
        "            return history\n",
        "    \n",
        "    def fit_single_step(self, params, x_batch, y_batch):\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = self(x_batch, training=True)\n",
        "            raw_loss = tf.keras.losses.sparse_categorical_crossentropy(y_batch, y_pred)\n",
        "            loss_value = tf.reduce_mean(raw_loss)\n",
        "            loss_value += sum(self.losses)  # Add losses registered by model.add_loss\n",
        "\n",
        "            loss = tf.reduce_sum(raw_loss)\n",
        "            accuracy = float(tf.reduce_sum(tf.keras.metrics.sparse_categorical_accuracy(y_batch, y_pred)))\n",
        "\n",
        "        grads = tape.gradient(loss_value, self.trainable_variables)\n",
        "        params.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
        "\n",
        "        return loss, accuracy\n",
        "    \n",
        "    def fit_single_epoch(self, params):\n",
        "        summed_loss = 0\n",
        "        summed_accuracy = 0\n",
        "        \n",
        "        for mini_epoch in range(params.mini_epochs_per_epoch):\n",
        "            summed_loss = 0\n",
        "            summed_accuracy = 0\n",
        "\n",
        "            if params.use_static_graph:\n",
        "                fit_single_step_function = tf.function(self.fit_single_step)\n",
        "            else:\n",
        "                fit_single_step_function = self.fit_single_step\n",
        "            for step, (x_batch, y_batch) in enumerate(params.train_dataset):\n",
        "                loss, accuracy = fit_single_step_function(params, x_batch, y_batch)\n",
        "                summed_loss += loss\n",
        "                summed_accuracy += accuracy\n",
        "        \n",
        "        return summed_loss, summed_accuracy\n",
        "\n",
        "    def fit(self, x, y, optimizer, schedule, batch_size, min_new_neurons, validation_data, pruning_threshold=0.001, regularization_penalty_multiplier=1., \n",
        "            stall_coefficient=1, growth_percentage=0.2, mini_epochs_per_epoch=1, verbose=True, print_neurons=False, use_static_graph=True):\n",
        "        params = self.ParameterContainer(x=x, y=y, optimizer=optimizer, batch_size=batch_size, min_new_neurons=min_new_neurons, validation_data=validation_data, \n",
        "                                         pruning_threshold=pruning_threshold, regularization_penalty_multiplier=regularization_penalty_multiplier, stall_coefficient=stall_coefficient, \n",
        "                                         growth_percentage=growth_percentage, mini_epochs_per_epoch=mini_epochs_per_epoch, verbose=verbose, print_neurons=print_neurons, \n",
        "                                         use_static_graph=use_static_graph)\n",
        "        self.build(x.shape)  # Necessary when verbose == False\n",
        "\n",
        "        for epoch_no, epoch_type in enumerate(schedule):\n",
        "            if verbose:\n",
        "                print(\"##########################################################\")\n",
        "                print(f\"Epoch {epoch_no + 1}/{len(schedule)}\")\n",
        "\n",
        "            if epoch_type in (EpochType.DYNAMIC, EpochType.GROWTH_ONLY):\n",
        "                self.grow_wrapper(params)\n",
        "            \n",
        "            if epoch_type == EpochType.STATIC_NO_REGULARIZATION:\n",
        "                self.set_regularization_penalty(0.)\n",
        "            \n",
        "            summed_loss, summed_accuracy = self.fit_single_epoch(params)\n",
        "\n",
        "            if epoch_type in (EpochType.DYNAMIC, EpochType.PRUNING_ONLY):\n",
        "                self.prune_wrapper(params, summed_loss, summed_accuracy)\n",
        "            else:\n",
        "                loss, accuracy, val_loss, val_accuracy = self.print_epoch_statistics(params, summed_loss, summed_accuracy, require_result=True)\n",
        "                self.update_history(params, loss, accuracy, val_loss, val_accuracy)\n",
        "        \n",
        "        return params.history\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# HELPER FUNCTIONS\n",
        "################################################################################\n",
        "\n",
        "\n",
        "def get_statistics_from_history(history):\n",
        "    best_epoch_number = np.argmax(history['val_accuracy'])\n",
        "    best_val_accuracy = history['val_accuracy'][best_epoch_number]\n",
        "    best_hidden_layer_sizes = history['hidden_layer_sizes'][best_epoch_number]\n",
        "    return best_val_accuracy, best_hidden_layer_sizes\n",
        "\n",
        "\n",
        "def get_statistics_from_histories(histories):\n",
        "    best_val_accuracies = list()\n",
        "    all_best_hidden_layer_sizes = list()\n",
        "\n",
        "    for history in histories:\n",
        "        best_val_accuracy, best_hidden_layer_sizes = get_statistics_from_history(history)\n",
        "        best_val_accuracies.append(best_val_accuracy)\n",
        "        all_best_hidden_layer_sizes.append(best_hidden_layer_sizes)\n",
        "    \n",
        "    mean_best_val_accuracy = np.mean(best_val_accuracies)\n",
        "    mean_best_hidden_layer_sizes = [np.mean(layer) for layer in list(zip(*all_best_hidden_layer_sizes))]\n",
        "    \n",
        "    return mean_best_val_accuracy, mean_best_hidden_layer_sizes\n",
        "\n",
        "\n",
        "def cross_validate(train_fn, x, y, n_splits, random_state=42, *args, **kwargs):\n",
        "    from sklearn.model_selection import KFold\n",
        "\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
        "\n",
        "    histories = list()\n",
        "    for i, (train_index, test_index) in enumerate(kf.split(x)):\n",
        "        xtrain, xtest = x[train_index], x[test_index]\n",
        "        ytrain, ytest = y[train_index], y[test_index]\n",
        "\n",
        "        history = train_fn(xtrain, ytrain, validation_data=(xtest, ytest), *args, **kwargs)\n",
        "        histories.append(history)\n",
        "\n",
        "        best_val_accuracy, best_hidden_layer_sizes = get_statistics_from_history(history)\n",
        "        print(f\"Run {i} completed, best_val_accuracy: {best_val_accuracy}, best_hidden_layer_sizes: {best_hidden_layer_sizes}\")\n",
        "\n",
        "    mean_best_val_accuracy, mean_best_hidden_layer_sizes = get_statistics_from_histories(histories)\n",
        "    print(f'mean_best_val_accuracy: {mean_best_val_accuracy}')\n",
        "    print(f'mean_best_hidden_layer_sizes: {mean_best_hidden_layer_sizes}')\n",
        "\n",
        "    return histories\n",
        "\n",
        "\n",
        "def hyperparameter_search(train_fn, x, y, validation_data, *args, **kwargs):\n",
        "    from itertools import product\n",
        "\n",
        "    all_params = [*args] + list(kwargs.values())\n",
        "    histories = list()\n",
        "\n",
        "    best_overall_val_accuracy = -np.inf\n",
        "    best_overall_combination = None\n",
        "\n",
        "    for combination in product(*all_params):\n",
        "        combination_args = combination[:len(args)]\n",
        "\n",
        "        combination_kwargs_values = combination[len(args):]\n",
        "        combination_kwargs = dict(zip(kwargs.keys(), combination_kwargs_values))\n",
        "\n",
        "        history = train_fn(x, y, validation_data, *combination_args, **combination_kwargs)\n",
        "        history['parameters'] = combination\n",
        "        histories.append(history)\n",
        "\n",
        "        best_val_accuracy, best_hidden_layer_sizes = get_statistics_from_history(history)\n",
        "        print(f\"Run with parameters {combination} completed, best_val_accuracy: {best_val_accuracy}, best_hidden_layer_sizes sizes: {best_hidden_layer_sizes}\")\n",
        "\n",
        "        if best_val_accuracy > best_overall_val_accuracy:\n",
        "            best_overall_val_accuracy = best_val_accuracy\n",
        "            best_overall_combination = combination\n",
        "    \n",
        "    print(f'Best overall combination: {best_overall_combination}, val_accuracy: {best_overall_val_accuracy}')\n",
        "\n",
        "    return histories\n",
        "\n",
        "\n",
        "def get_convolutional_model(x, regularization_penalty, regularization_method, layer_sizes, output_neurons=10):\n",
        "    model = Sequential([\n",
        "        Conv2D(layer_sizes[0], filter_size=(3, 3), activation='selu', strides=(1, 1), padding='SAME', \n",
        "            regularization_penalty=regularization_penalty, regularization_method=regularization_method, \n",
        "            kernel_initializer='lecun_normal', input_shape=x[0,:,:,:].shape),\n",
        "        Conv2D(layer_sizes[1], filter_size=(3, 3), activation='selu', strides=(2, 2), padding='SAME', \n",
        "            regularization_penalty=regularization_penalty, regularization_method=regularization_method, \n",
        "            kernel_initializer='lecun_normal'),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        Conv2D(layer_sizes[2], filter_size=(3, 3), activation='selu', strides=(1, 1), padding='SAME', \n",
        "            regularization_penalty=regularization_penalty, regularization_method=regularization_method, \n",
        "            kernel_initializer='lecun_normal'),\n",
        "        Conv2D(layer_sizes[3], filter_size=(3, 3), activation='selu', strides=(2, 2), padding='SAME', \n",
        "            regularization_penalty=regularization_penalty, regularization_method=regularization_method, \n",
        "            kernel_initializer='lecun_normal'),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        Flatten(),\n",
        "        Dense(layer_sizes[4], activation='selu', regularization_penalty=regularization_penalty, \n",
        "            regularization_method=regularization_method, kernel_initializer='lecun_normal'),\n",
        "        Dense(output_neurons, activation='softmax', regularization_penalty=0., \n",
        "            regularization_method=None, fixed_size=True),\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_fn(x, y, validation_data, learning_rate, schedule, regularization_penalty, regularization_method, layer_sizes, \n",
        "             output_neurons=10, min_new_neurons=20, growth_percentage=0.2, verbose=False, use_static_graph=True):\n",
        "    batch_size = 128\n",
        "\n",
        "    model = get_convolutional_model(x, regularization_penalty, regularization_method, layer_sizes, output_neurons)\n",
        "    \n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "    history = model.fit(x=x, y=y, optimizer=optimizer, schedule=schedule, batch_size=batch_size, min_new_neurons=min_new_neurons, \n",
        "                        validation_data=validation_data, growth_percentage=growth_percentage, verbose=verbose, use_static_graph=use_static_graph)\n",
        "    \n",
        "    return history"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1MrQXUTFwOe"
      },
      "source": [
        "# Accuracy benchmark - FF and convolutions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsivpauwveEK"
      },
      "source": [
        "## CIFAR100"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cifar100 = get_cifar_100_dataset()"
      ],
      "metadata": {
        "id": "SjJ2e9njMl04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "model = get_convolutional_model(cifar100.X_train_norm, regularization_penalty=0.00002, regularization_method='weighted_l1', layer_sizes=[100, 100, 100, 100, 100], output_neurons=100)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002)\n",
        "\n",
        "history = model.fit(cifar100.X_train_norm, cifar100.y_train, optimizer, epochs=40, self_scaling_epochs=20, batch_size=batch_size, \n",
        "                    min_new_neurons=20, validation_data=(cifar100.X_test_norm, cifar100.y_test), pruning_only_epochs=0, \n",
        "                    growth_percentage=0.2, verbose=True)"
      ],
      "metadata": {
        "id": "7934TYSFhFKN",
        "outputId": "bbd2acce-4228-4634-b3e7-20616475dff0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##########################################################\n",
            "Epoch 1/40\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 4.9665093421936035 - val_accuracy: 0.0102 - penalty: 2e-05\n",
            "hidden layer sizes: [100, 100, 100, 100, 100], total units: 500\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 4.9665093421936035 - val_accuracy: 0.0102 - penalty: 2e-05\n",
            "hidden layer sizes: [120, 120, 120, 120, 120], total units: 600\n",
            "Before pruning:\n",
            "loss: 4.0280256271362305 - accuracy: 0.1080000028014183 - val_loss: 3.6404457092285156 - val_accuracy: 0.1663 - penalty: 2e-05\n",
            "hidden layer sizes: [120, 120, 120, 120, 120], total units: 600\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 3.64062762260437 - val_accuracy: 0.1661 - penalty: 2e-05\n",
            "hidden layer sizes: [100, 100, 100, 100, 100], total units: 500\n",
            "##########################################################\n",
            "Epoch 2/40\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 3.64062762260437 - val_accuracy: 0.1661 - penalty: 2e-05\n",
            "hidden layer sizes: [100, 100, 100, 100, 100], total units: 500\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 3.64062762260437 - val_accuracy: 0.1661 - penalty: 2e-05\n",
            "hidden layer sizes: [120, 120, 120, 120, 120], total units: 600\n",
            "Before pruning:\n",
            "loss: 3.6307544708251953 - accuracy: 0.15977999567985535 - val_loss: 3.4752750396728516 - val_accuracy: 0.1883 - penalty: 2e-05\n",
            "hidden layer sizes: [120, 120, 120, 120, 120], total units: 600\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 3.475353240966797 - val_accuracy: 0.1886 - penalty: 2e-05\n",
            "hidden layer sizes: [100, 100, 100, 100, 106], total units: 506\n",
            "##########################################################\n",
            "Epoch 3/40\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 3.475353240966797 - val_accuracy: 0.1886 - penalty: 2e-05\n",
            "hidden layer sizes: [100, 100, 100, 100, 106], total units: 506\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 3.4753527641296387 - val_accuracy: 0.1886 - penalty: 2e-05\n",
            "hidden layer sizes: [120, 120, 120, 120, 127], total units: 607\n",
            "Before pruning:\n",
            "loss: 3.447922706604004 - accuracy: 0.18661999702453613 - val_loss: 3.241731882095337 - val_accuracy: 0.2306 - penalty: 2e-05\n",
            "hidden layer sizes: [120, 120, 120, 120, 127], total units: 607\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 3.241814374923706 - val_accuracy: 0.2302 - penalty: 2e-05\n",
            "hidden layer sizes: [100, 98, 91, 91, 120], total units: 500\n",
            "##########################################################\n",
            "Epoch 4/40\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 3.241814374923706 - val_accuracy: 0.2302 - penalty: 2e-05\n",
            "hidden layer sizes: [100, 98, 91, 91, 120], total units: 500\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 3.241814374923706 - val_accuracy: 0.2302 - penalty: 2e-05\n",
            "hidden layer sizes: [120, 118, 111, 111, 144], total units: 604\n",
            "Before pruning:\n",
            "loss: 3.231048345565796 - accuracy: 0.22458000481128693 - val_loss: 3.0473365783691406 - val_accuracy: 0.261 - penalty: 2e-05\n",
            "hidden layer sizes: [120, 118, 111, 111, 144], total units: 604\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 3.0472774505615234 - val_accuracy: 0.2609 - penalty: 2e-05\n",
            "hidden layer sizes: [100, 60, 80, 78, 134], total units: 452\n",
            "##########################################################\n",
            "Epoch 5/40\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 3.0472774505615234 - val_accuracy: 0.2609 - penalty: 2e-05\n",
            "hidden layer sizes: [100, 60, 80, 78, 134], total units: 452\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 3.0472774505615234 - val_accuracy: 0.2609 - penalty: 2e-05\n",
            "hidden layer sizes: [120, 80, 100, 98, 160], total units: 558\n",
            "Before pruning:\n",
            "loss: 3.100306272506714 - accuracy: 0.24726000428199768 - val_loss: 2.9273619651794434 - val_accuracy: 0.2843 - penalty: 2e-05\n",
            "hidden layer sizes: [120, 80, 100, 98, 160], total units: 558\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 2.9273147583007812 - val_accuracy: 0.2843 - penalty: 2e-05\n",
            "hidden layer sizes: [100, 48, 76, 67, 156], total units: 447\n",
            "##########################################################\n",
            "Epoch 6/40\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 2.9273147583007812 - val_accuracy: 0.2843 - penalty: 2e-05\n",
            "hidden layer sizes: [100, 48, 76, 67, 156], total units: 447\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 2.9273147583007812 - val_accuracy: 0.2843 - penalty: 2e-05\n",
            "hidden layer sizes: [120, 68, 96, 87, 187], total units: 558\n",
            "Before pruning:\n",
            "loss: 3.0037763118743896 - accuracy: 0.26600000262260437 - val_loss: 2.8310389518737793 - val_accuracy: 0.3046 - penalty: 2e-05\n",
            "hidden layer sizes: [120, 68, 96, 87, 187], total units: 558\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 2.831169605255127 - val_accuracy: 0.3046 - penalty: 2e-05\n",
            "hidden layer sizes: [100, 38, 72, 60, 161], total units: 431\n",
            "##########################################################\n",
            "Epoch 7/40\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 2.831169605255127 - val_accuracy: 0.3046 - penalty: 2e-05\n",
            "hidden layer sizes: [100, 38, 72, 60, 161], total units: 431\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 2.831169605255127 - val_accuracy: 0.3046 - penalty: 2e-05\n",
            "hidden layer sizes: [120, 58, 92, 80, 193], total units: 543\n",
            "Before pruning:\n",
            "loss: 2.9358506202697754 - accuracy: 0.280239999294281 - val_loss: 2.748509407043457 - val_accuracy: 0.3227 - penalty: 2e-05\n",
            "hidden layer sizes: [120, 58, 92, 80, 193], total units: 543\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 2.7486495971679688 - val_accuracy: 0.3233 - penalty: 2e-05\n",
            "hidden layer sizes: [99, 35, 72, 58, 170], total units: 434\n",
            "##########################################################\n",
            "Epoch 8/40\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 2.7486495971679688 - val_accuracy: 0.3233 - penalty: 2e-05\n",
            "hidden layer sizes: [99, 35, 72, 58, 170], total units: 434\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 2.7486495971679688 - val_accuracy: 0.3233 - penalty: 2e-05\n",
            "hidden layer sizes: [119, 55, 92, 78, 204], total units: 548\n",
            "Before pruning:\n",
            "loss: 2.873830556869507 - accuracy: 0.28804001212120056 - val_loss: 2.69982647895813 - val_accuracy: 0.3309 - penalty: 2e-05\n",
            "hidden layer sizes: [119, 55, 92, 78, 204], total units: 548\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 2.6998634338378906 - val_accuracy: 0.3306 - penalty: 2e-05\n",
            "hidden layer sizes: [95, 30, 62, 55, 179], total units: 421\n",
            "##########################################################\n",
            "Epoch 9/40\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 2.6998634338378906 - val_accuracy: 0.3306 - penalty: 2e-05\n",
            "hidden layer sizes: [95, 30, 62, 55, 179], total units: 421\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 2.6998634338378906 - val_accuracy: 0.3306 - penalty: 2e-05\n",
            "hidden layer sizes: [115, 50, 82, 75, 214], total units: 536\n",
            "Before pruning:\n",
            "loss: 2.817314624786377 - accuracy: 0.3005000054836273 - val_loss: 2.655538558959961 - val_accuracy: 0.3356 - penalty: 2e-05\n",
            "hidden layer sizes: [115, 50, 82, 75, 214], total units: 536\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 2.655611515045166 - val_accuracy: 0.3352 - penalty: 2e-05\n",
            "hidden layer sizes: [93, 29, 59, 52, 175], total units: 408\n",
            "##########################################################\n",
            "Epoch 10/40\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 2.655611515045166 - val_accuracy: 0.3352 - penalty: 2e-05\n",
            "hidden layer sizes: [93, 29, 59, 52, 175], total units: 408\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 2.655611991882324 - val_accuracy: 0.3352 - penalty: 2e-05\n",
            "hidden layer sizes: [113, 49, 79, 72, 210], total units: 523\n",
            "Before pruning:\n",
            "loss: 2.7702877521514893 - accuracy: 0.3056800067424774 - val_loss: 2.607609748840332 - val_accuracy: 0.3445 - penalty: 2e-05\n",
            "hidden layer sizes: [113, 49, 79, 72, 210], total units: 523\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 2.60766339302063 - val_accuracy: 0.3443 - penalty: 2e-05\n",
            "hidden layer sizes: [91, 26, 56, 53, 179], total units: 405\n",
            "##########################################################\n",
            "Epoch 11/40\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 2.60766339302063 - val_accuracy: 0.3443 - penalty: 2e-05\n",
            "hidden layer sizes: [91, 26, 56, 53, 179], total units: 405\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 2.60766339302063 - val_accuracy: 0.3443 - penalty: 2e-05\n",
            "hidden layer sizes: [111, 46, 76, 73, 214], total units: 520\n",
            "Before pruning:\n",
            "loss: 2.7229537963867188 - accuracy: 0.31727999448776245 - val_loss: 2.568338632583618 - val_accuracy: 0.3548 - penalty: 2e-05\n",
            "hidden layer sizes: [111, 46, 76, 73, 214], total units: 520\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 2.568450927734375 - val_accuracy: 0.3545 - penalty: 2e-05\n",
            "hidden layer sizes: [88, 25, 55, 52, 181], total units: 401\n",
            "##########################################################\n",
            "Epoch 12/40\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 2.568450927734375 - val_accuracy: 0.3545 - penalty: 2e-05\n",
            "hidden layer sizes: [88, 25, 55, 52, 181], total units: 401\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 2.568450927734375 - val_accuracy: 0.3545 - penalty: 2e-05\n",
            "hidden layer sizes: [108, 45, 75, 72, 217], total units: 517\n",
            "Before pruning:\n",
            "loss: 2.687681198120117 - accuracy: 0.32589998841285706 - val_loss: 2.5235960483551025 - val_accuracy: 0.3684 - penalty: 2e-05\n",
            "hidden layer sizes: [108, 45, 75, 72, 217], total units: 517\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 2.5236635208129883 - val_accuracy: 0.3677 - penalty: 2e-05\n",
            "hidden layer sizes: [85, 25, 52, 56, 186], total units: 404\n",
            "##########################################################\n",
            "Epoch 13/40\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 2.5236635208129883 - val_accuracy: 0.3677 - penalty: 2e-05\n",
            "hidden layer sizes: [85, 25, 52, 56, 186], total units: 404\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 2.5236637592315674 - val_accuracy: 0.3677 - penalty: 2e-05\n",
            "hidden layer sizes: [105, 45, 72, 76, 223], total units: 521\n",
            "Before pruning:\n",
            "loss: 2.654783010482788 - accuracy: 0.33118000626564026 - val_loss: 2.5015439987182617 - val_accuracy: 0.3698 - penalty: 2e-05\n",
            "hidden layer sizes: [105, 45, 72, 76, 223], total units: 521\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 2.5015714168548584 - val_accuracy: 0.3696 - penalty: 2e-05\n",
            "hidden layer sizes: [82, 24, 49, 54, 186], total units: 395\n",
            "##########################################################\n",
            "Epoch 14/40\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 2.5015714168548584 - val_accuracy: 0.3696 - penalty: 2e-05\n",
            "hidden layer sizes: [82, 24, 49, 54, 186], total units: 395\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 2.5015714168548584 - val_accuracy: 0.3696 - penalty: 2e-05\n",
            "hidden layer sizes: [102, 44, 69, 74, 223], total units: 512\n",
            "Before pruning:\n",
            "loss: 2.6212122440338135 - accuracy: 0.33643999695777893 - val_loss: 2.471114158630371 - val_accuracy: 0.3666 - penalty: 2e-05\n",
            "hidden layer sizes: [102, 44, 69, 74, 223], total units: 512\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 2.4711081981658936 - val_accuracy: 0.3659 - penalty: 2e-05\n",
            "hidden layer sizes: [79, 22, 44, 59, 187], total units: 391\n",
            "##########################################################\n",
            "Epoch 15/40\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 2.4711081981658936 - val_accuracy: 0.3659 - penalty: 2e-05\n",
            "hidden layer sizes: [79, 22, 44, 59, 187], total units: 391\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 2.4711081981658936 - val_accuracy: 0.3659 - penalty: 2e-05\n",
            "hidden layer sizes: [99, 42, 64, 79, 224], total units: 508\n",
            "Before pruning:\n",
            "loss: 2.5820822715759277 - accuracy: 0.3453199863433838 - val_loss: 2.451876640319824 - val_accuracy: 0.3776 - penalty: 2e-05\n",
            "hidden layer sizes: [99, 42, 64, 79, 224], total units: 508\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 2.45180344581604 - val_accuracy: 0.3774 - penalty: 2e-05\n",
            "hidden layer sizes: [76, 21, 39, 62, 188], total units: 386\n",
            "##########################################################\n",
            "Epoch 16/40\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 2.45180344581604 - val_accuracy: 0.3774 - penalty: 2e-05\n",
            "hidden layer sizes: [76, 21, 39, 62, 188], total units: 386\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 2.4518039226531982 - val_accuracy: 0.3774 - penalty: 2e-05\n",
            "hidden layer sizes: [96, 41, 59, 82, 225], total units: 503\n",
            "Before pruning:\n",
            "loss: 2.5633888244628906 - accuracy: 0.3481200039386749 - val_loss: 2.418726682662964 - val_accuracy: 0.3866 - penalty: 2e-05\n",
            "hidden layer sizes: [96, 41, 59, 82, 225], total units: 503\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 2.418813943862915 - val_accuracy: 0.3868 - penalty: 2e-05\n",
            "hidden layer sizes: [73, 20, 37, 66, 189], total units: 385\n",
            "##########################################################\n",
            "Epoch 17/40\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 2.418813943862915 - val_accuracy: 0.3868 - penalty: 2e-05\n",
            "hidden layer sizes: [73, 20, 37, 66, 189], total units: 385\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 2.418813943862915 - val_accuracy: 0.3868 - penalty: 2e-05\n",
            "hidden layer sizes: [93, 40, 57, 86, 226], total units: 502\n",
            "Before pruning:\n",
            "loss: 2.536714792251587 - accuracy: 0.35109999775886536 - val_loss: 2.380084991455078 - val_accuracy: 0.395 - penalty: 2e-05\n",
            "hidden layer sizes: [93, 40, 57, 86, 226], total units: 502\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 2.3800809383392334 - val_accuracy: 0.3948 - penalty: 2e-05\n",
            "hidden layer sizes: [70, 20, 37, 60, 190], total units: 377\n",
            "##########################################################\n",
            "Epoch 18/40\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 2.3800809383392334 - val_accuracy: 0.3948 - penalty: 2e-05\n",
            "hidden layer sizes: [70, 20, 37, 60, 190], total units: 377\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 2.3800809383392334 - val_accuracy: 0.3948 - penalty: 2e-05\n",
            "hidden layer sizes: [90, 40, 57, 80, 228], total units: 495\n",
            "Before pruning:\n",
            "loss: 2.5163424015045166 - accuracy: 0.3587999939918518 - val_loss: 2.3841278553009033 - val_accuracy: 0.3908 - penalty: 2e-05\n",
            "hidden layer sizes: [90, 40, 57, 80, 228], total units: 495\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 2.3841354846954346 - val_accuracy: 0.3911 - penalty: 2e-05\n",
            "hidden layer sizes: [68, 20, 33, 61, 194], total units: 376\n",
            "##########################################################\n",
            "Epoch 19/40\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 2.3841354846954346 - val_accuracy: 0.3911 - penalty: 2e-05\n",
            "hidden layer sizes: [68, 20, 33, 61, 194], total units: 376\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 2.3841354846954346 - val_accuracy: 0.3911 - penalty: 2e-05\n",
            "hidden layer sizes: [88, 40, 53, 81, 232], total units: 494\n",
            "Before pruning:\n",
            "loss: 2.4894819259643555 - accuracy: 0.36188000440597534 - val_loss: 2.359753370285034 - val_accuracy: 0.3929 - penalty: 2e-05\n",
            "hidden layer sizes: [88, 40, 53, 81, 232], total units: 494\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 2.359687566757202 - val_accuracy: 0.3929 - penalty: 2e-05\n",
            "hidden layer sizes: [66, 20, 32, 63, 194], total units: 375\n",
            "##########################################################\n",
            "Epoch 20/40\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 2.359687566757202 - val_accuracy: 0.3929 - penalty: 2e-05\n",
            "hidden layer sizes: [66, 20, 32, 63, 194], total units: 375\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 2.359687328338623 - val_accuracy: 0.3929 - penalty: 2e-05\n",
            "hidden layer sizes: [86, 40, 52, 83, 232], total units: 493\n",
            "Before pruning:\n",
            "loss: 2.478177547454834 - accuracy: 0.36344000697135925 - val_loss: 2.3493340015411377 - val_accuracy: 0.3983 - penalty: 2e-05\n",
            "hidden layer sizes: [86, 40, 52, 83, 232], total units: 493\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 2.3494250774383545 - val_accuracy: 0.3981 - penalty: 2e-05\n",
            "hidden layer sizes: [62, 17, 31, 65, 192], total units: 367\n",
            "##########################################################\n",
            "Epoch 21/40\n",
            "loss: 2.4997246265411377 - accuracy: 0.36320000886917114 - val_loss: 2.2836287021636963 - val_accuracy: 0.4075 - penalty: 0.0\n",
            "hidden layer sizes: [62, 17, 31, 65, 192], total units: 367\n",
            "##########################################################\n",
            "Epoch 22/40\n",
            "loss: 2.212423086166382 - accuracy: 0.42250001430511475 - val_loss: 2.251293659210205 - val_accuracy: 0.4182 - penalty: 0.0\n",
            "hidden layer sizes: [62, 17, 31, 65, 192], total units: 367\n",
            "##########################################################\n",
            "Epoch 23/40\n",
            "loss: 2.101757287979126 - accuracy: 0.4499000012874603 - val_loss: 2.2030065059661865 - val_accuracy: 0.4336 - penalty: 0.0\n",
            "hidden layer sizes: [62, 17, 31, 65, 192], total units: 367\n",
            "##########################################################\n",
            "Epoch 24/40\n",
            "loss: 2.0088319778442383 - accuracy: 0.4698199927806854 - val_loss: 2.189648151397705 - val_accuracy: 0.4373 - penalty: 0.0\n",
            "hidden layer sizes: [62, 17, 31, 65, 192], total units: 367\n",
            "##########################################################\n",
            "Epoch 25/40\n",
            "loss: 1.9249986410140991 - accuracy: 0.4891600012779236 - val_loss: 2.2084591388702393 - val_accuracy: 0.4328 - penalty: 0.0\n",
            "hidden layer sizes: [62, 17, 31, 65, 192], total units: 367\n",
            "##########################################################\n",
            "Epoch 26/40\n",
            "loss: 1.8579679727554321 - accuracy: 0.5050399899482727 - val_loss: 2.1898374557495117 - val_accuracy: 0.442 - penalty: 0.0\n",
            "hidden layer sizes: [62, 17, 31, 65, 192], total units: 367\n",
            "##########################################################\n",
            "Epoch 27/40\n",
            "loss: 1.7782809734344482 - accuracy: 0.5248000025749207 - val_loss: 2.1925439834594727 - val_accuracy: 0.4414 - penalty: 0.0\n",
            "hidden layer sizes: [62, 17, 31, 65, 192], total units: 367\n",
            "##########################################################\n",
            "Epoch 28/40\n",
            "loss: 1.7154521942138672 - accuracy: 0.5369200110435486 - val_loss: 2.1960043907165527 - val_accuracy: 0.4485 - penalty: 0.0\n",
            "hidden layer sizes: [62, 17, 31, 65, 192], total units: 367\n",
            "##########################################################\n",
            "Epoch 29/40\n",
            "loss: 1.6548138856887817 - accuracy: 0.5501000285148621 - val_loss: 2.191196918487549 - val_accuracy: 0.447 - penalty: 0.0\n",
            "hidden layer sizes: [62, 17, 31, 65, 192], total units: 367\n",
            "##########################################################\n",
            "Epoch 30/40\n",
            "loss: 1.6023191213607788 - accuracy: 0.561460018157959 - val_loss: 2.2147793769836426 - val_accuracy: 0.448 - penalty: 0.0\n",
            "hidden layer sizes: [62, 17, 31, 65, 192], total units: 367\n",
            "##########################################################\n",
            "Epoch 31/40\n",
            "loss: 1.552045464515686 - accuracy: 0.5742800235748291 - val_loss: 2.23360013961792 - val_accuracy: 0.4526 - penalty: 0.0\n",
            "hidden layer sizes: [62, 17, 31, 65, 192], total units: 367\n",
            "##########################################################\n",
            "Epoch 32/40\n",
            "loss: 1.5061599016189575 - accuracy: 0.5844799876213074 - val_loss: 2.23874568939209 - val_accuracy: 0.45 - penalty: 0.0\n",
            "hidden layer sizes: [62, 17, 31, 65, 192], total units: 367\n",
            "##########################################################\n",
            "Epoch 33/40\n",
            "loss: 1.45633065700531 - accuracy: 0.5946400165557861 - val_loss: 2.2481303215026855 - val_accuracy: 0.4481 - penalty: 0.0\n",
            "hidden layer sizes: [62, 17, 31, 65, 192], total units: 367\n",
            "##########################################################\n",
            "Epoch 34/40\n",
            "loss: 1.4107677936553955 - accuracy: 0.6059200167655945 - val_loss: 2.2804975509643555 - val_accuracy: 0.4479 - penalty: 0.0\n",
            "hidden layer sizes: [62, 17, 31, 65, 192], total units: 367\n",
            "##########################################################\n",
            "Epoch 35/40\n",
            "loss: 1.368695616722107 - accuracy: 0.6142399907112122 - val_loss: 2.275789260864258 - val_accuracy: 0.4519 - penalty: 0.0\n",
            "hidden layer sizes: [62, 17, 31, 65, 192], total units: 367\n",
            "##########################################################\n",
            "Epoch 36/40\n",
            "loss: 1.3367382287979126 - accuracy: 0.6235399842262268 - val_loss: 2.3016200065612793 - val_accuracy: 0.45 - penalty: 0.0\n",
            "hidden layer sizes: [62, 17, 31, 65, 192], total units: 367\n",
            "##########################################################\n",
            "Epoch 37/40\n",
            "loss: 1.29282808303833 - accuracy: 0.6343399882316589 - val_loss: 2.2924118041992188 - val_accuracy: 0.4496 - penalty: 0.0\n",
            "hidden layer sizes: [62, 17, 31, 65, 192], total units: 367\n",
            "##########################################################\n",
            "Epoch 38/40\n",
            "loss: 1.2675926685333252 - accuracy: 0.6386200189590454 - val_loss: 2.350851058959961 - val_accuracy: 0.4456 - penalty: 0.0\n",
            "hidden layer sizes: [62, 17, 31, 65, 192], total units: 367\n",
            "##########################################################\n",
            "Epoch 39/40\n",
            "loss: 1.2334692478179932 - accuracy: 0.6450600028038025 - val_loss: 2.3547112941741943 - val_accuracy: 0.4492 - penalty: 0.0\n",
            "hidden layer sizes: [62, 17, 31, 65, 192], total units: 367\n",
            "##########################################################\n",
            "Epoch 40/40\n",
            "loss: 1.2020186185836792 - accuracy: 0.6534799933433533 - val_loss: 2.3694541454315186 - val_accuracy: 0.4496 - penalty: 0.0\n",
            "hidden layer sizes: [62, 17, 31, 65, 192], total units: 367\n",
            "CPU times: user 3min 36s, sys: 8.33 s, total: 3min 45s\n",
            "Wall time: 3min 16s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "model = get_convolutional_model(cifar100.X_train_norm, regularization_penalty=0.00002, regularization_method='weighted_l1', layer_sizes=[100, 100, 100, 100, 100], output_neurons=100)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002)\n",
        "\n",
        "history = model.fit(cifar100.X_train_norm, cifar100.y_train, optimizer, epochs=40, self_scaling_epochs=20, batch_size=batch_size, \n",
        "                    min_new_neurons=20, validation_data=(cifar100.X_test_norm, cifar100.y_test), pruning_only_epochs=0, \n",
        "                    growth_percentage=0.2, verbose=True)"
      ],
      "metadata": {
        "id": "TZjoY4LyiG7Z",
        "outputId": "5bef9841-66e8-40d6-90a6-161e97733677",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##########################################################\n",
            "Epoch 1/40\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 4.954936504364014 - val_accuracy: 0.011 - penalty: 2e-05\n",
            "hidden layer sizes: [100, 100, 100, 100, 100], total units: 500\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 4.954936504364014 - val_accuracy: 0.011 - penalty: 2e-05\n",
            "hidden layer sizes: [120, 120, 120, 120, 120], total units: 600\n",
            "Before pruning:\n",
            "loss: 3.9630274772644043 - accuracy: 0.11682000011205673 - val_loss: 3.6065542697906494 - val_accuracy: 0.166 - penalty: 2e-05\n",
            "hidden layer sizes: [120, 120, 120, 120, 120], total units: 600\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 3.6065804958343506 - val_accuracy: 0.166 - penalty: 2e-05\n",
            "hidden layer sizes: [100, 100, 100, 100, 107], total units: 507\n",
            "##########################################################\n",
            "Epoch 2/40\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 3.6065804958343506 - val_accuracy: 0.166 - penalty: 2e-05\n",
            "hidden layer sizes: [100, 100, 100, 100, 107], total units: 507\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 3.606580972671509 - val_accuracy: 0.166 - penalty: 2e-05\n",
            "hidden layer sizes: [120, 120, 120, 120, 128], total units: 608\n",
            "Before pruning:\n",
            "loss: 3.6110033988952637 - accuracy: 0.1626800000667572 - val_loss: 3.4388890266418457 - val_accuracy: 0.188 - penalty: 2e-05\n",
            "hidden layer sizes: [120, 120, 120, 120, 128], total units: 608\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 3.4389023780822754 - val_accuracy: 0.1878 - penalty: 2e-05\n",
            "hidden layer sizes: [100, 99, 100, 100, 124], total units: 523\n",
            "##########################################################\n",
            "Epoch 3/40\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 3.4389023780822754 - val_accuracy: 0.1878 - penalty: 2e-05\n",
            "hidden layer sizes: [100, 99, 100, 100, 124], total units: 523\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 3.4389023780822754 - val_accuracy: 0.1878 - penalty: 2e-05\n",
            "hidden layer sizes: [120, 119, 120, 120, 148], total units: 627\n",
            "Before pruning:\n",
            "loss: 3.403515100479126 - accuracy: 0.19582000374794006 - val_loss: 3.1985278129577637 - val_accuracy: 0.2324 - penalty: 2e-05\n",
            "hidden layer sizes: [120, 119, 120, 120, 148], total units: 627\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 3.1977641582489014 - val_accuracy: 0.2326 - penalty: 2e-05\n",
            "hidden layer sizes: [100, 57, 93, 83, 148], total units: 481\n",
            "##########################################################\n",
            "Epoch 4/40\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 3.1977641582489014 - val_accuracy: 0.2326 - penalty: 2e-05\n",
            "hidden layer sizes: [100, 57, 93, 83, 148], total units: 481\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 3.1977641582489014 - val_accuracy: 0.2326 - penalty: 2e-05\n",
            "hidden layer sizes: [120, 77, 113, 103, 177], total units: 590\n",
            "Before pruning:\n",
            "loss: 3.1989669799804688 - accuracy: 0.22991999983787537 - val_loss: 3.001176357269287 - val_accuracy: 0.2702 - penalty: 2e-05\n",
            "hidden layer sizes: [120, 77, 113, 103, 177], total units: 590\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 3.0010879039764404 - val_accuracy: 0.2708 - penalty: 2e-05\n",
            "hidden layer sizes: [100, 43, 84, 75, 171], total units: 473\n",
            "##########################################################\n",
            "Epoch 5/40\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 3.0010879039764404 - val_accuracy: 0.2708 - penalty: 2e-05\n",
            "hidden layer sizes: [100, 43, 84, 75, 171], total units: 473\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 3.0010876655578613 - val_accuracy: 0.2708 - penalty: 2e-05\n",
            "hidden layer sizes: [120, 63, 104, 95, 205], total units: 587\n",
            "Before pruning:\n",
            "loss: 3.0719592571258545 - accuracy: 0.25064000487327576 - val_loss: 2.8911893367767334 - val_accuracy: 0.2947 - penalty: 2e-05\n",
            "hidden layer sizes: [120, 63, 104, 95, 205], total units: 587\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 2.891228437423706 - val_accuracy: 0.2949 - penalty: 2e-05\n",
            "hidden layer sizes: [100, 32, 70, 64, 194], total units: 460\n",
            "##########################################################\n",
            "Epoch 6/40\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 2.891228437423706 - val_accuracy: 0.2949 - penalty: 2e-05\n",
            "hidden layer sizes: [100, 32, 70, 64, 194], total units: 460\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 2.891228437423706 - val_accuracy: 0.2949 - penalty: 2e-05\n",
            "hidden layer sizes: [120, 52, 90, 84, 232], total units: 578\n",
            "Before pruning:\n",
            "loss: 2.9779112339019775 - accuracy: 0.268779993057251 - val_loss: 2.810983419418335 - val_accuracy: 0.3049 - penalty: 2e-05\n",
            "hidden layer sizes: [120, 52, 90, 84, 232], total units: 578\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 2.8109703063964844 - val_accuracy: 0.305 - penalty: 2e-05\n",
            "hidden layer sizes: [98, 30, 66, 61, 209], total units: 464\n",
            "##########################################################\n",
            "Epoch 7/40\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 2.8109703063964844 - val_accuracy: 0.305 - penalty: 2e-05\n",
            "hidden layer sizes: [98, 30, 66, 61, 209], total units: 464\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 2.8109703063964844 - val_accuracy: 0.305 - penalty: 2e-05\n",
            "hidden layer sizes: [118, 50, 86, 81, 250], total units: 585\n",
            "Before pruning:\n",
            "loss: 2.909067153930664 - accuracy: 0.28181999921798706 - val_loss: 2.7352731227874756 - val_accuracy: 0.3175 - penalty: 2e-05\n",
            "hidden layer sizes: [118, 50, 86, 81, 250], total units: 585\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 2.7352294921875 - val_accuracy: 0.3176 - penalty: 2e-05\n",
            "hidden layer sizes: [93, 29, 64, 64, 214], total units: 464\n",
            "##########################################################\n",
            "Epoch 8/40\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 2.7352294921875 - val_accuracy: 0.3176 - penalty: 2e-05\n",
            "hidden layer sizes: [93, 29, 64, 64, 214], total units: 464\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 2.735229730606079 - val_accuracy: 0.3176 - penalty: 2e-05\n",
            "hidden layer sizes: [113, 49, 84, 84, 256], total units: 586\n",
            "Before pruning:\n",
            "loss: 2.8367090225219727 - accuracy: 0.2979399859905243 - val_loss: 2.6538636684417725 - val_accuracy: 0.3382 - penalty: 2e-05\n",
            "hidden layer sizes: [113, 49, 84, 84, 256], total units: 586\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 2.653963565826416 - val_accuracy: 0.338 - penalty: 2e-05\n",
            "hidden layer sizes: [89, 27, 59, 63, 214], total units: 452\n",
            "##########################################################\n",
            "Epoch 9/40\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 2.653963565826416 - val_accuracy: 0.338 - penalty: 2e-05\n",
            "hidden layer sizes: [89, 27, 59, 63, 214], total units: 452\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 2.653963565826416 - val_accuracy: 0.338 - penalty: 2e-05\n",
            "hidden layer sizes: [109, 47, 79, 83, 256], total units: 574\n",
            "Before pruning:\n",
            "loss: 2.7689056396484375 - accuracy: 0.3087800145149231 - val_loss: 2.599806308746338 - val_accuracy: 0.3454 - penalty: 2e-05\n",
            "hidden layer sizes: [109, 47, 79, 83, 256], total units: 574\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 2.5998740196228027 - val_accuracy: 0.3454 - penalty: 2e-05\n",
            "hidden layer sizes: [89, 27, 54, 65, 218], total units: 453\n",
            "##########################################################\n",
            "Epoch 10/40\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 2.5998740196228027 - val_accuracy: 0.3454 - penalty: 2e-05\n",
            "hidden layer sizes: [89, 27, 54, 65, 218], total units: 453\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 2.5998740196228027 - val_accuracy: 0.3454 - penalty: 2e-05\n",
            "hidden layer sizes: [109, 47, 74, 85, 261], total units: 576\n",
            "Before pruning:\n",
            "loss: 2.7054312229156494 - accuracy: 0.32058000564575195 - val_loss: 2.568729877471924 - val_accuracy: 0.3512 - penalty: 2e-05\n",
            "hidden layer sizes: [109, 47, 74, 85, 261], total units: 576\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 2.568925619125366 - val_accuracy: 0.3512 - penalty: 2e-05\n",
            "hidden layer sizes: [82, 27, 50, 66, 227], total units: 452\n",
            "##########################################################\n",
            "Epoch 11/40\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 2.568925619125366 - val_accuracy: 0.3512 - penalty: 2e-05\n",
            "hidden layer sizes: [82, 27, 50, 66, 227], total units: 452\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 2.568925619125366 - val_accuracy: 0.3512 - penalty: 2e-05\n",
            "hidden layer sizes: [102, 47, 70, 86, 272], total units: 577\n",
            "Before pruning:\n",
            "loss: 2.670111656188965 - accuracy: 0.32923999428749084 - val_loss: 2.506255626678467 - val_accuracy: 0.3626 - penalty: 2e-05\n",
            "hidden layer sizes: [102, 47, 70, 86, 272], total units: 577\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 2.5063090324401855 - val_accuracy: 0.3627 - penalty: 2e-05\n",
            "hidden layer sizes: [78, 27, 45, 63, 227], total units: 440\n",
            "##########################################################\n",
            "Epoch 12/40\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 2.5063090324401855 - val_accuracy: 0.3627 - penalty: 2e-05\n",
            "hidden layer sizes: [78, 27, 45, 63, 227], total units: 440\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 2.5063092708587646 - val_accuracy: 0.3627 - penalty: 2e-05\n",
            "hidden layer sizes: [98, 47, 65, 83, 272], total units: 565\n",
            "Before pruning:\n",
            "loss: 2.629441261291504 - accuracy: 0.3353399932384491 - val_loss: 2.46703839302063 - val_accuracy: 0.3713 - penalty: 2e-05\n",
            "hidden layer sizes: [98, 47, 65, 83, 272], total units: 565\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 2.4670207500457764 - val_accuracy: 0.3719 - penalty: 2e-05\n",
            "hidden layer sizes: [75, 26, 43, 60, 228], total units: 432\n",
            "##########################################################\n",
            "Epoch 13/40\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 2.4670207500457764 - val_accuracy: 0.3719 - penalty: 2e-05\n",
            "hidden layer sizes: [75, 26, 43, 60, 228], total units: 432\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 2.4670207500457764 - val_accuracy: 0.3719 - penalty: 2e-05\n",
            "hidden layer sizes: [95, 46, 63, 80, 273], total units: 557\n",
            "Before pruning:\n",
            "loss: 2.598825454711914 - accuracy: 0.3413200080394745 - val_loss: 2.436244487762451 - val_accuracy: 0.3804 - penalty: 2e-05\n",
            "hidden layer sizes: [95, 46, 63, 80, 273], total units: 557\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 2.4362549781799316 - val_accuracy: 0.3801 - penalty: 2e-05\n",
            "hidden layer sizes: [72, 26, 37, 63, 229], total units: 427\n",
            "##########################################################\n",
            "Epoch 14/40\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 2.4362549781799316 - val_accuracy: 0.3801 - penalty: 2e-05\n",
            "hidden layer sizes: [72, 26, 37, 63, 229], total units: 427\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 2.4362549781799316 - val_accuracy: 0.3801 - penalty: 2e-05\n",
            "hidden layer sizes: [92, 46, 57, 83, 274], total units: 552\n",
            "Before pruning:\n",
            "loss: 2.5661332607269287 - accuracy: 0.3470599949359894 - val_loss: 2.4128286838531494 - val_accuracy: 0.3834 - penalty: 2e-05\n",
            "hidden layer sizes: [92, 46, 57, 83, 274], total units: 552\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 2.4127368927001953 - val_accuracy: 0.3836 - penalty: 2e-05\n",
            "hidden layer sizes: [69, 24, 34, 64, 226], total units: 417\n",
            "##########################################################\n",
            "Epoch 15/40\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 2.4127368927001953 - val_accuracy: 0.3836 - penalty: 2e-05\n",
            "hidden layer sizes: [69, 24, 34, 64, 226], total units: 417\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 2.412736654281616 - val_accuracy: 0.3836 - penalty: 2e-05\n",
            "hidden layer sizes: [89, 44, 54, 84, 271], total units: 542\n",
            "Before pruning:\n",
            "loss: 2.5488932132720947 - accuracy: 0.35076001286506653 - val_loss: 2.3928070068359375 - val_accuracy: 0.3845 - penalty: 2e-05\n",
            "hidden layer sizes: [89, 44, 54, 84, 271], total units: 542\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 2.3928215503692627 - val_accuracy: 0.385 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 22, 32, 67, 231], total units: 417\n",
            "##########################################################\n",
            "Epoch 16/40\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 2.3928215503692627 - val_accuracy: 0.385 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 22, 32, 67, 231], total units: 417\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 2.3928215503692627 - val_accuracy: 0.385 - penalty: 2e-05\n",
            "hidden layer sizes: [85, 42, 52, 87, 277], total units: 543\n",
            "Before pruning:\n",
            "loss: 2.522462844848633 - accuracy: 0.35580000281333923 - val_loss: 2.367138385772705 - val_accuracy: 0.3955 - penalty: 2e-05\n",
            "hidden layer sizes: [85, 42, 52, 87, 277], total units: 543\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 2.367140769958496 - val_accuracy: 0.3949 - penalty: 2e-05\n",
            "hidden layer sizes: [64, 22, 31, 69, 228], total units: 414\n",
            "##########################################################\n",
            "Epoch 17/40\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 2.367140769958496 - val_accuracy: 0.3949 - penalty: 2e-05\n",
            "hidden layer sizes: [64, 22, 31, 69, 228], total units: 414\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 2.367140769958496 - val_accuracy: 0.3949 - penalty: 2e-05\n",
            "hidden layer sizes: [84, 42, 51, 89, 273], total units: 539\n",
            "Before pruning:\n",
            "loss: 2.5087778568267822 - accuracy: 0.35760000348091125 - val_loss: 2.35250186920166 - val_accuracy: 0.4018 - penalty: 2e-05\n",
            "hidden layer sizes: [84, 42, 51, 89, 273], total units: 539\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 2.3525657653808594 - val_accuracy: 0.402 - penalty: 2e-05\n",
            "hidden layer sizes: [63, 22, 30, 67, 229], total units: 411\n",
            "##########################################################\n",
            "Epoch 18/40\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 2.3525657653808594 - val_accuracy: 0.402 - penalty: 2e-05\n",
            "hidden layer sizes: [63, 22, 30, 67, 229], total units: 411\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 2.3525655269622803 - val_accuracy: 0.402 - penalty: 2e-05\n",
            "hidden layer sizes: [83, 42, 50, 87, 274], total units: 536\n",
            "Before pruning:\n",
            "loss: 2.49056339263916 - accuracy: 0.36395999789237976 - val_loss: 2.335096597671509 - val_accuracy: 0.4071 - penalty: 2e-05\n",
            "hidden layer sizes: [83, 42, 50, 87, 274], total units: 536\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 2.3351385593414307 - val_accuracy: 0.4074 - penalty: 2e-05\n",
            "hidden layer sizes: [60, 22, 28, 68, 229], total units: 407\n",
            "##########################################################\n",
            "Epoch 19/40\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 2.3351385593414307 - val_accuracy: 0.4074 - penalty: 2e-05\n",
            "hidden layer sizes: [60, 22, 28, 68, 229], total units: 407\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 2.3351387977600098 - val_accuracy: 0.4074 - penalty: 2e-05\n",
            "hidden layer sizes: [80, 42, 48, 88, 274], total units: 532\n",
            "Before pruning:\n",
            "loss: 2.474853754043579 - accuracy: 0.36434000730514526 - val_loss: 2.322746992111206 - val_accuracy: 0.4009 - penalty: 2e-05\n",
            "hidden layer sizes: [80, 42, 48, 88, 274], total units: 532\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 2.322770595550537 - val_accuracy: 0.4007 - penalty: 2e-05\n",
            "hidden layer sizes: [59, 22, 28, 66, 229], total units: 404\n",
            "##########################################################\n",
            "Epoch 20/40\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 2.322770595550537 - val_accuracy: 0.4007 - penalty: 2e-05\n",
            "hidden layer sizes: [59, 22, 28, 66, 229], total units: 404\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 2.322770595550537 - val_accuracy: 0.4007 - penalty: 2e-05\n",
            "hidden layer sizes: [79, 42, 48, 86, 274], total units: 529\n",
            "Before pruning:\n",
            "loss: 2.456420421600342 - accuracy: 0.3689599931240082 - val_loss: 2.323631525039673 - val_accuracy: 0.4012 - penalty: 2e-05\n",
            "hidden layer sizes: [79, 42, 48, 86, 274], total units: 529\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 2.323789119720459 - val_accuracy: 0.4013 - penalty: 2e-05\n",
            "hidden layer sizes: [57, 22, 27, 69, 232], total units: 407\n",
            "##########################################################\n",
            "Epoch 21/40\n",
            "loss: 2.4838144779205322 - accuracy: 0.3684999942779541 - val_loss: 2.2542717456817627 - val_accuracy: 0.4208 - penalty: 0.0\n",
            "hidden layer sizes: [57, 22, 27, 69, 232], total units: 407\n",
            "##########################################################\n",
            "Epoch 22/40\n",
            "loss: 2.1958389282226562 - accuracy: 0.42838001251220703 - val_loss: 2.1808149814605713 - val_accuracy: 0.4331 - penalty: 0.0\n",
            "hidden layer sizes: [57, 22, 27, 69, 232], total units: 407\n",
            "##########################################################\n",
            "Epoch 23/40\n",
            "loss: 2.0788934230804443 - accuracy: 0.45767998695373535 - val_loss: 2.1656699180603027 - val_accuracy: 0.4434 - penalty: 0.0\n",
            "hidden layer sizes: [57, 22, 27, 69, 232], total units: 407\n",
            "##########################################################\n",
            "Epoch 24/40\n",
            "loss: 1.9762729406356812 - accuracy: 0.47863999009132385 - val_loss: 2.1576790809631348 - val_accuracy: 0.4434 - penalty: 0.0\n",
            "hidden layer sizes: [57, 22, 27, 69, 232], total units: 407\n",
            "##########################################################\n",
            "Epoch 25/40\n",
            "loss: 1.8821520805358887 - accuracy: 0.498879998922348 - val_loss: 2.1508190631866455 - val_accuracy: 0.4489 - penalty: 0.0\n",
            "hidden layer sizes: [57, 22, 27, 69, 232], total units: 407\n",
            "##########################################################\n",
            "Epoch 26/40\n",
            "loss: 1.784244418144226 - accuracy: 0.5199800133705139 - val_loss: 2.1646292209625244 - val_accuracy: 0.4508 - penalty: 0.0\n",
            "hidden layer sizes: [57, 22, 27, 69, 232], total units: 407\n",
            "##########################################################\n",
            "Epoch 27/40\n",
            "loss: 1.692240834236145 - accuracy: 0.5414999723434448 - val_loss: 2.170741558074951 - val_accuracy: 0.4492 - penalty: 0.0\n",
            "hidden layer sizes: [57, 22, 27, 69, 232], total units: 407\n",
            "##########################################################\n",
            "Epoch 28/40\n",
            "loss: 1.6143081188201904 - accuracy: 0.5579599738121033 - val_loss: 2.188553810119629 - val_accuracy: 0.4504 - penalty: 0.0\n",
            "hidden layer sizes: [57, 22, 27, 69, 232], total units: 407\n",
            "##########################################################\n",
            "Epoch 29/40\n",
            "loss: 1.5340301990509033 - accuracy: 0.5781199932098389 - val_loss: 2.195892095565796 - val_accuracy: 0.4511 - penalty: 0.0\n",
            "hidden layer sizes: [57, 22, 27, 69, 232], total units: 407\n",
            "##########################################################\n",
            "Epoch 30/40\n",
            "loss: 1.4521870613098145 - accuracy: 0.593779981136322 - val_loss: 2.223339319229126 - val_accuracy: 0.4492 - penalty: 0.0\n",
            "hidden layer sizes: [57, 22, 27, 69, 232], total units: 407\n",
            "##########################################################\n",
            "Epoch 31/40\n",
            "loss: 1.3904294967651367 - accuracy: 0.6098999977111816 - val_loss: 2.2660305500030518 - val_accuracy: 0.453 - penalty: 0.0\n",
            "hidden layer sizes: [57, 22, 27, 69, 232], total units: 407\n",
            "##########################################################\n",
            "Epoch 32/40\n",
            "loss: 1.3354294300079346 - accuracy: 0.622219979763031 - val_loss: 2.268756151199341 - val_accuracy: 0.4459 - penalty: 0.0\n",
            "hidden layer sizes: [57, 22, 27, 69, 232], total units: 407\n",
            "##########################################################\n",
            "Epoch 33/40\n",
            "loss: 1.2802320718765259 - accuracy: 0.6340600252151489 - val_loss: 2.2888760566711426 - val_accuracy: 0.4497 - penalty: 0.0\n",
            "hidden layer sizes: [57, 22, 27, 69, 232], total units: 407\n",
            "##########################################################\n",
            "Epoch 34/40\n",
            "loss: 1.226779580116272 - accuracy: 0.6456999778747559 - val_loss: 2.3313632011413574 - val_accuracy: 0.4481 - penalty: 0.0\n",
            "hidden layer sizes: [57, 22, 27, 69, 232], total units: 407\n",
            "##########################################################\n",
            "Epoch 35/40\n",
            "loss: 1.1743972301483154 - accuracy: 0.6586400270462036 - val_loss: 2.354907989501953 - val_accuracy: 0.4513 - penalty: 0.0\n",
            "hidden layer sizes: [57, 22, 27, 69, 232], total units: 407\n",
            "##########################################################\n",
            "Epoch 36/40\n",
            "loss: 1.1288738250732422 - accuracy: 0.6688600182533264 - val_loss: 2.370312213897705 - val_accuracy: 0.4536 - penalty: 0.0\n",
            "hidden layer sizes: [57, 22, 27, 69, 232], total units: 407\n",
            "##########################################################\n",
            "Epoch 37/40\n",
            "loss: 1.0819816589355469 - accuracy: 0.6825399994850159 - val_loss: 2.437800884246826 - val_accuracy: 0.4469 - penalty: 0.0\n",
            "hidden layer sizes: [57, 22, 27, 69, 232], total units: 407\n",
            "##########################################################\n",
            "Epoch 38/40\n",
            "loss: 1.0508049726486206 - accuracy: 0.6901599764823914 - val_loss: 2.437711238861084 - val_accuracy: 0.4499 - penalty: 0.0\n",
            "hidden layer sizes: [57, 22, 27, 69, 232], total units: 407\n",
            "##########################################################\n",
            "Epoch 39/40\n",
            "loss: 1.0040401220321655 - accuracy: 0.7031199932098389 - val_loss: 2.4660301208496094 - val_accuracy: 0.4479 - penalty: 0.0\n",
            "hidden layer sizes: [57, 22, 27, 69, 232], total units: 407\n",
            "##########################################################\n",
            "Epoch 40/40\n",
            "loss: 0.9733282327651978 - accuracy: 0.7082399725914001 - val_loss: 2.5028750896453857 - val_accuracy: 0.4489 - penalty: 0.0\n",
            "hidden layer sizes: [57, 22, 27, 69, 232], total units: 407\n",
            "CPU times: user 5min 18s, sys: 14.9 s, total: 5min 33s\n",
            "Wall time: 4min 37s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "histories = hyperparameter_search(train_fn, x=cifar100.X_train_norm, y=cifar100.y_train, validation_data=(cifar100.X_test_norm, cifar100.y_test), \n",
        "                                  learning_rate=[0.0002], regularization_penalty=[0.00002], regularization_method=['weighted_l1'], \n",
        "                                  self_scaling_epochs=[20], layer_sizes=[[65, 23, 29, 72, 201]], output_neurons=[100], min_new_neurons=[0, 1, 2, 4, 8, 16, 32], growth_percentage=[0.])"
      ],
      "metadata": {
        "id": "ShwjKpNOnXdM",
        "outputId": "5b3ac8e6-dca6-4673-d70c-37b0a6c04039",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run with parameters (0.0002, 2e-05, 'weighted_l1', 20, [65, 23, 29, 72, 201], 100, 0, 0.0) completed, best_val_accuracy: 0.4445, best_hidden_layer_sizes sizes: [59, 23, 29, 68, 201]\n",
            "Run with parameters (0.0002, 2e-05, 'weighted_l1', 20, [65, 23, 29, 72, 201], 100, 1, 0.0) completed, best_val_accuracy: 0.4427, best_hidden_layer_sizes sizes: [60, 23, 29, 70, 201]\n",
            "Run with parameters (0.0002, 2e-05, 'weighted_l1', 20, [65, 23, 29, 72, 201], 100, 2, 0.0) completed, best_val_accuracy: 0.4516, best_hidden_layer_sizes sizes: [62, 23, 29, 65, 203]\n",
            "Run with parameters (0.0002, 2e-05, 'weighted_l1', 20, [65, 23, 29, 72, 201], 100, 4, 0.0) completed, best_val_accuracy: 0.4449, best_hidden_layer_sizes sizes: [61, 23, 29, 72, 206]\n",
            "Run with parameters (0.0002, 2e-05, 'weighted_l1', 20, [65, 23, 29, 72, 201], 100, 8, 0.0) completed, best_val_accuracy: 0.4507, best_hidden_layer_sizes sizes: [59, 23, 29, 69, 210]\n",
            "Run with parameters (0.0002, 2e-05, 'weighted_l1', 20, [65, 23, 29, 72, 201], 100, 16, 0.0) completed, best_val_accuracy: 0.4421, best_hidden_layer_sizes sizes: [61, 23, 29, 70, 205]\n",
            "Run with parameters (0.0002, 2e-05, 'weighted_l1', 20, [65, 23, 29, 72, 201], 100, 32, 0.0) completed, best_val_accuracy: 0.4589, best_hidden_layer_sizes sizes: [58, 23, 29, 71, 207]\n",
            "Best overall combination: (0.0002, 2e-05, 'weighted_l1', 20, [65, 23, 29, 72, 201], 100, 32, 0.0), val_accuracy: 0.4589\n",
            "CPU times: user 21min 40s, sys: 50.7 s, total: 22min 30s\n",
            "Wall time: 18min\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "histories = hyperparameter_search(train_fn, x=cifar100.X_train_norm, y=cifar100.y_train, validation_data=(cifar100.X_test_norm, cifar100.y_test), \n",
        "                                  learning_rate=[0.0002], regularization_penalty=[0.00002], regularization_method=[None, 'weighted_l1'], \n",
        "                                  self_scaling_epochs=[0], layer_sizes=[[65, 23, 29, 72, 201]], output_neurons=[100], min_new_neurons=[0], growth_percentage=[0.])"
      ],
      "metadata": {
        "id": "YaYpOv8c4eRx",
        "outputId": "55f6edcf-238e-4001-ee73-38c94817660f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run with parameters (0.0002, 2e-05, None, 0, [65, 23, 29, 72, 201], 100, 0, 0.0) completed, best_val_accuracy: 0.3236, best_hidden_layer_sizes sizes: [65, 23, 29, 72, 201]\n",
            "Run with parameters (0.0002, 2e-05, 'weighted_l1', 0, [65, 23, 29, 72, 201], 100, 0, 0.0) completed, best_val_accuracy: 0.3375, best_hidden_layer_sizes sizes: [65, 23, 29, 72, 201]\n",
            "Best overall combination: (0.0002, 2e-05, 'weighted_l1', 0, [65, 23, 29, 72, 201], 100, 0, 0.0), val_accuracy: 0.3375\n",
            "CPU times: user 4min 49s, sys: 12.2 s, total: 5min 1s\n",
            "Wall time: 3min 51s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "histories = hyperparameter_search(train_fn, x=cifar100.X_train_norm, y=cifar100.y_train, validation_data=(cifar100.X_test_norm, cifar100.y_test), \n",
        "                                  learning_rate=[0.0002], regularization_penalty=[0.00002], regularization_method=['weighted_l1'], \n",
        "                                  self_scaling_epochs=[20], layer_sizes=[[65, 23, 29, 72, 201]], output_neurons=[100], min_new_neurons=[0], growth_percentage=[0.])"
      ],
      "metadata": {
        "id": "l96IUzPz69YX",
        "outputId": "33ebcf9e-6ac6-4fc4-dc4f-5536467d4a0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run with parameters (0.0002, 2e-05, 'weighted_l1', 20, [65, 23, 29, 72, 201], 100, 0, 0.0) completed, best_val_accuracy: 0.4537, best_hidden_layer_sizes sizes: [60, 23, 29, 65, 201]\n",
            "Best overall combination: (0.0002, 2e-05, 'weighted_l1', 20, [65, 23, 29, 72, 201], 100, 0, 0.0), val_accuracy: 0.4537\n",
            "CPU times: user 3min 1s, sys: 7.44 s, total: 3min 9s\n",
            "Wall time: 2min 29s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "histories = hyperparameter_search(train_fn, x=cifar100.X_train_norm, y=cifar100.y_train, validation_data=(cifar100.X_test_norm, cifar100.y_test), \n",
        "                                  learning_rate=[0.0002], regularization_penalty=[0.00002], regularization_method=['weighted_l1'], \n",
        "                                  self_scaling_epochs=[0, 20], layer_sizes=[[65, 23, 29, 72, 201]], output_neurons=[100], min_new_neurons=[0], growth_percentage=[0.])"
      ],
      "metadata": {
        "id": "D8vf4fag9jkV",
        "outputId": "371cef82-2a07-402e-e354-10fcacecca31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run with parameters (0.0002, 2e-05, 'weighted_l1', 0, [65, 23, 29, 72, 201], 100, 0, 0.0) completed, best_val_accuracy: 0.3259, best_hidden_layer_sizes sizes: [65, 23, 29, 72, 201]\n",
            "Run with parameters (0.0002, 2e-05, 'weighted_l1', 20, [65, 23, 29, 72, 201], 100, 0, 0.0) completed, best_val_accuracy: 0.4573, best_hidden_layer_sizes sizes: [61, 23, 29, 62, 200]\n",
            "Best overall combination: (0.0002, 2e-05, 'weighted_l1', 20, [65, 23, 29, 72, 201], 100, 0, 0.0), val_accuracy: 0.4573\n",
            "CPU times: user 5min 35s, sys: 14 s, total: 5min 49s\n",
            "Wall time: 4min 33s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "schedule = Schedule([EpochType.STATIC_WITH_REGULARIZATION] * 40)\n",
        "histories = hyperparameter_search(train_fn, x=cifar100.X_train_norm, y=cifar100.y_train, validation_data=(cifar100.X_test_norm, cifar100.y_test), \n",
        "                                  learning_rate=[0.0002], schedule=[schedule], regularization_penalty=[0.00002], regularization_method=[None, 'weighted_l1'], \n",
        "                                  layer_sizes=[[65, 23, 29, 72, 201]], output_neurons=[100], min_new_neurons=[0], growth_percentage=[0.])"
      ],
      "metadata": {
        "id": "6_BpUNyhsbHc",
        "outputId": "e4a2f7a0-2238-4c89-e80b-c9db41a15f75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run with parameters (0.0002, 1111111111111111111111111111111111111111, 2e-05, None, [65, 23, 29, 72, 201], 100, 0, 0.0) completed, best_val_accuracy: 0.3213, best_hidden_layer_sizes sizes: [65, 23, 29, 72, 201]\n",
            "Run with parameters (0.0002, 1111111111111111111111111111111111111111, 2e-05, 'weighted_l1', [65, 23, 29, 72, 201], 100, 0, 0.0) completed, best_val_accuracy: 0.4173, best_hidden_layer_sizes sizes: [65, 23, 29, 72, 201]\n",
            "Best overall combination: (0.0002, 1111111111111111111111111111111111111111, 2e-05, 'weighted_l1', [65, 23, 29, 72, 201], 100, 0, 0.0), val_accuracy: 0.4173\n",
            "CPU times: user 5min 3s, sys: 13.3 s, total: 5min 16s\n",
            "Wall time: 4min 2s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "schedule = Schedule([EpochType.STATIC_WITH_REGULARIZATION] * 20 + [EpochType.STATIC_NO_REGULARIZATION] * 20)\n",
        "histories = hyperparameter_search(train_fn, x=cifar100.X_train_norm, y=cifar100.y_train, validation_data=(cifar100.X_test_norm, cifar100.y_test), \n",
        "                                  learning_rate=[0.0002], schedule=[schedule], regularization_penalty=[0.00002], regularization_method=[None, 'weighted_l1'], \n",
        "                                  layer_sizes=[[65, 23, 29, 72, 201]], output_neurons=[100], min_new_neurons=[0], growth_percentage=[0.])"
      ],
      "metadata": {
        "id": "itosCeQJ1I1H",
        "outputId": "6a6a71e3-7d56-4a21-9aeb-233329844b76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run with parameters (0.0002, 1111111111111111111122222222222222222222, 2e-05, None, [65, 23, 29, 72, 201], 100, 0, 0.0) completed, best_val_accuracy: 0.3227, best_hidden_layer_sizes sizes: [65, 23, 29, 72, 201]\n",
            "Run with parameters (0.0002, 1111111111111111111122222222222222222222, 2e-05, 'weighted_l1', [65, 23, 29, 72, 201], 100, 0, 0.0) completed, best_val_accuracy: 0.4239, best_hidden_layer_sizes sizes: [65, 23, 29, 72, 201]\n",
            "Best overall combination: (0.0002, 1111111111111111111122222222222222222222, 2e-05, 'weighted_l1', [65, 23, 29, 72, 201], 100, 0, 0.0), val_accuracy: 0.4239\n",
            "CPU times: user 4min 56s, sys: 12.7 s, total: 5min 9s\n",
            "Wall time: 3min 56s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "schedule = Schedule([EpochType.DYNAMIC] * 20 + [EpochType.STATIC_NO_REGULARIZATION] * 20)\n",
        "histories = hyperparameter_search(train_fn, x=cifar100.X_train_norm, y=cifar100.y_train, validation_data=(cifar100.X_test_norm, cifar100.y_test), \n",
        "                                  learning_rate=[0.0002], schedule=[schedule], regularization_penalty=[0.00002], regularization_method=[None, 'weighted_l1'], \n",
        "                                  layer_sizes=[[65, 23, 29, 72, 201]], output_neurons=[100], min_new_neurons=[0], growth_percentage=[0.])"
      ],
      "metadata": {
        "id": "aC_kttDS8SuN",
        "outputId": "63eaf670-7b0d-4748-f090-73e76907ca35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run with parameters (0.0002, 0000000000000000000022222222222222222222, 2e-05, None, [65, 23, 29, 72, 201], 100, 0, 0.0) completed, best_val_accuracy: 0.3333, best_hidden_layer_sizes sizes: [65, 23, 29, 72, 201]\n",
            "Run with parameters (0.0002, 0000000000000000000022222222222222222222, 2e-05, 'weighted_l1', [65, 23, 29, 72, 201], 100, 0, 0.0) completed, best_val_accuracy: 0.4584, best_hidden_layer_sizes sizes: [61, 23, 29, 60, 201]\n",
            "Best overall combination: (0.0002, 0000000000000000000022222222222222222222, 2e-05, 'weighted_l1', [65, 23, 29, 72, 201], 100, 0, 0.0), val_accuracy: 0.4584\n",
            "CPU times: user 5min 46s, sys: 13.7 s, total: 6min\n",
            "Wall time: 4min 43s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## L1 regularization"
      ],
      "metadata": {
        "id": "fWKv7M3RQ9q5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "schedule = Schedule([EpochType.STATIC_WITH_REGULARIZATION] * 40)\n",
        "histories = hyperparameter_search(train_fn, x=cifar100.X_train_norm, y=cifar100.y_train, validation_data=(cifar100.X_test_norm, cifar100.y_test), \n",
        "                                  learning_rate=[0.0002], schedule=[schedule], regularization_penalty=[0., 0.00002, 0.0002, 0.002, 0.02], regularization_method=['l1'], \n",
        "                                  layer_sizes=[[65, 23, 29, 72, 201]], output_neurons=[100], min_new_neurons=[0], growth_percentage=[0.])"
      ],
      "metadata": {
        "id": "8DAHs2dAJQoe",
        "outputId": "8c8b874d-4f5e-4b88-f396-ec51f2cb19af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run with parameters (0.0002, 1111111111111111111111111111111111111111, 0.0, 'l1', [65, 23, 29, 72, 201], 100, 0, 0.0) completed, best_val_accuracy: 0.3278, best_hidden_layer_sizes sizes: [65, 23, 29, 72, 201]\n",
            "Run with parameters (0.0002, 1111111111111111111111111111111111111111, 2e-05, 'l1', [65, 23, 29, 72, 201], 100, 0, 0.0) completed, best_val_accuracy: 0.3348, best_hidden_layer_sizes sizes: [65, 23, 29, 72, 201]\n",
            "Run with parameters (0.0002, 1111111111111111111111111111111111111111, 0.0002, 'l1', [65, 23, 29, 72, 201], 100, 0, 0.0) completed, best_val_accuracy: 0.4063, best_hidden_layer_sizes sizes: [65, 23, 29, 72, 201]\n",
            "Run with parameters (0.0002, 1111111111111111111111111111111111111111, 0.002, 'l1', [65, 23, 29, 72, 201], 100, 0, 0.0) completed, best_val_accuracy: 0.2332, best_hidden_layer_sizes sizes: [65, 23, 29, 72, 201]\n",
            "Run with parameters (0.0002, 1111111111111111111111111111111111111111, 0.02, 'l1', [65, 23, 29, 72, 201], 100, 0, 0.0) completed, best_val_accuracy: 0.0103, best_hidden_layer_sizes sizes: [65, 23, 29, 72, 201]\n",
            "Best overall combination: (0.0002, 1111111111111111111111111111111111111111, 0.0002, 'l1', [65, 23, 29, 72, 201], 100, 0, 0.0), val_accuracy: 0.4063\n",
            "CPU times: user 12min 58s, sys: 35.9 s, total: 13min 34s\n",
            "Wall time: 10min 24s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "schedule = Schedule([EpochType.STATIC_WITH_REGULARIZATION] * 40)\n",
        "histories = hyperparameter_search(train_fn, x=cifar100.X_train_norm, y=cifar100.y_train, validation_data=(cifar100.X_test_norm, cifar100.y_test), \n",
        "                                  learning_rate=[0.0002], schedule=[schedule], regularization_penalty=[0.00005, 0.0001, 0.0002, 0.0004, 0.0008], regularization_method=['l1'], \n",
        "                                  layer_sizes=[[65, 23, 29, 72, 201]], output_neurons=[100], min_new_neurons=[0], growth_percentage=[0.])"
      ],
      "metadata": {
        "id": "8HBU2HJyNL46",
        "outputId": "d0a60ba4-ca1c-48fb-c7b4-1161abbe7b45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run with parameters (0.0002, 1111111111111111111111111111111111111111, 5e-05, 'l1', [65, 23, 29, 72, 201], 100, 0, 0.0) completed, best_val_accuracy: 0.3447, best_hidden_layer_sizes sizes: [65, 23, 29, 72, 201]\n",
            "Run with parameters (0.0002, 1111111111111111111111111111111111111111, 0.0001, 'l1', [65, 23, 29, 72, 201], 100, 0, 0.0) completed, best_val_accuracy: 0.3606, best_hidden_layer_sizes sizes: [65, 23, 29, 72, 201]\n",
            "Run with parameters (0.0002, 1111111111111111111111111111111111111111, 0.0002, 'l1', [65, 23, 29, 72, 201], 100, 0, 0.0) completed, best_val_accuracy: 0.4194, best_hidden_layer_sizes sizes: [65, 23, 29, 72, 201]\n",
            "Run with parameters (0.0002, 1111111111111111111111111111111111111111, 0.0004, 'l1', [65, 23, 29, 72, 201], 100, 0, 0.0) completed, best_val_accuracy: 0.4177, best_hidden_layer_sizes sizes: [65, 23, 29, 72, 201]\n",
            "Run with parameters (0.0002, 1111111111111111111111111111111111111111, 0.0008, 'l1', [65, 23, 29, 72, 201], 100, 0, 0.0) completed, best_val_accuracy: 0.3714, best_hidden_layer_sizes sizes: [65, 23, 29, 72, 201]\n",
            "Best overall combination: (0.0002, 1111111111111111111111111111111111111111, 0.0002, 'l1', [65, 23, 29, 72, 201], 100, 0, 0.0), val_accuracy: 0.4194\n",
            "CPU times: user 12min 54s, sys: 36.2 s, total: 13min 30s\n",
            "Wall time: 10min 18s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "schedule = Schedule([EpochType.STATIC_WITH_REGULARIZATION] * 40)\n",
        "histories = hyperparameter_search(train_fn, x=cifar100.X_train_norm, y=cifar100.y_train, validation_data=(cifar100.X_test_norm, cifar100.y_test), \n",
        "                                  learning_rate=[0.0002], schedule=[schedule], regularization_penalty=[0.0003], regularization_method=['l1'], \n",
        "                                  layer_sizes=[[65, 23, 29, 72, 201]], output_neurons=[100], min_new_neurons=[0], growth_percentage=[0.])"
      ],
      "metadata": {
        "id": "sdvVVnSIPsdA",
        "outputId": "fb596f80-2d2d-4fa1-e1bf-3b383093d0b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run with parameters (0.0002, 1111111111111111111111111111111111111111, 0.0003, 'l1', [65, 23, 29, 72, 201], 100, 0, 0.0) completed, best_val_accuracy: 0.4293, best_hidden_layer_sizes sizes: [65, 23, 29, 72, 201]\n",
            "Best overall combination: (0.0002, 1111111111111111111111111111111111111111, 0.0003, 'l1', [65, 23, 29, 72, 201], 100, 0, 0.0), val_accuracy: 0.4293\n",
            "CPU times: user 2min 33s, sys: 7.19 s, total: 2min 40s\n",
            "Wall time: 2min 2s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "schedule = Schedule([EpochType.STATIC_WITH_REGULARIZATION] * 40)\n",
        "histories = hyperparameter_search(train_fn, x=cifar100.X_train_norm, y=cifar100.y_train, validation_data=(cifar100.X_test_norm, cifar100.y_test), \n",
        "                                  learning_rate=[0.0002], schedule=[schedule], regularization_penalty=[0.0002, 0.0003, 0.0004], regularization_method=['l1'], \n",
        "                                  layer_sizes=[[66, 66, 66, 66, 201]], output_neurons=[100], min_new_neurons=[0], growth_percentage=[0.])"
      ],
      "metadata": {
        "id": "aqJHRNzZS0O9",
        "outputId": "c82747f0-e52b-49cc-c076-22821cbe00e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run with parameters (0.0002, 1111111111111111111111111111111111111111, 0.0002, 'l1', [66, 66, 66, 66, 201], 100, 0, 0.0) completed, best_val_accuracy: 0.4416, best_hidden_layer_sizes sizes: [66, 66, 66, 66, 201]\n",
            "Run with parameters (0.0002, 1111111111111111111111111111111111111111, 0.0003, 'l1', [66, 66, 66, 66, 201], 100, 0, 0.0) completed, best_val_accuracy: 0.4535, best_hidden_layer_sizes sizes: [66, 66, 66, 66, 201]\n",
            "Run with parameters (0.0002, 1111111111111111111111111111111111111111, 0.0004, 'l1', [66, 66, 66, 66, 201], 100, 0, 0.0) completed, best_val_accuracy: 0.441, best_hidden_layer_sizes sizes: [66, 66, 66, 66, 201]\n",
            "Best overall combination: (0.0002, 1111111111111111111111111111111111111111, 0.0003, 'l1', [66, 66, 66, 66, 201], 100, 0, 0.0), val_accuracy: 0.4535\n",
            "CPU times: user 8min, sys: 17.1 s, total: 8min 17s\n",
            "Wall time: 7min 14s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "model = get_convolutional_model(cifar100.X_train_norm, regularization_penalty=0.00002, regularization_method=None, layer_sizes=[65, 23, 29, 72, 201], output_neurons=100)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002)\n",
        "\n",
        "history = model.fit(cifar100.X_train_norm, cifar100.y_train, optimizer, epochs=40, self_scaling_epochs=20, batch_size=batch_size, \n",
        "                    min_new_neurons=0, validation_data=(cifar100.X_test_norm, cifar100.y_test), pruning_only_epochs=0, \n",
        "                    growth_percentage=0., verbose=True)"
      ],
      "metadata": {
        "id": "lZrp9FkE7_5n",
        "outputId": "db5be5c6-9677-408f-fbd5-0e0bf177d81d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##########################################################\n",
            "Epoch 1/40\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 5.157856464385986 - val_accuracy: 0.0083 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 5.157856464385986 - val_accuracy: 0.0083 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "Before pruning:\n",
            "loss: 4.065201759338379 - accuracy: 0.10670000314712524 - val_loss: 3.5247750282287598 - val_accuracy: 0.1916 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 3.5247750282287598 - val_accuracy: 0.1916 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "##########################################################\n",
            "Epoch 2/40\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 3.5247750282287598 - val_accuracy: 0.1916 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 3.5247750282287598 - val_accuracy: 0.1916 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "Before pruning:\n",
            "loss: 3.5729212760925293 - accuracy: 0.17825999855995178 - val_loss: 3.3440968990325928 - val_accuracy: 0.2164 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 3.3440968990325928 - val_accuracy: 0.2164 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "##########################################################\n",
            "Epoch 3/40\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 3.3440968990325928 - val_accuracy: 0.2164 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 3.3440968990325928 - val_accuracy: 0.2164 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "Before pruning:\n",
            "loss: 3.349431037902832 - accuracy: 0.21291999518871307 - val_loss: 3.198848247528076 - val_accuracy: 0.2406 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 3.198848247528076 - val_accuracy: 0.2406 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "##########################################################\n",
            "Epoch 4/40\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 3.198848247528076 - val_accuracy: 0.2406 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 3.198848247528076 - val_accuracy: 0.2406 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "Before pruning:\n",
            "loss: 3.187880039215088 - accuracy: 0.24267999827861786 - val_loss: 3.1080782413482666 - val_accuracy: 0.2564 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 3.1080782413482666 - val_accuracy: 0.2564 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "##########################################################\n",
            "Epoch 5/40\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 3.1080782413482666 - val_accuracy: 0.2564 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 3.1080782413482666 - val_accuracy: 0.2564 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "Before pruning:\n",
            "loss: 3.033339738845825 - accuracy: 0.2681399881839752 - val_loss: 3.058805465698242 - val_accuracy: 0.2718 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 3.058805465698242 - val_accuracy: 0.2718 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "##########################################################\n",
            "Epoch 6/40\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 3.058805465698242 - val_accuracy: 0.2718 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 3.058805465698242 - val_accuracy: 0.2718 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "Before pruning:\n",
            "loss: 2.9115965366363525 - accuracy: 0.2906799912452698 - val_loss: 2.978360414505005 - val_accuracy: 0.2797 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 2.978360414505005 - val_accuracy: 0.2797 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "##########################################################\n",
            "Epoch 7/40\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 2.978360414505005 - val_accuracy: 0.2797 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 2.978360414505005 - val_accuracy: 0.2797 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "Before pruning:\n",
            "loss: 2.796330213546753 - accuracy: 0.31130000948905945 - val_loss: 2.9387381076812744 - val_accuracy: 0.293 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 2.9387381076812744 - val_accuracy: 0.293 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "##########################################################\n",
            "Epoch 8/40\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 2.9387381076812744 - val_accuracy: 0.293 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 2.9387381076812744 - val_accuracy: 0.293 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "Before pruning:\n",
            "loss: 2.6931912899017334 - accuracy: 0.33215999603271484 - val_loss: 2.8953444957733154 - val_accuracy: 0.2977 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 2.8953444957733154 - val_accuracy: 0.2977 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "##########################################################\n",
            "Epoch 9/40\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 2.8953444957733154 - val_accuracy: 0.2977 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 2.8953444957733154 - val_accuracy: 0.2977 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "Before pruning:\n",
            "loss: 2.578977346420288 - accuracy: 0.35468000173568726 - val_loss: 2.886378765106201 - val_accuracy: 0.3036 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 2.886378765106201 - val_accuracy: 0.3036 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "##########################################################\n",
            "Epoch 10/40\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 2.886378765106201 - val_accuracy: 0.3036 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 2.886378765106201 - val_accuracy: 0.3036 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "Before pruning:\n",
            "loss: 2.487252950668335 - accuracy: 0.3716199994087219 - val_loss: 2.8758556842803955 - val_accuracy: 0.3082 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 2.8758556842803955 - val_accuracy: 0.3082 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "##########################################################\n",
            "Epoch 11/40\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 2.8758556842803955 - val_accuracy: 0.3082 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 2.8758556842803955 - val_accuracy: 0.3082 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "Before pruning:\n",
            "loss: 2.4155020713806152 - accuracy: 0.3852800130844116 - val_loss: 2.877117872238159 - val_accuracy: 0.3078 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 2.877117872238159 - val_accuracy: 0.3078 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "##########################################################\n",
            "Epoch 12/40\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 2.877117872238159 - val_accuracy: 0.3078 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 2.877117872238159 - val_accuracy: 0.3078 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "Before pruning:\n",
            "loss: 2.329409122467041 - accuracy: 0.402319997549057 - val_loss: 2.8944742679595947 - val_accuracy: 0.3127 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 2.8944742679595947 - val_accuracy: 0.3127 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "##########################################################\n",
            "Epoch 13/40\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 2.8944742679595947 - val_accuracy: 0.3127 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 2.8944742679595947 - val_accuracy: 0.3127 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "Before pruning:\n",
            "loss: 2.2553887367248535 - accuracy: 0.41822001338005066 - val_loss: 2.842076539993286 - val_accuracy: 0.3198 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 2.842076539993286 - val_accuracy: 0.3198 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "##########################################################\n",
            "Epoch 14/40\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 2.842076539993286 - val_accuracy: 0.3198 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 2.842076539993286 - val_accuracy: 0.3198 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "Before pruning:\n",
            "loss: 2.2035233974456787 - accuracy: 0.4280399978160858 - val_loss: 2.846912145614624 - val_accuracy: 0.3214 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 2.846912145614624 - val_accuracy: 0.3214 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "##########################################################\n",
            "Epoch 15/40\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 2.846912145614624 - val_accuracy: 0.3214 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 2.846912145614624 - val_accuracy: 0.3214 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "Before pruning:\n",
            "loss: 2.1467857360839844 - accuracy: 0.4435400068759918 - val_loss: 2.8552422523498535 - val_accuracy: 0.3189 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 2.8552422523498535 - val_accuracy: 0.3189 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "##########################################################\n",
            "Epoch 16/40\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 2.8552422523498535 - val_accuracy: 0.3189 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 2.8552422523498535 - val_accuracy: 0.3189 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "Before pruning:\n",
            "loss: 2.0946402549743652 - accuracy: 0.45155999064445496 - val_loss: 2.903836250305176 - val_accuracy: 0.3116 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 2.903836250305176 - val_accuracy: 0.3116 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "##########################################################\n",
            "Epoch 17/40\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 2.903836250305176 - val_accuracy: 0.3116 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 2.903836250305176 - val_accuracy: 0.3116 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "Before pruning:\n",
            "loss: 2.04494309425354 - accuracy: 0.4598200023174286 - val_loss: 2.9353861808776855 - val_accuracy: 0.3214 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 2.9353861808776855 - val_accuracy: 0.3214 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "##########################################################\n",
            "Epoch 18/40\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 2.9353861808776855 - val_accuracy: 0.3214 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 2.9353861808776855 - val_accuracy: 0.3214 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "Before pruning:\n",
            "loss: 2.009782552719116 - accuracy: 0.46658000349998474 - val_loss: 2.9045708179473877 - val_accuracy: 0.3198 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 2.9045708179473877 - val_accuracy: 0.3198 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "##########################################################\n",
            "Epoch 19/40\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 2.9045708179473877 - val_accuracy: 0.3198 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 2.9045708179473877 - val_accuracy: 0.3198 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "Before pruning:\n",
            "loss: 1.9685176610946655 - accuracy: 0.4777800142765045 - val_loss: 2.8966259956359863 - val_accuracy: 0.3257 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 2.8966259956359863 - val_accuracy: 0.3257 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "##########################################################\n",
            "Epoch 20/40\n",
            "Before growing:\n",
            "loss: None - accuracy: None - val_loss: 2.8966259956359863 - val_accuracy: 0.3257 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "After growing:\n",
            "loss: None - accuracy: None - val_loss: 2.8966259956359863 - val_accuracy: 0.3257 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "Before pruning:\n",
            "loss: 1.9309577941894531 - accuracy: 0.4853399991989136 - val_loss: 2.911456823348999 - val_accuracy: 0.3221 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "After pruning:\n",
            "loss: None - accuracy: None - val_loss: 2.911456823348999 - val_accuracy: 0.3221 - penalty: 2e-05\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "##########################################################\n",
            "Epoch 21/40\n",
            "loss: 1.895734190940857 - accuracy: 0.4927999973297119 - val_loss: 2.953035354614258 - val_accuracy: 0.323 - penalty: 0.0\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "##########################################################\n",
            "Epoch 22/40\n",
            "loss: 1.630110502243042 - accuracy: 0.5537400245666504 - val_loss: 2.9599382877349854 - val_accuracy: 0.3228 - penalty: 0.0\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "##########################################################\n",
            "Epoch 23/40\n",
            "loss: 1.5496588945388794 - accuracy: 0.5740000009536743 - val_loss: 2.960631847381592 - val_accuracy: 0.3258 - penalty: 0.0\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "##########################################################\n",
            "Epoch 24/40\n",
            "loss: 1.4891995191574097 - accuracy: 0.5875599980354309 - val_loss: 2.979832649230957 - val_accuracy: 0.3254 - penalty: 0.0\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "##########################################################\n",
            "Epoch 25/40\n",
            "loss: 1.456904649734497 - accuracy: 0.5933799743652344 - val_loss: 2.9904520511627197 - val_accuracy: 0.3281 - penalty: 0.0\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "##########################################################\n",
            "Epoch 26/40\n",
            "loss: 1.4324601888656616 - accuracy: 0.5974000096321106 - val_loss: 3.0426883697509766 - val_accuracy: 0.327 - penalty: 0.0\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "##########################################################\n",
            "Epoch 27/40\n",
            "loss: 1.385547161102295 - accuracy: 0.6122599840164185 - val_loss: 3.0661776065826416 - val_accuracy: 0.3266 - penalty: 0.0\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "##########################################################\n",
            "Epoch 28/40\n",
            "loss: 1.370500922203064 - accuracy: 0.6154400110244751 - val_loss: 3.084153413772583 - val_accuracy: 0.3302 - penalty: 0.0\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "##########################################################\n",
            "Epoch 29/40\n",
            "loss: 1.3446838855743408 - accuracy: 0.619379997253418 - val_loss: 3.058830738067627 - val_accuracy: 0.3281 - penalty: 0.0\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "##########################################################\n",
            "Epoch 30/40\n",
            "loss: 1.314578890800476 - accuracy: 0.6263399720191956 - val_loss: 3.11197829246521 - val_accuracy: 0.3288 - penalty: 0.0\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "##########################################################\n",
            "Epoch 31/40\n",
            "loss: 1.3052502870559692 - accuracy: 0.629859983921051 - val_loss: 3.16294527053833 - val_accuracy: 0.3263 - penalty: 0.0\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "##########################################################\n",
            "Epoch 32/40\n",
            "loss: 1.2803997993469238 - accuracy: 0.6313400268554688 - val_loss: 3.1292505264282227 - val_accuracy: 0.3315 - penalty: 0.0\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "##########################################################\n",
            "Epoch 33/40\n",
            "loss: 1.2594624757766724 - accuracy: 0.6357600092887878 - val_loss: 3.1777091026306152 - val_accuracy: 0.3275 - penalty: 0.0\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "##########################################################\n",
            "Epoch 34/40\n",
            "loss: 1.2384108304977417 - accuracy: 0.6413599848747253 - val_loss: 3.1524760723114014 - val_accuracy: 0.3294 - penalty: 0.0\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "##########################################################\n",
            "Epoch 35/40\n",
            "loss: 1.223309874534607 - accuracy: 0.6459599733352661 - val_loss: 3.2311999797821045 - val_accuracy: 0.3227 - penalty: 0.0\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "##########################################################\n",
            "Epoch 36/40\n",
            "loss: 1.2037484645843506 - accuracy: 0.6497799754142761 - val_loss: 3.1915900707244873 - val_accuracy: 0.3298 - penalty: 0.0\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "##########################################################\n",
            "Epoch 37/40\n",
            "loss: 1.200270414352417 - accuracy: 0.6505200266838074 - val_loss: 3.2524917125701904 - val_accuracy: 0.3253 - penalty: 0.0\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "##########################################################\n",
            "Epoch 38/40\n",
            "loss: 1.1940999031066895 - accuracy: 0.6520400047302246 - val_loss: 3.2098562717437744 - val_accuracy: 0.3284 - penalty: 0.0\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "##########################################################\n",
            "Epoch 39/40\n",
            "loss: 1.1589621305465698 - accuracy: 0.661080002784729 - val_loss: 3.237009048461914 - val_accuracy: 0.3242 - penalty: 0.0\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "##########################################################\n",
            "Epoch 40/40\n",
            "loss: 1.1579691171646118 - accuracy: 0.6603800058364868 - val_loss: 3.270716428756714 - val_accuracy: 0.3272 - penalty: 0.0\n",
            "hidden layer sizes: [65, 23, 29, 72, 201], total units: 390\n",
            "CPU times: user 3min 6s, sys: 7.57 s, total: 3min 14s\n",
            "Wall time: 2min 34s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBbX9M1TacAv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}