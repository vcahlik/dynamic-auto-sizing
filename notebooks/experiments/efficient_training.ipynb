{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "tf_multi_layer_ssnet_inverse.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_deAUKlniFk",
        "outputId": "10187627-fbba-435c-e2c8-519295842786"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Aug 18 14:12:00 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.57.02    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKwUwV_NneIo"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOoXBq05neIt"
      },
      "source": [
        "dtype = 'float32'\n",
        "tf.keras.backend.set_floatx(dtype)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfMGB9KZneIu"
      },
      "source": [
        "# fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "# (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# X_train = X_train.astype(dtype) / 255.0\n",
        "# y_train = y_train.astype(dtype)\n",
        "# X_test = X_test.astype(dtype)  / 255.0\n",
        "# y_test = y_test.astype(dtype)\n",
        "\n",
        "# X_train = np.reshape(X_train, (-1, 784))\n",
        "# X_test = np.reshape(X_test, (-1, 784))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BrJPdkBneIv",
        "outputId": "47db79e3-cba9-44da-aed6-222457c807e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "cifar10 = tf.keras.datasets.cifar10\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "X_train = X_train.astype(dtype) / 255.0\n",
        "y_train = y_train.astype(dtype)\n",
        "X_test = X_test.astype(dtype)  / 255.0\n",
        "y_test = y_test.astype(dtype)\n",
        "\n",
        "X_train = np.reshape(X_train, (-1, 3072))\n",
        "X_test = np.reshape(X_test, (-1, 3072))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 6s 0us/step\n",
            "170508288/170498071 [==============================] - 6s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FPbe4cCrHtb"
      },
      "source": [
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# glass = pd.read_csv('glass.data', header=None)\n",
        "# y = glass.pop(10)\n",
        "# X = glass\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# X_train = X_train.values.astype(dtype)\n",
        "# X_test = X_test.values.astype(dtype)\n",
        "# y_train = np.reshape(y_train.values, (-1, 1)).astype(dtype)\n",
        "# y_test = np.reshape(y_test.values, (-1, 1)).astype(dtype)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5uTvu5kxF-b"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "\n",
        "X_train_norm = scaler.transform(X_train)\n",
        "X_test_norm = scaler.transform(X_test)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTZq4KMpneIv"
      },
      "source": [
        "class SSRegularizer(tf.keras.regularizers.Regularizer):\n",
        "    def __init__(self, regularization_penalty, regularization_method):\n",
        "        self.regularization_penalty = regularization_penalty\n",
        "        self.regularization_method = regularization_method\n",
        "\n",
        "    def __call__(self, x):\n",
        "        if self.regularization_method == 'weighted_l1':\n",
        "            return self.weighted_l1(x)\n",
        "        elif self.regularization_method == 'group_sparsity':\n",
        "            return self.group_sparsity(x)\n",
        "        else:\n",
        "            raise NotImplementedError(f\"Unknown regularization method {self.regularization_method}\")\n",
        "    \n",
        "    def weighted_l1(self, x):\n",
        "        # I.e. for a parameter matrix of 4 input and 10 output neurons:\n",
        "        #\n",
        "        # [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
        "        #  [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
        "        #  [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
        "        #  [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]\n",
        "        #\n",
        "        # The scaling vector could be [0., 1., 2., 3., 4., 5., 6., 7., 8., 9.],\n",
        "        # and the resulting weighted values could be\n",
        "        #\n",
        "        # [[0., 1., 2., 3., 4., 5., 6., 7., 8., 9.],\n",
        "        #  [0., 1., 2., 3., 4., 5., 6., 7., 8., 9.],\n",
        "        #  [0., 1., 2., 3., 4., 5., 6., 7., 8., 9.],\n",
        "        #  [0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]]\n",
        "        #\n",
        "        # Therefore every additional output neuron is regularized more.\n",
        "\n",
        "        scaling_vector = tf.cumsum(tf.constant(self.regularization_penalty, shape=(x.shape[-1],), dtype=dtype), axis=0)\n",
        "        weighted_values = scaling_vector * tf.abs(x)\n",
        "        return tf.reduce_sum(weighted_values)\n",
        "    \n",
        "    def group_sparsity(self, x):\n",
        "        # I.e. for a parameter matrix of 3 input and 5 output neurons:\n",
        "        #\n",
        "        # [[1., 1., 1., 1., 1.],\n",
        "        #  [1., 2., 2., 1., 2.],\n",
        "        #  [2., 2., 3., 1., 3.]]\n",
        "        #\n",
        "        # The resulting vector of group norms is [2., 2., 3., 1., 3.], therefore for\n",
        "        # every output neuron, its incoming connections form a group.\n",
        "\n",
        "        group_norms = tf.norm(x, ord=2, axis=0)\n",
        "        # assert group_norms.shape[0] == x.shape[1]\n",
        "        return self.regularization_penalty * tf.reduce_sum(group_norms)\n",
        "\n",
        "    def get_config(self):\n",
        "        return {'regularization_penalty': float(self.regularization_penalty)}\n",
        "\n",
        "\n",
        "class SSLayer(tf.keras.Model):\n",
        "    def __init__(self, input_units, units, activation, regularization_penalty, regularization_method, kernel_initializer, bias_initializer, regularize=True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_units = input_units\n",
        "        self.units = units\n",
        "        self.activation = activation\n",
        "        self.regularization_penalty = regularization_penalty\n",
        "        self.regularization_method = regularization_method\n",
        "        self.kernel_initializer = kernel_initializer\n",
        "        self.bias_initializer = bias_initializer\n",
        "        \n",
        "        self.A = tf.keras.activations.get(activation)\n",
        "        self.W_init = tf.keras.initializers.get(kernel_initializer)\n",
        "        self.b_init = tf.keras.initializers.get(bias_initializer)\n",
        "        self.regularizer = SSRegularizer(self.regularization_penalty, self.regularization_method)\n",
        "        \n",
        "        self.W = tf.Variable(\n",
        "            name='W',\n",
        "            initial_value=self.W_init(shape=(input_units, units), dtype=dtype),\n",
        "            trainable=True)\n",
        "        \n",
        "        self.b = tf.Variable(\n",
        "            name='b',\n",
        "            initial_value=self.b_init(shape=(units,), dtype=dtype),\n",
        "            trainable=True)\n",
        "        \n",
        "        if self.regularization_method is not None:\n",
        "            self.add_loss(lambda: self.regularizer(tf.concat([self.W, tf.reshape(self.b, (1, -1))], axis=0)))\n",
        "    \n",
        "    def call(self, inputs):\n",
        "        return self.A(tf.matmul(inputs, self.W) + self.b)\n",
        "    \n",
        "    def copy_without_regularization(self):\n",
        "        copy = SSLayer(\n",
        "            self.input_units, \n",
        "            self.units, \n",
        "            self.activation, \n",
        "            regularization_penalty=self.regularization_penalty, \n",
        "            regularization_method=None, \n",
        "            kernel_initializer=self.kernel_initializer, \n",
        "            bias_initializer=self.bias_initializer\n",
        "        )\n",
        "        copy.W = self.W\n",
        "        copy.b = self.b\n",
        "        return copy\n",
        "\n",
        "\n",
        "class SSModel(tf.keras.Model):\n",
        "    def __init__(self, layer_sizes, activation=None, regularization_penalty=0.01, regularization_method='weighted_l1', kernel_initializer='glorot_uniform', bias_initializer='zeros'):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.sslayers = list()\n",
        "        for l in range(len(layer_sizes) - 1):\n",
        "            input_units = layer_sizes[l]\n",
        "            units = layer_sizes[l + 1]\n",
        "            if l < len(layer_sizes) - 2:\n",
        "                layer = SSLayer(input_units, units, activation, regularization_penalty, regularization_method, kernel_initializer, bias_initializer)\n",
        "            else:  # Last layer\n",
        "                layer = SSLayer(input_units, units, 'softmax', 0., regularization_method, kernel_initializer, bias_initializer)\n",
        "            self.sslayers.append(layer)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = inputs\n",
        "        for layer in self.sslayers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "    \n",
        "    def get_layer_sizes(self):\n",
        "        layer_sizes = list()\n",
        "        for l in range(len(self.sslayers)):\n",
        "            layer = self.sslayers[l]\n",
        "            layer_sizes.append(layer.W.shape[0])\n",
        "            if l == len(self.sslayers) - 1:  # Last layer\n",
        "                layer_sizes.append(layer.W.shape[1])\n",
        "        return layer_sizes\n",
        "    \n",
        "    def get_hidden_layer_sizes(self):\n",
        "        return self.get_layer_sizes()[1:-1]\n",
        "    \n",
        "    def print_neurons(self):\n",
        "        for layer in self.sslayers[:-1]:\n",
        "            print(get_param_string(layer.W, layer.b))\n",
        "    \n",
        "    def remove_regularization(self):\n",
        "        for l in range(len(self.sslayers)):\n",
        "            self.sslayers[l] = self.sslayers[l].copy_without_regularization()\n",
        "    \n",
        "    def get_regularization_penalty(self):\n",
        "        return self.sslayers[0].regularizer.regularization_penalty\n",
        "    \n",
        "    def set_regularization_penalty(self, regularization_penalty):\n",
        "        for l in range(0, len(self.sslayers) - 1):  # Every layer except of the last is regularized\n",
        "            self.sslayers[l].regularizer.regularization_penalty = regularization_penalty\n",
        "    \n",
        "    def prune(self, threshold=0.001):\n",
        "        for l in range(len(self.sslayers) - 1):\n",
        "            layer1 = self.sslayers[l]\n",
        "            layer2 = self.sslayers[l + 1]\n",
        "            \n",
        "            W1 = layer1.W.value()\n",
        "            b1 = layer1.b.value()\n",
        "            W2 = layer2.W.value()\n",
        "\n",
        "            weights_with_biases = tf.concat([W1, tf.reshape(b1, (1, -1))], axis=0)\n",
        "            neurons_are_active = tf.math.reduce_max(tf.abs(weights_with_biases), axis=0) >= threshold\n",
        "            active_neurons_indices = tf.reshape(tf.where(neurons_are_active), (-1,))\n",
        "            \n",
        "            new_W1 = tf.gather(W1, active_neurons_indices, axis=1)\n",
        "            new_b1 = tf.gather(b1, active_neurons_indices, axis=0)\n",
        "            new_W2 = tf.gather(W2, active_neurons_indices, axis=0)\n",
        "\n",
        "            layer1.W = tf.Variable(name='W', initial_value=new_W1, trainable=True)\n",
        "            layer1.b = tf.Variable(name='b', initial_value=new_b1, trainable=True)\n",
        "            layer2.W = tf.Variable(name='W', initial_value=new_W2, trainable=True)\n",
        "    \n",
        "    def grow(self, percentage, min_new_neurons, scaling_factor=0.001):\n",
        "        for l in range(len(self.sslayers) - 1):\n",
        "            layer1 = self.sslayers[l]\n",
        "            layer2 = self.sslayers[l + 1]\n",
        "       \n",
        "            W1 = layer1.W.value()\n",
        "            b1 = layer1.b.value()\n",
        "            W2 = layer2.W.value()\n",
        "\n",
        "            n_new_neurons = max(min_new_neurons, int(W1.shape[1] * percentage))\n",
        "\n",
        "            W1_growth = layer1.W_init(shape=(W1.shape[0], W1.shape[1] + n_new_neurons), dtype=dtype)[:, -n_new_neurons:] * scaling_factor\n",
        "            b1_growth = layer1.b_init(shape=(n_new_neurons,), dtype=dtype)\n",
        "            W2_growth = layer2.W_init(shape=(W2.shape[0] + n_new_neurons, W2.shape[1]), dtype=dtype)[-n_new_neurons:, :] #* scaling_factor  # TODO is it better to be multiplying here by scaling_factor? It does help with not increasing the max weights of existing neurons when new neurons are added.\n",
        "\n",
        "            new_W1 = tf.concat([W1, W1_growth], axis=1)\n",
        "            new_b1 = tf.concat([b1, b1_growth], axis=0)\n",
        "            new_W2 = tf.concat([W2, W2_growth], axis=0)\n",
        "\n",
        "            layer1.W = tf.Variable(name='W1', initial_value=new_W1, trainable=True)\n",
        "            layer1.b = tf.Variable(name='b1', initial_value=new_b1, trainable=True)\n",
        "            layer2.W = tf.Variable(name='W2', initial_value=new_W2, trainable=True)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WPKvoQQneIx"
      },
      "source": [
        "def get_param_string(weights, bias):\n",
        "    param_string = \"\"\n",
        "    weights_with_bias = tf.concat([weights, tf.reshape(bias, (1, -1))], axis=0)\n",
        "    max_parameters = tf.math.reduce_max(tf.abs(weights_with_bias), axis=0).numpy()\n",
        "    magnitudes = np.floor(np.log10(max_parameters))\n",
        "    for m in magnitudes:\n",
        "        if m > 0:\n",
        "            m = 0\n",
        "        param_string += str(int(-m))\n",
        "    return param_string\n",
        "\n",
        "\n",
        "def print_epoch_statistics(model, x, y, validation_data, print_neurons):\n",
        "    x_val = validation_data[0]\n",
        "    y_val = validation_data[1]\n",
        "\n",
        "    y_pred = model(x)\n",
        "    loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(y, y_pred))\n",
        "    accuracy = tf.reduce_mean(tf.keras.metrics.sparse_categorical_accuracy(y, y_pred))\n",
        "    \n",
        "    y_val_pred = model(x_val)\n",
        "    val_loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(y_val, y_val_pred))\n",
        "    val_accuracy = tf.reduce_mean(tf.keras.metrics.sparse_categorical_accuracy(y_val, y_val_pred))\n",
        "    print(f\"loss: {loss} - accuracy: {accuracy} - val_loss: {val_loss} - val_accuracy: {val_accuracy} - penalty: {model.get_regularization_penalty()}\")\n",
        "    hidden_layer_sizes = model.get_hidden_layer_sizes()\n",
        "    print(f\"hidden layer sizes: {hidden_layer_sizes}, total neurons: {sum(hidden_layer_sizes)}\")\n",
        "    if print_neurons:\n",
        "        model.print_neurons()\n",
        "\n",
        "    return val_loss\n",
        "\n",
        "\n",
        "def train_model(model, x, y, optimizer, epochs, self_scaling_epochs, batch_size, min_new_neurons, validation_data, \n",
        "                pruning_threshold=0.001, regularization_penalty_multiplier=1., growth_percentage=0.2, print_neurons=False):\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
        "    train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "\n",
        "    best_val_loss = np.inf\n",
        "    training_stalled = False\n",
        "    for epoch in range(epochs):\n",
        "        print(\"##########################################################\")\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "\n",
        "        if epoch < self_scaling_epochs:\n",
        "            print(\"Before growing:\")\n",
        "\n",
        "            val_loss = print_epoch_statistics(model, x, y, validation_data, print_neurons)\n",
        "            if val_loss >= best_val_loss:\n",
        "                if not training_stalled:\n",
        "                    penalty = model.get_regularization_penalty() * regularization_penalty_multiplier\n",
        "                    model.set_regularization_penalty(penalty)\n",
        "                    training_stalled = True\n",
        "            else:\n",
        "                best_val_loss = val_loss\n",
        "                training_stalled = False\n",
        "\n",
        "            model.grow(percentage=growth_percentage, min_new_neurons=min_new_neurons, scaling_factor=pruning_threshold)\n",
        "            print(\"After growing:\")\n",
        "            print_epoch_statistics(model, x, y, validation_data, print_neurons)\n",
        "        \n",
        "        if epoch == self_scaling_epochs:\n",
        "            model.remove_regularization()\n",
        "\n",
        "        for step, (x_batch, y_batch) in enumerate(train_dataset):\n",
        "            with tf.GradientTape() as tape:\n",
        "                y_pred = model(x_batch, training=True)\n",
        "                loss_value = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(y_batch, y_pred))\n",
        "                loss_value += sum(model.losses)\n",
        "\n",
        "            grads = tape.gradient(loss_value, model.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "        \n",
        "        if epoch < self_scaling_epochs:\n",
        "            print(\"Before pruning:\")\n",
        "            print_epoch_statistics(model, x, y, validation_data, print_neurons)\n",
        "            model.prune(threshold=pruning_threshold)\n",
        "            print(\"After pruning:\")\n",
        "            print_epoch_statistics(model, x, y, validation_data, print_neurons)\n",
        "        else:\n",
        "            print_epoch_statistics(model, x, y, validation_data, print_neurons)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1MrQXUTFwOe"
      },
      "source": [
        "# Efficient training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vn1eykLxn7hx"
      },
      "source": [
        "## Cifar 10 dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOgW8Vb5Ghcg"
      },
      "source": [
        "epochs = 10\n",
        "self_scaling_epochs = 10\n",
        "batch_size = 32\n",
        "min_new_neurons = 20"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdrNMmzqGBMV",
        "outputId": "466d3fea-3f4b-4e32-8d6a-e24ea986aad7"
      },
      "source": [
        "%%time\n",
        "\n",
        "model = SSModel(layer_sizes=[3072, 1000, 1000, 1000, 1000, 10], activation='selu', regularization_penalty=0.0001, regularization_method='weighted_l1', kernel_initializer='lecun_normal')\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002)\n",
        "\n",
        "train_model(model, X_train_norm, y_train, optimizer, epochs, self_scaling_epochs, batch_size, min_new_neurons, validation_data=(X_test_norm, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "Before growing:\n",
            "loss: 2.7915027141571045 - accuracy: 0.11311999708414078 - val_loss: 2.766880750656128 - val_accuracy: 0.11949999630451202\n",
            "layer sizes: [3072, 1000, 1000, 1000, 1000, 10]\n",
            "2222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "2222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "2222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "2222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "After growing:\n",
            "loss: 2.7915027141571045 - accuracy: 0.11311999708414078 - val_loss: 2.766880750656128 - val_accuracy: 0.11949999630451202\n",
            "layer sizes: [3072, 1200, 1200, 1200, 1200, 10]\n",
            "222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222255555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555\n",
            "222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222255555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555\n",
            "222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222255555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555\n",
            "222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222255555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555\n",
            "Before pruning:\n",
            "loss: 2.006772518157959 - accuracy: 0.186039999127388 - val_loss: 2.010066270828247 - val_accuracy: 0.1859000027179718\n",
            "layer sizes: [3072, 1200, 1200, 1200, 1200, 10]\n",
            "222222222444444554444544454444544555544454454454544455455545455545544455555555555445554545445555455555555455554455555455454444554555555554555555555545554555544555555555555555555455455555555555555555555555555455455545555555555555455555554555555555555545555555555555555455555445455455545555555555555555555555555555555544555545554545555555555555555555555555545555544555555555555554555555545455555555555555555555555555555545555545545555554555555555555555555555555555555455555555555555555555555555555455555555555555555555555555554555555555454545555555455555555555555554555555545545555555555555555555555545555555554555555545555555555555554555455554555554555555554555555555555455555555554455455455545555555555545555555545555555555555554555545554455555555555554555555455545455555555555555545555555455555555555555555555555545544455555445454554555554455545554545454554554555455545555545454555555545554555555555555454555555545555545554455545554555454545555555545555555455554544455555545555454554554545455455555455555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555\n",
            "122111121252111224422255425552244445444444455554554545445544454444454544444444444444454454454444444454444444445444445454544454545544444444444445444444544444444444444454544444444444444444444445445444545444444544555444444444544444544554545444444444454444444444445454445454454444445445454444444444444444444444545444544454445444444444444445445444555544544444444444444445444454444444454444455454454445454444544444444454544544445545445444454444444444444445444444444544545445454444554445444444444444444544544444444444454454444444444445444454444444455444454544444444444444454454445544444544444444544444544444444444444455454444444444444444444444444444444554444444444444444444444455445544444544444544445445444545444444454454444444454444444444454445444444444544444454444444444444444455444444445444444445544444454444444444444444544444444444444444444454444445444544444444444544445445444444444444445444445444444454445444444444444444444444444445444444444444444444454444444444544455455444444444454444445544444444444455555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555\n",
            "111111511121144455155245444545544444555444455444455445454545544554545545444455445444444444545544555454455555454555544454445455455544445544455545444455455545444544545445445554545454554444455544445554455555445444445454545454445455445554444544545545554454554445544454444554554445555555554444554555445445545445545454545445544554554554444454545445444455454444444555445555445455454444555444454445444454555544555455545445445445545444554444454555544555455445555445555555444455454544444554454444445544444444445544454444455444445545444545445544554444544454444454454454555445444545544544444454444454554444554444544545454454544545445554444554455544454454444444554454445444454444544545554445544444454444554555454544455544455445444444455444455444445444544554444544544444444555444444444544544544455444544444544544554444444444545444444445444444545444454544444444554444455445454445554455454554444444444555554544444444454445454454445445454554454444445445545545444444444545544444444454444445444444544445544555454444545455555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555\n",
            "111121111212211112142222322422422444434342444244444444444444444444445444444454444444445445444444444444445454444454454444445444444544554545554444444445444545544454544444555444545444445444444444544545445554444444445455444445454444545545545444544445544455454444454445554554554554554455555445454545555545445554544455445445445454444445445454544454444544454445454554445545545544544555545544554445445545445445544445555445444545554555455445445544445554544554454544444544544444445445445455555554455555444445555445544554544454444445555455444454455554455555555554554544555444554544454454554554545445454444444445454445545554445555544454455545444445545455545444544445455554455554544445545545444454544445545555554444555444555444545544455444454545545454445444454444545444545545455454444544445455545444455454444454444444555454444444444455444545554554454445454444454455455544444444455445555455444544454544454544545454454544555444445444544444444444445445545444454545444554444444444544445445544444445555454444454545444555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555\n",
            "After pruning:\n",
            "loss: 2.008625030517578 - accuracy: 0.1859000027179718 - val_loss: 2.0118825435638428 - val_accuracy: 0.1850000023841858\n",
            "layer sizes: [3072, 9, 21, 14, 34, 10]\n",
            "222222222\n",
            "122111121211122222222\n",
            "11111111121112\n",
            "1111211112122111121222232222223322\n",
            "Epoch 2/10\n",
            "Before growing:\n",
            "loss: 2.008625030517578 - accuracy: 0.1859000027179718 - val_loss: 2.0118825435638428 - val_accuracy: 0.1850000023841858\n",
            "layer sizes: [3072, 9, 21, 14, 34, 10]\n",
            "222222222\n",
            "122111121211122222222\n",
            "11111111121112\n",
            "1111211112122111121222232222223322\n",
            "After growing:\n",
            "loss: 2.008624792098999 - accuracy: 0.1859000027179718 - val_loss: 2.011882781982422 - val_accuracy: 0.1850000023841858\n",
            "layer sizes: [3072, 29, 41, 34, 54, 10]\n",
            "22222222255555555555555555555\n",
            "12211112121112222222244444444444444444444\n",
            "1111111112111244444444444444444444\n",
            "111121111212211112122223222222332244444444444444444444\n",
            "Before pruning:\n",
            "loss: 1.9791231155395508 - accuracy: 0.2035599946975708 - val_loss: 1.9832959175109863 - val_accuracy: 0.19869999587535858\n",
            "layer sizes: [3072, 29, 41, 34, 54, 10]\n",
            "12214512445554445444445445445\n",
            "11111112151125522252255555555555555555555\n",
            "1111111113121555555555555555555555\n",
            "111121111211221222122234322422444454445444455544444444\n",
            "After pruning:\n",
            "loss: 1.9792321920394897 - accuracy: 0.20374000072479248 - val_loss: 1.9834089279174805 - val_accuracy: 0.1987999975681305\n",
            "layer sizes: [3072, 6, 17, 13, 28, 10]\n",
            "122112\n",
            "11111112111222222\n",
            "1111111113121\n",
            "1111211112112212221222332222\n",
            "Epoch 3/10\n",
            "Before growing:\n",
            "loss: 1.9792321920394897 - accuracy: 0.20374000072479248 - val_loss: 1.9834089279174805 - val_accuracy: 0.1987999975681305\n",
            "layer sizes: [3072, 6, 17, 13, 28, 10]\n",
            "122112\n",
            "11111112111222222\n",
            "1111111113121\n",
            "1111211112112212221222332222\n",
            "After growing:\n",
            "loss: 1.9792320728302002 - accuracy: 0.20374000072479248 - val_loss: 1.9834089279174805 - val_accuracy: 0.1987999975681305\n",
            "layer sizes: [3072, 26, 37, 33, 48, 10]\n",
            "12211255555555555555555555\n",
            "1111111211122222244444444444444444444\n",
            "111111111312144444444444444444444\n",
            "111121111211221222122233222244444444444444444444\n",
            "Before pruning:\n",
            "loss: 1.8738772869110107 - accuracy: 0.2679600119590759 - val_loss: 1.8785133361816406 - val_accuracy: 0.2687000036239624\n",
            "layer sizes: [3072, 26, 37, 33, 48, 10]\n",
            "11212244444444444444444444\n",
            "1111111111121251255555555545555555545\n",
            "111111111511155555555555555555555\n",
            "111111111111221122121223422143114431444222444444\n",
            "After pruning:\n",
            "loss: 1.8738850355148315 - accuracy: 0.2679600119590759 - val_loss: 1.8785232305526733 - val_accuracy: 0.2689000070095062\n",
            "layer sizes: [3072, 6, 16, 12, 35, 10]\n",
            "112122\n",
            "1111111111121212\n",
            "111111111111\n",
            "11111111111122112212122322131131222\n",
            "Epoch 4/10\n",
            "Before growing:\n",
            "loss: 1.8738850355148315 - accuracy: 0.2679600119590759 - val_loss: 1.8785232305526733 - val_accuracy: 0.2689000070095062\n",
            "layer sizes: [3072, 6, 16, 12, 35, 10]\n",
            "112122\n",
            "1111111111121212\n",
            "111111111111\n",
            "11111111111122112212122322131131222\n",
            "After growing:\n",
            "loss: 1.8738850355148315 - accuracy: 0.2679600119590759 - val_loss: 1.8785232305526733 - val_accuracy: 0.2689000070095062\n",
            "layer sizes: [3072, 26, 36, 32, 55, 10]\n",
            "11212255555555555555555555\n",
            "111111111112121244444444444444444444\n",
            "11111111111144444444444444444444\n",
            "1111111111112211221212232213113122244444444444444444444\n",
            "Before pruning:\n",
            "loss: 1.8299658298492432 - accuracy: 0.3102400004863739 - val_loss: 1.8361601829528809 - val_accuracy: 0.31040000915527344\n",
            "layer sizes: [3072, 26, 36, 32, 55, 10]\n",
            "11112145444444444445444444\n",
            "111111111112111155555555555555555555\n",
            "11111111111155555555555555555555\n",
            "1111111111111111121111232114114122344444444444444454444\n",
            "After pruning:\n",
            "loss: 1.830153465270996 - accuracy: 0.3100399971008301 - val_loss: 1.8363468647003174 - val_accuracy: 0.3100999891757965\n",
            "layer sizes: [3072, 6, 16, 12, 33, 10]\n",
            "111121\n",
            "1111111111121111\n",
            "111111111111\n",
            "111111111111111112111123211111223\n",
            "Epoch 5/10\n",
            "Before growing:\n",
            "loss: 1.830153465270996 - accuracy: 0.3100399971008301 - val_loss: 1.8363468647003174 - val_accuracy: 0.3100999891757965\n",
            "layer sizes: [3072, 6, 16, 12, 33, 10]\n",
            "111121\n",
            "1111111111121111\n",
            "111111111111\n",
            "111111111111111112111123211111223\n",
            "After growing:\n",
            "loss: 1.8301535844802856 - accuracy: 0.3100399971008301 - val_loss: 1.8363471031188965 - val_accuracy: 0.3100999891757965\n",
            "layer sizes: [3072, 26, 36, 32, 53, 10]\n",
            "11112155555555555555555555\n",
            "111111111112111144444444444444444444\n",
            "11111111111144444444444444444444\n",
            "11111111111111111211112321111122344444444444444444444\n",
            "Before pruning:\n",
            "loss: 1.7904762029647827 - accuracy: 0.33281999826431274 - val_loss: 1.7992058992385864 - val_accuracy: 0.32760000228881836\n",
            "layer sizes: [3072, 26, 36, 32, 53, 10]\n",
            "11112144444444444444444444\n",
            "111111111112111155555555455555555555\n",
            "11111111121155555555555555555555\n",
            "11111111111111111211112311111132245444444454444444545\n",
            "After pruning:\n",
            "loss: 1.7905278205871582 - accuracy: 0.3328000009059906 - val_loss: 1.799251914024353 - val_accuracy: 0.3276999890804291\n",
            "layer sizes: [3072, 6, 16, 12, 33, 10]\n",
            "111121\n",
            "1111111111121111\n",
            "111111111211\n",
            "111111111111111112111123111111322\n",
            "Epoch 6/10\n",
            "Before growing:\n",
            "loss: 1.7905278205871582 - accuracy: 0.3328000009059906 - val_loss: 1.799251914024353 - val_accuracy: 0.3276999890804291\n",
            "layer sizes: [3072, 6, 16, 12, 33, 10]\n",
            "111121\n",
            "1111111111121111\n",
            "111111111211\n",
            "111111111111111112111123111111322\n",
            "After growing:\n",
            "loss: 1.7905279397964478 - accuracy: 0.3328000009059906 - val_loss: 1.7992521524429321 - val_accuracy: 0.3276999890804291\n",
            "layer sizes: [3072, 26, 36, 32, 53, 10]\n",
            "11112155555555555555555555\n",
            "111111111112111144444444444444444444\n",
            "11111111121144444444444444444444\n",
            "11111111111111111211112311111132244444444444444444444\n",
            "Before pruning:\n",
            "loss: 1.7708803415298462 - accuracy: 0.3397200107574463 - val_loss: 1.7796789407730103 - val_accuracy: 0.3361000120639801\n",
            "layer sizes: [3072, 26, 36, 32, 53, 10]\n",
            "11112145545444444444444444\n",
            "111111111112111155554555555555555555\n",
            "11111111122155555555555555555555\n",
            "11111111111111111311112312111233244454444444444544444\n",
            "After pruning:\n",
            "loss: 1.7708762884140015 - accuracy: 0.33965998888015747 - val_loss: 1.779669165611267 - val_accuracy: 0.3361000120639801\n",
            "layer sizes: [3072, 6, 16, 12, 33, 10]\n",
            "111121\n",
            "1111111111121111\n",
            "111111111221\n",
            "111111111111111113111123121112332\n",
            "Epoch 7/10\n",
            "Before growing:\n",
            "loss: 1.7708762884140015 - accuracy: 0.33965998888015747 - val_loss: 1.779669165611267 - val_accuracy: 0.3361000120639801\n",
            "layer sizes: [3072, 6, 16, 12, 33, 10]\n",
            "111121\n",
            "1111111111121111\n",
            "111111111221\n",
            "111111111111111113111123121112332\n",
            "After growing:\n",
            "loss: 1.7708762884140015 - accuracy: 0.33965998888015747 - val_loss: 1.779669165611267 - val_accuracy: 0.3361000120639801\n",
            "layer sizes: [3072, 26, 36, 32, 53, 10]\n",
            "11112155555555555555555555\n",
            "111111111112111144444444444444444444\n",
            "11111111122144444444444444444444\n",
            "11111111111111111311112312111233244444444444444444444\n",
            "Before pruning:\n",
            "loss: 1.7550849914550781 - accuracy: 0.3478600084781647 - val_loss: 1.7649648189544678 - val_accuracy: 0.3425999879837036\n",
            "layer sizes: [3072, 26, 36, 32, 53, 10]\n",
            "11111145444545544454455444\n",
            "111111111112111155555555555555555555\n",
            "11111111122155555555555555555555\n",
            "11111111111111111311112312122244244444454444444444444\n",
            "After pruning:\n",
            "loss: 1.7550926208496094 - accuracy: 0.34790000319480896 - val_loss: 1.7649720907211304 - val_accuracy: 0.3425000011920929\n",
            "layer sizes: [3072, 6, 16, 12, 31, 10]\n",
            "111111\n",
            "1111111111121111\n",
            "111111111221\n",
            "1111111111111111131111231212222\n",
            "Epoch 8/10\n",
            "Before growing:\n",
            "loss: 1.7550926208496094 - accuracy: 0.34790000319480896 - val_loss: 1.7649720907211304 - val_accuracy: 0.3425000011920929\n",
            "layer sizes: [3072, 6, 16, 12, 31, 10]\n",
            "111111\n",
            "1111111111121111\n",
            "111111111221\n",
            "1111111111111111131111231212222\n",
            "After growing:\n",
            "loss: 1.7550925016403198 - accuracy: 0.34790000319480896 - val_loss: 1.7649720907211304 - val_accuracy: 0.3425000011920929\n",
            "layer sizes: [3072, 26, 36, 32, 51, 10]\n",
            "11111155555555555555555555\n",
            "111111111112111144444444444444444444\n",
            "11111111122144444444444444444444\n",
            "111111111111111113111123121222244444444444444444444\n",
            "Before pruning:\n",
            "loss: 1.75437593460083 - accuracy: 0.34553998708724976 - val_loss: 1.7662519216537476 - val_accuracy: 0.3379000127315521\n",
            "layer sizes: [3072, 26, 36, 32, 51, 10]\n",
            "11111154445554444444444444\n",
            "111111111112111155455455555554555555\n",
            "11111111122155555555455555555555\n",
            "111111111111111113211223121222344444444444444444444\n",
            "After pruning:\n",
            "loss: 1.7543823719024658 - accuracy: 0.34558001160621643 - val_loss: 1.7662583589553833 - val_accuracy: 0.337799996137619\n",
            "layer sizes: [3072, 6, 16, 12, 31, 10]\n",
            "111111\n",
            "1111111111121111\n",
            "111111111221\n",
            "1111111111111111132112231212223\n",
            "Epoch 9/10\n",
            "Before growing:\n",
            "loss: 1.7543823719024658 - accuracy: 0.34558001160621643 - val_loss: 1.7662583589553833 - val_accuracy: 0.337799996137619\n",
            "layer sizes: [3072, 6, 16, 12, 31, 10]\n",
            "111111\n",
            "1111111111121111\n",
            "111111111221\n",
            "1111111111111111132112231212223\n",
            "After growing:\n",
            "loss: 1.7543823719024658 - accuracy: 0.34558001160621643 - val_loss: 1.7662583589553833 - val_accuracy: 0.337799996137619\n",
            "layer sizes: [3072, 26, 36, 32, 51, 10]\n",
            "11111155555555555555555555\n",
            "111111111112111144444444444444444444\n",
            "11111111122144444444444444444444\n",
            "111111111111111113211223121222344444444444444444444\n",
            "Before pruning:\n",
            "loss: 1.7563860416412354 - accuracy: 0.3457399904727936 - val_loss: 1.7703708410263062 - val_accuracy: 0.33719998598098755\n",
            "layer sizes: [3072, 26, 36, 32, 51, 10]\n",
            "11111145544455544444444454\n",
            "111111111112111155555555555555555555\n",
            "11111111122255555555455555555545\n",
            "111111111111111113211224121222344444444444454445444\n",
            "After pruning:\n",
            "loss: 1.7564443349838257 - accuracy: 0.34564000368118286 - val_loss: 1.7704277038574219 - val_accuracy: 0.33709999918937683\n",
            "layer sizes: [3072, 6, 16, 12, 30, 10]\n",
            "111111\n",
            "1111111111121111\n",
            "111111111222\n",
            "111111111111111113211221212223\n",
            "Epoch 10/10\n",
            "Before growing:\n",
            "loss: 1.7564443349838257 - accuracy: 0.34564000368118286 - val_loss: 1.7704277038574219 - val_accuracy: 0.33709999918937683\n",
            "layer sizes: [3072, 6, 16, 12, 30, 10]\n",
            "111111\n",
            "1111111111121111\n",
            "111111111222\n",
            "111111111111111113211221212223\n",
            "After growing:\n",
            "loss: 1.7564443349838257 - accuracy: 0.34564000368118286 - val_loss: 1.7704277038574219 - val_accuracy: 0.33709999918937683\n",
            "layer sizes: [3072, 26, 36, 32, 50, 10]\n",
            "11111155555555555555555555\n",
            "111111111112111144444444444444444444\n",
            "11111111122244444444444444444444\n",
            "11111111111111111321122121222344444444444444444444\n",
            "Before pruning:\n",
            "loss: 1.7317959070205688 - accuracy: 0.35662001371383667 - val_loss: 1.7456492185592651 - val_accuracy: 0.3537999987602234\n",
            "layer sizes: [3072, 26, 36, 32, 50, 10]\n",
            "11111144445444444444444444\n",
            "111111111112111155555555555555555555\n",
            "11111111122255555555555555555555\n",
            "11111111111111111421122121222344444544544445445445\n",
            "After pruning:\n",
            "loss: 1.731806755065918 - accuracy: 0.3566800057888031 - val_loss: 1.7456566095352173 - val_accuracy: 0.3537999987602234\n",
            "layer sizes: [3072, 6, 16, 12, 29, 10]\n",
            "111111\n",
            "1111111111121111\n",
            "111111111222\n",
            "11111111111111111211221212223\n",
            "CPU times: user 5min 46s, sys: 4.84 s, total: 5min 51s\n",
            "Wall time: 5min 47s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74i_-dRhyAgv",
        "outputId": "df556a6f-3858-426a-91cb-561d680d9424"
      },
      "source": [
        "%%time\n",
        "\n",
        "model = SSModel(layer_sizes=[3072, 300, 300, 300, 300, 10], activation='selu', regularization_penalty=0.0001, regularization_method='weighted_l1', kernel_initializer='lecun_normal')\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002)\n",
        "\n",
        "train_model(model, X_train_norm, y_train, optimizer, epochs, self_scaling_epochs, batch_size, min_new_neurons, validation_data=(X_test_norm, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "##########################################################\n",
            "Epoch 1/10\n",
            "Before growing:\n",
            "loss: 2.771711587905884 - accuracy: 0.09074000269174576 - val_loss: 2.757340908050537 - val_accuracy: 0.09019999951124191 - penalty: 0.0001\n",
            "layer sizes: [3072, 300, 300, 300, 300, 10]\n",
            "After growing:\n",
            "loss: 2.7717113494873047 - accuracy: 0.09074000269174576 - val_loss: 2.757340669631958 - val_accuracy: 0.09019999951124191 - penalty: 0.0001\n",
            "layer sizes: [3072, 360, 360, 360, 360, 10]\n",
            "Before pruning:\n",
            "loss: 1.831411361694336 - accuracy: 0.3278200030326843 - val_loss: 1.832931637763977 - val_accuracy: 0.32679998874664307 - penalty: 0.0001\n",
            "layer sizes: [3072, 360, 360, 360, 360, 10]\n",
            "After pruning:\n",
            "loss: 1.8334648609161377 - accuracy: 0.32638001441955566 - val_loss: 1.834941029548645 - val_accuracy: 0.32429999113082886 - penalty: 0.0001\n",
            "layer sizes: [3072, 14, 59, 38, 60, 10]\n",
            "##########################################################\n",
            "Epoch 2/10\n",
            "Before growing:\n",
            "loss: 1.8334648609161377 - accuracy: 0.32638001441955566 - val_loss: 1.834941029548645 - val_accuracy: 0.32429999113082886 - penalty: 0.0001\n",
            "layer sizes: [3072, 14, 59, 38, 60, 10]\n",
            "After growing:\n",
            "loss: 1.8334647417068481 - accuracy: 0.32638001441955566 - val_loss: 1.834941029548645 - val_accuracy: 0.32429999113082886 - penalty: 0.0001\n",
            "layer sizes: [3072, 34, 79, 58, 80, 10]\n",
            "Before pruning:\n",
            "loss: 1.7579070329666138 - accuracy: 0.355320006608963 - val_loss: 1.764008641242981 - val_accuracy: 0.3508000075817108 - penalty: 0.0001\n",
            "layer sizes: [3072, 34, 79, 58, 80, 10]\n",
            "After pruning:\n",
            "loss: 1.7578190565109253 - accuracy: 0.35523998737335205 - val_loss: 1.7639102935791016 - val_accuracy: 0.3504999876022339 - penalty: 0.0001\n",
            "layer sizes: [3072, 9, 40, 23, 43, 10]\n",
            "##########################################################\n",
            "Epoch 3/10\n",
            "Before growing:\n",
            "loss: 1.7578190565109253 - accuracy: 0.35523998737335205 - val_loss: 1.7639102935791016 - val_accuracy: 0.3504999876022339 - penalty: 0.0001\n",
            "layer sizes: [3072, 9, 40, 23, 43, 10]\n",
            "After growing:\n",
            "loss: 1.7578190565109253 - accuracy: 0.35523998737335205 - val_loss: 1.7639102935791016 - val_accuracy: 0.3504999876022339 - penalty: 0.0001\n",
            "layer sizes: [3072, 29, 60, 43, 63, 10]\n",
            "Before pruning:\n",
            "loss: 1.7497155666351318 - accuracy: 0.35736000537872314 - val_loss: 1.7571394443511963 - val_accuracy: 0.352400004863739 - penalty: 0.0001\n",
            "layer sizes: [3072, 29, 60, 43, 63, 10]\n",
            "After pruning:\n",
            "loss: 1.7496867179870605 - accuracy: 0.357340008020401 - val_loss: 1.7571035623550415 - val_accuracy: 0.352400004863739 - penalty: 0.0001\n",
            "layer sizes: [3072, 9, 31, 18, 40, 10]\n",
            "##########################################################\n",
            "Epoch 4/10\n",
            "Before growing:\n",
            "loss: 1.7496867179870605 - accuracy: 0.357340008020401 - val_loss: 1.7571035623550415 - val_accuracy: 0.352400004863739 - penalty: 0.0001\n",
            "layer sizes: [3072, 9, 31, 18, 40, 10]\n",
            "After growing:\n",
            "loss: 1.7496867179870605 - accuracy: 0.357340008020401 - val_loss: 1.7571035623550415 - val_accuracy: 0.352400004863739 - penalty: 0.0001\n",
            "layer sizes: [3072, 29, 51, 38, 60, 10]\n",
            "Before pruning:\n",
            "loss: 1.7161794900894165 - accuracy: 0.3773599863052368 - val_loss: 1.725213646888733 - val_accuracy: 0.374099999666214 - penalty: 0.0001\n",
            "layer sizes: [3072, 29, 51, 38, 60, 10]\n",
            "After pruning:\n",
            "loss: 1.7161937952041626 - accuracy: 0.3772599995136261 - val_loss: 1.725233793258667 - val_accuracy: 0.3741999864578247 - penalty: 0.0001\n",
            "layer sizes: [3072, 9, 28, 14, 39, 10]\n",
            "##########################################################\n",
            "Epoch 5/10\n",
            "Before growing:\n",
            "loss: 1.7161937952041626 - accuracy: 0.3772599995136261 - val_loss: 1.725233793258667 - val_accuracy: 0.3741999864578247 - penalty: 0.0001\n",
            "layer sizes: [3072, 9, 28, 14, 39, 10]\n",
            "After growing:\n",
            "loss: 1.7161937952041626 - accuracy: 0.3772599995136261 - val_loss: 1.725233793258667 - val_accuracy: 0.3741999864578247 - penalty: 0.0001\n",
            "layer sizes: [3072, 29, 48, 34, 59, 10]\n",
            "Before pruning:\n",
            "loss: 1.6997604370117188 - accuracy: 0.3824799954891205 - val_loss: 1.7117668390274048 - val_accuracy: 0.38269999623298645 - penalty: 0.0001\n",
            "layer sizes: [3072, 29, 48, 34, 59, 10]\n",
            "After pruning:\n",
            "loss: 1.699770450592041 - accuracy: 0.38245999813079834 - val_loss: 1.7117478847503662 - val_accuracy: 0.38260000944137573 - penalty: 0.0001\n",
            "layer sizes: [3072, 8, 27, 14, 39, 10]\n",
            "##########################################################\n",
            "Epoch 6/10\n",
            "Before growing:\n",
            "loss: 1.699770450592041 - accuracy: 0.38245999813079834 - val_loss: 1.7117478847503662 - val_accuracy: 0.38260000944137573 - penalty: 0.0001\n",
            "layer sizes: [3072, 8, 27, 14, 39, 10]\n",
            "After growing:\n",
            "loss: 1.6997705698013306 - accuracy: 0.38245999813079834 - val_loss: 1.7117478847503662 - val_accuracy: 0.38260000944137573 - penalty: 0.0001\n",
            "layer sizes: [3072, 28, 47, 34, 59, 10]\n",
            "Before pruning:\n",
            "loss: 1.687454104423523 - accuracy: 0.38802000880241394 - val_loss: 1.7044605016708374 - val_accuracy: 0.37860000133514404 - penalty: 0.0001\n",
            "layer sizes: [3072, 28, 47, 34, 59, 10]\n",
            "After pruning:\n",
            "loss: 1.6875388622283936 - accuracy: 0.3880400061607361 - val_loss: 1.7045443058013916 - val_accuracy: 0.37860000133514404 - penalty: 0.0001\n",
            "layer sizes: [3072, 8, 27, 14, 37, 10]\n",
            "##########################################################\n",
            "Epoch 7/10\n",
            "Before growing:\n",
            "loss: 1.6875388622283936 - accuracy: 0.3880400061607361 - val_loss: 1.7045443058013916 - val_accuracy: 0.37860000133514404 - penalty: 0.0001\n",
            "layer sizes: [3072, 8, 27, 14, 37, 10]\n",
            "After growing:\n",
            "loss: 1.6875388622283936 - accuracy: 0.3880400061607361 - val_loss: 1.7045443058013916 - val_accuracy: 0.37860000133514404 - penalty: 0.0001\n",
            "layer sizes: [3072, 28, 47, 34, 57, 10]\n",
            "Before pruning:\n",
            "loss: 1.6774787902832031 - accuracy: 0.3918600082397461 - val_loss: 1.6936542987823486 - val_accuracy: 0.38019999861717224 - penalty: 0.0001\n",
            "layer sizes: [3072, 28, 47, 34, 57, 10]\n",
            "After pruning:\n",
            "loss: 1.6774768829345703 - accuracy: 0.39184001088142395 - val_loss: 1.693649172782898 - val_accuracy: 0.38019999861717224 - penalty: 0.0001\n",
            "layer sizes: [3072, 8, 26, 13, 35, 10]\n",
            "##########################################################\n",
            "Epoch 8/10\n",
            "Before growing:\n",
            "loss: 1.6774768829345703 - accuracy: 0.39184001088142395 - val_loss: 1.693649172782898 - val_accuracy: 0.38019999861717224 - penalty: 0.0001\n",
            "layer sizes: [3072, 8, 26, 13, 35, 10]\n",
            "After growing:\n",
            "loss: 1.6774768829345703 - accuracy: 0.39184001088142395 - val_loss: 1.693649172782898 - val_accuracy: 0.38019999861717224 - penalty: 0.0001\n",
            "layer sizes: [3072, 28, 46, 33, 55, 10]\n",
            "Before pruning:\n",
            "loss: 1.6747316122055054 - accuracy: 0.3924799859523773 - val_loss: 1.6931262016296387 - val_accuracy: 0.38339999318122864 - penalty: 0.0001\n",
            "layer sizes: [3072, 28, 46, 33, 55, 10]\n",
            "After pruning:\n",
            "loss: 1.6746928691864014 - accuracy: 0.392300009727478 - val_loss: 1.6930644512176514 - val_accuracy: 0.3833000063896179 - penalty: 0.0001\n",
            "layer sizes: [3072, 8, 26, 13, 35, 10]\n",
            "##########################################################\n",
            "Epoch 9/10\n",
            "Before growing:\n",
            "loss: 1.6746928691864014 - accuracy: 0.392300009727478 - val_loss: 1.6930644512176514 - val_accuracy: 0.3833000063896179 - penalty: 0.0001\n",
            "layer sizes: [3072, 8, 26, 13, 35, 10]\n",
            "After growing:\n",
            "loss: 1.6746928691864014 - accuracy: 0.392300009727478 - val_loss: 1.6930644512176514 - val_accuracy: 0.3833000063896179 - penalty: 0.0001\n",
            "layer sizes: [3072, 28, 46, 33, 55, 10]\n",
            "Before pruning:\n",
            "loss: 1.6702289581298828 - accuracy: 0.3941799998283386 - val_loss: 1.6928426027297974 - val_accuracy: 0.3824999928474426 - penalty: 0.0001\n",
            "layer sizes: [3072, 28, 46, 33, 55, 10]\n",
            "After pruning:\n",
            "loss: 1.6702271699905396 - accuracy: 0.3942199945449829 - val_loss: 1.6928398609161377 - val_accuracy: 0.3824999928474426 - penalty: 0.0001\n",
            "layer sizes: [3072, 8, 26, 12, 34, 10]\n",
            "##########################################################\n",
            "Epoch 10/10\n",
            "Before growing:\n",
            "loss: 1.6702271699905396 - accuracy: 0.3942199945449829 - val_loss: 1.6928398609161377 - val_accuracy: 0.3824999928474426 - penalty: 0.0001\n",
            "layer sizes: [3072, 8, 26, 12, 34, 10]\n",
            "After growing:\n",
            "loss: 1.6702271699905396 - accuracy: 0.3942199945449829 - val_loss: 1.6928398609161377 - val_accuracy: 0.3824999928474426 - penalty: 0.0001\n",
            "layer sizes: [3072, 28, 46, 32, 54, 10]\n",
            "Before pruning:\n",
            "loss: 1.6601228713989258 - accuracy: 0.3979800045490265 - val_loss: 1.683695673942566 - val_accuracy: 0.38510000705718994 - penalty: 0.0001\n",
            "layer sizes: [3072, 28, 46, 32, 54, 10]\n",
            "After pruning:\n",
            "loss: 1.6601637601852417 - accuracy: 0.3980199992656708 - val_loss: 1.683748483657837 - val_accuracy: 0.38499999046325684 - penalty: 0.0001\n",
            "layer sizes: [3072, 8, 26, 12, 34, 10]\n",
            "CPU times: user 3min 40s, sys: 3.42 s, total: 3min 44s\n",
            "Wall time: 3min 45s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2isqn13d00Q3",
        "outputId": "e8876dac-a51c-4dee-cea9-33d4cab839ff"
      },
      "source": [
        "%%time\n",
        "\n",
        "model = SSModel(layer_sizes=[3072, 1000, 1000, 1000, 1000, 10], activation='selu', regularization_penalty=0.00001, regularization_method='weighted_l1', kernel_initializer='lecun_normal')\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002)\n",
        "\n",
        "train_model(model, X_train_norm, y_train, optimizer, epochs, self_scaling_epochs, batch_size, min_new_neurons, validation_data=(X_test_norm, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "Before growing:\n",
            "loss: 2.701932430267334 - accuracy: 0.09616000205278397 - val_loss: 2.6937167644500732 - val_accuracy: 0.10040000081062317\n",
            "layer sizes: [3072, 1000, 1000, 1000, 1000, 10]\n",
            "2222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "2222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "2222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "2222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "After growing:\n",
            "loss: 2.701932430267334 - accuracy: 0.09616000205278397 - val_loss: 2.6937172412872314 - val_accuracy: 0.10040000081062317\n",
            "layer sizes: [3072, 1200, 1200, 1200, 1200, 10]\n",
            "222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222255555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555\n",
            "222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222255555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555\n",
            "222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222255555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555\n",
            "222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222255555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555\n",
            "Before pruning:\n",
            "loss: 1.6422853469848633 - accuracy: 0.4048599898815155 - val_loss: 1.6402077674865723 - val_accuracy: 0.4049000144004822\n",
            "layer sizes: [3072, 1200, 1200, 1200, 1200, 10]\n",
            "222222222222222222222222222222222222223222223232234434443444444442442444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444454444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444445444444444444444444444444444444544444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444445445445444444444444444444444444455555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555\n",
            "112112111121122222212212122222222212222212222222212222122222222222222222222222222222222222222222222222242222322223222222222223222224232222222224332442242224323244223334343223434443323244424224324234344434244434442434422344444244434442444444424444444444443444444444444344443443444444344444444344434444444444444444443444444444444444444444444444444444444444444544444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444454445444444444444444444444444444444454444444444444444444444444444444444444444444444454444444545444444444445445444444444444454444444444445444444454444444444444444444445444454444454544454444454444444454445444444444444444444544444454444444444444444444444544454445444444444444444454444444544444445444544444454444444444554455444444454444454444544444444444444444444454444444444544445444444444444444544445445554444444444554454445444444444544444444544444445454444444554444544454544444444444444544445544445554454445444544444444545444454454454555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555\n",
            "121111111112221212211222221221211222222222222222222222222222222223222242222222232324232242223222224322242322243442444434444424424424444444454444434544444444444454455444544444445444444454444444444444544354444444444544445544454444445444444444554444444445444544444444444445445554444554444555454445444445445445455544455444444454544445445444545544554454444545444444555554445554444455544554444444444545454444545554544554444554454555554444545454444544454545554544545455454454455444555545544455545544444455455544444544555445545455544444545455554545454454544544455545555454454545555455444555455444544545545454455445545545454545555455554555545554455454545455545555455444555544544544554554455554544444545554545554555455454554554544554555444555545454554555455445554554445555555555555555455554555544555544545555545545545554554455554554455444455545455455454555545555555555555555555545554555545444555455445454555555545454545545545455545555545554455555455555455555555555545555554445454545455454555455555554545555554555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555\n",
            "111112111211111121212212221221122212222211112222122222222222222222222222122212222222222222222222222222222222222222222223222222222222223222223332222222322232232234222342322322332323333322324442332344433334323323423233333333343433332342334233444344422343442432444344333434344333343243443434324343443444334444443444444443244434444433434434443444433434244444433344443344434343444344343444443444344444444444434343344434444444443444444444444443444444444444444444444344444444444444444444444344444443444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444344444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444454444444444444444444444454444444444444444444444444444444444444444444444444444544444544444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444445444444455555454555554555545554455545455455555555444455555455555555455555545554555555454555554455555454555555554554554555554555555555555555555555545554555445555555555555555554555555545555555554555555555544455\n",
            "After pruning:\n",
            "loss: 1.6407244205474854 - accuracy: 0.4054200053215027 - val_loss: 1.638981819152832 - val_accuracy: 0.40869998931884766\n",
            "layer sizes: [3072, 54, 197, 112, 293, 10]\n",
            "222222222222222222222222222222222222223222223232233322\n",
            "11211211112112222221221212222222221222221222222221222212222222222222222222222222222222222222222222222222222322223222222222223222222322222222233222222323222333332233332322223223332323223232233333333\n",
            "1211111111122212122112222212212112222222222222222222222222222222232222222222223232232222232222232222322232322233\n",
            "11111211121111112121221222122112221222221111222212222222222222222222222212221222222222222222222222222222222222222222222322222222222222322222333222222232223223223222323223223323233333223223323333332332323233333333333333232332333223323233333333333233332333333323333333332333333333333333333333333\n",
            "Epoch 2/10\n",
            "Before growing:\n",
            "loss: 1.6407244205474854 - accuracy: 0.4054200053215027 - val_loss: 1.638981819152832 - val_accuracy: 0.40869998931884766\n",
            "layer sizes: [3072, 54, 197, 112, 293, 10]\n",
            "222222222222222222222222222222222222223222223232233322\n",
            "11211211112112222221221212222222221222221222222221222212222222222222222222222222222222222222222222222222222322223222222222223222222322222222233222222323222333332233332322223223332323223232233333333\n",
            "1211111111122212122112222212212112222222222222222222222222222222232222222222223232232222232222232222322232322233\n",
            "11111211121111112121221222122112221222221111222212222222222222222222222212221222222222222222222222222222222222222222222322222222222222322222333222222232223223223222323223223323233333223223323333332332323233333333333333232332333223323233333333333233332333333323333333332333333333333333333333333\n",
            "After growing:\n",
            "loss: 1.6407244205474854 - accuracy: 0.4054200053215027 - val_loss: 1.6389816999435425 - val_accuracy: 0.40869998931884766\n",
            "layer sizes: [3072, 74, 236, 134, 351, 10]\n",
            "22222222222222222222222222222222222222322222323223332255555555555555555555\n",
            "11211211112112222221221212222222221222221222222221222212222222222222222222222222222222222222222222222222222322223222222222223222222322222222233222222323222333332233332322223223332323223232233333333444444444444444444444444444444444444444\n",
            "12111111111222121221122222122121122222222222222222222222222222222322222222222232322322222322222322223222323222334444444444444444444444\n",
            "111112111211111121212212221221122212222211112222122222222222222222222222122212222222222222222222222222222222222222222223222222222222223222223332222222322232232232223232232233232333332232233233333323323232333333333333332323323332233232333333333332333323333333233333333323333333333333333333333334444444444444444444444444444444444444444444444444444444444\n",
            "Before pruning:\n",
            "loss: 1.556762456893921 - accuracy: 0.43751999735832214 - val_loss: 1.571761131286621 - val_accuracy: 0.4309999942779541\n",
            "layer sizes: [3072, 74, 236, 134, 351, 10]\n",
            "12122211211122212222122423323223442244443314344444334444444444444444444454\n",
            "11111111111111112211211212121222221221221212222221222212222222222421121222331222322222222222422412222324232532255422442244225425422542355442244234552535522554543454553424555545554545535453455554554555555555555555555555555555555555555555\n",
            "11111111111111121121111222112111122122222222224422254225422542225522424225244554554542525554545555555454545555555555555555555555555555\n",
            "111111111111111111212112221111122211221211111122122222212222122222221222122212232232222222214222222222233242222222422234422332244342234333434443243222423344243443424442233344342444434344444444444444434243444444444344344443443343244342444434344444444444444444444444344444444444444444434444443344444444444444444444444444444444444444444444444444444444444\n",
            "After pruning:\n",
            "loss: 1.5569236278533936 - accuracy: 0.4376400113105774 - val_loss: 1.571966290473938 - val_accuracy: 0.42989999055862427\n",
            "layer sizes: [3072, 39, 130, 64, 170, 10]\n",
            "121222112111222122221222332322322331333\n",
            "1111111111111111221121121212122222122122121222222122221222222222221121222331222322222222222221222232232322222222222232223232233233\n",
            "1111111111111112112111122211211112212222222222222222222222222222\n",
            "11111111111111111121211222111112221122121111112212222221222212222222122212221223223222222221222222222233222222222223223322322333333232222332332223333233323333333232333333\n",
            "Epoch 3/10\n",
            "Before growing:\n",
            "loss: 1.5569236278533936 - accuracy: 0.4376400113105774 - val_loss: 1.571966290473938 - val_accuracy: 0.42989999055862427\n",
            "layer sizes: [3072, 39, 130, 64, 170, 10]\n",
            "121222112111222122221222332322322331333\n",
            "1111111111111111221121121212122222122122121222222122221222222222221121222331222322222222222221222232232322222222222232223232233233\n",
            "1111111111111112112111122211211112212222222222222222222222222222\n",
            "11111111111111111121211222111112221122121111112212222221222212222222122212221223223222222221222222222233222222222223223322322333333232222332332223333233323333333232333333\n",
            "After growing:\n",
            "loss: 1.5569233894348145 - accuracy: 0.4376400113105774 - val_loss: 1.571966290473938 - val_accuracy: 0.42989999055862427\n",
            "layer sizes: [3072, 59, 156, 84, 204, 10]\n",
            "12122211211122212222122233232232233133355555555555555555555\n",
            "111111111111111122112112121212222212212212122222212222122222222222112122233122232222222222222122223223232222222222223222323223323344444444444444444444444444\n",
            "111111111111111211211112221121111221222222222222222222222222222244444444444444444444\n",
            "111111111111111111212112221111122211221211111122122222212222122222221222122212232232222222212222222222332222222222232233223223333332322223323322233332333233333332323333334444444444444444444444444444444444\n",
            "Before pruning:\n",
            "loss: 1.5130418539047241 - accuracy: 0.45427998900413513 - val_loss: 1.5379393100738525 - val_accuracy: 0.44620001316070557\n",
            "layer sizes: [3072, 59, 156, 84, 204, 10]\n",
            "11121211111122212331112334232233433134444444444444444444444\n",
            "111111111111111122111112121112222211112112141222212222122222222222112124245115244343222222242122324325353242222222325234544224424455555555554555555555555554\n",
            "111111111111111111111112241121111241122242252522224222344445244545555555445555555545\n",
            "111111111111111111211112211111112111221211111122122222222212122222221222132212242243222222212222232222442222223222442444434444444342422433424424243443344344333433334443433343443444443434444444444444444434\n",
            "After pruning:\n",
            "loss: 1.5129624605178833 - accuracy: 0.4542999863624573 - val_loss: 1.5378872156143188 - val_accuracy: 0.44679999351501465\n",
            "layer sizes: [3072, 35, 108, 50, 141, 10]\n",
            "11121211111122212331112332322333313\n",
            "111111111111111122111112121112222211112112112222122221222222222221121221123322222222122323233222222223223222\n",
            "11111111111111111111111221121111211222222222222232\n",
            "111111111111111111211112211111112111221211111122122222222212122222221222132212222322222221222223222222222232222332223322233333333333333333333\n",
            "Epoch 4/10\n",
            "Before growing:\n",
            "loss: 1.5129624605178833 - accuracy: 0.4542999863624573 - val_loss: 1.5378872156143188 - val_accuracy: 0.44679999351501465\n",
            "layer sizes: [3072, 35, 108, 50, 141, 10]\n",
            "11121211111122212331112332322333313\n",
            "111111111111111122111112121112222211112112112222122221222222222221121221123322222222122323233222222223223222\n",
            "11111111111111111111111221121111211222222222222232\n",
            "111111111111111111211112211111112111221211111122122222222212122222221222132212222322222221222223222222222232222332223322233333333333333333333\n",
            "After growing:\n",
            "loss: 1.5129624605178833 - accuracy: 0.4542999863624573 - val_loss: 1.5378872156143188 - val_accuracy: 0.44679999351501465\n",
            "layer sizes: [3072, 55, 129, 70, 169, 10]\n",
            "1112121111112221233111233232233331355555555555555555555\n",
            "111111111111111122111112121112222211112112112222122221222222222221121221123322222222122323233222222223223222444444444444444444444\n",
            "1111111111111111111111122112111121122222222222223244444444444444444444\n",
            "1111111111111111112111122111111121112212111111221222222222121222222212221322122223222222212222232222222222322223322233222333333333333333333334444444444444444444444444444\n",
            "Before pruning:\n",
            "loss: 1.4740278720855713 - accuracy: 0.4695799946784973 - val_loss: 1.50686776638031 - val_accuracy: 0.4593000113964081\n",
            "layer sizes: [3072, 55, 129, 70, 169, 10]\n",
            "1111111111111221133121234232343444344444444444444444444\n",
            "111111111111111112111112121112212211112111112221122221222222222221122222234452222341123434344212222225234422555555555555555555555\n",
            "1111111111111111111111122112111121155354124424425255445455554555555555\n",
            "1111111111111111111111112111111121111212111111221222212222111212222212221422122224322222212223342222222232422223422344233434444434444444444444444444444444444443444444444\n",
            "After pruning:\n",
            "loss: 1.4739677906036377 - accuracy: 0.46959999203681946 - val_loss: 1.5068124532699585 - val_accuracy: 0.45879998803138733\n",
            "layer sizes: [3072, 30, 97, 41, 117, 10]\n",
            "111111111111122113312123232333\n",
            "1111111111111111121111121211122122111121111122211222212222222222211222222322223112333212222222322\n",
            "11111111111111111111111221121111211312222\n",
            "111111111111111111111111211111112111121211111122122221222211121222221222122122223222222122233222222223222223223233333\n",
            "Epoch 5/10\n",
            "Before growing:\n",
            "loss: 1.4739677906036377 - accuracy: 0.46959999203681946 - val_loss: 1.5068124532699585 - val_accuracy: 0.45879998803138733\n",
            "layer sizes: [3072, 30, 97, 41, 117, 10]\n",
            "111111111111122113312123232333\n",
            "1111111111111111121111121211122122111121111122211222212222222222211222222322223112333212222222322\n",
            "11111111111111111111111221121111211312222\n",
            "111111111111111111111111211111112111121211111122122221222211121222221222122122223222222122233222222223222223223233333\n",
            "After growing:\n",
            "loss: 1.4739677906036377 - accuracy: 0.46959999203681946 - val_loss: 1.5068124532699585 - val_accuracy: 0.45879998803138733\n",
            "layer sizes: [3072, 50, 117, 61, 140, 10]\n",
            "11111111111112211331212323233355555555555555555555\n",
            "111111111111111112111112121112212211112111112221122221222222222221122222232222311233321222222232244444444444444444444\n",
            "1111111111111111111111122112111121131222244444444444444444444\n",
            "11111111111111111111111121111111211112121111112212222122221112122222122212212222322222212223322222222322222322323333344444444444444444444444\n",
            "Before pruning:\n",
            "loss: 1.4465709924697876 - accuracy: 0.4802800118923187 - val_loss: 1.4885493516921997 - val_accuracy: 0.4666000008583069\n",
            "layer sizes: [3072, 50, 117, 61, 140, 10]\n",
            "11111111111112211341312423233444444444444444444444\n",
            "111111111111111112111111111111112211112112112221122221222222223221122232242222411144421223223244345555555555555555555\n",
            "1111111111111111111111144111211121141243355455555555555555555\n",
            "11111111111111111111111111111111111112122111112212222121221122122222123212212222422233212224422322222322223432323334334334444444444443443444\n",
            "After pruning:\n",
            "loss: 1.446600317955017 - accuracy: 0.4800800085067749 - val_loss: 1.4885764122009277 - val_accuracy: 0.4668000042438507\n",
            "layer sizes: [3072, 27, 90, 37, 117, 10]\n",
            "111111111111122113131223233\n",
            "111111111111111112111111111111112211112112112221122221222222223221122232222221112122322323\n",
            "1111111111111111111111111121112111233\n",
            "111111111111111111111111111111111111121221111122122221212211221222221232122122222223321222223222223222233232333333333\n",
            "Epoch 6/10\n",
            "Before growing:\n",
            "loss: 1.446600317955017 - accuracy: 0.4800800085067749 - val_loss: 1.4885764122009277 - val_accuracy: 0.4668000042438507\n",
            "layer sizes: [3072, 27, 90, 37, 117, 10]\n",
            "111111111111122113131223233\n",
            "111111111111111112111111111111112211112112112221122221222222223221122232222221112122322323\n",
            "1111111111111111111111111121112111233\n",
            "111111111111111111111111111111111111121221111122122221212211221222221232122122222223321222223222223222233232333333333\n",
            "After growing:\n",
            "loss: 1.446600317955017 - accuracy: 0.4800800085067749 - val_loss: 1.4885764122009277 - val_accuracy: 0.4668000042438507\n",
            "layer sizes: [3072, 47, 110, 57, 140, 10]\n",
            "11111111111112211313122323355555555555555555555\n",
            "11111111111111111211111111111111221111211211222112222122222222322112223222222111212232232344444444444444444444\n",
            "111111111111111111111111112111211123344444444444444444444\n",
            "11111111111111111111111111111111111112122111112212222121221122122222123212212222222332122222322222322223323233333333344444444444444444444444\n",
            "Before pruning:\n",
            "loss: 1.4308992624282837 - accuracy: 0.4865399897098541 - val_loss: 1.4813296794891357 - val_accuracy: 0.4675000011920929\n",
            "layer sizes: [3072, 47, 110, 57, 140, 10]\n",
            "11111111111111211313123324344444444444444444444\n",
            "11111111111111111211111111111111221111211211222112222112222231422111223222223111312242332455555555555555555555\n",
            "111111111111111111111111112111211144555555555555555554545\n",
            "11111111111111111111111111111111111112122111112212212122221122122222224213112232222442122222422222422234434244444444444444444444444444443434\n",
            "After pruning:\n",
            "loss: 1.4308849573135376 - accuracy: 0.48664000630378723 - val_loss: 1.4813146591186523 - val_accuracy: 0.46799999475479126\n",
            "layer sizes: [3072, 26, 87, 34, 102, 10]\n",
            "11111111111111211313123323\n",
            "111111111111111112111111111111112211112112112221122221122222312211122322222311131222332\n",
            "1111111111111111111111111121112111\n",
            "111111111111111111111111111111111111121221111122122121222211221222222221311223222221222222222222233233\n",
            "Epoch 7/10\n",
            "Before growing:\n",
            "loss: 1.4308849573135376 - accuracy: 0.48664000630378723 - val_loss: 1.4813146591186523 - val_accuracy: 0.46799999475479126\n",
            "layer sizes: [3072, 26, 87, 34, 102, 10]\n",
            "11111111111111211313123323\n",
            "111111111111111112111111111111112211112112112221122221122222312211122322222311131222332\n",
            "1111111111111111111111111121112111\n",
            "111111111111111111111111111111111111121221111122122121222211221222222221311223222221222222222222233233\n",
            "After growing:\n",
            "loss: 1.4308849573135376 - accuracy: 0.48664000630378723 - val_loss: 1.4813146591186523 - val_accuracy: 0.46799999475479126\n",
            "layer sizes: [3072, 46, 107, 54, 122, 10]\n",
            "1111111111111121131312332355555555555555555555\n",
            "11111111111111111211111111111111221111211211222112222112222231221112232222231113122233244444444444444444444\n",
            "111111111111111111111111112111211144444444444444444444\n",
            "11111111111111111111111111111111111112122111112212212122221122122222222131122322222122222222222223323344444444444444444444\n",
            "Before pruning:\n",
            "loss: 1.4076555967330933 - accuracy: 0.49535998702049255 - val_loss: 1.465956687927246 - val_accuracy: 0.47350001335144043\n",
            "layer sizes: [3072, 46, 107, 54, 122, 10]\n",
            "1111111111111121131312342444444444444444444444\n",
            "11111111111111111211111111111111221111211211222113222113222241231112242222231124132244255455555555555555555\n",
            "111111111111111111111111112111211155555555555555555554\n",
            "11111111111111111111111111111111111112122211112212212122221222122222222132122422222132222233223223424444444444444444444444\n",
            "After pruning:\n",
            "loss: 1.4078251123428345 - accuracy: 0.495279997587204 - val_loss: 1.4661099910736084 - val_accuracy: 0.47360000014305115\n",
            "layer sizes: [3072, 24, 82, 34, 98, 10]\n",
            "111111111111112113131232\n",
            "1111111111111111121111111111111122111121121122211322211322221231112222222311213222\n",
            "1111111111111111111111111121112111\n",
            "11111111111111111111111111111111111112122211112212212122221222122222222132122222221322222332232232\n",
            "Epoch 8/10\n",
            "Before growing:\n",
            "loss: 1.4078251123428345 - accuracy: 0.495279997587204 - val_loss: 1.4661099910736084 - val_accuracy: 0.47360000014305115\n",
            "layer sizes: [3072, 24, 82, 34, 98, 10]\n",
            "111111111111112113131232\n",
            "1111111111111111121111111111111122111121121122211322211322221231112222222311213222\n",
            "1111111111111111111111111121112111\n",
            "11111111111111111111111111111111111112122211112212212122221222122222222132122222221322222332232232\n",
            "After growing:\n",
            "loss: 1.4078251123428345 - accuracy: 0.495279997587204 - val_loss: 1.4661098718643188 - val_accuracy: 0.47360000014305115\n",
            "layer sizes: [3072, 44, 102, 54, 118, 10]\n",
            "11111111111111211313123255555555555555555555\n",
            "111111111111111112111111111111112211112112112221132221132222123111222222231121322244444444444444444444\n",
            "111111111111111111111111112111211144444444444444444444\n",
            "1111111111111111111111111111111111111212221111221221212222122212222222213212222222132222233223223244444444444444444444\n",
            "Before pruning:\n",
            "loss: 1.3994313478469849 - accuracy: 0.4967400133609772 - val_loss: 1.4618608951568604 - val_accuracy: 0.47540000081062317\n",
            "layer sizes: [3072, 44, 102, 54, 118, 10]\n",
            "11111111111111111313113344334443333434334434\n",
            "111111111111111111111111111111112211112112112231132251142222223111322224241121322345555554545555555555\n",
            "111111111111111111111121112111211155554555555555555545\n",
            "1111111111111111111111112111111111121212221111221221212222122222223222214212222322132232234224224243444344444444444444\n",
            "After pruning:\n",
            "loss: 1.3994557857513428 - accuracy: 0.4967400133609772 - val_loss: 1.4618722200393677 - val_accuracy: 0.47600001096725464\n",
            "layer sizes: [3072, 34, 78, 34, 96, 10]\n",
            "1111111111111111131311333333333333\n",
            "111111111111111111111111111111112211112112112231132211222222311132222211213223\n",
            "1111111111111111111111211121112111\n",
            "111111111111111111111111211111111112121222111122122121222212222222322221212222322132232232222233\n",
            "Epoch 9/10\n",
            "Before growing:\n",
            "loss: 1.3994557857513428 - accuracy: 0.4967400133609772 - val_loss: 1.4618722200393677 - val_accuracy: 0.47600001096725464\n",
            "layer sizes: [3072, 34, 78, 34, 96, 10]\n",
            "1111111111111111131311333333333333\n",
            "111111111111111111111111111111112211112112112231132211222222311132222211213223\n",
            "1111111111111111111111211121112111\n",
            "111111111111111111111111211111111112121222111122122121222212222222322221212222322132232232222233\n",
            "After growing:\n",
            "loss: 1.3994559049606323 - accuracy: 0.4967400133609772 - val_loss: 1.4618722200393677 - val_accuracy: 0.47600001096725464\n",
            "layer sizes: [3072, 54, 98, 54, 116, 10]\n",
            "111111111111111113131133333333333355555555555555555555\n",
            "11111111111111111111111111111111221111211211223113221122222231113222221121322344444444444444444444\n",
            "111111111111111111111121112111211144444444444444444444\n",
            "11111111111111111111111121111111111212122211112212212122221222222232222121222232213223223222223344444444444444444444\n",
            "Before pruning:\n",
            "loss: 1.3949309587478638 - accuracy: 0.4991999864578247 - val_loss: 1.4618464708328247 - val_accuracy: 0.47269999980926514\n",
            "layer sizes: [3072, 54, 98, 54, 116, 10]\n",
            "111111111111111113131133444434444444444444444444444444\n",
            "11111111111111111111111111111111221111211211224115221132222241114222221121432355555555555555555545\n",
            "111111111111111111111121112111311155555555555555455555\n",
            "11111111111111111111111121111111111212122211112212212122221222222232222121322232214224323322223444444444444444444344\n",
            "After pruning:\n",
            "loss: 1.3949415683746338 - accuracy: 0.4991999864578247 - val_loss: 1.4618685245513916 - val_accuracy: 0.47269999980926514\n",
            "layer sizes: [3072, 25, 73, 34, 94, 10]\n",
            "1111111111111111131311333\n",
            "1111111111111111111111111111111122111121121122112211322222111222221121323\n",
            "1111111111111111111111211121113111\n",
            "1111111111111111111111112111111111121212221111221221212222122222223222212132223221223233222233\n",
            "Epoch 10/10\n",
            "Before growing:\n",
            "loss: 1.3949415683746338 - accuracy: 0.4991999864578247 - val_loss: 1.4618685245513916 - val_accuracy: 0.47269999980926514\n",
            "layer sizes: [3072, 25, 73, 34, 94, 10]\n",
            "1111111111111111131311333\n",
            "1111111111111111111111111111111122111121121122112211322222111222221121323\n",
            "1111111111111111111111211121113111\n",
            "1111111111111111111111112111111111121212221111221221212222122222223222212132223221223233222233\n",
            "After growing:\n",
            "loss: 1.3949416875839233 - accuracy: 0.4991999864578247 - val_loss: 1.4618685245513916 - val_accuracy: 0.47269999980926514\n",
            "layer sizes: [3072, 45, 93, 54, 114, 10]\n",
            "111111111111111113131133355555555555555555555\n",
            "111111111111111111111111111111112211112112112211221132222211122222112132344444444444444444444\n",
            "111111111111111111111121112111311144444444444444444444\n",
            "111111111111111111111111211111111112121222111122122121222212222222322221213222322122323322223344444444444444444444\n",
            "Before pruning:\n",
            "loss: 1.3745191097259521 - accuracy: 0.5087800025939941 - val_loss: 1.450331449508667 - val_accuracy: 0.4821000099182129\n",
            "layer sizes: [3072, 45, 93, 54, 114, 10]\n",
            "111111111111111113131133333344433434333434444\n",
            "111111111111111111111111111111112111112112112211221142232311132322112143455555555555555554555\n",
            "111111111111111111111121112111411155555554555455555545\n",
            "111111111111111111111111211111111112121222111123121131222212222222422221213223422122324432324333444343443444344443\n",
            "After pruning:\n",
            "loss: 1.3745399713516235 - accuracy: 0.5087599754333496 - val_loss: 1.4503381252288818 - val_accuracy: 0.48179998993873596\n",
            "layer sizes: [3072, 35, 70, 33, 96, 10]\n",
            "11111111111111111313113333333333333\n",
            "1111111111111111111111111111111121111121121122112211223231113232211213\n",
            "111111111111111111111121112111111\n",
            "111111111111111111111111211111111112121222111123121131222212222222222212132232212232323233333333\n",
            "CPU times: user 5min 47s, sys: 5.01 s, total: 5min 52s\n",
            "Wall time: 5min 48s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Jpk54f_yEap",
        "outputId": "de17d888-d183-4bf7-83a4-1c6e3bddcee0"
      },
      "source": [
        "%%time\n",
        "\n",
        "model = SSModel(layer_sizes=[3072, 300, 300, 300, 300, 10], activation='selu', regularization_penalty=0.00001, regularization_method='weighted_l1', kernel_initializer='lecun_normal')\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002)\n",
        "\n",
        "train_model(model, X_train_norm, y_train, optimizer, epochs, self_scaling_epochs, batch_size, min_new_neurons, validation_data=(X_test_norm, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "##########################################################\n",
            "Epoch 1/10\n",
            "Before growing:\n",
            "loss: 2.739136219024658 - accuracy: 0.0854400023818016 - val_loss: 2.7438509464263916 - val_accuracy: 0.08449999988079071 - penalty: 1e-05\n",
            "layer sizes: [3072, 300, 300, 300, 300, 10]\n",
            "After growing:\n",
            "loss: 2.739136219024658 - accuracy: 0.0854400023818016 - val_loss: 2.7438507080078125 - val_accuracy: 0.08449999988079071 - penalty: 1e-05\n",
            "layer sizes: [3072, 360, 360, 360, 360, 10]\n",
            "Before pruning:\n",
            "loss: 1.5972710847854614 - accuracy: 0.42989999055862427 - val_loss: 1.6050333976745605 - val_accuracy: 0.4284000098705292 - penalty: 1e-05\n",
            "layer sizes: [3072, 360, 360, 360, 360, 10]\n",
            "After pruning:\n",
            "loss: 1.5968269109725952 - accuracy: 0.4307200014591217 - val_loss: 1.6047106981277466 - val_accuracy: 0.4296000003814697 - penalty: 1e-05\n",
            "layer sizes: [3072, 108, 299, 298, 299, 10]\n",
            "##########################################################\n",
            "Epoch 2/10\n",
            "Before growing:\n",
            "loss: 1.5968269109725952 - accuracy: 0.4307200014591217 - val_loss: 1.6047106981277466 - val_accuracy: 0.4296000003814697 - penalty: 1e-05\n",
            "layer sizes: [3072, 108, 299, 298, 299, 10]\n",
            "After growing:\n",
            "loss: 1.5968266725540161 - accuracy: 0.4307200014591217 - val_loss: 1.6047106981277466 - val_accuracy: 0.4296000003814697 - penalty: 1e-05\n",
            "layer sizes: [3072, 129, 358, 357, 358, 10]\n",
            "Before pruning:\n",
            "loss: 1.5173356533050537 - accuracy: 0.45805999636650085 - val_loss: 1.5390710830688477 - val_accuracy: 0.44999998807907104 - penalty: 1e-05\n",
            "layer sizes: [3072, 129, 358, 357, 358, 10]\n",
            "After pruning:\n",
            "loss: 1.516478180885315 - accuracy: 0.45848000049591064 - val_loss: 1.5380620956420898 - val_accuracy: 0.4499000012874603 - penalty: 1e-05\n",
            "layer sizes: [3072, 75, 218, 136, 259, 10]\n",
            "##########################################################\n",
            "Epoch 3/10\n",
            "Before growing:\n",
            "loss: 1.516478180885315 - accuracy: 0.45848000049591064 - val_loss: 1.5380620956420898 - val_accuracy: 0.4499000012874603 - penalty: 1e-05\n",
            "layer sizes: [3072, 75, 218, 136, 259, 10]\n",
            "After growing:\n",
            "loss: 1.516478180885315 - accuracy: 0.45848000049591064 - val_loss: 1.5380620956420898 - val_accuracy: 0.4499000012874603 - penalty: 1e-05\n",
            "layer sizes: [3072, 95, 261, 163, 310, 10]\n",
            "Before pruning:\n",
            "loss: 1.4655240774154663 - accuracy: 0.47666001319885254 - val_loss: 1.491287350654602 - val_accuracy: 0.46470001339912415 - penalty: 1e-05\n",
            "layer sizes: [3072, 95, 261, 163, 310, 10]\n",
            "After pruning:\n",
            "loss: 1.465552806854248 - accuracy: 0.47683998942375183 - val_loss: 1.4913910627365112 - val_accuracy: 0.46540001034736633 - penalty: 1e-05\n",
            "layer sizes: [3072, 56, 154, 87, 200, 10]\n",
            "##########################################################\n",
            "Epoch 4/10\n",
            "Before growing:\n",
            "loss: 1.465552806854248 - accuracy: 0.47683998942375183 - val_loss: 1.4913910627365112 - val_accuracy: 0.46540001034736633 - penalty: 1e-05\n",
            "layer sizes: [3072, 56, 154, 87, 200, 10]\n",
            "After growing:\n",
            "loss: 1.465552806854248 - accuracy: 0.47683998942375183 - val_loss: 1.4913910627365112 - val_accuracy: 0.46540001034736633 - penalty: 1e-05\n",
            "layer sizes: [3072, 76, 184, 107, 240, 10]\n",
            "Before pruning:\n",
            "loss: 1.4367706775665283 - accuracy: 0.4908199906349182 - val_loss: 1.4739652872085571 - val_accuracy: 0.48089998960494995 - penalty: 1e-05\n",
            "layer sizes: [3072, 76, 184, 107, 240, 10]\n",
            "After pruning:\n",
            "loss: 1.4367541074752808 - accuracy: 0.49074000120162964 - val_loss: 1.473986268043518 - val_accuracy: 0.48089998960494995 - penalty: 1e-05\n",
            "layer sizes: [3072, 50, 131, 69, 167, 10]\n",
            "##########################################################\n",
            "Epoch 5/10\n",
            "Before growing:\n",
            "loss: 1.4367541074752808 - accuracy: 0.49074000120162964 - val_loss: 1.473986268043518 - val_accuracy: 0.48089998960494995 - penalty: 1e-05\n",
            "layer sizes: [3072, 50, 131, 69, 167, 10]\n",
            "After growing:\n",
            "loss: 1.4367541074752808 - accuracy: 0.49074000120162964 - val_loss: 1.4739863872528076 - val_accuracy: 0.48089998960494995 - penalty: 1e-05\n",
            "layer sizes: [3072, 70, 157, 89, 200, 10]\n",
            "Before pruning:\n",
            "loss: 1.4129078388214111 - accuracy: 0.49702000617980957 - val_loss: 1.455810308456421 - val_accuracy: 0.4821999967098236 - penalty: 1e-05\n",
            "layer sizes: [3072, 70, 157, 89, 200, 10]\n",
            "After pruning:\n",
            "loss: 1.412892460823059 - accuracy: 0.49709999561309814 - val_loss: 1.4557645320892334 - val_accuracy: 0.4821000099182129 - penalty: 1e-05\n",
            "layer sizes: [3072, 51, 110, 59, 133, 10]\n",
            "##########################################################\n",
            "Epoch 6/10\n",
            "Before growing:\n",
            "loss: 1.412892460823059 - accuracy: 0.49709999561309814 - val_loss: 1.4557645320892334 - val_accuracy: 0.4821000099182129 - penalty: 1e-05\n",
            "layer sizes: [3072, 51, 110, 59, 133, 10]\n",
            "After growing:\n",
            "loss: 1.412892460823059 - accuracy: 0.49709999561309814 - val_loss: 1.4557644128799438 - val_accuracy: 0.4821000099182129 - penalty: 1e-05\n",
            "layer sizes: [3072, 71, 132, 79, 159, 10]\n",
            "Before pruning:\n",
            "loss: 1.3859832286834717 - accuracy: 0.5061799883842468 - val_loss: 1.4389244318008423 - val_accuracy: 0.4900999963283539 - penalty: 1e-05\n",
            "layer sizes: [3072, 71, 132, 79, 159, 10]\n",
            "After pruning:\n",
            "loss: 1.3860278129577637 - accuracy: 0.5062599778175354 - val_loss: 1.4389939308166504 - val_accuracy: 0.48969998955726624 - penalty: 1e-05\n",
            "layer sizes: [3072, 42, 102, 55, 113, 10]\n",
            "##########################################################\n",
            "Epoch 7/10\n",
            "Before growing:\n",
            "loss: 1.3860278129577637 - accuracy: 0.5062599778175354 - val_loss: 1.4389939308166504 - val_accuracy: 0.48969998955726624 - penalty: 1e-05\n",
            "layer sizes: [3072, 42, 102, 55, 113, 10]\n",
            "After growing:\n",
            "loss: 1.3860278129577637 - accuracy: 0.5062599778175354 - val_loss: 1.4389939308166504 - val_accuracy: 0.48969998955726624 - penalty: 1e-05\n",
            "layer sizes: [3072, 62, 122, 75, 135, 10]\n",
            "Before pruning:\n",
            "loss: 1.3695404529571533 - accuracy: 0.5149800181388855 - val_loss: 1.4275572299957275 - val_accuracy: 0.49070000648498535 - penalty: 1e-05\n",
            "layer sizes: [3072, 62, 122, 75, 135, 10]\n",
            "After pruning:\n",
            "loss: 1.3696156740188599 - accuracy: 0.5147799849510193 - val_loss: 1.4276379346847534 - val_accuracy: 0.49079999327659607 - penalty: 1e-05\n",
            "layer sizes: [3072, 41, 90, 50, 104, 10]\n",
            "##########################################################\n",
            "Epoch 8/10\n",
            "Before growing:\n",
            "loss: 1.3696156740188599 - accuracy: 0.5147799849510193 - val_loss: 1.4276379346847534 - val_accuracy: 0.49079999327659607 - penalty: 1e-05\n",
            "layer sizes: [3072, 41, 90, 50, 104, 10]\n",
            "After growing:\n",
            "loss: 1.3696156740188599 - accuracy: 0.5147799849510193 - val_loss: 1.4276379346847534 - val_accuracy: 0.49079999327659607 - penalty: 1e-05\n",
            "layer sizes: [3072, 61, 110, 70, 124, 10]\n",
            "Before pruning:\n",
            "loss: 1.35245943069458 - accuracy: 0.5188199877738953 - val_loss: 1.4141274690628052 - val_accuracy: 0.49880000948905945 - penalty: 1e-05\n",
            "layer sizes: [3072, 61, 110, 70, 124, 10]\n",
            "After pruning:\n",
            "loss: 1.352471113204956 - accuracy: 0.5187600255012512 - val_loss: 1.414123773574829 - val_accuracy: 0.4991999864578247 - penalty: 1e-05\n",
            "layer sizes: [3072, 40, 84, 45, 95, 10]\n",
            "##########################################################\n",
            "Epoch 9/10\n",
            "Before growing:\n",
            "loss: 1.352471113204956 - accuracy: 0.5187600255012512 - val_loss: 1.414123773574829 - val_accuracy: 0.4991999864578247 - penalty: 1e-05\n",
            "layer sizes: [3072, 40, 84, 45, 95, 10]\n",
            "After growing:\n",
            "loss: 1.3524712324142456 - accuracy: 0.5187600255012512 - val_loss: 1.4141238927841187 - val_accuracy: 0.4991999864578247 - penalty: 1e-05\n",
            "layer sizes: [3072, 60, 104, 65, 115, 10]\n",
            "Before pruning:\n",
            "loss: 1.349268913269043 - accuracy: 0.5199599862098694 - val_loss: 1.4160902500152588 - val_accuracy: 0.4945000112056732 - penalty: 1e-05\n",
            "layer sizes: [3072, 60, 104, 65, 115, 10]\n",
            "After pruning:\n",
            "loss: 1.3493249416351318 - accuracy: 0.5199199914932251 - val_loss: 1.4161128997802734 - val_accuracy: 0.49470001459121704 - penalty: 1e-05\n",
            "layer sizes: [3072, 32, 81, 43, 97, 10]\n",
            "##########################################################\n",
            "Epoch 10/10\n",
            "Before growing:\n",
            "loss: 1.3493249416351318 - accuracy: 0.5199199914932251 - val_loss: 1.4161128997802734 - val_accuracy: 0.49470001459121704 - penalty: 1e-05\n",
            "layer sizes: [3072, 32, 81, 43, 97, 10]\n",
            "After growing:\n",
            "loss: 1.3493249416351318 - accuracy: 0.5199199914932251 - val_loss: 1.4161131381988525 - val_accuracy: 0.49470001459121704 - penalty: 1e-05\n",
            "layer sizes: [3072, 52, 101, 63, 117, 10]\n",
            "Before pruning:\n",
            "loss: 1.3346582651138306 - accuracy: 0.5247600078582764 - val_loss: 1.4080772399902344 - val_accuracy: 0.49810001254081726 - penalty: 1e-05\n",
            "layer sizes: [3072, 52, 101, 63, 117, 10]\n",
            "After pruning:\n",
            "loss: 1.3346558809280396 - accuracy: 0.5246999859809875 - val_loss: 1.4080613851547241 - val_accuracy: 0.49799999594688416 - penalty: 1e-05\n",
            "layer sizes: [3072, 34, 76, 42, 88, 10]\n",
            "CPU times: user 3min 39s, sys: 2.86 s, total: 3min 42s\n",
            "Wall time: 3min 39s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDRfUEnIstzy",
        "outputId": "4311e3ce-310b-4ed5-9237-f0881c19e05c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "model = SSModel(layer_sizes=[3072, 300, 300, 300, 300, 10], activation='selu', regularization_penalty=0.00001, regularization_method='weighted_l1', kernel_initializer='lecun_normal')\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002)\n",
        "\n",
        "train_model(model, X_train_norm, y_train, optimizer, epochs, self_scaling_epochs, batch_size, min_new_neurons, validation_data=(X_test_norm, y_test))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "##########################################################\n",
            "Epoch 1/10\n",
            "Before growing:\n",
            "loss: 2.6930086612701416 - accuracy: 0.10617999732494354 - val_loss: 2.6887967586517334 - val_accuracy: 0.10769999772310257 - penalty: 1e-05\n",
            "hidden layer sizes: [300, 300, 300, 300], total neurons: 1200\n",
            "After growing:\n",
            "loss: 2.6930086612701416 - accuracy: 0.10617999732494354 - val_loss: 2.6887967586517334 - val_accuracy: 0.10769999772310257 - penalty: 1e-05\n",
            "hidden layer sizes: [360, 360, 360, 360], total neurons: 1440\n",
            "Before pruning:\n",
            "loss: 1.5993088483810425 - accuracy: 0.42566001415252686 - val_loss: 1.6107486486434937 - val_accuracy: 0.424699991941452 - penalty: 1e-05\n",
            "hidden layer sizes: [360, 360, 360, 360], total neurons: 1440\n",
            "After pruning:\n",
            "loss: 1.5944675207138062 - accuracy: 0.42941999435424805 - val_loss: 1.6057299375534058 - val_accuracy: 0.4277999997138977 - penalty: 1e-05\n",
            "hidden layer sizes: [99, 300, 300, 300], total neurons: 999\n",
            "##########################################################\n",
            "Epoch 2/10\n",
            "Before growing:\n",
            "loss: 1.5944675207138062 - accuracy: 0.42941999435424805 - val_loss: 1.6057299375534058 - val_accuracy: 0.4277999997138977 - penalty: 1e-05\n",
            "hidden layer sizes: [99, 300, 300, 300], total neurons: 999\n",
            "After growing:\n",
            "loss: 1.5944675207138062 - accuracy: 0.42941999435424805 - val_loss: 1.6057299375534058 - val_accuracy: 0.4277999997138977 - penalty: 1e-05\n",
            "hidden layer sizes: [119, 360, 360, 360], total neurons: 1199\n",
            "Before pruning:\n",
            "loss: 1.5161465406417847 - accuracy: 0.458079993724823 - val_loss: 1.5329468250274658 - val_accuracy: 0.4544000029563904 - penalty: 1e-05\n",
            "hidden layer sizes: [119, 360, 360, 360], total neurons: 1199\n",
            "After pruning:\n",
            "loss: 1.5154483318328857 - accuracy: 0.4587799906730652 - val_loss: 1.53243088722229 - val_accuracy: 0.45500001311302185 - penalty: 1e-05\n",
            "hidden layer sizes: [57, 220, 120, 248], total neurons: 645\n",
            "##########################################################\n",
            "Epoch 3/10\n",
            "Before growing:\n",
            "loss: 1.5154483318328857 - accuracy: 0.4587799906730652 - val_loss: 1.53243088722229 - val_accuracy: 0.45500001311302185 - penalty: 1e-05\n",
            "hidden layer sizes: [57, 220, 120, 248], total neurons: 645\n",
            "After growing:\n",
            "loss: 1.5154483318328857 - accuracy: 0.4587799906730652 - val_loss: 1.53243088722229 - val_accuracy: 0.45500001311302185 - penalty: 1e-05\n",
            "hidden layer sizes: [77, 264, 144, 297], total neurons: 782\n",
            "Before pruning:\n",
            "loss: 1.466373085975647 - accuracy: 0.4771600067615509 - val_loss: 1.4946460723876953 - val_accuracy: 0.46639999747276306 - penalty: 1e-05\n",
            "hidden layer sizes: [77, 264, 144, 297], total neurons: 782\n",
            "After pruning:\n",
            "loss: 1.466406226158142 - accuracy: 0.47718000411987305 - val_loss: 1.494635820388794 - val_accuracy: 0.46630001068115234 - penalty: 1e-05\n",
            "hidden layer sizes: [49, 167, 75, 203], total neurons: 494\n",
            "##########################################################\n",
            "Epoch 4/10\n",
            "Before growing:\n",
            "loss: 1.466406226158142 - accuracy: 0.47718000411987305 - val_loss: 1.494635820388794 - val_accuracy: 0.46630001068115234 - penalty: 1e-05\n",
            "hidden layer sizes: [49, 167, 75, 203], total neurons: 494\n",
            "After growing:\n",
            "loss: 1.466405987739563 - accuracy: 0.47718000411987305 - val_loss: 1.494635820388794 - val_accuracy: 0.46630001068115234 - penalty: 1e-05\n",
            "hidden layer sizes: [69, 200, 95, 243], total neurons: 607\n",
            "Before pruning:\n",
            "loss: 1.4152148962020874 - accuracy: 0.49498000741004944 - val_loss: 1.4572367668151855 - val_accuracy: 0.48030000925064087 - penalty: 1e-05\n",
            "hidden layer sizes: [69, 200, 95, 243], total neurons: 607\n",
            "After pruning:\n",
            "loss: 1.415077805519104 - accuracy: 0.4950000047683716 - val_loss: 1.4570783376693726 - val_accuracy: 0.4794999957084656 - penalty: 1e-05\n",
            "hidden layer sizes: [44, 138, 65, 148], total neurons: 395\n",
            "##########################################################\n",
            "Epoch 5/10\n",
            "Before growing:\n",
            "loss: 1.415077805519104 - accuracy: 0.4950000047683716 - val_loss: 1.4570783376693726 - val_accuracy: 0.4794999957084656 - penalty: 1e-05\n",
            "hidden layer sizes: [44, 138, 65, 148], total neurons: 395\n",
            "After growing:\n",
            "loss: 1.4150776863098145 - accuracy: 0.4950000047683716 - val_loss: 1.457078456878662 - val_accuracy: 0.4794999957084656 - penalty: 1e-05\n",
            "hidden layer sizes: [64, 165, 85, 177], total neurons: 491\n",
            "Before pruning:\n",
            "loss: 1.3911945819854736 - accuracy: 0.5065799951553345 - val_loss: 1.4413169622421265 - val_accuracy: 0.4864000082015991 - penalty: 1e-05\n",
            "hidden layer sizes: [64, 165, 85, 177], total neurons: 491\n",
            "After pruning:\n",
            "loss: 1.3911978006362915 - accuracy: 0.5064200162887573 - val_loss: 1.441332221031189 - val_accuracy: 0.4860000014305115 - penalty: 1e-05\n",
            "hidden layer sizes: [41, 117, 56, 128], total neurons: 342\n",
            "##########################################################\n",
            "Epoch 6/10\n",
            "Before growing:\n",
            "loss: 1.3911978006362915 - accuracy: 0.5064200162887573 - val_loss: 1.441332221031189 - val_accuracy: 0.4860000014305115 - penalty: 1e-05\n",
            "hidden layer sizes: [41, 117, 56, 128], total neurons: 342\n",
            "After growing:\n",
            "loss: 1.3911978006362915 - accuracy: 0.5064200162887573 - val_loss: 1.4413321018218994 - val_accuracy: 0.4860000014305115 - penalty: 1e-05\n",
            "hidden layer sizes: [61, 140, 76, 153], total neurons: 430\n",
            "Before pruning:\n",
            "loss: 1.3830540180206299 - accuracy: 0.5055999755859375 - val_loss: 1.4381896257400513 - val_accuracy: 0.4900999963283539 - penalty: 1e-05\n",
            "hidden layer sizes: [61, 140, 76, 153], total neurons: 430\n",
            "After pruning:\n",
            "loss: 1.3830615282058716 - accuracy: 0.5055400133132935 - val_loss: 1.4382244348526 - val_accuracy: 0.490200012922287 - penalty: 1e-05\n",
            "hidden layer sizes: [38, 103, 53, 114], total neurons: 308\n",
            "##########################################################\n",
            "Epoch 7/10\n",
            "Before growing:\n",
            "loss: 1.3830615282058716 - accuracy: 0.5055400133132935 - val_loss: 1.4382244348526 - val_accuracy: 0.490200012922287 - penalty: 1e-05\n",
            "hidden layer sizes: [38, 103, 53, 114], total neurons: 308\n",
            "After growing:\n",
            "loss: 1.383061408996582 - accuracy: 0.5055400133132935 - val_loss: 1.4382244348526 - val_accuracy: 0.490200012922287 - penalty: 1e-05\n",
            "hidden layer sizes: [58, 123, 73, 136], total neurons: 390\n",
            "Before pruning:\n",
            "loss: 1.3567003011703491 - accuracy: 0.5184199810028076 - val_loss: 1.4218944311141968 - val_accuracy: 0.4952000081539154 - penalty: 1e-05\n",
            "hidden layer sizes: [58, 123, 73, 136], total neurons: 390\n",
            "After pruning:\n",
            "loss: 1.3566465377807617 - accuracy: 0.5184800028800964 - val_loss: 1.4218517541885376 - val_accuracy: 0.4952999949455261 - penalty: 1e-05\n",
            "hidden layer sizes: [47, 93, 51, 116], total neurons: 307\n",
            "##########################################################\n",
            "Epoch 8/10\n",
            "Before growing:\n",
            "loss: 1.3566465377807617 - accuracy: 0.5184800028800964 - val_loss: 1.4218517541885376 - val_accuracy: 0.4952999949455261 - penalty: 1e-05\n",
            "hidden layer sizes: [47, 93, 51, 116], total neurons: 307\n",
            "After growing:\n",
            "loss: 1.3566465377807617 - accuracy: 0.5184800028800964 - val_loss: 1.4218517541885376 - val_accuracy: 0.4952999949455261 - penalty: 1e-05\n",
            "hidden layer sizes: [67, 113, 71, 139], total neurons: 390\n",
            "Before pruning:\n",
            "loss: 1.3409053087234497 - accuracy: 0.5222799777984619 - val_loss: 1.4136013984680176 - val_accuracy: 0.4999000132083893 - penalty: 1e-05\n",
            "hidden layer sizes: [67, 113, 71, 139], total neurons: 390\n",
            "After pruning:\n",
            "loss: 1.3407387733459473 - accuracy: 0.5223000049591064 - val_loss: 1.4134204387664795 - val_accuracy: 0.5004000067710876 - penalty: 1e-05\n",
            "hidden layer sizes: [42, 85, 47, 98], total neurons: 272\n",
            "##########################################################\n",
            "Epoch 9/10\n",
            "Before growing:\n",
            "loss: 1.3407387733459473 - accuracy: 0.5223000049591064 - val_loss: 1.4134204387664795 - val_accuracy: 0.5004000067710876 - penalty: 1e-05\n",
            "hidden layer sizes: [42, 85, 47, 98], total neurons: 272\n",
            "After growing:\n",
            "loss: 1.3407387733459473 - accuracy: 0.5223000049591064 - val_loss: 1.4134204387664795 - val_accuracy: 0.5004000067710876 - penalty: 1e-05\n",
            "hidden layer sizes: [62, 105, 67, 118], total neurons: 352\n",
            "Before pruning:\n",
            "loss: 1.328075647354126 - accuracy: 0.526960015296936 - val_loss: 1.404915452003479 - val_accuracy: 0.5027999877929688 - penalty: 1e-05\n",
            "hidden layer sizes: [62, 105, 67, 118], total neurons: 352\n",
            "After pruning:\n",
            "loss: 1.3281511068344116 - accuracy: 0.5267199873924255 - val_loss: 1.404951810836792 - val_accuracy: 0.5029000043869019 - penalty: 1e-05\n",
            "hidden layer sizes: [36, 82, 46, 94], total neurons: 258\n",
            "##########################################################\n",
            "Epoch 10/10\n",
            "Before growing:\n",
            "loss: 1.3281511068344116 - accuracy: 0.5267199873924255 - val_loss: 1.404951810836792 - val_accuracy: 0.5029000043869019 - penalty: 1e-05\n",
            "hidden layer sizes: [36, 82, 46, 94], total neurons: 258\n",
            "After growing:\n",
            "loss: 1.328150987625122 - accuracy: 0.5267199873924255 - val_loss: 1.404951810836792 - val_accuracy: 0.5029000043869019 - penalty: 1e-05\n",
            "hidden layer sizes: [56, 102, 66, 114], total neurons: 338\n",
            "Before pruning:\n",
            "loss: 1.3299870491027832 - accuracy: 0.5278800129890442 - val_loss: 1.4119497537612915 - val_accuracy: 0.5005000233650208 - penalty: 1e-05\n",
            "hidden layer sizes: [56, 102, 66, 114], total neurons: 338\n",
            "After pruning:\n",
            "loss: 1.329940915107727 - accuracy: 0.5278599858283997 - val_loss: 1.411912202835083 - val_accuracy: 0.5008000135421753 - penalty: 1e-05\n",
            "hidden layer sizes: [34, 79, 44, 89], total neurons: 246\n",
            "CPU times: user 5min 42s, sys: 5.44 s, total: 5min 47s\n",
            "Wall time: 5min 43s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a29DTkin27Ai",
        "outputId": "c18fa3d9-2081-4836-baab-5ed34cae0638"
      },
      "source": [
        "%%time\n",
        "\n",
        "model = SSModel(layer_sizes=[3072, 1000, 1000, 1000, 1000, 10], activation='selu', regularization_penalty=0.000001, regularization_method='weighted_l1', kernel_initializer='lecun_normal')\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002)\n",
        "\n",
        "train_model(model, X_train_norm, y_train, optimizer, epochs, self_scaling_epochs, batch_size, min_new_neurons, validation_data=(X_test_norm, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "Before growing:\n",
            "loss: 2.662282705307007 - accuracy: 0.1032399982213974 - val_loss: 2.657341718673706 - val_accuracy: 0.10700000077486038\n",
            "layer sizes: [3072, 1000, 1000, 1000, 1000, 10]\n",
            "2222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "2222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "2222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "2222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "After growing:\n",
            "loss: 2.662282705307007 - accuracy: 0.1032399982213974 - val_loss: 2.657341718673706 - val_accuracy: 0.10700000077486038\n",
            "layer sizes: [3072, 1200, 1200, 1200, 1200, 10]\n",
            "222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222255555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555\n",
            "222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222255555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555\n",
            "222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222255555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555\n",
            "222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222255555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555\n",
            "Before pruning:\n",
            "loss: 1.6055657863616943 - accuracy: 0.424560010433197 - val_loss: 1.6237467527389526 - val_accuracy: 0.4180999994277954\n",
            "layer sizes: [3072, 1200, 1200, 1200, 1200, 10]\n",
            "222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222232222223222222233222223223222222232233233232333323222233223222223332323332343324243333432322343242333342344433432333224234224434423343333343444434443334443443443244344443324444343444444444334444444444443444444344344444444444442244444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444455555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555455555555555555555555555555555555555555555555555555555555555555555555555555555\n",
            "222222222222222222222222222222222222222222222222222222222222222222222222222221222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222232222222222222222222222222222222222232222222222222222222222222222222323222222222222222223233242324222222222222222222222222322222222223222223222222222232322223323222233232342333322323232232232222222322322332222333333233233222333222222223322432342323224434222422242422243232333233322333233223324332423224243333224332333234244322233342223324334344422334233332334343232224434322423444243432324333255555555555555555555555555545555555555555545555545555555544555545555555555555554455554554554555555555555555555555555555555555555555555555555555555555555455555555555555545555545555555545555555555555555\n",
            "222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222223222222222222222222222222222222222222222222222222232222222222222222222222222222222232222223232222222222322232322222322222222222222222222222332222234322232322222232222222332232333223232223222242332232223222222242222223232222222223423333242222342322322232234333222232223242224333422322422332442242432343242333324323322232232233324342223243222324222422422322222223223443322233234424234443344324434242224432434444423433322344442224443344443323434244234442444333433333432455555554554455555455445554555545545555455555545455555455455555555545554555545545554555455445555555554555445545555555455555545554455545454455454455545554454554545455555555455555555555555555555545545554\n",
            "222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222232222222223222222222222222222222222222222222222222222222322222222222222222222222222322222222222222222222222222222222222222222222222222222222222222222322222222222222222222222222222222222222222232222222222222223222222222222222444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444\n",
            "After pruning:\n",
            "loss: 1.5971894264221191 - accuracy: 0.4270800054073334 - val_loss: 1.6151868104934692 - val_accuracy: 0.42179998755455017\n",
            "layer sizes: [3072, 404, 961, 927, 999, 10]\n",
            "22222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222223222222322222223322222322322222223223323323233332322223322322222333232333233322333332322332233332333323332223223233333333333333323332333333322\n",
            "2222222222222222222222222222222222222222222222222222222222222222222222222222212222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222322222222222222222222222222222222222322222222222222222222222222222223232222222222222222232332232222222222222222222222222322222222223222223222222222232322223323222233232323333223232322322322222223223223322223333332332332223332222222233223232323223222222222232323332333223332332233233223222333322332333232322233322233233322332333323333232223322232332323332\n",
            "222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222223222222222222222222222222222222222222222222222222232222222222222222222222222222222232222223232222222222322232322222322222222222222222222222332222233222323222222322222223322323332232322232222233223222322222222222223232222222223233332222232322322232233332222322232222333223222233222232332233332323322232232233323222323222322222222322222223223332223222333323222232323333223222333323322323333333332\n",
            "222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222232222222223222222222222222222222222222222222222222222222322222222222222222222222222322222222222222222222222222222222222222222222222222222222222222222322222222222222222222222222222222222222222232222222222222223222222222222222\n",
            "Epoch 2/10\n",
            "Before growing:\n",
            "loss: 1.5971894264221191 - accuracy: 0.4270800054073334 - val_loss: 1.6151868104934692 - val_accuracy: 0.42179998755455017\n",
            "layer sizes: [3072, 404, 961, 927, 999, 10]\n",
            "22222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222223222222322222223322222322322222223223323323233332322223322322222333232333233322333332322332233332333323332223223233333333333333323332333333322\n",
            "2222222222222222222222222222222222222222222222222222222222222222222222222222212222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222322222222222222222222222222222222222322222222222222222222222222222223232222222222222222232332232222222222222222222222222322222222223222223222222222232322223323222233232323333223232322322322222223223223322223333332332332223332222222233223232323223222222222232323332333223332332233233223222333322332333232322233322233233322332333323333232223322232332323332\n",
            "222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222223222222222222222222222222222222222222222222222222232222222222222222222222222222222232222223232222222222322232322222322222222222222222222222332222233222323222222322222223322323332232322232222233223222322222222222223232222222223233332222232322322232233332222322232222333223222233222232332233332323322232232233323222323222322222222322222223223332223222333323222232323333223222333323322323333333332\n",
            "222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222232222222223222222222222222222222222222222222222222222222322222222222222222222222222322222222222222222222222222222222222222222222222222222222222222222322222222222222222222222222222222222222222232222222222222223222222222222222\n",
            "After growing:\n",
            "loss: 1.5971894264221191 - accuracy: 0.4270800054073334 - val_loss: 1.6151866912841797 - val_accuracy: 0.42179998755455017\n",
            "layer sizes: [3072, 484, 1153, 1112, 1198, 10]\n",
            "2222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222322222232222222332222232232222222322332332323333232222332232222233323233323332233333232233223333233332333222322323333333333333332333233333332255555555555555555555555555555555555555555555555555555555555555555555555555555555\n",
            "2222222222222222222222222222222222222222222222222222222222222222222222222222212222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222322222222222222222222222222222222222322222222222222222222222222222223232222222222222222232332232222222222222222222222222322222222223222223222222222232322223323222233232323333223232322322322222223223223322223333332332332223332222222233223232323223222222222232323332333223332332233233223222333322332333232322233322233233322332333323333232223322232332323332444444444544444444444444444444444445444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444544444544445444444444444444544444444444444454444444\n",
            "22222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222322222222222222222222222222222222222222222222222223222222222222222222222222222222223222222323222222222232223232222232222222222222222222222233222223322232322222232222222332232333223232223222223322322232222222222222323222222222323333222223232232223223333222232223222233322322223322223233223333232332223223223332322232322232222222232222222322333222322233332322223232333322322233332332232333333333255555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555\n",
            "2222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222322222222232222222222222222222222222222222222222222222223222222222222222222222222223222222222222222222222222222222222222222222222222222222222222222223222222222222222222222222222222222222222222322222222222222232222222222222225555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555\n",
            "Before pruning:\n",
            "loss: 1.4313263893127441 - accuracy: 0.48646000027656555 - val_loss: 1.4675519466400146 - val_accuracy: 0.4772000014781952\n",
            "layer sizes: [3072, 484, 1153, 1112, 1198, 10]\n",
            "2222222222221222212121221222222222222222222222222211222221222223212221222121223222222222222232323222312233323312333333223232232333333322323233323333333233334432333343333332332333323433344333334343333433234334333443434433334334333433343334433334334443433334333343443243444344444444344343434444344434443433344444444444444444444444443444444443433444434444444444444444434444344444444444444444444434444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444\n",
            "2211221122212122221122222222222122222222222222222212221222221222122222222221212212222222212222212212222222122222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222223222222222222322222222222222222222222222222232223222222222222422222232222232222222222322222222223224232233232222223223333322322423322223242323223223332322232223232222242232322443332333324423342342332343334423333423334322443433332323323343444334432334333234424324434434334424244333443444334244333332433424433344434424333434433443344334344424444333444433443444444444234444444343444334444444444433444344434344443434444444444444444344444444444344434443444443444344444444444443443444444444434343434444444444344444444444444343444444444444444434434434444444444444444444444444444444444444444444344444344344434444444444444444444444444444444444444433444433434444444444443434444443444344444444444444434444444444444443444344444444444444444444545545454555544454454555445445545554444555555544445454454455555554555545444545555555555555555554455545455544455555455555445445545555555545544544545555544545555545454455544444554555455455455545\n",
            "21222222222211122222212222221222222122222222222222222222222221222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222223222222222221222332242223222222222222332323222223223423222422432242222242242224422224224443444444423432244444444423424234444324222442432444444244432244243324444234444344433434244444244444422434442444434444344434444424444443444344444444444444444442434444444444444444444444444344434442444444444444444444334444444444444444444434244444444444444444444444444444444444444444444443444444443444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444443444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444445554545554454545445555545545444455454545445554544455445554545555545455445544555544554545445545555455444444455554544544444455554445555555545554555455445554554445545454455554545445554554\n",
            "2112222222222222122222222222222122222222212222222222222222222222222222222222222122222222222222222222222222222222222222212222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222242222222222222222222222222222222222222222223222222222322222232222222242222222222222222223222222222222222232232222322222222222222222422222222223224222223222442222222422222222222424434222223223223324224222222242422442222223222224222222222422222224222222223422243232322222422442232243222234422224222222324444343422242242332444323232332324224222224234324242222244442223442332332232244243232224434233324244423233242423423244432434224432343244232322422232423342422342242423444243343442242223444344424222324343334434242442444344443444443442442434443444444244333344424444434442342444434444442434344434444334333434444443444444424443444333344244443434424444344443444434244443422434444444444444443442444344442344442444344423334444434324444444444444434344434444443343444444444444444334444444434444443343444444444444444343444434444444444444444444443433434344334334444444344444444434443444444444433443434443444434344444444344334444443344433444444434444443444433434444443444443444443444434\n",
            "After pruning:\n",
            "loss: 1.4318718910217285 - accuracy: 0.48625999689102173 - val_loss: 1.4681748151779175 - val_accuracy: 0.476500004529953\n",
            "layer sizes: [3072, 255, 555, 294, 745, 10]\n",
            "222222222222122221212122122222222222222222222222221122222122222321222122212122322222222222223232322231223332331233333322323223233333332232323332333333323333323333333333233233332333333333333333323333333333333333333333333333333333333333233333333333333333333\n",
            "221122112221212222112222222222212222222222222222221222122222122212222222222121221222222221222221221222222212222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222322222222222232222222222222222222222222222223222322222222222222222232222232222222222322222222223222322332322222232233333223222332222322323223223332322232223232222222323223332333322332323323333233332333322333332323323333332333332323233332233333323333323323333233333333333233333323333333333333333333333333333333333333333333333\n",
            "212222222222111222222122222212222221222222222222222222222222212222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222232222222222212223322222322222222222233232322222322323222223222222222222222222323322232233222223223222332233333222232333233233323332333\n",
            "2112222222222222122222222222222122222222212222222222222222222222222222222222222122222222222222222222222222222222222222212222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222232222222223222222322222222222222222222222222322222222222222223223222232222222222222222222222222223222222232222222222222222222222322222322322332222222222222222222322222222222222222222222222222322232323222222222322322223222222222232332222223323232323323222222222332222222222323323322322232322232333222323322232323232232332232322222322332223222232333222223322223233333222333223323333232323233333333332333332332333232233232323233333233333333333333333333333333333333333333333333333333\n",
            "Epoch 3/10\n",
            "Before growing:\n",
            "loss: 1.4318718910217285 - accuracy: 0.48625999689102173 - val_loss: 1.4681748151779175 - val_accuracy: 0.476500004529953\n",
            "layer sizes: [3072, 255, 555, 294, 745, 10]\n",
            "222222222222122221212122122222222222222222222222221122222122222321222122212122322222222222223232322231223332331233333322323223233333332232323332333333323333323333333333233233332333333333333333323333333333333333333333333333333333333333233333333333333333333\n",
            "221122112221212222112222222222212222222222222222221222122222122212222222222121221222222221222221221222222212222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222322222222222232222222222222222222222222222223222322222222222222222232222232222222222322222222223222322332322222232233333223222332222322323223223332322232223232222222323223332333322332323323333233332333322333332323323333332333332323233332233333323333323323333233333333333233333323333333333333333333333333333333333333333333333\n",
            "212222222222111222222122222212222221222222222222222222222222212222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222232222222222212223322222322222222222233232322222322323222223222222222222222222323322232233222223223222332233333222232333233233323332333\n",
            "2112222222222222122222222222222122222222212222222222222222222222222222222222222122222222222222222222222222222222222222212222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222232222222223222222322222222222222222222222222322222222222222223223222232222222222222222222222222223222222232222222222222222222222322222322322332222222222222222222322222222222222222222222222222322232323222222222322322223222222222232332222223323232323323222222222332222222222323323322322232322232333222323322232323232232332232322222322332223222232333222223322223233333222333223323333232323233333333332333332332333232233232323233333233333333333333333333333333333333333333333333333333\n",
            "After growing:\n",
            "loss: 1.4318718910217285 - accuracy: 0.48625999689102173 - val_loss: 1.4681748151779175 - val_accuracy: 0.476500004529953\n",
            "layer sizes: [3072, 306, 666, 352, 894, 10]\n",
            "222222222222122221212122122222222222222222222222221122222122222321222122212122322222222222223232322231223332331233333322323223233333332232323332333333323333323333333333233233332333333333333333323333333333333333333333333333333333333333233333333333333333333555555555555555555555555555555555555555555555555555\n",
            "221122112221212222112222222222212222222222222222221222122222122212222222222121221222222221222221221222222212222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222322222222222232222222222222222222222222222223222322222222222222222232222232222222222322222222223222322332322222232233333223222332222322323223223332322232223232222222323223332333322332323323333233332333322333332323323333332333332323233332233333323333323323333233333333333233333323333333333333333333333333333333333333333333333444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444\n",
            "2122222222221112222221222222122222212222222222222222222222222122222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222322222222222122233222223222222222222332323222223223232222232222222222222222223233222322332222232232223322333332222323332332333233323335555555555555555555555555555555555555555555555555555555555\n",
            "211222222222222212222222222222212222222221222222222222222222222222222222222222212222222222222222222222222222222222222221222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222223222222222322222232222222222222222222222222232222222222222222322322223222222222222222222222222222322222223222222222222222222222232222232232233222222222222222222232222222222222222222222222222232223232322222222232232222322222222223233222222332323232332322222222233222222222232332332232223232223233322232332223232323223233223232222232233222322223233322222332222323333322233322332333323232323333333333233333233233323223323232323333323333333333333333333333333333333333333333333333333344444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444\n",
            "Before pruning:\n",
            "loss: 1.3631645441055298 - accuracy: 0.5158799886703491 - val_loss: 1.4235961437225342 - val_accuracy: 0.4952000081539154\n",
            "layer sizes: [3072, 306, 666, 352, 894, 10]\n",
            "112122111122111221111111112111121211211221121222111122222121222321223123212122313232322212233232333231323333331233333332333313333343331133433333433333313333333333333333333243333343333333333333333334343334443343433344443444434343433434334344433343344344443444444444443443444444444443444444444444444444444444\n",
            "111121111211212122112111121111212221222222112212121221112222121212212122222121211221212121222121221222112211212212222222222222212222222222222222222222222222212222221212212212222222112222222222332222232222222222222222224322222222222332222233222232232322222323323222232323322224223432322332332223232342333232233332334322323332333323432332333233332433344233324333233323343223443432442333333343333233433323343433343343434334344243442444333443344333443333443434334333434444334343443443434334344433443334434344444433444444434344443354444443444443444444444444434444454445454544444454445444544455444445544544454545455544444454544454444444454554444544545444454555444545554454\n",
            "2112112112221112112221222121112222211222212211222222222222222112222212232221221222222122211222224221243223222222224322222332123221233243322424324422222223224323424424244423144244242424342424444444344324432244234344444434444433442344444444444244342444444444444444444444444444444344444444344443445454544544455445545545454545555444544455454544545554545454\n",
            "111221122112211212211212222222211122122221222122221222222122222212222222221212212122122222222222221222222221222222221221222222221222212222242222222222222222422222222222222232222222222224222422222222222234232222212222322222222222242422224224222322223222424222232242244234222442442244242422222222432222223244222244322224422442422444444224242224424442222422442344242444344424344222232443442243344244422444244444424224424432322342444242444334432442344444242444444244244444324244444444442442443244424444443324444444442444444444342344424234444444444442444444442444444444444442444444444444444224242444443444434442444444444442444444424444444444434442444444444434444444444444444444444424444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444\n",
            "After pruning:\n",
            "loss: 1.3628919124603271 - accuracy: 0.515779972076416 - val_loss: 1.4233094453811646 - val_accuracy: 0.4952999949455261\n",
            "layer sizes: [3072, 222, 445, 184, 386, 10]\n",
            "112122111122111221111111112111121211211221121222111122222121222321223123212122313232322212233232333231323333331233333332333313333333311333333333333313333333333333333333233333333333333333333333333333333333333333333333333333\n",
            "1111211112112121221121111211112122212222221122121212211122221212122121222221212112212121212221212212221122112122122222222222222122222222222222222222222222222122222212122122122222221122222222223322222322222222222222222232222222222233222223322223223232222232332322223232332222223323223323322232323233323223333233322323332333323323323332333323332333233323332333223332233333333333233333233333333333332323333333333333333333333333333333333333333333333\n",
            "2112112112221112112221222121112222211222212211222222222222222112222212232221221222222122211222222212322322222222322222332123221233233222322222222322323222231222232233232223333323232333\n",
            "11122112211221121221121222222221112212222122212222122222212222221222222222121221212212222222222222122222222122222222122122222222122221222222222222222222222222222222222222322222222222222222222222222232322222122223222222222222222222222232222322222222322222322222222222222223222222322222322222222222222222222222322323222232322332222222232322322233322322223222232233223232232222222332223232\n",
            "Epoch 4/10\n",
            "Before growing:\n",
            "loss: 1.3628919124603271 - accuracy: 0.515779972076416 - val_loss: 1.4233094453811646 - val_accuracy: 0.4952999949455261\n",
            "layer sizes: [3072, 222, 445, 184, 386, 10]\n",
            "112122111122111221111111112111121211211221121222111122222121222321223123212122313232322212233232333231323333331233333332333313333333311333333333333313333333333333333333233333333333333333333333333333333333333333333333333333\n",
            "1111211112112121221121111211112122212222221122121212211122221212122121222221212112212121212221212212221122112122122222222222222122222222222222222222222222222122222212122122122222221122222222223322222322222222222222222232222222222233222223322223223232222232332322223232332222223323223323322232323233323223333233322323332333323323323332333323332333233323332333223332233333333333233333233333333333332323333333333333333333333333333333333333333333333\n",
            "2112112112221112112221222121112222211222212211222222222222222112222212232221221222222122211222222212322322222222322222332123221233233222322222222322323222231222232233232223333323232333\n",
            "11122112211221121221121222222221112212222122212222122222212222221222222222121221212212222222222222122222222122222222122122222222122221222222222222222222222222222222222222322222222222222222222222222232322222122223222222222222222222222232222322222222322222322222222222222223222222322222322222222222222222222222322323222232322332222222232322322233322322223222232233223232232222222332223232\n",
            "After growing:\n",
            "loss: 1.3628919124603271 - accuracy: 0.515779972076416 - val_loss: 1.423309564590454 - val_accuracy: 0.4952999949455261\n",
            "layer sizes: [3072, 266, 534, 220, 463, 10]\n",
            "11212211112211122111111111211112121121122112122211112222212122232122312321212231323232221223323233323132333333123333333233331333333331133333333333331333333333333333333323333333333333333333333333333333333333333333333333333355555555555555555555555555555555555555555555\n",
            "111121111211212122112111121111212221222222112212121221112222121212212122222121211221212121222121221222112211212212222222222222212222222222222222222222222222212222221212212212222222112222222222332222232222222222222222223222222222223322222332222322323222223233232222323233222222332322332332223232323332322333323332232333233332332332333233332333233323332333233322333223333333333323333323333333333333232333333333333333333333333333333333333333333333344444444444444444444444444444444444444444444444444444444444444444444444444444444444444444\n",
            "2112112112221112112221222121112222211222212211222222222222222112222212232221221222222122211222222212322322222222322222332123221233233222322222222322323222231222232233232223333323232333555555555555555555555555555555555555\n",
            "1112211221122112122112122222222111221222212221222212222221222222122222222212122121221222222222222212222222212222222212212222222212222122222222222222222222222222222222222232222222222222222222222222223232222212222322222222222222222222223222232222222232222232222222222222222322222232222232222222222222222222222232232322223232233222222223232232223332232222322223223322323223222222233222323244444444444444444444444444444444444444444444444444444444444444444444444444444\n",
            "Before pruning:\n",
            "loss: 1.3042412996292114 - accuracy: 0.5326799750328064 - val_loss: 1.3891916275024414 - val_accuracy: 0.5052000284194946\n",
            "layer sizes: [3072, 266, 534, 220, 463, 10]\n",
            "11111111111111111111111111111112121111111112121111112222121123232122312331213331323231331333323233333133333333123333333333331333333331133333333333331333333333433333334323333333333333343443333333333333343334343333333333443444343434343433344443443444444343444443443444\n",
            "111121111111111121112111121111111211212111112211111111111222111212112112222111211221112111222111211122112211112212222222212122211222222222122222221222222322212222221212212312232222112222222322432323232213232234122232323223222223223332233443333422334232223333233222334333232323433422332432333343433434432333333332342433244333343443443343442444233423343434244433444233443333444333344433333444433344243433344344434344444443443443444444444444434343455555554445445455555555555455554455445555355555555555544555555545455355555555455554555555\n",
            "1112111111221112112211111121111112211211112111221222121122222113223212233331321132222122411422233212432333423334322223342144331433333223422333324334434413341233432144243344433423344433544454454445545454555545444455454445\n",
            "1112111221122112122112122222221111212222111221221212222211222222122122122212111121221232221221222112232212212412222212412222122212222122222222222222422224222421222222224342222242242222244422222222323232422212224422322242222322422222244222242222222242242242222232322222424423224242224242244422422222444222424442434442324442344444442244342343444444442222443324244424444234444444444424424444444444443444443444443444444444444444443444444434444444444444444444434444344\n",
            "After pruning:\n",
            "loss: 1.3042476177215576 - accuracy: 0.5328999757766724 - val_loss: 1.3890563249588013 - val_accuracy: 0.5049999952316284\n",
            "layer sizes: [3072, 225, 358, 157, 301, 10]\n",
            "111111111111111111111111111111121211111111121211111122221211232321223123312133313232313313333232333331333333331233333333333313333333311333333333333313333333333333333323333333333333333333333333333333333333333333333333333333333\n",
            "1111211111111111211121111211111112112121111122111111111112221112121121122221112112211121112221112111221122111122122222222121222112222222221222222212222223222122222212122123122322221122222223223232323221323223122232323223222223223332233333322332322233332332223333323232333223323233333333323333333323233233333333322332333323323333333333333333332333333333333333\n",
            "1112111111221112112211111121111112211211112111221222121122222113223212233331321132222122112223321232333233332222332133133333223223333233313312333212333323333\n",
            "1112111221122112122112122222221111212222111221221212222211222222122122122212111121221232221221222112232212212122222121222212221222212222222222222222222222122222222322222222222222222222323232222122222322222223222222222222222222222222222223232222222322222222222222222222232322322323322223322223223333333\n",
            "Epoch 5/10\n",
            "Before growing:\n",
            "loss: 1.3042476177215576 - accuracy: 0.5328999757766724 - val_loss: 1.3890563249588013 - val_accuracy: 0.5049999952316284\n",
            "layer sizes: [3072, 225, 358, 157, 301, 10]\n",
            "111111111111111111111111111111121211111111121211111122221211232321223123312133313232313313333232333331333333331233333333333313333333311333333333333313333333333333333323333333333333333333333333333333333333333333333333333333333\n",
            "1111211111111111211121111211111112112121111122111111111112221112121121122221112112211121112221112111221122111122122222222121222112222222221222222212222223222122222212122123122322221122222223223232323221323223122232323223222223223332233333322332322233332332223333323232333223323233333333323333333323233233333333322332333323323333333333333333332333333333333333\n",
            "1112111111221112112211111121111112211211112111221222121122222113223212233331321132222122112223321232333233332222332133133333223223333233313312333212333323333\n",
            "1112111221122112122112122222221111212222111221221212222211222222122122122212111121221232221221222112232212212122222121222212221222212222222222222222222222122222222322222222222222222222323232222122222322222223222222222222222222222222222223232222222322222222222222222222232322322323322223322223223333333\n",
            "After growing:\n",
            "loss: 1.3042476177215576 - accuracy: 0.5328999757766724 - val_loss: 1.3890563249588013 - val_accuracy: 0.5049999952316284\n",
            "layer sizes: [3072, 270, 429, 188, 361, 10]\n",
            "111111111111111111111111111111121211111111121211111122221211232321223123312133313232313313333232333331333333331233333333333313333333311333333333333313333333333333333323333333333333333333333333333333333333333333333333333333333555555555555555555555555555555555555555555555\n",
            "111121111111111121112111121111111211212111112211111111111222111212112112222111211221112111222111211122112211112212222222212122211222222222122222221222222322212222221212212312232222112222222322323232322132322312223232322322222322333223333332233232223333233222333332323233322332323333333332333333332323323333333332233233332332333333333333333333233333333333333344444444444444444444444444444444444444444444444444444444444444444444444\n",
            "11121111112211121122111111211111122112111121112212221211222221132232122333313211322221221122233212323332333322223321331333332232233332333133123332123333233334444444444444444444444444444444\n",
            "1112111221122112122112122222221111212222111221221212222211222222122122122212111121221232221221222112232212212122222121222212221222212222222222222222222222122222222322222222222222222222323232222122222322222223222222222222222222222222222223232222222322222222222222222222232322322323322223322223223333333444444444444444444444444444444444444444444444444444444444444\n",
            "Before pruning:\n",
            "loss: 1.2620947360992432 - accuracy: 0.5509600043296814 - val_loss: 1.3633660078048706 - val_accuracy: 0.5145999789237976\n",
            "layer sizes: [3072, 270, 429, 188, 361, 10]\n",
            "111111111111111111111111111111111211111111111211111122221212232311223123312133323232313323333233333331333333331233333333333313333333311333333343433423333333333333333324333443333344333343333333343333333343334444444444444444444444444444444444444444443444444444444444444434\n",
            "111111111111111111112111111111111211211111111111111111111212111111111112222111211111112111122111211112112211112211222122212112311222132222122232221222223332312232221212212312242223113222222312323233423132322312333333322322332422333223333333333232233443233323433442333334332432323333434332433333433433434334434433234334342332434333344333343344234344444443444444544444444444444444454444454444444444444444444444444444444444444444445\n",
            "11111111112211121112111111111111121111111121112212221211222321142342123443313311432231221111443212334343443434343431341444434233233332434133124441124344244435454545554455555545554444454545\n",
            "1112111211122112112112121222221111111222111221211212222212122224122122122212111121211242221221221112242212212121222121242212221222212422242222224222222222112232222422222222222221222224424242222122424424222244224344242224224122242444424324244222424442242243422342442244342433444444422424422444224444444444444444444444444443444443444444444444444444444444444444444\n",
            "After pruning:\n",
            "loss: 1.262412667274475 - accuracy: 0.5507799983024597 - val_loss: 1.363599181175232 - val_accuracy: 0.5142999887466431\n",
            "layer sizes: [3072, 197, 315, 125, 236, 10]\n",
            "11111111111111111111111111111111121111111111121111112222121223231122312331213332323231332333323333333133333333123333333333331333333331133333333332333333333333333332333333333333333333333333333333333\n",
            "111111111111111111112111111111111211211111111111111111111212111111111112222111211111112111122111211112112211112211222122212112311222132222122232221222223332312232221212212312222231132222223123232332313232231233333332232233222333223333333333232233323332333233333332323233333332333333333333333233332332333333333332333\n",
            "11111111112211121112111111111111121111111121112212221211222321123212333133113223122111132123333333331313233233332313312112323\n",
            "11121112111221121121121212222211111112221112212112122222121222212212212221211112121122221221221112222122121212221212221222122221222222222222222222211223222222222222222221222222222221222222222232222221222223222222222232232223233222222233\n",
            "Epoch 6/10\n",
            "Before growing:\n",
            "loss: 1.262412667274475 - accuracy: 0.5507799983024597 - val_loss: 1.363599181175232 - val_accuracy: 0.5142999887466431\n",
            "layer sizes: [3072, 197, 315, 125, 236, 10]\n",
            "11111111111111111111111111111111121111111111121111112222121223231122312331213332323231332333323333333133333333123333333333331333333331133333333332333333333333333332333333333333333333333333333333333\n",
            "111111111111111111112111111111111211211111111111111111111212111111111112222111211111112111122111211112112211112211222122212112311222132222122232221222223332312232221212212312222231132222223123232332313232231233333332232233222333223333333333232233323332333233333332323233333332333333333333333233332332333333333332333\n",
            "11111111112211121112111111111111121111111121112212221211222321123212333133113223122111132123333333331313233233332313312112323\n",
            "11121112111221121121121212222211111112221112212112122222121222212212212221211112121122221221221112222122121212221212221222122221222222222222222222211223222222222222222221222222222221222222222232222221222223222222222232232223233222222233\n",
            "After growing:\n",
            "loss: 1.262412667274475 - accuracy: 0.5507799983024597 - val_loss: 1.363599181175232 - val_accuracy: 0.5142999887466431\n",
            "layer sizes: [3072, 236, 378, 150, 283, 10]\n",
            "11111111111111111111111111111111121111111111121111112222121223231122312331213332323231332333323333333133333333123333333333331333333331133333333332333333333333333332333333333333333333333333333333333555555555555555555555555555555555555555\n",
            "111111111111111111112111111111111211211111111111111111111212111111111112222111211111112111122111211112112211112211222122212112311222132222122232221222223332312232221212212312222231132222223123232332313232231233333332232233222333223333333333232233323332333233333332323233333332333333333333333233332332333333333332333444444444444444444444444444444444444444444444444444444444444444\n",
            "111111111122111211121111111111111211111111211122122212112223211232123331331132231221111321233333333313132332333323133121123234444444444444444444444444\n",
            "1112111211122112112112121222221111111222111221211212222212122221221221222121111212112222122122111222212212121222121222122212222122222222222222222221122322222222222222222122222222222122222222223222222122222322222222223223222323322222223344444444444444444444444444444444444444444444444\n",
            "Before pruning:\n",
            "loss: 1.2218199968338013 - accuracy: 0.5629199743270874 - val_loss: 1.3430185317993164 - val_accuracy: 0.5236999988555908\n",
            "layer sizes: [3072, 236, 378, 150, 283, 10]\n",
            "11111111111111111111111111111111121111111111121111112222131213231122311331213333323331332333313333333133333333133333333333331333333431133333333333433333433433333332333333333334333343333334333343344344333334333433434344434433433433443443\n",
            "111111111111111111112111111111111111111111111111111111111211111111111112112111111111112111122111211112113211113211212122212112311322132232122232221223223332312232221212213312322231132322213124232432313332341333333342333233322333323343443344242233424333343243333333434233434433344344333344343343343432343444444432444444444444444444444444444544454444444554444544445444444444444444\n",
            "111111111122111111221111111111111211111111211122122312112223211332123341331132341231111321243333333313132333333423134111124345545554555555544445455555\n",
            "1112111111121112111112121221121111111122111121221412222211122211221421222111111212112212122122111422212212121222121222222212222122222222222222242221122422224224222222222132222222222122244222224242223124242442442242244224422434422334444444444443444344443444444434444444444434444444444\n",
            "After pruning:\n",
            "loss: 1.2218291759490967 - accuracy: 0.5627400279045105 - val_loss: 1.343051552772522 - val_accuracy: 0.5235000252723694\n",
            "layer sizes: [3072, 209, 275, 118, 211, 10]\n",
            "11111111111111111111111111111111121111111111121111112222131213231122311331213333323331332333313333333133333333133333333333331333333311333333333333333333333333323333333333333333333333333333333333333333333333333\n",
            "11111111111111111111211111111111111111111111111111111111121111111111111211211111111111211112211121111211321111321121212221211231132213223212223222122322333231223222121221331232223113232221312232323133323133333332333233322333323333322233233333233333333233333333333333333323332\n",
            "1111111111221111112211111111111112111111112111221223121122232113321233133113231231111321233333333131323333332313111123\n",
            "1112111111121112111112121221121111111122111121221122222111222112212122211111121211221212212211122212212121222121222222212222122222222222222222211222222222222222221322222222221222222222222312222222222223223333333\n",
            "Epoch 7/10\n",
            "Before growing:\n",
            "loss: 1.2218291759490967 - accuracy: 0.5627400279045105 - val_loss: 1.343051552772522 - val_accuracy: 0.5235000252723694\n",
            "layer sizes: [3072, 209, 275, 118, 211, 10]\n",
            "11111111111111111111111111111111121111111111121111112222131213231122311331213333323331332333313333333133333333133333333333331333333311333333333333333333333333323333333333333333333333333333333333333333333333333\n",
            "11111111111111111111211111111111111111111111111111111111121111111111111211211111111111211112211121111211321111321121212221211231132213223212223222122322333231223222121221331232223113232221312232323133323133333332333233322333323333322233233333233333333233333333333333333323332\n",
            "1111111111221111112211111111111112111111112111221223121122232113321233133113231231111321233333333131323333332313111123\n",
            "1112111111121112111112121221121111111122111121221122222111222112212122211111121211221212212211122212212121222121222222212222122222222222222222211222222222222222221322222222221222222222222312222222222223223333333\n",
            "After growing:\n",
            "loss: 1.2218291759490967 - accuracy: 0.5627400279045105 - val_loss: 1.343051552772522 - val_accuracy: 0.5235000252723694\n",
            "layer sizes: [3072, 250, 330, 141, 253, 10]\n",
            "1111111111111111111111111111111112111111111112111111222213121323112231133121333332333133233331333333313333333313333333333333133333331133333333333333333333333332333333333333333333333333333333333333333333333333355555555555555555555555555555555555555555\n",
            "111111111111111111112111111111111111111111111111111111111211111111111112112111111111112111122111211112113211113211212122212112311322132232122232221223223332312232221212213312322231132322213122323231333231333333323332333223333233333222332333332333333332333333333333333333233324444444444444444444444444444444444444444444444444444444\n",
            "111111111122111111221111111111111211111111211122122312112223211332123313311323123111132123333333313132333333231311112344444444444444444444444\n",
            "1112111111121112111112121221121111111122111121221122222111222112212122211111121211221212212211122212212121222121222222212222122222222222222222211222222222222222221322222222221222222222222312222222222223223333333444444444444444444444444444444444444444444\n",
            "Before pruning:\n",
            "loss: 1.2028262615203857 - accuracy: 0.57396000623703 - val_loss: 1.3463820219039917 - val_accuracy: 0.5214999914169312\n",
            "layer sizes: [3072, 250, 330, 141, 253, 10]\n",
            "1111111111111111111111111111111111111111111113111111222213121323113331133111333333333133333331333343313334333313333333333333133333331133333333333333334333333332333333433333333343343434333444343433444433444343444443344443434444344343443333344444344344\n",
            "111111111111111111111111111111111111111111111111111111111211111111111112111111121111112111123111211112113211113211212122212113311322132232122231221333223342422232221313313312322231143322223122333231434331333433323432334234333343333322342443442333333332334434434434333333234435444445444454444544444544444444445444444454454445445454\n",
            "111111111122111111211111111111111211111111211132122312111323311432124313311423134111132124343443413142434343231311113345444455444554454544454\n",
            "1112111111121112111112111221121111111122111141121122222111122112212121211111121211221212212211141212212121214111422222214224122222222223242422211224222222421242222442232224231244222322242312243422244444324434443444443444433443444434444433443444344334334\n",
            "After pruning:\n",
            "loss: 1.2026666402816772 - accuracy: 0.5739799737930298 - val_loss: 1.3462284803390503 - val_accuracy: 0.5213000178337097\n",
            "layer sizes: [3072, 201, 251, 105, 195, 10]\n",
            "111111111111111111111111111111111111111111111311111122221312132311333113311133333333313333333133333313333333133333333333331333333311333333333333333333333333233333333333333333333333333333333333333333333\n",
            "11111111111111111111111111111111111111111111111111111111121111111111111211111112111111211112311121111211321111321121212221211331132213223212223122133322332222322213133133123222311332222312233323133313333332332332333333333322323233333333233333333333233\n",
            "111111111122111111211111111111111211111111211132122312111323311321231331123131111321233313123332313111133\n",
            "111211111112111211111211122112111111112211111121122222111122112212121211111121211221212212211112122121212111122222212212222222222322222112222222221222222232222312222322223122322232333333333333333\n",
            "Epoch 8/10\n",
            "Before growing:\n",
            "loss: 1.2026666402816772 - accuracy: 0.5739799737930298 - val_loss: 1.3462284803390503 - val_accuracy: 0.5213000178337097\n",
            "layer sizes: [3072, 201, 251, 105, 195, 10]\n",
            "111111111111111111111111111111111111111111111311111122221312132311333113311133333333313333333133333313333333133333333333331333333311333333333333333333333333233333333333333333333333333333333333333333333\n",
            "11111111111111111111111111111111111111111111111111111111121111111111111211111112111111211112311121111211321111321121212221211331132213223212223122133322332222322213133133123222311332222312233323133313333332332332333333333322323233333333233333333333233\n",
            "111111111122111111211111111111111211111111211132122312111323311321231331123131111321233313123332313111133\n",
            "111211111112111211111211122112111111112211111121122222111122112212121211111121211221212212211112122121212111122222212212222222222322222112222222221222222232222312222322223122322232333333333333333\n",
            "After growing:\n",
            "loss: 1.2026665210723877 - accuracy: 0.5739799737930298 - val_loss: 1.3462284803390503 - val_accuracy: 0.5213000178337097\n",
            "layer sizes: [3072, 241, 301, 126, 234, 10]\n",
            "1111111111111111111111111111111111111111111113111111222213121323113331133111333333333133333331333333133333331333333333333313333333113333333333333333333333332333333333333333333333333333333333333333333335555555555555555555555555555555555555555\n",
            "1111111111111111111111111111111111111111111111111111111112111111111111121111111211111121111231112111121132111132112121222121133113221322321222312213332233222232221313313312322231133222231223332313331333333233233233333333332232323333333323333333333323344444444444444444444444444444444444444444444444444\n",
            "111111111122111111211111111111111211111111211132122312111323311321231331123131111321233313123332313111133444444444444444444444\n",
            "111211111112111211111211122112111111112211111121122222111122112212121211111121211221212212211112122121212111122222212212222222222322222112222222221222222232222312222322223122322232333333333333333444444444444444444444444444444444444444\n",
            "Before pruning:\n",
            "loss: 1.1618068218231201 - accuracy: 0.5839599967002869 - val_loss: 1.3246874809265137 - val_accuracy: 0.5256999731063843\n",
            "layer sizes: [3072, 241, 301, 126, 234, 10]\n",
            "1111111111111111111111111111111111111111111113111111122213131333123331133111333333333133333331333433133333331333333333333313443333113333333334333433433333332343333334343433333333344443433444334443444444334443344444444443334433444444434344344\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111121111111211111121111221113111121132111132112121232121134113221322321212312213332233212242221313313322322231233222141223332413331333344233243234433443433333334444444323434343443324344444444454544444444544444444444454445444455444455\n",
            "111111111122111111211111111111111311111111211132132412111313411321331331123131111421244414123343413111133454444444444444545544\n",
            "111211111112111111111111122111111111112111111121121222111122111212121211111131211211212211211112122141212111122222212212242222224422222112222222221222322444422412222422424122422232444444444444444444444444444444444344344444344444344344\n",
            "After pruning:\n",
            "loss: 1.1616326570510864 - accuracy: 0.5839800238609314 - val_loss: 1.3244218826293945 - val_accuracy: 0.5260999798774719\n",
            "layer sizes: [3072, 187, 226, 96, 172, 10]\n",
            "1111111111111111111111111111111111111111111113111111122213131333123331133111333333333133333331333331333333313333333333333133333113333333333333333333332333333333333333333333333333333333333\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111121111111211111121111221113111121132111132112121232121131132213223212123122133322332122222131331332232223123322211223332133313333233232333333333333233333323\n",
            "111111111122111111211111111111111311111111211132132121113131132133133112313111121211233313111133\n",
            "1112111111121111111111111221111111111121111111211212221111221112121212111111312112112122112111121221121211112222221221222222222222211222222222122232222122222221222223233333\n",
            "Epoch 9/10\n",
            "Before growing:\n",
            "loss: 1.1616326570510864 - accuracy: 0.5839800238609314 - val_loss: 1.3244218826293945 - val_accuracy: 0.5260999798774719\n",
            "layer sizes: [3072, 187, 226, 96, 172, 10]\n",
            "1111111111111111111111111111111111111111111113111111122213131333123331133111333333333133333331333331333333313333333333333133333113333333333333333333332333333333333333333333333333333333333\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111121111111211111121111221113111121132111132112121232121131132213223212123122133322332122222131331332232223123322211223332133313333233232333333333333233333323\n",
            "111111111122111111211111111111111311111111211132132121113131132133133112313111121211233313111133\n",
            "1112111111121111111111111221111111111121111111211212221111221112121212111111312112112122112111121221121211112222221221222222222222211222222222122232222122222221222223233333\n",
            "After growing:\n",
            "loss: 1.1616326570510864 - accuracy: 0.5839800238609314 - val_loss: 1.3244218826293945 - val_accuracy: 0.5260999798774719\n",
            "layer sizes: [3072, 224, 271, 116, 206, 10]\n",
            "11111111111111111111111111111111111111111111131111111222131313331233311331113333333331333333313333313333333133333333333331333331133333333333333333333323333333333333333333333333333333333335555555555555555555555555555555555555\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111121111111211111121111221113111121132111132112121232121131132213223212123122133322332122222131331332232223123322211223332133313333233232333333333333233333323444444444444444444444444444444444444444444444\n",
            "11111111112211111121111111111111131111111121113213212111313113213313311231311112121123331311113344444444444444444444\n",
            "11121111111211111111111112211111111111211111112112122211112211121212121111113121121121221121111212211212111122222212212222222222222112222222221222322221222222212222232333334444444444444444444444444444444444\n",
            "Before pruning:\n",
            "loss: 1.1420552730560303 - accuracy: 0.5938599705696106 - val_loss: 1.3206034898757935 - val_accuracy: 0.5289000272750854\n",
            "layer sizes: [3072, 224, 271, 116, 206, 10]\n",
            "11111111111111111111111111111111121111111111131111111131131313331233311331113333333331333333313333313333333133333333333331334332133333333333333333344323343333334433333333333334343344443343443343444443333344444434444343344444\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111121111111211111121111231113111121132111132112121232121131132214224212124122133322342122222131331332232123123433211223332143413433234242333333333333333333333444454554455444545444554544454555444444545544\n",
            "11111111112211111121111111111111131111111121114313213111413114214314411331311112132114331411113455554445545555455545\n",
            "11121111111211111111111112211111111111211111112112112211111211121212121111114121121121221121111212211212111122122222212224222422334112242222241222423241222242214242242344444444444444444444433443443434344443\n",
            "After pruning:\n",
            "loss: 1.1420304775238037 - accuracy: 0.5938400030136108 - val_loss: 1.320600152015686 - val_accuracy: 0.5285999774932861\n",
            "layer sizes: [3072, 187, 216, 87, 163, 10]\n",
            "1111111111111111111111111111111112111111111113111111113113131333123331133111333333333133333331333331333333313333333333333133332133333333333333333332333333333333333333333333333333333333333\n",
            "111111111111111111111111111111111111111111111111111111111111111111111112111111121111112111123111311112113211113211212123212113113221222121212213332232122222131331332232123123332112233321313332322333333333333333333333\n",
            "111111111122111111211111111111111311111111211131321311113112131113313111121321133111113\n",
            "1112111111121111111111111221111111111121111111211211221111121112121212111111121121121221121111212211212111122122222212222222233112222222122223212222221222233333333\n",
            "Epoch 10/10\n",
            "Before growing:\n",
            "loss: 1.1420304775238037 - accuracy: 0.5938400030136108 - val_loss: 1.320600152015686 - val_accuracy: 0.5285999774932861\n",
            "layer sizes: [3072, 187, 216, 87, 163, 10]\n",
            "1111111111111111111111111111111112111111111113111111113113131333123331133111333333333133333331333331333333313333333333333133332133333333333333333332333333333333333333333333333333333333333\n",
            "111111111111111111111111111111111111111111111111111111111111111111111112111111121111112111123111311112113211113211212123212113113221222121212213332232122222131331332232123123332112233321313332322333333333333333333333\n",
            "111111111122111111211111111111111311111111211131321311113112131113313111121321133111113\n",
            "1112111111121111111111111221111111111121111111211211221111121112121212111111121121121221121111212211212111122122222212222222233112222222122223212222221222233333333\n",
            "After growing:\n",
            "loss: 1.1420304775238037 - accuracy: 0.5938400030136108 - val_loss: 1.320600152015686 - val_accuracy: 0.5285999774932861\n",
            "layer sizes: [3072, 224, 259, 107, 195, 10]\n",
            "11111111111111111111111111111111121111111111131111111131131313331233311331113333333331333333313333313333333133333333333331333321333333333333333333323333333333333333333333333333333333333335555555555555555555555555555555555555\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111121111111211111121111231113111121132111132112121232121131132212221212122133322321222221313313322321231233321122333213133323223333333333333333333334444444444444444444444444444444444444444444\n",
            "11111111112211111121111111111111131111111121113132131111311213111331311112132113311111344444444444444444444\n",
            "111211111112111111111111122111111111112111111121121122111112111212121211111112112112122112111121221121211112212222221222222223311222222212222321222222122223333333344444444444444444444444444444444\n",
            "Before pruning:\n",
            "loss: 1.1198385953903198 - accuracy: 0.6008999943733215 - val_loss: 1.3173203468322754 - val_accuracy: 0.5303000211715698\n",
            "layer sizes: [3072, 224, 259, 107, 195, 10]\n",
            "11111111111111111111111111111111111111111111131111111131131313331333312331113333333331333333313333313333333133333333333331333332333343333333333333313333333333333334344434333433444433343433334344444433434444444444444444433434\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111111111211111121111231113111121132111132112121332221131132212221212112133322331222221313313332321231233321122333213133423223333333433333333343333343444443444445444444444443445444444443444\n",
            "11111111112211111121111111111111131111111121113142131111411214211331311113132124311111344444554444444444444\n",
            "111211111112111111111111122111111111112111111121121122111114111211121211111112122112122112111121221141211112312221221222222224311332222212222321232232122243333344443444444433444334344444444443444\n",
            "After pruning:\n",
            "loss: 1.1199510097503662 - accuracy: 0.6007999777793884 - val_loss: 1.3174477815628052 - val_accuracy: 0.5303999781608582\n",
            "layer sizes: [3072, 184, 219, 83, 163, 10]\n",
            "1111111111111111111111111111111111111111111113111111113113131333133331233111333333333133333331333331333333313333333333333133333233333333333333333313333333333333333333333333333333333333\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111121111112111123111311112113211113211212133222113113221222121211213332233122222131331333232123123332112233321313323223333333333333333333333333\n",
            "11111111112211111121111111111111131111111121113121311111121211331311113132123111113\n",
            "1112111111121111111111111221111111111121111111211211221111111121112121111111212211212211211112122111211112312221221222222223113322222122223212322321222333333333333\n",
            "CPU times: user 5min 44s, sys: 15.3 s, total: 6min\n",
            "Wall time: 5min 56s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nQWwgTYyG0q",
        "outputId": "405d96ce-0d31-4d8f-b679-3985084f090b"
      },
      "source": [
        "%%time\n",
        "\n",
        "model = SSModel(layer_sizes=[3072, 300, 300, 300, 300, 10], activation='selu', regularization_penalty=0.000001, regularization_method='weighted_l1', kernel_initializer='lecun_normal')\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002)\n",
        "\n",
        "train_model(model, X_train_norm, y_train, optimizer, epochs, self_scaling_epochs, batch_size, min_new_neurons, validation_data=(X_test_norm, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "##########################################################\n",
            "Epoch 1/10\n",
            "Before growing:\n",
            "loss: 2.653571605682373 - accuracy: 0.10862000286579132 - val_loss: 2.6458799839019775 - val_accuracy: 0.11469999700784683 - penalty: 1e-06\n",
            "layer sizes: [3072, 300, 300, 300, 300, 10]\n",
            "After growing:\n",
            "loss: 2.653571605682373 - accuracy: 0.10862000286579132 - val_loss: 2.6458799839019775 - val_accuracy: 0.11469999700784683 - penalty: 1e-06\n",
            "layer sizes: [3072, 360, 360, 360, 360, 10]\n",
            "Before pruning:\n",
            "loss: 1.44719660282135 - accuracy: 0.4853399991989136 - val_loss: 1.5112738609313965 - val_accuracy: 0.4641999900341034 - penalty: 1e-06\n",
            "layer sizes: [3072, 360, 360, 360, 360, 10]\n",
            "After pruning:\n",
            "loss: 1.4471937417984009 - accuracy: 0.4853000044822693 - val_loss: 1.5112707614898682 - val_accuracy: 0.4641999900341034 - penalty: 1e-06\n",
            "layer sizes: [3072, 300, 300, 300, 341, 10]\n",
            "##########################################################\n",
            "Epoch 2/10\n",
            "Before growing:\n",
            "loss: 1.4471937417984009 - accuracy: 0.4853000044822693 - val_loss: 1.5112707614898682 - val_accuracy: 0.4641999900341034 - penalty: 1e-06\n",
            "layer sizes: [3072, 300, 300, 300, 341, 10]\n",
            "After growing:\n",
            "loss: 1.4471933841705322 - accuracy: 0.4853000044822693 - val_loss: 1.511270523071289 - val_accuracy: 0.4641999900341034 - penalty: 1e-06\n",
            "layer sizes: [3072, 360, 360, 360, 409, 10]\n",
            "Before pruning:\n",
            "loss: 1.3646430969238281 - accuracy: 0.5124599933624268 - val_loss: 1.437365174293518 - val_accuracy: 0.4846999943256378 - penalty: 1e-06\n",
            "layer sizes: [3072, 360, 360, 360, 409, 10]\n",
            "After pruning:\n",
            "loss: 1.3646459579467773 - accuracy: 0.5123800039291382 - val_loss: 1.4373588562011719 - val_accuracy: 0.4846999943256378 - penalty: 1e-06\n",
            "layer sizes: [3072, 300, 300, 300, 331, 10]\n",
            "##########################################################\n",
            "Epoch 3/10\n",
            "Before growing:\n",
            "loss: 1.3646459579467773 - accuracy: 0.5123800039291382 - val_loss: 1.4373588562011719 - val_accuracy: 0.4846999943256378 - penalty: 1e-06\n",
            "layer sizes: [3072, 300, 300, 300, 331, 10]\n",
            "After growing:\n",
            "loss: 1.364646077156067 - accuracy: 0.5123800039291382 - val_loss: 1.4373588562011719 - val_accuracy: 0.4846999943256378 - penalty: 1e-06\n",
            "layer sizes: [3072, 360, 360, 360, 397, 10]\n",
            "Before pruning:\n",
            "loss: 1.3035646677017212 - accuracy: 0.5348399877548218 - val_loss: 1.3855372667312622 - val_accuracy: 0.5078999996185303 - penalty: 1e-06\n",
            "layer sizes: [3072, 360, 360, 360, 397, 10]\n",
            "After pruning:\n",
            "loss: 1.3035662174224854 - accuracy: 0.534820020198822 - val_loss: 1.3855317831039429 - val_accuracy: 0.5077000260353088 - penalty: 1e-06\n",
            "layer sizes: [3072, 300, 300, 300, 304, 10]\n",
            "##########################################################\n",
            "Epoch 4/10\n",
            "Before growing:\n",
            "loss: 1.3035662174224854 - accuracy: 0.534820020198822 - val_loss: 1.3855317831039429 - val_accuracy: 0.5077000260353088 - penalty: 1e-06\n",
            "layer sizes: [3072, 300, 300, 300, 304, 10]\n",
            "After growing:\n",
            "loss: 1.3035662174224854 - accuracy: 0.534820020198822 - val_loss: 1.3855317831039429 - val_accuracy: 0.5077000260353088 - penalty: 1e-06\n",
            "layer sizes: [3072, 360, 360, 360, 364, 10]\n",
            "Before pruning:\n",
            "loss: 1.2757153511047363 - accuracy: 0.54093998670578 - val_loss: 1.376749038696289 - val_accuracy: 0.5070000290870667 - penalty: 1e-06\n",
            "layer sizes: [3072, 360, 360, 360, 364, 10]\n",
            "After pruning:\n",
            "loss: 1.2757315635681152 - accuracy: 0.54093998670578 - val_loss: 1.3767669200897217 - val_accuracy: 0.507099986076355 - penalty: 1e-06\n",
            "layer sizes: [3072, 299, 300, 300, 322, 10]\n",
            "##########################################################\n",
            "Epoch 5/10\n",
            "Before growing:\n",
            "loss: 1.2757315635681152 - accuracy: 0.54093998670578 - val_loss: 1.3767669200897217 - val_accuracy: 0.507099986076355 - penalty: 1e-06\n",
            "layer sizes: [3072, 299, 300, 300, 322, 10]\n",
            "After growing:\n",
            "loss: 1.2757315635681152 - accuracy: 0.54093998670578 - val_loss: 1.3767669200897217 - val_accuracy: 0.507099986076355 - penalty: 1e-06\n",
            "layer sizes: [3072, 358, 360, 360, 386, 10]\n",
            "Before pruning:\n",
            "loss: 1.2404656410217285 - accuracy: 0.5494599938392639 - val_loss: 1.3632210493087769 - val_accuracy: 0.5110999941825867 - penalty: 1e-06\n",
            "layer sizes: [3072, 358, 360, 360, 386, 10]\n",
            "After pruning:\n",
            "loss: 1.2405256032943726 - accuracy: 0.5495799779891968 - val_loss: 1.363307237625122 - val_accuracy: 0.5109999775886536 - penalty: 1e-06\n",
            "layer sizes: [3072, 288, 300, 300, 326, 10]\n",
            "##########################################################\n",
            "Epoch 6/10\n",
            "Before growing:\n",
            "loss: 1.2405256032943726 - accuracy: 0.5495799779891968 - val_loss: 1.363307237625122 - val_accuracy: 0.5109999775886536 - penalty: 1e-06\n",
            "layer sizes: [3072, 288, 300, 300, 326, 10]\n",
            "After growing:\n",
            "loss: 1.2405256032943726 - accuracy: 0.5495799779891968 - val_loss: 1.363307237625122 - val_accuracy: 0.5109999775886536 - penalty: 1e-06\n",
            "layer sizes: [3072, 345, 360, 360, 391, 10]\n",
            "Before pruning:\n",
            "loss: 1.215855598449707 - accuracy: 0.563539981842041 - val_loss: 1.3565062284469604 - val_accuracy: 0.5130000114440918 - penalty: 1e-06\n",
            "layer sizes: [3072, 345, 360, 360, 391, 10]\n",
            "After pruning:\n",
            "loss: 1.2157418727874756 - accuracy: 0.563480019569397 - val_loss: 1.3563646078109741 - val_accuracy: 0.5134000182151794 - penalty: 1e-06\n",
            "layer sizes: [3072, 270, 300, 300, 295, 10]\n",
            "##########################################################\n",
            "Epoch 7/10\n",
            "Before growing:\n",
            "loss: 1.2157418727874756 - accuracy: 0.563480019569397 - val_loss: 1.3563646078109741 - val_accuracy: 0.5134000182151794 - penalty: 1e-06\n",
            "layer sizes: [3072, 270, 300, 300, 295, 10]\n",
            "After growing:\n",
            "loss: 1.215741753578186 - accuracy: 0.563480019569397 - val_loss: 1.3563646078109741 - val_accuracy: 0.5134000182151794 - penalty: 1e-06\n",
            "layer sizes: [3072, 324, 360, 360, 354, 10]\n",
            "Before pruning:\n",
            "loss: 1.175469994544983 - accuracy: 0.5804399847984314 - val_loss: 1.333858847618103 - val_accuracy: 0.5256999731063843 - penalty: 1e-06\n",
            "layer sizes: [3072, 324, 360, 360, 354, 10]\n",
            "After pruning:\n",
            "loss: 1.1756181716918945 - accuracy: 0.5802199840545654 - val_loss: 1.3340142965316772 - val_accuracy: 0.5260999798774719 - penalty: 1e-06\n",
            "layer sizes: [3072, 249, 300, 299, 281, 10]\n",
            "##########################################################\n",
            "Epoch 8/10\n",
            "Before growing:\n",
            "loss: 1.1756181716918945 - accuracy: 0.5802199840545654 - val_loss: 1.3340142965316772 - val_accuracy: 0.5260999798774719 - penalty: 1e-06\n",
            "layer sizes: [3072, 249, 300, 299, 281, 10]\n",
            "After growing:\n",
            "loss: 1.1756181716918945 - accuracy: 0.5802199840545654 - val_loss: 1.3340142965316772 - val_accuracy: 0.5260999798774719 - penalty: 1e-06\n",
            "layer sizes: [3072, 298, 360, 358, 337, 10]\n",
            "Before pruning:\n",
            "loss: 1.1509491205215454 - accuracy: 0.5869399905204773 - val_loss: 1.3288652896881104 - val_accuracy: 0.5281000137329102 - penalty: 1e-06\n",
            "layer sizes: [3072, 298, 360, 358, 337, 10]\n",
            "After pruning:\n",
            "loss: 1.1509894132614136 - accuracy: 0.5867999792098999 - val_loss: 1.3289036750793457 - val_accuracy: 0.5282999873161316 - penalty: 1e-06\n",
            "layer sizes: [3072, 231, 300, 291, 277, 10]\n",
            "##########################################################\n",
            "Epoch 9/10\n",
            "Before growing:\n",
            "loss: 1.1509894132614136 - accuracy: 0.5867999792098999 - val_loss: 1.3289036750793457 - val_accuracy: 0.5282999873161316 - penalty: 1e-06\n",
            "layer sizes: [3072, 231, 300, 291, 277, 10]\n",
            "After growing:\n",
            "loss: 1.1509895324707031 - accuracy: 0.5867999792098999 - val_loss: 1.3289036750793457 - val_accuracy: 0.5282999873161316 - penalty: 1e-06\n",
            "layer sizes: [3072, 277, 360, 349, 332, 10]\n",
            "Before pruning:\n",
            "loss: 1.132772445678711 - accuracy: 0.5921000242233276 - val_loss: 1.3332191705703735 - val_accuracy: 0.5282999873161316 - penalty: 1e-06\n",
            "layer sizes: [3072, 277, 360, 349, 332, 10]\n",
            "After pruning:\n",
            "loss: 1.1327868700027466 - accuracy: 0.5920799970626831 - val_loss: 1.333269476890564 - val_accuracy: 0.5289000272750854 - penalty: 1e-06\n",
            "layer sizes: [3072, 212, 294, 274, 271, 10]\n",
            "##########################################################\n",
            "Epoch 10/10\n",
            "Before growing:\n",
            "loss: 1.1327868700027466 - accuracy: 0.5920799970626831 - val_loss: 1.333269476890564 - val_accuracy: 0.5289000272750854 - penalty: 1e-06\n",
            "layer sizes: [3072, 212, 294, 274, 271, 10]\n",
            "After growing:\n",
            "loss: 1.1327868700027466 - accuracy: 0.5920799970626831 - val_loss: 1.333269476890564 - val_accuracy: 0.5289000272750854 - penalty: 1e-06\n",
            "layer sizes: [3072, 254, 352, 328, 325, 10]\n",
            "Before pruning:\n",
            "loss: 1.0994240045547485 - accuracy: 0.6055600047111511 - val_loss: 1.3229185342788696 - val_accuracy: 0.5313000082969666 - penalty: 1e-06\n",
            "layer sizes: [3072, 254, 352, 328, 325, 10]\n",
            "After pruning:\n",
            "loss: 1.0994789600372314 - accuracy: 0.6056600213050842 - val_loss: 1.3229448795318604 - val_accuracy: 0.5311999917030334 - penalty: 1e-06\n",
            "layer sizes: [3072, 190, 289, 255, 253, 10]\n",
            "CPU times: user 3min 41s, sys: 2.74 s, total: 3min 44s\n",
            "Wall time: 3min 41s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltB1QmpIMG7D"
      },
      "source": [
        "epochs = 50\n",
        "self_scaling_epochs = 50\n",
        "batch_size = 32\n",
        "min_new_neurons = 50"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptA-TKCwnU-X",
        "outputId": "f4864d86-af3a-48e1-a220-3017de74e42b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "model = SSModel(layer_sizes=[3072, 300, 300, 300, 300, 10], activation='selu', regularization_penalty=0.0001, regularization_method='weighted_l1', kernel_initializer='lecun_normal')\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002)\n",
        "\n",
        "train_model(model, X_train_norm, y_train, optimizer, epochs, self_scaling_epochs, batch_size, min_new_neurons, validation_data=(X_test_norm, y_test), regularization_penalty_multiplier=0.1)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "##########################################################\n",
            "Epoch 1/50\n",
            "Before growing:\n",
            "loss: 2.707078695297241 - accuracy: 0.10412000119686127 - val_loss: 2.7202649116516113 - val_accuracy: 0.10530000180006027 - penalty: 0.0001\n",
            "hidden layer sizes: [300, 300, 300, 300], total neurons: 1200\n",
            "After growing:\n",
            "loss: 2.707078695297241 - accuracy: 0.10412000119686127 - val_loss: 2.7202649116516113 - val_accuracy: 0.10530000180006027 - penalty: 0.0001\n",
            "hidden layer sizes: [360, 360, 360, 360], total neurons: 1440\n",
            "Before pruning:\n",
            "loss: 1.840686559677124 - accuracy: 0.33399999141693115 - val_loss: 1.84286367893219 - val_accuracy: 0.33480000495910645 - penalty: 0.0001\n",
            "hidden layer sizes: [360, 360, 360, 360], total neurons: 1440\n",
            "After pruning:\n",
            "loss: 1.8411165475845337 - accuracy: 0.33410000801086426 - val_loss: 1.8433187007904053 - val_accuracy: 0.33309999108314514 - penalty: 0.0001\n",
            "hidden layer sizes: [15, 58, 34, 62], total neurons: 169\n",
            "##########################################################\n",
            "Epoch 2/50\n",
            "Before growing:\n",
            "loss: 1.8411165475845337 - accuracy: 0.33410000801086426 - val_loss: 1.8433187007904053 - val_accuracy: 0.33309999108314514 - penalty: 0.0001\n",
            "hidden layer sizes: [15, 58, 34, 62], total neurons: 169\n",
            "After growing:\n",
            "loss: 1.8411165475845337 - accuracy: 0.33410000801086426 - val_loss: 1.8433187007904053 - val_accuracy: 0.33309999108314514 - penalty: 0.0001\n",
            "hidden layer sizes: [65, 108, 84, 112], total neurons: 369\n",
            "Before pruning:\n",
            "loss: 1.7565593719482422 - accuracy: 0.3594000041484833 - val_loss: 1.7631562948226929 - val_accuracy: 0.35929998755455017 - penalty: 0.0001\n",
            "hidden layer sizes: [65, 108, 84, 112], total neurons: 369\n",
            "After pruning:\n",
            "loss: 1.7566578388214111 - accuracy: 0.35938000679016113 - val_loss: 1.7632745504379272 - val_accuracy: 0.35929998755455017 - penalty: 0.0001\n",
            "hidden layer sizes: [10, 42, 20, 46], total neurons: 118\n",
            "##########################################################\n",
            "Epoch 3/50\n",
            "Before growing:\n",
            "loss: 1.7566578388214111 - accuracy: 0.35938000679016113 - val_loss: 1.7632745504379272 - val_accuracy: 0.35929998755455017 - penalty: 0.0001\n",
            "hidden layer sizes: [10, 42, 20, 46], total neurons: 118\n",
            "After growing:\n",
            "loss: 1.7566578388214111 - accuracy: 0.35938000679016113 - val_loss: 1.7632745504379272 - val_accuracy: 0.35929998755455017 - penalty: 0.0001\n",
            "hidden layer sizes: [60, 92, 70, 96], total neurons: 318\n",
            "Before pruning:\n",
            "loss: 1.739669680595398 - accuracy: 0.36493998765945435 - val_loss: 1.7466872930526733 - val_accuracy: 0.36309999227523804 - penalty: 0.0001\n",
            "hidden layer sizes: [60, 92, 70, 96], total neurons: 318\n",
            "After pruning:\n",
            "loss: 1.7397090196609497 - accuracy: 0.3650200068950653 - val_loss: 1.7467093467712402 - val_accuracy: 0.36340001225471497 - penalty: 0.0001\n",
            "hidden layer sizes: [9, 34, 20, 41], total neurons: 104\n",
            "##########################################################\n",
            "Epoch 4/50\n",
            "Before growing:\n",
            "loss: 1.7397090196609497 - accuracy: 0.3650200068950653 - val_loss: 1.7467093467712402 - val_accuracy: 0.36340001225471497 - penalty: 0.0001\n",
            "hidden layer sizes: [9, 34, 20, 41], total neurons: 104\n",
            "After growing:\n",
            "loss: 1.7397090196609497 - accuracy: 0.3650200068950653 - val_loss: 1.7467093467712402 - val_accuracy: 0.36340001225471497 - penalty: 0.0001\n",
            "hidden layer sizes: [59, 84, 70, 91], total neurons: 304\n",
            "Before pruning:\n",
            "loss: 1.7446671724319458 - accuracy: 0.3713800013065338 - val_loss: 1.7532503604888916 - val_accuracy: 0.3637999892234802 - penalty: 0.0001\n",
            "hidden layer sizes: [59, 84, 70, 91], total neurons: 304\n",
            "After pruning:\n",
            "loss: 1.744680643081665 - accuracy: 0.3712199926376343 - val_loss: 1.753267526626587 - val_accuracy: 0.3637999892234802 - penalty: 0.0001\n",
            "hidden layer sizes: [9, 31, 20, 40], total neurons: 100\n",
            "##########################################################\n",
            "Epoch 5/50\n",
            "Before growing:\n",
            "loss: 1.744680643081665 - accuracy: 0.3712199926376343 - val_loss: 1.753267526626587 - val_accuracy: 0.3637999892234802 - penalty: 0.0001\n",
            "hidden layer sizes: [9, 31, 20, 40], total neurons: 100\n",
            "After growing:\n",
            "loss: 1.744680643081665 - accuracy: 0.3712199926376343 - val_loss: 1.7532674074172974 - val_accuracy: 0.3637999892234802 - penalty: 1e-05\n",
            "hidden layer sizes: [59, 81, 70, 90], total neurons: 300\n",
            "Before pruning:\n",
            "loss: 1.6463587284088135 - accuracy: 0.40206000208854675 - val_loss: 1.6582187414169312 - val_accuracy: 0.39480000734329224 - penalty: 1e-05\n",
            "hidden layer sizes: [59, 81, 70, 90], total neurons: 300\n",
            "After pruning:\n",
            "loss: 1.646688461303711 - accuracy: 0.40143999457359314 - val_loss: 1.6585334539413452 - val_accuracy: 0.39469999074935913 - penalty: 1e-05\n",
            "hidden layer sizes: [31, 46, 20, 68], total neurons: 165\n",
            "##########################################################\n",
            "Epoch 6/50\n",
            "Before growing:\n",
            "loss: 1.646688461303711 - accuracy: 0.40143999457359314 - val_loss: 1.6585334539413452 - val_accuracy: 0.39469999074935913 - penalty: 1e-05\n",
            "hidden layer sizes: [31, 46, 20, 68], total neurons: 165\n",
            "After growing:\n",
            "loss: 1.646688461303711 - accuracy: 0.40143999457359314 - val_loss: 1.6585334539413452 - val_accuracy: 0.39469999074935913 - penalty: 1e-05\n",
            "hidden layer sizes: [81, 96, 70, 118], total neurons: 365\n",
            "Before pruning:\n",
            "loss: 1.5956834554672241 - accuracy: 0.4223400056362152 - val_loss: 1.61345374584198 - val_accuracy: 0.4133000075817108 - penalty: 1e-05\n",
            "hidden layer sizes: [81, 96, 70, 118], total neurons: 365\n",
            "After pruning:\n",
            "loss: 1.5957282781600952 - accuracy: 0.42239999771118164 - val_loss: 1.6134636402130127 - val_accuracy: 0.4131999909877777 - penalty: 1e-05\n",
            "hidden layer sizes: [35, 43, 20, 67], total neurons: 165\n",
            "##########################################################\n",
            "Epoch 7/50\n",
            "Before growing:\n",
            "loss: 1.5957282781600952 - accuracy: 0.42239999771118164 - val_loss: 1.6134636402130127 - val_accuracy: 0.4131999909877777 - penalty: 1e-05\n",
            "hidden layer sizes: [35, 43, 20, 67], total neurons: 165\n",
            "After growing:\n",
            "loss: 1.5957283973693848 - accuracy: 0.42239999771118164 - val_loss: 1.6134636402130127 - val_accuracy: 0.4131999909877777 - penalty: 1e-05\n",
            "hidden layer sizes: [85, 93, 70, 117], total neurons: 365\n",
            "Before pruning:\n",
            "loss: 1.5713945627212524 - accuracy: 0.4314599931240082 - val_loss: 1.593837022781372 - val_accuracy: 0.4221000075340271 - penalty: 1e-05\n",
            "hidden layer sizes: [85, 93, 70, 117], total neurons: 365\n",
            "After pruning:\n",
            "loss: 1.571315884590149 - accuracy: 0.4316200017929077 - val_loss: 1.5937583446502686 - val_accuracy: 0.4221999943256378 - penalty: 1e-05\n",
            "hidden layer sizes: [47, 42, 20, 54], total neurons: 163\n",
            "##########################################################\n",
            "Epoch 8/50\n",
            "Before growing:\n",
            "loss: 1.571315884590149 - accuracy: 0.4316200017929077 - val_loss: 1.5937583446502686 - val_accuracy: 0.4221999943256378 - penalty: 1e-05\n",
            "hidden layer sizes: [47, 42, 20, 54], total neurons: 163\n",
            "After growing:\n",
            "loss: 1.571315884590149 - accuracy: 0.4316200017929077 - val_loss: 1.5937583446502686 - val_accuracy: 0.4221999943256378 - penalty: 1e-05\n",
            "hidden layer sizes: [97, 92, 70, 104], total neurons: 363\n",
            "Before pruning:\n",
            "loss: 1.5532028675079346 - accuracy: 0.4399600028991699 - val_loss: 1.5814316272735596 - val_accuracy: 0.4255000054836273 - penalty: 1e-05\n",
            "hidden layer sizes: [97, 92, 70, 104], total neurons: 363\n",
            "After pruning:\n",
            "loss: 1.5534952878952026 - accuracy: 0.4399000108242035 - val_loss: 1.581739902496338 - val_accuracy: 0.4251999855041504 - penalty: 1e-05\n",
            "hidden layer sizes: [39, 42, 20, 52], total neurons: 153\n",
            "##########################################################\n",
            "Epoch 9/50\n",
            "Before growing:\n",
            "loss: 1.5534952878952026 - accuracy: 0.4399000108242035 - val_loss: 1.581739902496338 - val_accuracy: 0.4251999855041504 - penalty: 1e-05\n",
            "hidden layer sizes: [39, 42, 20, 52], total neurons: 153\n",
            "After growing:\n",
            "loss: 1.5534952878952026 - accuracy: 0.4399000108242035 - val_loss: 1.581739902496338 - val_accuracy: 0.4251999855041504 - penalty: 1e-05\n",
            "hidden layer sizes: [89, 92, 70, 102], total neurons: 353\n",
            "Before pruning:\n",
            "loss: 1.5380096435546875 - accuracy: 0.44464001059532166 - val_loss: 1.5689035654067993 - val_accuracy: 0.43070000410079956 - penalty: 1e-05\n",
            "hidden layer sizes: [89, 92, 70, 102], total neurons: 353\n",
            "After pruning:\n",
            "loss: 1.537901759147644 - accuracy: 0.444599986076355 - val_loss: 1.5687371492385864 - val_accuracy: 0.4309000074863434 - penalty: 1e-05\n",
            "hidden layer sizes: [23, 42, 20, 50], total neurons: 135\n",
            "##########################################################\n",
            "Epoch 10/50\n",
            "Before growing:\n",
            "loss: 1.537901759147644 - accuracy: 0.444599986076355 - val_loss: 1.5687371492385864 - val_accuracy: 0.4309000074863434 - penalty: 1e-05\n",
            "hidden layer sizes: [23, 42, 20, 50], total neurons: 135\n",
            "After growing:\n",
            "loss: 1.5379018783569336 - accuracy: 0.444599986076355 - val_loss: 1.568737506866455 - val_accuracy: 0.4309000074863434 - penalty: 1e-05\n",
            "hidden layer sizes: [73, 92, 70, 100], total neurons: 335\n",
            "Before pruning:\n",
            "loss: 1.5215792655944824 - accuracy: 0.4525800049304962 - val_loss: 1.5573960542678833 - val_accuracy: 0.4390000104904175 - penalty: 1e-05\n",
            "hidden layer sizes: [73, 92, 70, 100], total neurons: 335\n",
            "After pruning:\n",
            "loss: 1.5215801000595093 - accuracy: 0.4524399936199188 - val_loss: 1.5574016571044922 - val_accuracy: 0.43880000710487366 - penalty: 1e-05\n",
            "hidden layer sizes: [28, 41, 20, 45], total neurons: 134\n",
            "##########################################################\n",
            "Epoch 11/50\n",
            "Before growing:\n",
            "loss: 1.5215801000595093 - accuracy: 0.4524399936199188 - val_loss: 1.5574016571044922 - val_accuracy: 0.43880000710487366 - penalty: 1e-05\n",
            "hidden layer sizes: [28, 41, 20, 45], total neurons: 134\n",
            "After growing:\n",
            "loss: 1.5215799808502197 - accuracy: 0.4524399936199188 - val_loss: 1.557401418685913 - val_accuracy: 0.43880000710487366 - penalty: 1e-05\n",
            "hidden layer sizes: [78, 91, 70, 95], total neurons: 334\n",
            "Before pruning:\n",
            "loss: 1.5103923082351685 - accuracy: 0.4576199948787689 - val_loss: 1.5543004274368286 - val_accuracy: 0.44020000100135803 - penalty: 1e-05\n",
            "hidden layer sizes: [78, 91, 70, 95], total neurons: 334\n",
            "After pruning:\n",
            "loss: 1.510074257850647 - accuracy: 0.45774000883102417 - val_loss: 1.5540162324905396 - val_accuracy: 0.4401000142097473 - penalty: 1e-05\n",
            "hidden layer sizes: [26, 41, 20, 85], total neurons: 172\n",
            "##########################################################\n",
            "Epoch 12/50\n",
            "Before growing:\n",
            "loss: 1.510074257850647 - accuracy: 0.45774000883102417 - val_loss: 1.5540162324905396 - val_accuracy: 0.4401000142097473 - penalty: 1e-05\n",
            "hidden layer sizes: [26, 41, 20, 85], total neurons: 172\n",
            "After growing:\n",
            "loss: 1.5100740194320679 - accuracy: 0.45774000883102417 - val_loss: 1.5540162324905396 - val_accuracy: 0.4401000142097473 - penalty: 1e-05\n",
            "hidden layer sizes: [76, 91, 70, 135], total neurons: 372\n",
            "Before pruning:\n",
            "loss: 1.501613736152649 - accuracy: 0.46077999472618103 - val_loss: 1.5484626293182373 - val_accuracy: 0.4413999915122986 - penalty: 1e-05\n",
            "hidden layer sizes: [76, 91, 70, 135], total neurons: 372\n",
            "After pruning:\n",
            "loss: 1.5015785694122314 - accuracy: 0.46066001057624817 - val_loss: 1.5484647750854492 - val_accuracy: 0.4415000081062317 - penalty: 1e-05\n",
            "hidden layer sizes: [28, 41, 20, 42], total neurons: 131\n",
            "##########################################################\n",
            "Epoch 13/50\n",
            "Before growing:\n",
            "loss: 1.5015785694122314 - accuracy: 0.46066001057624817 - val_loss: 1.5484647750854492 - val_accuracy: 0.4415000081062317 - penalty: 1e-05\n",
            "hidden layer sizes: [28, 41, 20, 42], total neurons: 131\n",
            "After growing:\n",
            "loss: 1.5015785694122314 - accuracy: 0.46066001057624817 - val_loss: 1.5484646558761597 - val_accuracy: 0.4415000081062317 - penalty: 1e-05\n",
            "hidden layer sizes: [78, 91, 70, 92], total neurons: 331\n",
            "Before pruning:\n",
            "loss: 1.492538332939148 - accuracy: 0.4652000069618225 - val_loss: 1.5454291105270386 - val_accuracy: 0.44429999589920044 - penalty: 1e-05\n",
            "hidden layer sizes: [78, 91, 70, 92], total neurons: 331\n",
            "After pruning:\n",
            "loss: 1.492706537246704 - accuracy: 0.46540001034736633 - val_loss: 1.5456162691116333 - val_accuracy: 0.4440000057220459 - penalty: 1e-05\n",
            "hidden layer sizes: [31, 42, 20, 69], total neurons: 162\n",
            "##########################################################\n",
            "Epoch 14/50\n",
            "Before growing:\n",
            "loss: 1.492706537246704 - accuracy: 0.46540001034736633 - val_loss: 1.5456162691116333 - val_accuracy: 0.4440000057220459 - penalty: 1e-05\n",
            "hidden layer sizes: [31, 42, 20, 69], total neurons: 162\n",
            "After growing:\n",
            "loss: 1.492706537246704 - accuracy: 0.46540001034736633 - val_loss: 1.5456162691116333 - val_accuracy: 0.4440000057220459 - penalty: 1e-05\n",
            "hidden layer sizes: [81, 92, 70, 119], total neurons: 362\n",
            "Before pruning:\n",
            "loss: 1.4816348552703857 - accuracy: 0.46869999170303345 - val_loss: 1.5371618270874023 - val_accuracy: 0.4456999897956848 - penalty: 1e-05\n",
            "hidden layer sizes: [81, 92, 70, 119], total neurons: 362\n",
            "After pruning:\n",
            "loss: 1.48137629032135 - accuracy: 0.46884000301361084 - val_loss: 1.53694748878479 - val_accuracy: 0.4456999897956848 - penalty: 1e-05\n",
            "hidden layer sizes: [40, 41, 20, 77], total neurons: 178\n",
            "##########################################################\n",
            "Epoch 15/50\n",
            "Before growing:\n",
            "loss: 1.48137629032135 - accuracy: 0.46884000301361084 - val_loss: 1.53694748878479 - val_accuracy: 0.4456999897956848 - penalty: 1e-05\n",
            "hidden layer sizes: [40, 41, 20, 77], total neurons: 178\n",
            "After growing:\n",
            "loss: 1.48137629032135 - accuracy: 0.46884000301361084 - val_loss: 1.5369473695755005 - val_accuracy: 0.4456999897956848 - penalty: 1e-05\n",
            "hidden layer sizes: [90, 91, 70, 127], total neurons: 378\n",
            "Before pruning:\n",
            "loss: 1.4742859601974487 - accuracy: 0.47341999411582947 - val_loss: 1.5341838598251343 - val_accuracy: 0.4505999982357025 - penalty: 1e-05\n",
            "hidden layer sizes: [90, 91, 70, 127], total neurons: 378\n",
            "After pruning:\n",
            "loss: 1.474366545677185 - accuracy: 0.47341999411582947 - val_loss: 1.5342085361480713 - val_accuracy: 0.4505999982357025 - penalty: 1e-05\n",
            "hidden layer sizes: [42, 41, 20, 54], total neurons: 157\n",
            "##########################################################\n",
            "Epoch 16/50\n",
            "Before growing:\n",
            "loss: 1.474366545677185 - accuracy: 0.47341999411582947 - val_loss: 1.5342085361480713 - val_accuracy: 0.4505999982357025 - penalty: 1e-05\n",
            "hidden layer sizes: [42, 41, 20, 54], total neurons: 157\n",
            "After growing:\n",
            "loss: 1.474366545677185 - accuracy: 0.47341999411582947 - val_loss: 1.5342085361480713 - val_accuracy: 0.4505999982357025 - penalty: 1e-05\n",
            "hidden layer sizes: [92, 91, 70, 104], total neurons: 357\n",
            "Before pruning:\n",
            "loss: 1.4652478694915771 - accuracy: 0.475739985704422 - val_loss: 1.530095100402832 - val_accuracy: 0.44760000705718994 - penalty: 1e-05\n",
            "hidden layer sizes: [92, 91, 70, 104], total neurons: 357\n",
            "After pruning:\n",
            "loss: 1.465015172958374 - accuracy: 0.47585999965667725 - val_loss: 1.529810905456543 - val_accuracy: 0.44760000705718994 - penalty: 1e-05\n",
            "hidden layer sizes: [32, 41, 20, 66], total neurons: 159\n",
            "##########################################################\n",
            "Epoch 17/50\n",
            "Before growing:\n",
            "loss: 1.465015172958374 - accuracy: 0.47585999965667725 - val_loss: 1.529810905456543 - val_accuracy: 0.44760000705718994 - penalty: 1e-05\n",
            "hidden layer sizes: [32, 41, 20, 66], total neurons: 159\n",
            "After growing:\n",
            "loss: 1.465015172958374 - accuracy: 0.47585999965667725 - val_loss: 1.529810905456543 - val_accuracy: 0.44760000705718994 - penalty: 1e-05\n",
            "hidden layer sizes: [82, 91, 70, 116], total neurons: 359\n",
            "Before pruning:\n",
            "loss: 1.4538140296936035 - accuracy: 0.480679988861084 - val_loss: 1.523227572441101 - val_accuracy: 0.45159998536109924 - penalty: 1e-05\n",
            "hidden layer sizes: [82, 91, 70, 116], total neurons: 359\n",
            "After pruning:\n",
            "loss: 1.4538812637329102 - accuracy: 0.4807400107383728 - val_loss: 1.5233131647109985 - val_accuracy: 0.4507000148296356 - penalty: 1e-05\n",
            "hidden layer sizes: [31, 41, 20, 51], total neurons: 143\n",
            "##########################################################\n",
            "Epoch 18/50\n",
            "Before growing:\n",
            "loss: 1.4538812637329102 - accuracy: 0.4807400107383728 - val_loss: 1.5233131647109985 - val_accuracy: 0.4507000148296356 - penalty: 1e-05\n",
            "hidden layer sizes: [31, 41, 20, 51], total neurons: 143\n",
            "After growing:\n",
            "loss: 1.4538809061050415 - accuracy: 0.4807400107383728 - val_loss: 1.5233131647109985 - val_accuracy: 0.4507000148296356 - penalty: 1e-05\n",
            "hidden layer sizes: [81, 91, 70, 101], total neurons: 343\n",
            "Before pruning:\n",
            "loss: 1.446455955505371 - accuracy: 0.48330000042915344 - val_loss: 1.5183929204940796 - val_accuracy: 0.45410001277923584 - penalty: 1e-05\n",
            "hidden layer sizes: [81, 91, 70, 101], total neurons: 343\n",
            "After pruning:\n",
            "loss: 1.4464956521987915 - accuracy: 0.4830000102519989 - val_loss: 1.5183900594711304 - val_accuracy: 0.4544999897480011 - penalty: 1e-05\n",
            "hidden layer sizes: [23, 41, 20, 55], total neurons: 139\n",
            "##########################################################\n",
            "Epoch 19/50\n",
            "Before growing:\n",
            "loss: 1.4464956521987915 - accuracy: 0.4830000102519989 - val_loss: 1.5183900594711304 - val_accuracy: 0.4544999897480011 - penalty: 1e-05\n",
            "hidden layer sizes: [23, 41, 20, 55], total neurons: 139\n",
            "After growing:\n",
            "loss: 1.4464956521987915 - accuracy: 0.4830000102519989 - val_loss: 1.5183900594711304 - val_accuracy: 0.4544999897480011 - penalty: 1e-05\n",
            "hidden layer sizes: [73, 91, 70, 105], total neurons: 339\n",
            "Before pruning:\n",
            "loss: 1.4395115375518799 - accuracy: 0.4866200089454651 - val_loss: 1.5167855024337769 - val_accuracy: 0.45339998602867126 - penalty: 1e-05\n",
            "hidden layer sizes: [73, 91, 70, 105], total neurons: 339\n",
            "After pruning:\n",
            "loss: 1.4396029710769653 - accuracy: 0.48660001158714294 - val_loss: 1.5169039964675903 - val_accuracy: 0.4537000060081482 - penalty: 1e-05\n",
            "hidden layer sizes: [27, 40, 20, 74], total neurons: 161\n",
            "##########################################################\n",
            "Epoch 20/50\n",
            "Before growing:\n",
            "loss: 1.4396029710769653 - accuracy: 0.48660001158714294 - val_loss: 1.5169039964675903 - val_accuracy: 0.4537000060081482 - penalty: 1e-05\n",
            "hidden layer sizes: [27, 40, 20, 74], total neurons: 161\n",
            "After growing:\n",
            "loss: 1.4396030902862549 - accuracy: 0.48660001158714294 - val_loss: 1.5169039964675903 - val_accuracy: 0.4537000060081482 - penalty: 1e-05\n",
            "hidden layer sizes: [77, 90, 70, 124], total neurons: 361\n",
            "Before pruning:\n",
            "loss: 1.4411765336990356 - accuracy: 0.487280011177063 - val_loss: 1.5218719244003296 - val_accuracy: 0.45339998602867126 - penalty: 1e-05\n",
            "hidden layer sizes: [77, 90, 70, 124], total neurons: 361\n",
            "After pruning:\n",
            "loss: 1.4414623975753784 - accuracy: 0.48726001381874084 - val_loss: 1.5221574306488037 - val_accuracy: 0.453000009059906 - penalty: 1e-05\n",
            "hidden layer sizes: [30, 40, 20, 71], total neurons: 161\n",
            "##########################################################\n",
            "Epoch 21/50\n",
            "Before growing:\n",
            "loss: 1.4414623975753784 - accuracy: 0.48726001381874084 - val_loss: 1.5221574306488037 - val_accuracy: 0.453000009059906 - penalty: 1e-05\n",
            "hidden layer sizes: [30, 40, 20, 71], total neurons: 161\n",
            "After growing:\n",
            "loss: 1.441462516784668 - accuracy: 0.48726001381874084 - val_loss: 1.5221575498580933 - val_accuracy: 0.453000009059906 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [80, 90, 70, 121], total neurons: 361\n",
            "Before pruning:\n",
            "loss: 1.401272177696228 - accuracy: 0.5025399923324585 - val_loss: 1.4882224798202515 - val_accuracy: 0.4675000011920929 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [80, 90, 70, 121], total neurons: 361\n",
            "After pruning:\n",
            "loss: 1.401268482208252 - accuracy: 0.5026000142097473 - val_loss: 1.4882185459136963 - val_accuracy: 0.4675000011920929 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [80, 90, 47, 70], total neurons: 287\n",
            "##########################################################\n",
            "Epoch 22/50\n",
            "Before growing:\n",
            "loss: 1.401268482208252 - accuracy: 0.5026000142097473 - val_loss: 1.4882185459136963 - val_accuracy: 0.4675000011920929 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [80, 90, 47, 70], total neurons: 287\n",
            "After growing:\n",
            "loss: 1.401268482208252 - accuracy: 0.5026000142097473 - val_loss: 1.4882185459136963 - val_accuracy: 0.4675000011920929 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [130, 140, 97, 120], total neurons: 487\n",
            "Before pruning:\n",
            "loss: 1.3667157888412476 - accuracy: 0.5170000195503235 - val_loss: 1.465758204460144 - val_accuracy: 0.48030000925064087 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [130, 140, 97, 120], total neurons: 487\n",
            "After pruning:\n",
            "loss: 1.366714358329773 - accuracy: 0.5170000195503235 - val_loss: 1.4657554626464844 - val_accuracy: 0.48030000925064087 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [130, 138, 58, 93], total neurons: 419\n",
            "##########################################################\n",
            "Epoch 23/50\n",
            "Before growing:\n",
            "loss: 1.366714358329773 - accuracy: 0.5170000195503235 - val_loss: 1.4657554626464844 - val_accuracy: 0.48030000925064087 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [130, 138, 58, 93], total neurons: 419\n",
            "After growing:\n",
            "loss: 1.366714358329773 - accuracy: 0.5170000195503235 - val_loss: 1.4657554626464844 - val_accuracy: 0.48030000925064087 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [180, 188, 108, 143], total neurons: 619\n",
            "Before pruning:\n",
            "loss: 1.3384240865707397 - accuracy: 0.5258399844169617 - val_loss: 1.4425467252731323 - val_accuracy: 0.4869999885559082 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [180, 188, 108, 143], total neurons: 619\n",
            "After pruning:\n",
            "loss: 1.3383910655975342 - accuracy: 0.5258600115776062 - val_loss: 1.4425228834152222 - val_accuracy: 0.48669999837875366 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [176, 168, 79, 83], total neurons: 506\n",
            "##########################################################\n",
            "Epoch 24/50\n",
            "Before growing:\n",
            "loss: 1.3383910655975342 - accuracy: 0.5258600115776062 - val_loss: 1.4425228834152222 - val_accuracy: 0.48669999837875366 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [176, 168, 79, 83], total neurons: 506\n",
            "After growing:\n",
            "loss: 1.3383910655975342 - accuracy: 0.5258600115776062 - val_loss: 1.4425228834152222 - val_accuracy: 0.48669999837875366 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [226, 218, 129, 133], total neurons: 706\n",
            "Before pruning:\n",
            "loss: 1.3089076280593872 - accuracy: 0.5353800058364868 - val_loss: 1.4252873659133911 - val_accuracy: 0.492900013923645 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [226, 218, 129, 133], total neurons: 706\n",
            "After pruning:\n",
            "loss: 1.3088592290878296 - accuracy: 0.5356000065803528 - val_loss: 1.425246000289917 - val_accuracy: 0.49300000071525574 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [212, 202, 60, 60], total neurons: 534\n",
            "##########################################################\n",
            "Epoch 25/50\n",
            "Before growing:\n",
            "loss: 1.3088592290878296 - accuracy: 0.5356000065803528 - val_loss: 1.425246000289917 - val_accuracy: 0.49300000071525574 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [212, 202, 60, 60], total neurons: 534\n",
            "After growing:\n",
            "loss: 1.3088592290878296 - accuracy: 0.5356000065803528 - val_loss: 1.4252461194992065 - val_accuracy: 0.49300000071525574 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [262, 252, 110, 110], total neurons: 734\n",
            "Before pruning:\n",
            "loss: 1.2971007823944092 - accuracy: 0.5394600033760071 - val_loss: 1.4192912578582764 - val_accuracy: 0.4925999939441681 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [262, 252, 110, 110], total neurons: 734\n",
            "After pruning:\n",
            "loss: 1.297129511833191 - accuracy: 0.5393000245094299 - val_loss: 1.4192885160446167 - val_accuracy: 0.4927999973297119 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [235, 218, 73, 45], total neurons: 571\n",
            "##########################################################\n",
            "Epoch 26/50\n",
            "Before growing:\n",
            "loss: 1.297129511833191 - accuracy: 0.5393000245094299 - val_loss: 1.4192885160446167 - val_accuracy: 0.4927999973297119 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [235, 218, 73, 45], total neurons: 571\n",
            "After growing:\n",
            "loss: 1.2971293926239014 - accuracy: 0.5393000245094299 - val_loss: 1.4192886352539062 - val_accuracy: 0.4927999973297119 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [285, 268, 123, 95], total neurons: 771\n",
            "Before pruning:\n",
            "loss: 1.2788926362991333 - accuracy: 0.5494400262832642 - val_loss: 1.4153724908828735 - val_accuracy: 0.49390000104904175 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [285, 268, 123, 95], total neurons: 771\n",
            "After pruning:\n",
            "loss: 1.2788485288619995 - accuracy: 0.5493999719619751 - val_loss: 1.4152531623840332 - val_accuracy: 0.49390000104904175 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [256, 235, 77, 68], total neurons: 636\n",
            "##########################################################\n",
            "Epoch 27/50\n",
            "Before growing:\n",
            "loss: 1.2788485288619995 - accuracy: 0.5493999719619751 - val_loss: 1.4152531623840332 - val_accuracy: 0.49390000104904175 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [256, 235, 77, 68], total neurons: 636\n",
            "After growing:\n",
            "loss: 1.27884840965271 - accuracy: 0.5493999719619751 - val_loss: 1.4152531623840332 - val_accuracy: 0.49390000104904175 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [307, 285, 127, 118], total neurons: 837\n",
            "Before pruning:\n",
            "loss: 1.2676284313201904 - accuracy: 0.5531399846076965 - val_loss: 1.4139572381973267 - val_accuracy: 0.5006999969482422 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [307, 285, 127, 118], total neurons: 837\n",
            "After pruning:\n",
            "loss: 1.2675925493240356 - accuracy: 0.5535600185394287 - val_loss: 1.4139505624771118 - val_accuracy: 0.5011000037193298 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [228, 257, 79, 66], total neurons: 630\n",
            "##########################################################\n",
            "Epoch 28/50\n",
            "Before growing:\n",
            "loss: 1.2675925493240356 - accuracy: 0.5535600185394287 - val_loss: 1.4139505624771118 - val_accuracy: 0.5011000037193298 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [228, 257, 79, 66], total neurons: 630\n",
            "After growing:\n",
            "loss: 1.267592430114746 - accuracy: 0.5535600185394287 - val_loss: 1.4139505624771118 - val_accuracy: 0.5011000037193298 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [278, 308, 129, 116], total neurons: 831\n",
            "Before pruning:\n",
            "loss: 1.2530834674835205 - accuracy: 0.5567200183868408 - val_loss: 1.4069103002548218 - val_accuracy: 0.4984000027179718 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [278, 308, 129, 116], total neurons: 831\n",
            "After pruning:\n",
            "loss: 1.2529921531677246 - accuracy: 0.5569199919700623 - val_loss: 1.4069157838821411 - val_accuracy: 0.49810001254081726 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [256, 243, 77, 56], total neurons: 632\n",
            "##########################################################\n",
            "Epoch 29/50\n",
            "Before growing:\n",
            "loss: 1.2529921531677246 - accuracy: 0.5569199919700623 - val_loss: 1.4069157838821411 - val_accuracy: 0.49810001254081726 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [256, 243, 77, 56], total neurons: 632\n",
            "After growing:\n",
            "loss: 1.2529921531677246 - accuracy: 0.5569199919700623 - val_loss: 1.4069160223007202 - val_accuracy: 0.49810001254081726 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [307, 293, 127, 106], total neurons: 833\n",
            "Before pruning:\n",
            "loss: 1.2412924766540527 - accuracy: 0.5619199872016907 - val_loss: 1.4061775207519531 - val_accuracy: 0.5034999847412109 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [307, 293, 127, 106], total neurons: 833\n",
            "After pruning:\n",
            "loss: 1.2412656545639038 - accuracy: 0.5619199872016907 - val_loss: 1.4061460494995117 - val_accuracy: 0.5041000247001648 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [256, 264, 88, 77], total neurons: 685\n",
            "##########################################################\n",
            "Epoch 30/50\n",
            "Before growing:\n",
            "loss: 1.2412656545639038 - accuracy: 0.5619199872016907 - val_loss: 1.4061460494995117 - val_accuracy: 0.5041000247001648 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [256, 264, 88, 77], total neurons: 685\n",
            "After growing:\n",
            "loss: 1.2412655353546143 - accuracy: 0.5619199872016907 - val_loss: 1.4061460494995117 - val_accuracy: 0.5041000247001648 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [307, 316, 138, 127], total neurons: 888\n",
            "Before pruning:\n",
            "loss: 1.2221431732177734 - accuracy: 0.5698000192642212 - val_loss: 1.3928582668304443 - val_accuracy: 0.5081999897956848 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [307, 316, 138, 127], total neurons: 888\n",
            "After pruning:\n",
            "loss: 1.2222098112106323 - accuracy: 0.5694400072097778 - val_loss: 1.3928539752960205 - val_accuracy: 0.5080999732017517 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [241, 272, 87, 65], total neurons: 665\n",
            "##########################################################\n",
            "Epoch 31/50\n",
            "Before growing:\n",
            "loss: 1.2222098112106323 - accuracy: 0.5694400072097778 - val_loss: 1.3928539752960205 - val_accuracy: 0.5080999732017517 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [241, 272, 87, 65], total neurons: 665\n",
            "After growing:\n",
            "loss: 1.2222098112106323 - accuracy: 0.5694400072097778 - val_loss: 1.392853856086731 - val_accuracy: 0.5080999732017517 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [291, 326, 137, 115], total neurons: 869\n",
            "Before pruning:\n",
            "loss: 1.220065951347351 - accuracy: 0.5695599913597107 - val_loss: 1.398587703704834 - val_accuracy: 0.5037000179290771 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [291, 326, 137, 115], total neurons: 869\n",
            "After pruning:\n",
            "loss: 1.2200249433517456 - accuracy: 0.569599986076355 - val_loss: 1.398587703704834 - val_accuracy: 0.5037999749183655 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [264, 258, 87, 83], total neurons: 692\n",
            "##########################################################\n",
            "Epoch 32/50\n",
            "Before growing:\n",
            "loss: 1.2200249433517456 - accuracy: 0.569599986076355 - val_loss: 1.398587703704834 - val_accuracy: 0.5037999749183655 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [264, 258, 87, 83], total neurons: 692\n",
            "After growing:\n",
            "loss: 1.2200249433517456 - accuracy: 0.569599986076355 - val_loss: 1.3985875844955444 - val_accuracy: 0.5037999749183655 - penalty: 1.0000000000000002e-07\n",
            "hidden layer sizes: [316, 309, 137, 133], total neurons: 895\n",
            "Before pruning:\n",
            "loss: 1.2032402753829956 - accuracy: 0.5759000182151794 - val_loss: 1.3881710767745972 - val_accuracy: 0.5095000267028809 - penalty: 1.0000000000000002e-07\n",
            "hidden layer sizes: [316, 309, 137, 133], total neurons: 895\n",
            "After pruning:\n",
            "loss: 1.2032402753829956 - accuracy: 0.5759000182151794 - val_loss: 1.3881710767745972 - val_accuracy: 0.5095000267028809 - penalty: 1.0000000000000002e-07\n",
            "hidden layer sizes: [316, 309, 137, 130], total neurons: 892\n",
            "##########################################################\n",
            "Epoch 33/50\n",
            "Before growing:\n",
            "loss: 1.2032402753829956 - accuracy: 0.5759000182151794 - val_loss: 1.3881710767745972 - val_accuracy: 0.5095000267028809 - penalty: 1.0000000000000002e-07\n",
            "hidden layer sizes: [316, 309, 137, 130], total neurons: 892\n",
            "After growing:\n",
            "loss: 1.2032402753829956 - accuracy: 0.5759000182151794 - val_loss: 1.3881710767745972 - val_accuracy: 0.5095000267028809 - penalty: 1.0000000000000002e-07\n",
            "hidden layer sizes: [379, 370, 187, 180], total neurons: 1116\n",
            "Before pruning:\n",
            "loss: 1.192260503768921 - accuracy: 0.5782399773597717 - val_loss: 1.3888033628463745 - val_accuracy: 0.510699987411499 - penalty: 1.0000000000000002e-07\n",
            "hidden layer sizes: [379, 370, 187, 180], total neurons: 1116\n",
            "After pruning:\n",
            "loss: 1.1922601461410522 - accuracy: 0.5782399773597717 - val_loss: 1.3888030052185059 - val_accuracy: 0.510699987411499 - penalty: 1.0000000000000002e-07\n",
            "hidden layer sizes: [379, 370, 187, 174], total neurons: 1110\n",
            "##########################################################\n",
            "Epoch 34/50\n",
            "Before growing:\n",
            "loss: 1.1922601461410522 - accuracy: 0.5782399773597717 - val_loss: 1.3888030052185059 - val_accuracy: 0.510699987411499 - penalty: 1.0000000000000002e-07\n",
            "hidden layer sizes: [379, 370, 187, 174], total neurons: 1110\n",
            "After growing:\n",
            "loss: 1.1922601461410522 - accuracy: 0.5782399773597717 - val_loss: 1.3888030052185059 - val_accuracy: 0.510699987411499 - penalty: 1.0000000000000004e-08\n",
            "hidden layer sizes: [454, 444, 237, 224], total neurons: 1359\n",
            "Before pruning:\n",
            "loss: 1.1771529912948608 - accuracy: 0.5846999883651733 - val_loss: 1.3872171640396118 - val_accuracy: 0.5123000144958496 - penalty: 1.0000000000000004e-08\n",
            "hidden layer sizes: [454, 444, 237, 224], total neurons: 1359\n",
            "After pruning:\n",
            "loss: 1.1771529912948608 - accuracy: 0.5846999883651733 - val_loss: 1.3872171640396118 - val_accuracy: 0.5123000144958496 - penalty: 1.0000000000000004e-08\n",
            "hidden layer sizes: [454, 444, 237, 224], total neurons: 1359\n",
            "##########################################################\n",
            "Epoch 35/50\n",
            "Before growing:\n",
            "loss: 1.1771529912948608 - accuracy: 0.5846999883651733 - val_loss: 1.3872171640396118 - val_accuracy: 0.5123000144958496 - penalty: 1.0000000000000004e-08\n",
            "hidden layer sizes: [454, 444, 237, 224], total neurons: 1359\n",
            "After growing:\n",
            "loss: 1.1771529912948608 - accuracy: 0.5846999883651733 - val_loss: 1.3872171640396118 - val_accuracy: 0.5123000144958496 - penalty: 1.0000000000000004e-08\n",
            "hidden layer sizes: [544, 532, 287, 274], total neurons: 1637\n",
            "Before pruning:\n",
            "loss: 1.1670924425125122 - accuracy: 0.5885400176048279 - val_loss: 1.3839257955551147 - val_accuracy: 0.5163999795913696 - penalty: 1.0000000000000004e-08\n",
            "hidden layer sizes: [544, 532, 287, 274], total neurons: 1637\n",
            "After pruning:\n",
            "loss: 1.1670924425125122 - accuracy: 0.5885400176048279 - val_loss: 1.3839257955551147 - val_accuracy: 0.5163999795913696 - penalty: 1.0000000000000004e-08\n",
            "hidden layer sizes: [544, 532, 287, 274], total neurons: 1637\n",
            "##########################################################\n",
            "Epoch 36/50\n",
            "Before growing:\n",
            "loss: 1.1670924425125122 - accuracy: 0.5885400176048279 - val_loss: 1.3839257955551147 - val_accuracy: 0.5163999795913696 - penalty: 1.0000000000000004e-08\n",
            "hidden layer sizes: [544, 532, 287, 274], total neurons: 1637\n",
            "After growing:\n",
            "loss: 1.1670923233032227 - accuracy: 0.5885400176048279 - val_loss: 1.3839256763458252 - val_accuracy: 0.5163999795913696 - penalty: 1.0000000000000004e-08\n",
            "hidden layer sizes: [652, 638, 344, 328], total neurons: 1962\n",
            "Before pruning:\n",
            "loss: 1.1525970697402954 - accuracy: 0.5929399728775024 - val_loss: 1.3856379985809326 - val_accuracy: 0.5123000144958496 - penalty: 1.0000000000000004e-08\n",
            "hidden layer sizes: [652, 638, 344, 328], total neurons: 1962\n",
            "After pruning:\n",
            "loss: 1.1525970697402954 - accuracy: 0.5929399728775024 - val_loss: 1.3856379985809326 - val_accuracy: 0.5123000144958496 - penalty: 1.0000000000000004e-08\n",
            "hidden layer sizes: [652, 638, 344, 328], total neurons: 1962\n",
            "##########################################################\n",
            "Epoch 37/50\n",
            "Before growing:\n",
            "loss: 1.1525970697402954 - accuracy: 0.5929399728775024 - val_loss: 1.3856379985809326 - val_accuracy: 0.5123000144958496 - penalty: 1.0000000000000004e-08\n",
            "hidden layer sizes: [652, 638, 344, 328], total neurons: 1962\n",
            "After growing:\n",
            "loss: 1.1525970697402954 - accuracy: 0.5929399728775024 - val_loss: 1.3856379985809326 - val_accuracy: 0.5123000144958496 - penalty: 1.0000000000000005e-09\n",
            "hidden layer sizes: [782, 765, 412, 393], total neurons: 2352\n",
            "Before pruning:\n",
            "loss: 1.1226001977920532 - accuracy: 0.6033200025558472 - val_loss: 1.3707422018051147 - val_accuracy: 0.5170000195503235 - penalty: 1.0000000000000005e-09\n",
            "hidden layer sizes: [782, 765, 412, 393], total neurons: 2352\n",
            "After pruning:\n",
            "loss: 1.1226001977920532 - accuracy: 0.6033200025558472 - val_loss: 1.3707422018051147 - val_accuracy: 0.5170000195503235 - penalty: 1.0000000000000005e-09\n",
            "hidden layer sizes: [782, 765, 412, 393], total neurons: 2352\n",
            "##########################################################\n",
            "Epoch 38/50\n",
            "Before growing:\n",
            "loss: 1.1226001977920532 - accuracy: 0.6033200025558472 - val_loss: 1.3707422018051147 - val_accuracy: 0.5170000195503235 - penalty: 1.0000000000000005e-09\n",
            "hidden layer sizes: [782, 765, 412, 393], total neurons: 2352\n",
            "After growing:\n",
            "loss: 1.1226001977920532 - accuracy: 0.6033200025558472 - val_loss: 1.3707422018051147 - val_accuracy: 0.5170000195503235 - penalty: 1.0000000000000005e-09\n",
            "hidden layer sizes: [938, 918, 494, 471], total neurons: 2821\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-4429fd157a3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\nmodel = SSModel(layer_sizes=[3072, 300, 300, 300, 300, 10], activation='selu', regularization_penalty=0.0001, regularization_method='weighted_l1', kernel_initializer='lecun_normal')\\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.0002)\\n\\ntrain_model(model, X_train_norm, y_train, optimizer, epochs, self_scaling_epochs, batch_size, min_new_neurons, validation_data=(X_test_norm, y_test), regularization_penalty_multiplier=0.1)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-53>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-da6d44c360d4>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, x, y, optimizer, epochs, self_scaling_epochs, batch_size, min_new_neurons, validation_data, pruning_threshold, regularization_penalty_multiplier, print_neurons)\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_categorical_crossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0mloss_value\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mlosses\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1408\u001b[0m         \u001b[0mcollected_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mregularizer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_losses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1410\u001b[0;31m         \u001b[0mloss_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1411\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mloss_tensor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1412\u001b[0m           \u001b[0mcollected_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_tag_callable\u001b[0;34m(loss)\u001b[0m\n\u001b[1;32m   1484\u001b[0m         \u001b[0;31m# numerically unstable in float16.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_variable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m           \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1487\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# Will be filtered out when computing the .losses property\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-c98db545d31e>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregularization_method\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregularizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-c98db545d31e>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregularization_method\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'weighted_l1'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweighted_l1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregularization_method\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'group_sparsity'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup_sparsity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-c98db545d31e>\u001b[0m in \u001b[0;36mweighted_l1\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mscaling_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregularization_penalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mweighted_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaling_vector\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweighted_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mabs\u001b[0;34m(x, name)\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomplex_abs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreal_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_abs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7EQtWo_G8t2",
        "outputId": "1cca9586-6add-488e-d982-3e57ed0e1e38"
      },
      "source": [
        "%%time\n",
        "\n",
        "model = SSModel(layer_sizes=[3072, 300, 300, 300, 300, 10], activation='selu', regularization_penalty=0.0001, regularization_method='weighted_l1', kernel_initializer='lecun_normal')\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002)\n",
        "\n",
        "train_model(model, X_train_norm, y_train, optimizer, epochs, self_scaling_epochs, batch_size, min_new_neurons, validation_data=(X_test_norm, y_test), regularization_penalty_multiplier=0.1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "##########################################################\n",
            "Epoch 1/50\n",
            "Before growing:\n",
            "loss: 2.9734561443328857 - accuracy: 0.0878399983048439 - val_loss: 2.957174301147461 - val_accuracy: 0.0892999991774559 - penalty: 0.0001\n",
            "layer sizes: [3072, 300, 300, 300, 300, 10]\n",
            "After growing:\n",
            "loss: 2.973456621170044 - accuracy: 0.0878399983048439 - val_loss: 2.957174301147461 - val_accuracy: 0.0892999991774559 - penalty: 0.0001\n",
            "layer sizes: [3072, 360, 360, 360, 360, 10]\n",
            "Before pruning:\n",
            "loss: 1.8268446922302246 - accuracy: 0.3210600018501282 - val_loss: 1.8327789306640625 - val_accuracy: 0.3165000081062317 - penalty: 0.0001\n",
            "layer sizes: [3072, 360, 360, 360, 360, 10]\n",
            "After pruning:\n",
            "loss: 1.8263518810272217 - accuracy: 0.32137998938560486 - val_loss: 1.8324464559555054 - val_accuracy: 0.31679999828338623 - penalty: 0.0001\n",
            "layer sizes: [3072, 15, 58, 34, 70, 10]\n",
            "##########################################################\n",
            "Epoch 2/50\n",
            "Before growing:\n",
            "loss: 1.8263518810272217 - accuracy: 0.32137998938560486 - val_loss: 1.8324464559555054 - val_accuracy: 0.31679999828338623 - penalty: 0.0001\n",
            "layer sizes: [3072, 15, 58, 34, 70, 10]\n",
            "After growing:\n",
            "loss: 1.8263518810272217 - accuracy: 0.32137998938560486 - val_loss: 1.8324464559555054 - val_accuracy: 0.31679999828338623 - penalty: 0.0001\n",
            "layer sizes: [3072, 65, 108, 84, 120, 10]\n",
            "Before pruning:\n",
            "loss: 1.7552181482315063 - accuracy: 0.35148000717163086 - val_loss: 1.7627851963043213 - val_accuracy: 0.35010001063346863 - penalty: 0.0001\n",
            "layer sizes: [3072, 65, 108, 84, 120, 10]\n",
            "After pruning:\n",
            "loss: 1.755191683769226 - accuracy: 0.35172000527381897 - val_loss: 1.7627809047698975 - val_accuracy: 0.35019999742507935 - penalty: 0.0001\n",
            "layer sizes: [3072, 13, 41, 21, 55, 10]\n",
            "##########################################################\n",
            "Epoch 3/50\n",
            "Before growing:\n",
            "loss: 1.755191683769226 - accuracy: 0.35172000527381897 - val_loss: 1.7627809047698975 - val_accuracy: 0.35019999742507935 - penalty: 0.0001\n",
            "layer sizes: [3072, 13, 41, 21, 55, 10]\n",
            "After growing:\n",
            "loss: 1.7551919221878052 - accuracy: 0.35172000527381897 - val_loss: 1.7627809047698975 - val_accuracy: 0.35019999742507935 - penalty: 0.0001\n",
            "layer sizes: [3072, 63, 91, 71, 105, 10]\n",
            "Before pruning:\n",
            "loss: 1.7386107444763184 - accuracy: 0.3573000133037567 - val_loss: 1.7480753660202026 - val_accuracy: 0.3508000075817108 - penalty: 0.0001\n",
            "layer sizes: [3072, 63, 91, 71, 105, 10]\n",
            "After pruning:\n",
            "loss: 1.7385753393173218 - accuracy: 0.3576599955558777 - val_loss: 1.748012900352478 - val_accuracy: 0.35040000081062317 - penalty: 0.0001\n",
            "layer sizes: [3072, 8, 30, 17, 50, 10]\n",
            "##########################################################\n",
            "Epoch 4/50\n",
            "Before growing:\n",
            "loss: 1.7385753393173218 - accuracy: 0.3576599955558777 - val_loss: 1.748012900352478 - val_accuracy: 0.35040000081062317 - penalty: 0.0001\n",
            "layer sizes: [3072, 8, 30, 17, 50, 10]\n",
            "After growing:\n",
            "loss: 1.7385753393173218 - accuracy: 0.3576599955558777 - val_loss: 1.748012900352478 - val_accuracy: 0.35040000081062317 - penalty: 0.0001\n",
            "layer sizes: [3072, 58, 80, 67, 100, 10]\n",
            "Before pruning:\n",
            "loss: 1.7160193920135498 - accuracy: 0.3632200062274933 - val_loss: 1.7290109395980835 - val_accuracy: 0.35740000009536743 - penalty: 0.0001\n",
            "layer sizes: [3072, 58, 80, 67, 100, 10]\n",
            "After pruning:\n",
            "loss: 1.7160184383392334 - accuracy: 0.363180011510849 - val_loss: 1.729008436203003 - val_accuracy: 0.3573000133037567 - penalty: 0.0001\n",
            "layer sizes: [3072, 8, 24, 15, 48, 10]\n",
            "##########################################################\n",
            "Epoch 5/50\n",
            "Before growing:\n",
            "loss: 1.7160184383392334 - accuracy: 0.363180011510849 - val_loss: 1.729008436203003 - val_accuracy: 0.3573000133037567 - penalty: 0.0001\n",
            "layer sizes: [3072, 8, 24, 15, 48, 10]\n",
            "After growing:\n",
            "loss: 1.7160184383392334 - accuracy: 0.363180011510849 - val_loss: 1.7290081977844238 - val_accuracy: 0.3573000133037567 - penalty: 0.0001\n",
            "layer sizes: [3072, 58, 74, 65, 98, 10]\n",
            "Before pruning:\n",
            "loss: 1.7015681266784668 - accuracy: 0.37516000866889954 - val_loss: 1.7137393951416016 - val_accuracy: 0.3700999915599823 - penalty: 0.0001\n",
            "layer sizes: [3072, 58, 74, 65, 98, 10]\n",
            "After pruning:\n",
            "loss: 1.7015726566314697 - accuracy: 0.37512001395225525 - val_loss: 1.713741660118103 - val_accuracy: 0.3702999949455261 - penalty: 0.0001\n",
            "layer sizes: [3072, 7, 23, 13, 46, 10]\n",
            "##########################################################\n",
            "Epoch 6/50\n",
            "Before growing:\n",
            "loss: 1.7015726566314697 - accuracy: 0.37512001395225525 - val_loss: 1.713741660118103 - val_accuracy: 0.3702999949455261 - penalty: 0.0001\n",
            "layer sizes: [3072, 7, 23, 13, 46, 10]\n",
            "After growing:\n",
            "loss: 1.7015725374221802 - accuracy: 0.37512001395225525 - val_loss: 1.713741660118103 - val_accuracy: 0.3702999949455261 - penalty: 0.0001\n",
            "layer sizes: [3072, 57, 73, 63, 96, 10]\n",
            "Before pruning:\n",
            "loss: 1.6802533864974976 - accuracy: 0.38266000151634216 - val_loss: 1.6963818073272705 - val_accuracy: 0.37070000171661377 - penalty: 0.0001\n",
            "layer sizes: [3072, 57, 73, 63, 96, 10]\n",
            "After pruning:\n",
            "loss: 1.6802743673324585 - accuracy: 0.38273999094963074 - val_loss: 1.696408987045288 - val_accuracy: 0.37059998512268066 - penalty: 0.0001\n",
            "layer sizes: [3072, 7, 22, 13, 42, 10]\n",
            "##########################################################\n",
            "Epoch 7/50\n",
            "Before growing:\n",
            "loss: 1.6802743673324585 - accuracy: 0.38273999094963074 - val_loss: 1.696408987045288 - val_accuracy: 0.37059998512268066 - penalty: 0.0001\n",
            "layer sizes: [3072, 7, 22, 13, 42, 10]\n",
            "After growing:\n",
            "loss: 1.6802743673324585 - accuracy: 0.38273999094963074 - val_loss: 1.696408987045288 - val_accuracy: 0.37059998512268066 - penalty: 0.0001\n",
            "layer sizes: [3072, 57, 72, 63, 92, 10]\n",
            "Before pruning:\n",
            "loss: 1.6721540689468384 - accuracy: 0.38464000821113586 - val_loss: 1.6891363859176636 - val_accuracy: 0.37450000643730164 - penalty: 0.0001\n",
            "layer sizes: [3072, 57, 72, 63, 92, 10]\n",
            "After pruning:\n",
            "loss: 1.672197937965393 - accuracy: 0.3847399950027466 - val_loss: 1.68916916847229 - val_accuracy: 0.3749000132083893 - penalty: 0.0001\n",
            "layer sizes: [3072, 7, 20, 13, 41, 10]\n",
            "##########################################################\n",
            "Epoch 8/50\n",
            "Before growing:\n",
            "loss: 1.672197937965393 - accuracy: 0.3847399950027466 - val_loss: 1.68916916847229 - val_accuracy: 0.3749000132083893 - penalty: 0.0001\n",
            "layer sizes: [3072, 7, 20, 13, 41, 10]\n",
            "After growing:\n",
            "loss: 1.672197937965393 - accuracy: 0.3847399950027466 - val_loss: 1.689168930053711 - val_accuracy: 0.3749000132083893 - penalty: 0.0001\n",
            "layer sizes: [3072, 57, 70, 63, 91, 10]\n",
            "Before pruning:\n",
            "loss: 1.6482595205307007 - accuracy: 0.39664000272750854 - val_loss: 1.6642394065856934 - val_accuracy: 0.38199999928474426 - penalty: 0.0001\n",
            "layer sizes: [3072, 57, 70, 63, 91, 10]\n",
            "After pruning:\n",
            "loss: 1.6482621431350708 - accuracy: 0.39656001329421997 - val_loss: 1.664260983467102 - val_accuracy: 0.3822000026702881 - penalty: 0.0001\n",
            "layer sizes: [3072, 7, 20, 13, 38, 10]\n",
            "##########################################################\n",
            "Epoch 9/50\n",
            "Before growing:\n",
            "loss: 1.6482621431350708 - accuracy: 0.39656001329421997 - val_loss: 1.664260983467102 - val_accuracy: 0.3822000026702881 - penalty: 0.0001\n",
            "layer sizes: [3072, 7, 20, 13, 38, 10]\n",
            "After growing:\n",
            "loss: 1.6482621431350708 - accuracy: 0.39656001329421997 - val_loss: 1.664260983467102 - val_accuracy: 0.3822000026702881 - penalty: 0.0001\n",
            "layer sizes: [3072, 57, 70, 63, 88, 10]\n",
            "Before pruning:\n",
            "loss: 1.639958381652832 - accuracy: 0.39858001470565796 - val_loss: 1.6595935821533203 - val_accuracy: 0.3889000117778778 - penalty: 0.0001\n",
            "layer sizes: [3072, 57, 70, 63, 88, 10]\n",
            "After pruning:\n",
            "loss: 1.6399693489074707 - accuracy: 0.3985599875450134 - val_loss: 1.6596380472183228 - val_accuracy: 0.38920000195503235 - penalty: 0.0001\n",
            "layer sizes: [3072, 8, 20, 13, 39, 10]\n",
            "##########################################################\n",
            "Epoch 10/50\n",
            "Before growing:\n",
            "loss: 1.6399693489074707 - accuracy: 0.3985599875450134 - val_loss: 1.6596380472183228 - val_accuracy: 0.38920000195503235 - penalty: 0.0001\n",
            "layer sizes: [3072, 8, 20, 13, 39, 10]\n",
            "After growing:\n",
            "loss: 1.6399693489074707 - accuracy: 0.3985599875450134 - val_loss: 1.6596380472183228 - val_accuracy: 0.38920000195503235 - penalty: 0.0001\n",
            "layer sizes: [3072, 58, 70, 63, 89, 10]\n",
            "Before pruning:\n",
            "loss: 1.6317631006240845 - accuracy: 0.401419997215271 - val_loss: 1.6544878482818604 - val_accuracy: 0.390500009059906 - penalty: 0.0001\n",
            "layer sizes: [3072, 58, 70, 63, 89, 10]\n",
            "After pruning:\n",
            "loss: 1.6317174434661865 - accuracy: 0.401419997215271 - val_loss: 1.6544421911239624 - val_accuracy: 0.3905999958515167 - penalty: 0.0001\n",
            "layer sizes: [3072, 11, 20, 13, 36, 10]\n",
            "##########################################################\n",
            "Epoch 11/50\n",
            "Before growing:\n",
            "loss: 1.6317174434661865 - accuracy: 0.401419997215271 - val_loss: 1.6544421911239624 - val_accuracy: 0.3905999958515167 - penalty: 0.0001\n",
            "layer sizes: [3072, 11, 20, 13, 36, 10]\n",
            "After growing:\n",
            "loss: 1.6317174434661865 - accuracy: 0.401419997215271 - val_loss: 1.6544419527053833 - val_accuracy: 0.3905999958515167 - penalty: 0.0001\n",
            "layer sizes: [3072, 61, 70, 63, 86, 10]\n",
            "Before pruning:\n",
            "loss: 1.6235706806182861 - accuracy: 0.4086199998855591 - val_loss: 1.6457568407058716 - val_accuracy: 0.3978999853134155 - penalty: 0.0001\n",
            "layer sizes: [3072, 61, 70, 63, 86, 10]\n",
            "After pruning:\n",
            "loss: 1.6235668659210205 - accuracy: 0.4085800051689148 - val_loss: 1.6457544565200806 - val_accuracy: 0.3977999985218048 - penalty: 0.0001\n",
            "layer sizes: [3072, 7, 20, 13, 35, 10]\n",
            "##########################################################\n",
            "Epoch 12/50\n",
            "Before growing:\n",
            "loss: 1.6235668659210205 - accuracy: 0.4085800051689148 - val_loss: 1.6457544565200806 - val_accuracy: 0.3977999985218048 - penalty: 0.0001\n",
            "layer sizes: [3072, 7, 20, 13, 35, 10]\n",
            "After growing:\n",
            "loss: 1.6235668659210205 - accuracy: 0.4085800051689148 - val_loss: 1.6457544565200806 - val_accuracy: 0.3977999985218048 - penalty: 0.0001\n",
            "layer sizes: [3072, 57, 70, 63, 85, 10]\n",
            "Before pruning:\n",
            "loss: 1.617138385772705 - accuracy: 0.4111599922180176 - val_loss: 1.6445504426956177 - val_accuracy: 0.3953000009059906 - penalty: 0.0001\n",
            "layer sizes: [3072, 57, 70, 63, 85, 10]\n",
            "After pruning:\n",
            "loss: 1.6172422170639038 - accuracy: 0.4110400080680847 - val_loss: 1.6446533203125 - val_accuracy: 0.3952000141143799 - penalty: 0.0001\n",
            "layer sizes: [3072, 8, 19, 13, 35, 10]\n",
            "##########################################################\n",
            "Epoch 13/50\n",
            "Before growing:\n",
            "loss: 1.6172422170639038 - accuracy: 0.4110400080680847 - val_loss: 1.6446533203125 - val_accuracy: 0.3952000141143799 - penalty: 0.0001\n",
            "layer sizes: [3072, 8, 19, 13, 35, 10]\n",
            "After growing:\n",
            "loss: 1.6172422170639038 - accuracy: 0.4110400080680847 - val_loss: 1.6446533203125 - val_accuracy: 0.3952000141143799 - penalty: 0.0001\n",
            "layer sizes: [3072, 58, 69, 63, 85, 10]\n",
            "Before pruning:\n",
            "loss: 1.6178843975067139 - accuracy: 0.4103600084781647 - val_loss: 1.6414804458618164 - val_accuracy: 0.3993000090122223 - penalty: 0.0001\n",
            "layer sizes: [3072, 58, 69, 63, 85, 10]\n",
            "After pruning:\n",
            "loss: 1.61781644821167 - accuracy: 0.41054001450538635 - val_loss: 1.6413837671279907 - val_accuracy: 0.399399995803833 - penalty: 0.0001\n",
            "layer sizes: [3072, 7, 19, 13, 34, 10]\n",
            "##########################################################\n",
            "Epoch 14/50\n",
            "Before growing:\n",
            "loss: 1.61781644821167 - accuracy: 0.41054001450538635 - val_loss: 1.6413837671279907 - val_accuracy: 0.399399995803833 - penalty: 0.0001\n",
            "layer sizes: [3072, 7, 19, 13, 34, 10]\n",
            "After growing:\n",
            "loss: 1.6178165674209595 - accuracy: 0.41054001450538635 - val_loss: 1.6413837671279907 - val_accuracy: 0.399399995803833 - penalty: 0.0001\n",
            "layer sizes: [3072, 57, 69, 63, 84, 10]\n",
            "Before pruning:\n",
            "loss: 1.6128950119018555 - accuracy: 0.41207998991012573 - val_loss: 1.6364887952804565 - val_accuracy: 0.4016000032424927 - penalty: 0.0001\n",
            "layer sizes: [3072, 57, 69, 63, 84, 10]\n",
            "After pruning:\n",
            "loss: 1.6130259037017822 - accuracy: 0.4120599925518036 - val_loss: 1.6366331577301025 - val_accuracy: 0.4018999934196472 - penalty: 0.0001\n",
            "layer sizes: [3072, 7, 19, 13, 34, 10]\n",
            "##########################################################\n",
            "Epoch 15/50\n",
            "Before growing:\n",
            "loss: 1.6130259037017822 - accuracy: 0.4120599925518036 - val_loss: 1.6366331577301025 - val_accuracy: 0.4018999934196472 - penalty: 0.0001\n",
            "layer sizes: [3072, 7, 19, 13, 34, 10]\n",
            "After growing:\n",
            "loss: 1.6130259037017822 - accuracy: 0.4120599925518036 - val_loss: 1.6366331577301025 - val_accuracy: 0.4018999934196472 - penalty: 0.0001\n",
            "layer sizes: [3072, 57, 69, 63, 84, 10]\n",
            "Before pruning:\n",
            "loss: 1.606682300567627 - accuracy: 0.4142799973487854 - val_loss: 1.6321817636489868 - val_accuracy: 0.40380001068115234 - penalty: 0.0001\n",
            "layer sizes: [3072, 57, 69, 63, 84, 10]\n",
            "After pruning:\n",
            "loss: 1.6066315174102783 - accuracy: 0.414139986038208 - val_loss: 1.6321121454238892 - val_accuracy: 0.40400001406669617 - penalty: 0.0001\n",
            "layer sizes: [3072, 7, 18, 13, 33, 10]\n",
            "##########################################################\n",
            "Epoch 16/50\n",
            "Before growing:\n",
            "loss: 1.6066315174102783 - accuracy: 0.414139986038208 - val_loss: 1.6321121454238892 - val_accuracy: 0.40400001406669617 - penalty: 0.0001\n",
            "layer sizes: [3072, 7, 18, 13, 33, 10]\n",
            "After growing:\n",
            "loss: 1.6066315174102783 - accuracy: 0.414139986038208 - val_loss: 1.6321120262145996 - val_accuracy: 0.40400001406669617 - penalty: 0.0001\n",
            "layer sizes: [3072, 57, 68, 63, 83, 10]\n",
            "Before pruning:\n",
            "loss: 1.6030917167663574 - accuracy: 0.4169999957084656 - val_loss: 1.6305187940597534 - val_accuracy: 0.4050999879837036 - penalty: 0.0001\n",
            "layer sizes: [3072, 57, 68, 63, 83, 10]\n",
            "After pruning:\n",
            "loss: 1.6032369136810303 - accuracy: 0.41686001420021057 - val_loss: 1.6306980848312378 - val_accuracy: 0.4052000045776367 - penalty: 0.0001\n",
            "layer sizes: [3072, 7, 18, 13, 32, 10]\n",
            "##########################################################\n",
            "Epoch 17/50\n",
            "Before growing:\n",
            "loss: 1.6032369136810303 - accuracy: 0.41686001420021057 - val_loss: 1.6306980848312378 - val_accuracy: 0.4052000045776367 - penalty: 0.0001\n",
            "layer sizes: [3072, 7, 18, 13, 32, 10]\n",
            "After growing:\n",
            "loss: 1.6032369136810303 - accuracy: 0.41686001420021057 - val_loss: 1.6306980848312378 - val_accuracy: 0.4052000045776367 - penalty: 0.0001\n",
            "layer sizes: [3072, 57, 68, 63, 82, 10]\n",
            "Before pruning:\n",
            "loss: 1.6198312044143677 - accuracy: 0.4130600094795227 - val_loss: 1.6449429988861084 - val_accuracy: 0.40220001339912415 - penalty: 0.0001\n",
            "layer sizes: [3072, 57, 68, 63, 82, 10]\n",
            "After pruning:\n",
            "loss: 1.6204453706741333 - accuracy: 0.4130600094795227 - val_loss: 1.6455540657043457 - val_accuracy: 0.4016999900341034 - penalty: 0.0001\n",
            "layer sizes: [3072, 7, 18, 13, 32, 10]\n",
            "##########################################################\n",
            "Epoch 18/50\n",
            "Before growing:\n",
            "loss: 1.6204453706741333 - accuracy: 0.4130600094795227 - val_loss: 1.6455540657043457 - val_accuracy: 0.4016999900341034 - penalty: 0.0001\n",
            "layer sizes: [3072, 7, 18, 13, 32, 10]\n",
            "After growing:\n",
            "loss: 1.6204453706741333 - accuracy: 0.4130600094795227 - val_loss: 1.6455539464950562 - val_accuracy: 0.4016999900341034 - penalty: 1e-05\n",
            "layer sizes: [3072, 57, 68, 63, 82, 10]\n",
            "Before pruning:\n",
            "loss: 1.5841829776763916 - accuracy: 0.42107999324798584 - val_loss: 1.617518424987793 - val_accuracy: 0.412200003862381 - penalty: 1e-05\n",
            "layer sizes: [3072, 57, 68, 63, 82, 10]\n",
            "After pruning:\n",
            "loss: 1.5840712785720825 - accuracy: 0.4213399887084961 - val_loss: 1.6174110174179077 - val_accuracy: 0.41269999742507935 - penalty: 1e-05\n",
            "layer sizes: [3072, 39, 24, 13, 57, 10]\n",
            "##########################################################\n",
            "Epoch 19/50\n",
            "Before growing:\n",
            "loss: 1.5840712785720825 - accuracy: 0.4213399887084961 - val_loss: 1.6174110174179077 - val_accuracy: 0.41269999742507935 - penalty: 1e-05\n",
            "layer sizes: [3072, 39, 24, 13, 57, 10]\n",
            "After growing:\n",
            "loss: 1.5840712785720825 - accuracy: 0.4213399887084961 - val_loss: 1.6174110174179077 - val_accuracy: 0.41269999742507935 - penalty: 1e-05\n",
            "layer sizes: [3072, 89, 74, 63, 107, 10]\n",
            "Before pruning:\n",
            "loss: 1.5526175498962402 - accuracy: 0.4342400133609772 - val_loss: 1.5910006761550903 - val_accuracy: 0.4196000099182129 - penalty: 1e-05\n",
            "layer sizes: [3072, 89, 74, 63, 107, 10]\n",
            "After pruning:\n",
            "loss: 1.5528299808502197 - accuracy: 0.43397998809814453 - val_loss: 1.5910532474517822 - val_accuracy: 0.4198000133037567 - penalty: 1e-05\n",
            "layer sizes: [3072, 37, 26, 13, 43, 10]\n",
            "##########################################################\n",
            "Epoch 20/50\n",
            "Before growing:\n",
            "loss: 1.5528299808502197 - accuracy: 0.43397998809814453 - val_loss: 1.5910532474517822 - val_accuracy: 0.4198000133037567 - penalty: 1e-05\n",
            "layer sizes: [3072, 37, 26, 13, 43, 10]\n",
            "After growing:\n",
            "loss: 1.5528301000595093 - accuracy: 0.43397998809814453 - val_loss: 1.5910532474517822 - val_accuracy: 0.4198000133037567 - penalty: 1e-05\n",
            "layer sizes: [3072, 87, 76, 63, 93, 10]\n",
            "Before pruning:\n",
            "loss: 1.5383315086364746 - accuracy: 0.44071999192237854 - val_loss: 1.5824599266052246 - val_accuracy: 0.4275999963283539 - penalty: 1e-05\n",
            "layer sizes: [3072, 87, 76, 63, 93, 10]\n",
            "After pruning:\n",
            "loss: 1.538189172744751 - accuracy: 0.4408800005912781 - val_loss: 1.5822627544403076 - val_accuracy: 0.42800000309944153 - penalty: 1e-05\n",
            "layer sizes: [3072, 39, 28, 13, 65, 10]\n",
            "##########################################################\n",
            "Epoch 21/50\n",
            "Before growing:\n",
            "loss: 1.538189172744751 - accuracy: 0.4408800005912781 - val_loss: 1.5822627544403076 - val_accuracy: 0.42800000309944153 - penalty: 1e-05\n",
            "layer sizes: [3072, 39, 28, 13, 65, 10]\n",
            "After growing:\n",
            "loss: 1.5381890535354614 - accuracy: 0.4408800005912781 - val_loss: 1.5822627544403076 - val_accuracy: 0.42800000309944153 - penalty: 1e-05\n",
            "layer sizes: [3072, 89, 78, 63, 115, 10]\n",
            "Before pruning:\n",
            "loss: 1.5276154279708862 - accuracy: 0.44633999466896057 - val_loss: 1.5780283212661743 - val_accuracy: 0.42750000953674316 - penalty: 1e-05\n",
            "layer sizes: [3072, 89, 78, 63, 115, 10]\n",
            "After pruning:\n",
            "loss: 1.527547836303711 - accuracy: 0.44648000597953796 - val_loss: 1.5780357122421265 - val_accuracy: 0.42750000953674316 - penalty: 1e-05\n",
            "layer sizes: [3072, 43, 28, 14, 61, 10]\n",
            "##########################################################\n",
            "Epoch 22/50\n",
            "Before growing:\n",
            "loss: 1.527547836303711 - accuracy: 0.44648000597953796 - val_loss: 1.5780357122421265 - val_accuracy: 0.42750000953674316 - penalty: 1e-05\n",
            "layer sizes: [3072, 43, 28, 14, 61, 10]\n",
            "After growing:\n",
            "loss: 1.527547836303711 - accuracy: 0.44648000597953796 - val_loss: 1.5780357122421265 - val_accuracy: 0.42750000953674316 - penalty: 1e-05\n",
            "layer sizes: [3072, 93, 78, 64, 111, 10]\n",
            "Before pruning:\n",
            "loss: 1.509802222251892 - accuracy: 0.4552600085735321 - val_loss: 1.563576340675354 - val_accuracy: 0.4359999895095825 - penalty: 1e-05\n",
            "layer sizes: [3072, 93, 78, 64, 111, 10]\n",
            "After pruning:\n",
            "loss: 1.5095012187957764 - accuracy: 0.4553399980068207 - val_loss: 1.5633363723754883 - val_accuracy: 0.4359000027179718 - penalty: 1e-05\n",
            "layer sizes: [3072, 54, 28, 14, 68, 10]\n",
            "##########################################################\n",
            "Epoch 23/50\n",
            "Before growing:\n",
            "loss: 1.5095012187957764 - accuracy: 0.4553399980068207 - val_loss: 1.5633363723754883 - val_accuracy: 0.4359000027179718 - penalty: 1e-05\n",
            "layer sizes: [3072, 54, 28, 14, 68, 10]\n",
            "After growing:\n",
            "loss: 1.5095012187957764 - accuracy: 0.4553399980068207 - val_loss: 1.5633363723754883 - val_accuracy: 0.4359000027179718 - penalty: 1e-05\n",
            "layer sizes: [3072, 104, 78, 64, 118, 10]\n",
            "Before pruning:\n",
            "loss: 1.4971712827682495 - accuracy: 0.4598599970340729 - val_loss: 1.556594729423523 - val_accuracy: 0.43970000743865967 - penalty: 1e-05\n",
            "layer sizes: [3072, 104, 78, 64, 118, 10]\n",
            "After pruning:\n",
            "loss: 1.4973499774932861 - accuracy: 0.45989999175071716 - val_loss: 1.5568666458129883 - val_accuracy: 0.43939998745918274 - penalty: 1e-05\n",
            "layer sizes: [3072, 45, 28, 14, 56, 10]\n",
            "##########################################################\n",
            "Epoch 24/50\n",
            "Before growing:\n",
            "loss: 1.4973499774932861 - accuracy: 0.45989999175071716 - val_loss: 1.5568666458129883 - val_accuracy: 0.43939998745918274 - penalty: 1e-05\n",
            "layer sizes: [3072, 45, 28, 14, 56, 10]\n",
            "After growing:\n",
            "loss: 1.4973499774932861 - accuracy: 0.45989999175071716 - val_loss: 1.5568665266036987 - val_accuracy: 0.43939998745918274 - penalty: 1e-05\n",
            "layer sizes: [3072, 95, 78, 64, 106, 10]\n",
            "Before pruning:\n",
            "loss: 1.490395426750183 - accuracy: 0.4634400010108948 - val_loss: 1.5545532703399658 - val_accuracy: 0.43700000643730164 - penalty: 1e-05\n",
            "layer sizes: [3072, 95, 78, 64, 106, 10]\n",
            "After pruning:\n",
            "loss: 1.4906808137893677 - accuracy: 0.4629800021648407 - val_loss: 1.5547876358032227 - val_accuracy: 0.43720000982284546 - penalty: 1e-05\n",
            "layer sizes: [3072, 35, 28, 13, 49, 10]\n",
            "##########################################################\n",
            "Epoch 25/50\n",
            "Before growing:\n",
            "loss: 1.4906808137893677 - accuracy: 0.4629800021648407 - val_loss: 1.5547876358032227 - val_accuracy: 0.43720000982284546 - penalty: 1e-05\n",
            "layer sizes: [3072, 35, 28, 13, 49, 10]\n",
            "After growing:\n",
            "loss: 1.4906808137893677 - accuracy: 0.4629800021648407 - val_loss: 1.5547876358032227 - val_accuracy: 0.43720000982284546 - penalty: 1e-05\n",
            "layer sizes: [3072, 85, 78, 63, 99, 10]\n",
            "Before pruning:\n",
            "loss: 1.482893466949463 - accuracy: 0.465719997882843 - val_loss: 1.5524499416351318 - val_accuracy: 0.4413999915122986 - penalty: 1e-05\n",
            "layer sizes: [3072, 85, 78, 63, 99, 10]\n",
            "After pruning:\n",
            "loss: 1.4825448989868164 - accuracy: 0.46588000655174255 - val_loss: 1.5522544384002686 - val_accuracy: 0.4413999915122986 - penalty: 1e-05\n",
            "layer sizes: [3072, 33, 27, 13, 67, 10]\n",
            "##########################################################\n",
            "Epoch 26/50\n",
            "Before growing:\n",
            "loss: 1.4825448989868164 - accuracy: 0.46588000655174255 - val_loss: 1.5522544384002686 - val_accuracy: 0.4413999915122986 - penalty: 1e-05\n",
            "layer sizes: [3072, 33, 27, 13, 67, 10]\n",
            "After growing:\n",
            "loss: 1.4825448989868164 - accuracy: 0.46588000655174255 - val_loss: 1.5522544384002686 - val_accuracy: 0.4413999915122986 - penalty: 1e-05\n",
            "layer sizes: [3072, 83, 77, 63, 117, 10]\n",
            "Before pruning:\n",
            "loss: 1.480233073234558 - accuracy: 0.46696001291275024 - val_loss: 1.555190086364746 - val_accuracy: 0.4399000108242035 - penalty: 1e-05\n",
            "layer sizes: [3072, 83, 77, 63, 117, 10]\n",
            "After pruning:\n",
            "loss: 1.4800384044647217 - accuracy: 0.46682000160217285 - val_loss: 1.5550020933151245 - val_accuracy: 0.43959999084472656 - penalty: 1e-05\n",
            "layer sizes: [3072, 42, 27, 13, 77, 10]\n",
            "##########################################################\n",
            "Epoch 27/50\n",
            "Before growing:\n",
            "loss: 1.4800384044647217 - accuracy: 0.46682000160217285 - val_loss: 1.5550020933151245 - val_accuracy: 0.43959999084472656 - penalty: 1e-05\n",
            "layer sizes: [3072, 42, 27, 13, 77, 10]\n",
            "After growing:\n",
            "loss: 1.4800384044647217 - accuracy: 0.46682000160217285 - val_loss: 1.5550020933151245 - val_accuracy: 0.43959999084472656 - penalty: 1.0000000000000002e-06\n",
            "layer sizes: [3072, 92, 77, 63, 127, 10]\n",
            "Before pruning:\n",
            "loss: 1.4496673345565796 - accuracy: 0.47788000106811523 - val_loss: 1.5275428295135498 - val_accuracy: 0.45329999923706055 - penalty: 1.0000000000000002e-06\n",
            "layer sizes: [3072, 92, 77, 63, 127, 10]\n",
            "After pruning:\n",
            "loss: 1.4496662616729736 - accuracy: 0.4779199957847595 - val_loss: 1.5275418758392334 - val_accuracy: 0.45329999923706055 - penalty: 1.0000000000000002e-06\n",
            "layer sizes: [3072, 92, 77, 63, 77, 10]\n",
            "##########################################################\n",
            "Epoch 28/50\n",
            "Before growing:\n",
            "loss: 1.4496662616729736 - accuracy: 0.4779199957847595 - val_loss: 1.5275418758392334 - val_accuracy: 0.45329999923706055 - penalty: 1.0000000000000002e-06\n",
            "layer sizes: [3072, 92, 77, 63, 77, 10]\n",
            "After growing:\n",
            "loss: 1.4496662616729736 - accuracy: 0.4779199957847595 - val_loss: 1.5275417566299438 - val_accuracy: 0.45329999923706055 - penalty: 1.0000000000000002e-06\n",
            "layer sizes: [3072, 142, 127, 113, 127, 10]\n",
            "Before pruning:\n",
            "loss: 1.415971279144287 - accuracy: 0.4937399923801422 - val_loss: 1.5035312175750732 - val_accuracy: 0.4652000069618225 - penalty: 1.0000000000000002e-06\n",
            "layer sizes: [3072, 142, 127, 113, 127, 10]\n",
            "After pruning:\n",
            "loss: 1.4159611463546753 - accuracy: 0.4936800003051758 - val_loss: 1.5035231113433838 - val_accuracy: 0.4652000069618225 - penalty: 1.0000000000000002e-06\n",
            "layer sizes: [3072, 140, 126, 77, 104, 10]\n",
            "##########################################################\n",
            "Epoch 29/50\n",
            "Before growing:\n",
            "loss: 1.4159611463546753 - accuracy: 0.4936800003051758 - val_loss: 1.5035231113433838 - val_accuracy: 0.4652000069618225 - penalty: 1.0000000000000002e-06\n",
            "layer sizes: [3072, 140, 126, 77, 104, 10]\n",
            "After growing:\n",
            "loss: 1.4159612655639648 - accuracy: 0.4936800003051758 - val_loss: 1.5035229921340942 - val_accuracy: 0.4652000069618225 - penalty: 1.0000000000000002e-06\n",
            "layer sizes: [3072, 190, 176, 127, 154, 10]\n",
            "Before pruning:\n",
            "loss: 1.388051152229309 - accuracy: 0.5059000253677368 - val_loss: 1.4814006090164185 - val_accuracy: 0.47760000824928284 - penalty: 1.0000000000000002e-06\n",
            "layer sizes: [3072, 190, 176, 127, 154, 10]\n",
            "After pruning:\n",
            "loss: 1.3880300521850586 - accuracy: 0.505840003490448 - val_loss: 1.481391191482544 - val_accuracy: 0.47749999165534973 - penalty: 1.0000000000000002e-06\n",
            "layer sizes: [3072, 185, 155, 88, 93, 10]\n",
            "##########################################################\n",
            "Epoch 30/50\n",
            "Before growing:\n",
            "loss: 1.3880300521850586 - accuracy: 0.505840003490448 - val_loss: 1.481391191482544 - val_accuracy: 0.47749999165534973 - penalty: 1.0000000000000002e-06\n",
            "layer sizes: [3072, 185, 155, 88, 93, 10]\n",
            "After growing:\n",
            "loss: 1.3880300521850586 - accuracy: 0.505840003490448 - val_loss: 1.481391191482544 - val_accuracy: 0.47749999165534973 - penalty: 1.0000000000000002e-06\n",
            "layer sizes: [3072, 235, 205, 138, 143, 10]\n",
            "Before pruning:\n",
            "loss: 1.358595609664917 - accuracy: 0.5182600021362305 - val_loss: 1.459433674812317 - val_accuracy: 0.4853000044822693 - penalty: 1.0000000000000002e-06\n",
            "layer sizes: [3072, 235, 205, 138, 143, 10]\n",
            "After pruning:\n",
            "loss: 1.3586485385894775 - accuracy: 0.518060028553009 - val_loss: 1.4594615697860718 - val_accuracy: 0.48510000109672546 - penalty: 1.0000000000000002e-06\n",
            "layer sizes: [3072, 211, 176, 103, 93, 10]\n",
            "##########################################################\n",
            "Epoch 31/50\n",
            "Before growing:\n",
            "loss: 1.3586485385894775 - accuracy: 0.518060028553009 - val_loss: 1.4594615697860718 - val_accuracy: 0.48510000109672546 - penalty: 1.0000000000000002e-06\n",
            "layer sizes: [3072, 211, 176, 103, 93, 10]\n",
            "After growing:\n",
            "loss: 1.3586485385894775 - accuracy: 0.518060028553009 - val_loss: 1.4594616889953613 - val_accuracy: 0.48510000109672546 - penalty: 1.0000000000000002e-06\n",
            "layer sizes: [3072, 261, 226, 153, 143, 10]\n",
            "Before pruning:\n",
            "loss: 1.3334884643554688 - accuracy: 0.527180016040802 - val_loss: 1.4449195861816406 - val_accuracy: 0.4900999963283539 - penalty: 1.0000000000000002e-06\n",
            "layer sizes: [3072, 261, 226, 153, 143, 10]\n",
            "After pruning:\n",
            "loss: 1.3332840204238892 - accuracy: 0.527459979057312 - val_loss: 1.444771647453308 - val_accuracy: 0.4902999997138977 - penalty: 1.0000000000000002e-06\n",
            "layer sizes: [3072, 219, 201, 105, 48, 10]\n",
            "##########################################################\n",
            "Epoch 32/50\n",
            "Before growing:\n",
            "loss: 1.3332840204238892 - accuracy: 0.527459979057312 - val_loss: 1.444771647453308 - val_accuracy: 0.4902999997138977 - penalty: 1.0000000000000002e-06\n",
            "layer sizes: [3072, 219, 201, 105, 48, 10]\n",
            "After growing:\n",
            "loss: 1.3332840204238892 - accuracy: 0.527459979057312 - val_loss: 1.4447715282440186 - val_accuracy: 0.4902999997138977 - penalty: 1.0000000000000002e-06\n",
            "layer sizes: [3072, 269, 251, 155, 98, 10]\n",
            "Before pruning:\n",
            "loss: 1.330496907234192 - accuracy: 0.5290600061416626 - val_loss: 1.4535704851150513 - val_accuracy: 0.4862000048160553 - penalty: 1.0000000000000002e-06\n",
            "layer sizes: [3072, 269, 251, 155, 98, 10]\n",
            "After pruning:\n",
            "loss: 1.3302565813064575 - accuracy: 0.5290799736976624 - val_loss: 1.4533871412277222 - val_accuracy: 0.4862000048160553 - penalty: 1.0000000000000002e-06\n",
            "layer sizes: [3072, 227, 228, 95, 74, 10]\n",
            "##########################################################\n",
            "Epoch 33/50\n",
            "Before growing:\n",
            "loss: 1.3302565813064575 - accuracy: 0.5290799736976624 - val_loss: 1.4533871412277222 - val_accuracy: 0.4862000048160553 - penalty: 1.0000000000000002e-06\n",
            "layer sizes: [3072, 227, 228, 95, 74, 10]\n",
            "After growing:\n",
            "loss: 1.3302565813064575 - accuracy: 0.5290799736976624 - val_loss: 1.4533871412277222 - val_accuracy: 0.4862000048160553 - penalty: 1.0000000000000002e-07\n",
            "layer sizes: [3072, 277, 278, 145, 124, 10]\n",
            "Before pruning:\n",
            "loss: 1.2975372076034546 - accuracy: 0.5407199859619141 - val_loss: 1.4300810098648071 - val_accuracy: 0.492900013923645 - penalty: 1.0000000000000002e-07\n",
            "layer sizes: [3072, 277, 278, 145, 124, 10]\n",
            "After pruning:\n",
            "loss: 1.2975372076034546 - accuracy: 0.5407199859619141 - val_loss: 1.4300810098648071 - val_accuracy: 0.492900013923645 - penalty: 1.0000000000000002e-07\n",
            "layer sizes: [3072, 277, 278, 145, 124, 10]\n",
            "##########################################################\n",
            "Epoch 34/50\n",
            "Before growing:\n",
            "loss: 1.2975372076034546 - accuracy: 0.5407199859619141 - val_loss: 1.4300810098648071 - val_accuracy: 0.492900013923645 - penalty: 1.0000000000000002e-07\n",
            "layer sizes: [3072, 277, 278, 145, 124, 10]\n",
            "After growing:\n",
            "loss: 1.297537088394165 - accuracy: 0.5407199859619141 - val_loss: 1.4300810098648071 - val_accuracy: 0.492900013923645 - penalty: 1.0000000000000002e-07\n",
            "layer sizes: [3072, 332, 333, 195, 174, 10]\n",
            "Before pruning:\n",
            "loss: 1.2760323286056519 - accuracy: 0.5492200255393982 - val_loss: 1.4230144023895264 - val_accuracy: 0.4959000051021576 - penalty: 1.0000000000000002e-07\n",
            "layer sizes: [3072, 332, 333, 195, 174, 10]\n",
            "After pruning:\n",
            "loss: 1.2760323286056519 - accuracy: 0.5492200255393982 - val_loss: 1.4230142831802368 - val_accuracy: 0.4959000051021576 - penalty: 1.0000000000000002e-07\n",
            "layer sizes: [3072, 332, 333, 195, 173, 10]\n",
            "##########################################################\n",
            "Epoch 35/50\n",
            "Before growing:\n",
            "loss: 1.2760323286056519 - accuracy: 0.5492200255393982 - val_loss: 1.4230142831802368 - val_accuracy: 0.4959000051021576 - penalty: 1.0000000000000002e-07\n",
            "layer sizes: [3072, 332, 333, 195, 173, 10]\n",
            "After growing:\n",
            "loss: 1.2760323286056519 - accuracy: 0.5492200255393982 - val_loss: 1.4230144023895264 - val_accuracy: 0.4959000051021576 - penalty: 1.0000000000000002e-07\n",
            "layer sizes: [3072, 398, 399, 245, 223, 10]\n",
            "Before pruning:\n",
            "loss: 1.2480413913726807 - accuracy: 0.5601599812507629 - val_loss: 1.4040998220443726 - val_accuracy: 0.4991999864578247 - penalty: 1.0000000000000002e-07\n",
            "layer sizes: [3072, 398, 399, 245, 223, 10]\n",
            "After pruning:\n",
            "loss: 1.248040795326233 - accuracy: 0.5601599812507629 - val_loss: 1.4040992259979248 - val_accuracy: 0.4991999864578247 - penalty: 1.0000000000000002e-07\n",
            "layer sizes: [3072, 398, 399, 245, 188, 10]\n",
            "##########################################################\n",
            "Epoch 36/50\n",
            "Before growing:\n",
            "loss: 1.248040795326233 - accuracy: 0.5601599812507629 - val_loss: 1.4040992259979248 - val_accuracy: 0.4991999864578247 - penalty: 1.0000000000000002e-07\n",
            "layer sizes: [3072, 398, 399, 245, 188, 10]\n",
            "After growing:\n",
            "loss: 1.248040795326233 - accuracy: 0.5601599812507629 - val_loss: 1.4040992259979248 - val_accuracy: 0.4991999864578247 - penalty: 1.0000000000000002e-07\n",
            "layer sizes: [3072, 477, 478, 295, 238, 10]\n",
            "Before pruning:\n",
            "loss: 1.229069471359253 - accuracy: 0.565500020980835 - val_loss: 1.3987220525741577 - val_accuracy: 0.5044000148773193 - penalty: 1.0000000000000002e-07\n",
            "layer sizes: [3072, 477, 478, 295, 238, 10]\n",
            "After pruning:\n",
            "loss: 1.229068636894226 - accuracy: 0.565500020980835 - val_loss: 1.3987213373184204 - val_accuracy: 0.5044000148773193 - penalty: 1.0000000000000002e-07\n",
            "layer sizes: [3072, 477, 478, 295, 198, 10]\n",
            "##########################################################\n",
            "Epoch 37/50\n",
            "Before growing:\n",
            "loss: 1.229068636894226 - accuracy: 0.565500020980835 - val_loss: 1.3987213373184204 - val_accuracy: 0.5044000148773193 - penalty: 1.0000000000000002e-07\n",
            "layer sizes: [3072, 477, 478, 295, 198, 10]\n",
            "After growing:\n",
            "loss: 1.229068636894226 - accuracy: 0.565500020980835 - val_loss: 1.3987213373184204 - val_accuracy: 0.5044000148773193 - penalty: 1.0000000000000002e-07\n",
            "layer sizes: [3072, 572, 573, 354, 248, 10]\n",
            "Before pruning:\n",
            "loss: 1.192228078842163 - accuracy: 0.578279972076416 - val_loss: 1.3821673393249512 - val_accuracy: 0.5112000107765198 - penalty: 1.0000000000000002e-07\n",
            "layer sizes: [3072, 572, 573, 354, 248, 10]\n",
            "After pruning:\n",
            "loss: 1.1922270059585571 - accuracy: 0.578279972076416 - val_loss: 1.3821660280227661 - val_accuracy: 0.5112000107765198 - penalty: 1.0000000000000002e-07\n",
            "layer sizes: [3072, 572, 572, 354, 209, 10]\n",
            "##########################################################\n",
            "Epoch 38/50\n",
            "Before growing:\n",
            "loss: 1.1922270059585571 - accuracy: 0.578279972076416 - val_loss: 1.3821660280227661 - val_accuracy: 0.5112000107765198 - penalty: 1.0000000000000002e-07\n",
            "layer sizes: [3072, 572, 572, 354, 209, 10]\n",
            "After growing:\n",
            "loss: 1.1922270059585571 - accuracy: 0.578279972076416 - val_loss: 1.3821660280227661 - val_accuracy: 0.5112000107765198 - penalty: 1.0000000000000002e-07\n",
            "layer sizes: [3072, 686, 686, 424, 259, 10]\n",
            "Before pruning:\n",
            "loss: 1.1635173559188843 - accuracy: 0.5899199843406677 - val_loss: 1.3703094720840454 - val_accuracy: 0.5174000263214111 - penalty: 1.0000000000000002e-07\n",
            "layer sizes: [3072, 686, 686, 424, 259, 10]\n",
            "After pruning:\n",
            "loss: 1.163524866104126 - accuracy: 0.5898399949073792 - val_loss: 1.3703056573867798 - val_accuracy: 0.5171999931335449 - penalty: 1.0000000000000002e-07\n",
            "layer sizes: [3072, 679, 684, 424, 236, 10]\n",
            "##########################################################\n",
            "Epoch 39/50\n",
            "Before growing:\n",
            "loss: 1.163524866104126 - accuracy: 0.5898399949073792 - val_loss: 1.3703056573867798 - val_accuracy: 0.5171999931335449 - penalty: 1.0000000000000002e-07\n",
            "layer sizes: [3072, 679, 684, 424, 236, 10]\n",
            "After growing:\n",
            "loss: 1.163524866104126 - accuracy: 0.5898399949073792 - val_loss: 1.3703056573867798 - val_accuracy: 0.5171999931335449 - penalty: 1.0000000000000002e-07\n",
            "layer sizes: [3072, 814, 820, 508, 286, 10]\n",
            "Before pruning:\n",
            "loss: 1.1468786001205444 - accuracy: 0.5948600172996521 - val_loss: 1.3657418489456177 - val_accuracy: 0.5149999856948853 - penalty: 1.0000000000000002e-07\n",
            "layer sizes: [3072, 814, 820, 508, 286, 10]\n",
            "After pruning:\n",
            "loss: 1.1468708515167236 - accuracy: 0.5948799848556519 - val_loss: 1.3657286167144775 - val_accuracy: 0.5151000022888184 - penalty: 1.0000000000000002e-07\n",
            "layer sizes: [3072, 809, 815, 508, 232, 10]\n",
            "##########################################################\n",
            "Epoch 40/50\n",
            "Before growing:\n",
            "loss: 1.1468708515167236 - accuracy: 0.5948799848556519 - val_loss: 1.3657286167144775 - val_accuracy: 0.5151000022888184 - penalty: 1.0000000000000002e-07\n",
            "layer sizes: [3072, 809, 815, 508, 232, 10]\n",
            "After growing:\n",
            "loss: 1.146870732307434 - accuracy: 0.5948799848556519 - val_loss: 1.365728497505188 - val_accuracy: 0.5151000022888184 - penalty: 1.0000000000000002e-07\n",
            "layer sizes: [3072, 970, 978, 609, 282, 10]\n",
            "Before pruning:\n",
            "loss: 1.1286396980285645 - accuracy: 0.6017400026321411 - val_loss: 1.3652913570404053 - val_accuracy: 0.515500009059906 - penalty: 1.0000000000000002e-07\n",
            "layer sizes: [3072, 970, 978, 609, 282, 10]\n",
            "After pruning:\n",
            "loss: 1.1285964250564575 - accuracy: 0.6017400026321411 - val_loss: 1.3652578592300415 - val_accuracy: 0.5152999758720398 - penalty: 1.0000000000000002e-07\n",
            "layer sizes: [3072, 958, 955, 609, 250, 10]\n",
            "##########################################################\n",
            "Epoch 41/50\n",
            "Before growing:\n",
            "loss: 1.1285964250564575 - accuracy: 0.6017400026321411 - val_loss: 1.3652578592300415 - val_accuracy: 0.5152999758720398 - penalty: 1.0000000000000002e-07\n",
            "layer sizes: [3072, 958, 955, 609, 250, 10]\n",
            "After growing:\n",
            "loss: 1.1285964250564575 - accuracy: 0.6017400026321411 - val_loss: 1.3652578592300415 - val_accuracy: 0.5152999758720398 - penalty: 1.0000000000000002e-07\n",
            "layer sizes: [3072, 1149, 1146, 730, 300, 10]\n",
            "Before pruning:\n",
            "loss: 1.1135743856430054 - accuracy: 0.6068000197410583 - val_loss: 1.3654240369796753 - val_accuracy: 0.5170999765396118 - penalty: 1.0000000000000002e-07\n",
            "layer sizes: [3072, 1149, 1146, 730, 300, 10]\n",
            "After pruning:\n",
            "loss: 1.11354660987854 - accuracy: 0.6066200137138367 - val_loss: 1.3653552532196045 - val_accuracy: 0.5170000195503235 - penalty: 1.0000000000000002e-07\n",
            "layer sizes: [3072, 1108, 1141, 726, 273, 10]\n",
            "##########################################################\n",
            "Epoch 42/50\n",
            "Before growing:\n",
            "loss: 1.11354660987854 - accuracy: 0.6066200137138367 - val_loss: 1.3653552532196045 - val_accuracy: 0.5170000195503235 - penalty: 1.0000000000000002e-07\n",
            "layer sizes: [3072, 1108, 1141, 726, 273, 10]\n",
            "After growing:\n",
            "loss: 1.11354660987854 - accuracy: 0.6066200137138367 - val_loss: 1.365355134010315 - val_accuracy: 0.5170000195503235 - penalty: 1.0000000000000004e-08\n",
            "layer sizes: [3072, 1329, 1369, 871, 327, 10]\n",
            "Before pruning:\n",
            "loss: 1.1243542432785034 - accuracy: 0.6030799746513367 - val_loss: 1.3839462995529175 - val_accuracy: 0.513700008392334 - penalty: 1.0000000000000004e-08\n",
            "layer sizes: [3072, 1329, 1369, 871, 327, 10]\n",
            "After pruning:\n",
            "loss: 1.1243542432785034 - accuracy: 0.6030799746513367 - val_loss: 1.3839462995529175 - val_accuracy: 0.513700008392334 - penalty: 1.0000000000000004e-08\n",
            "layer sizes: [3072, 1329, 1369, 871, 327, 10]\n",
            "##########################################################\n",
            "Epoch 43/50\n",
            "Before growing:\n",
            "loss: 1.1243542432785034 - accuracy: 0.6030799746513367 - val_loss: 1.3839462995529175 - val_accuracy: 0.513700008392334 - penalty: 1.0000000000000004e-08\n",
            "layer sizes: [3072, 1329, 1369, 871, 327, 10]\n",
            "After growing:\n",
            "loss: 1.124354362487793 - accuracy: 0.6030799746513367 - val_loss: 1.3839462995529175 - val_accuracy: 0.513700008392334 - penalty: 1.0000000000000004e-08\n",
            "layer sizes: [3072, 1594, 1642, 1045, 392, 10]\n",
            "Before pruning:\n",
            "loss: 1.1195056438446045 - accuracy: 0.6016600131988525 - val_loss: 1.3772610425949097 - val_accuracy: 0.5116999745368958 - penalty: 1.0000000000000004e-08\n",
            "layer sizes: [3072, 1594, 1642, 1045, 392, 10]\n",
            "After pruning:\n",
            "loss: 1.1195056438446045 - accuracy: 0.6016600131988525 - val_loss: 1.3772610425949097 - val_accuracy: 0.5116999745368958 - penalty: 1.0000000000000004e-08\n",
            "layer sizes: [3072, 1594, 1642, 1045, 392, 10]\n",
            "##########################################################\n",
            "Epoch 44/50\n",
            "Before growing:\n",
            "loss: 1.1195056438446045 - accuracy: 0.6016600131988525 - val_loss: 1.3772610425949097 - val_accuracy: 0.5116999745368958 - penalty: 1.0000000000000004e-08\n",
            "layer sizes: [3072, 1594, 1642, 1045, 392, 10]\n",
            "After growing:\n",
            "loss: 1.1195056438446045 - accuracy: 0.6016600131988525 - val_loss: 1.3772611618041992 - val_accuracy: 0.5116999745368958 - penalty: 1.0000000000000004e-08\n",
            "layer sizes: [3072, 1912, 1970, 1254, 470, 10]\n",
            "Before pruning:\n",
            "loss: 1.1194733381271362 - accuracy: 0.6023200154304504 - val_loss: 1.3971736431121826 - val_accuracy: 0.5121999979019165 - penalty: 1.0000000000000004e-08\n",
            "layer sizes: [3072, 1912, 1970, 1254, 470, 10]\n",
            "After pruning:\n",
            "loss: 1.1194733381271362 - accuracy: 0.6023200154304504 - val_loss: 1.3971736431121826 - val_accuracy: 0.5121999979019165 - penalty: 1.0000000000000004e-08\n",
            "layer sizes: [3072, 1912, 1970, 1254, 470, 10]\n",
            "##########################################################\n",
            "Epoch 45/50\n",
            "Before growing:\n",
            "loss: 1.1194733381271362 - accuracy: 0.6023200154304504 - val_loss: 1.3971736431121826 - val_accuracy: 0.5121999979019165 - penalty: 1.0000000000000004e-08\n",
            "layer sizes: [3072, 1912, 1970, 1254, 470, 10]\n",
            "After growing:\n",
            "loss: 1.1194733381271362 - accuracy: 0.6023200154304504 - val_loss: 1.3971736431121826 - val_accuracy: 0.5121999979019165 - penalty: 1.0000000000000004e-08\n",
            "layer sizes: [3072, 2294, 2364, 1504, 564, 10]\n",
            "Before pruning:\n",
            "loss: 1.1225438117980957 - accuracy: 0.6032000184059143 - val_loss: 1.4037624597549438 - val_accuracy: 0.5181999802589417 - penalty: 1.0000000000000004e-08\n",
            "layer sizes: [3072, 2294, 2364, 1504, 564, 10]\n",
            "After pruning:\n",
            "loss: 1.1225438117980957 - accuracy: 0.6032000184059143 - val_loss: 1.4037624597549438 - val_accuracy: 0.5181999802589417 - penalty: 1.0000000000000004e-08\n",
            "layer sizes: [3072, 2294, 2364, 1504, 564, 10]\n",
            "##########################################################\n",
            "Epoch 46/50\n",
            "Before growing:\n",
            "loss: 1.1225438117980957 - accuracy: 0.6032000184059143 - val_loss: 1.4037624597549438 - val_accuracy: 0.5181999802589417 - penalty: 1.0000000000000004e-08\n",
            "layer sizes: [3072, 2294, 2364, 1504, 564, 10]\n",
            "After growing:\n",
            "loss: 1.1225439310073853 - accuracy: 0.6032000184059143 - val_loss: 1.4037624597549438 - val_accuracy: 0.5181999802589417 - penalty: 1.0000000000000004e-08\n",
            "layer sizes: [3072, 2752, 2836, 1804, 676, 10]\n",
            "Before pruning:\n",
            "loss: 1.0946334600448608 - accuracy: 0.6144999861717224 - val_loss: 1.3642951250076294 - val_accuracy: 0.5235999822616577 - penalty: 1.0000000000000004e-08\n",
            "layer sizes: [3072, 2752, 2836, 1804, 676, 10]\n",
            "After pruning:\n",
            "loss: 1.094636082649231 - accuracy: 0.6144999861717224 - val_loss: 1.364301323890686 - val_accuracy: 0.5235999822616577 - penalty: 1.0000000000000004e-08\n",
            "layer sizes: [3072, 2740, 2836, 1804, 676, 10]\n",
            "##########################################################\n",
            "Epoch 47/50\n",
            "Before growing:\n",
            "loss: 1.094636082649231 - accuracy: 0.6144999861717224 - val_loss: 1.364301323890686 - val_accuracy: 0.5235999822616577 - penalty: 1.0000000000000004e-08\n",
            "layer sizes: [3072, 2740, 2836, 1804, 676, 10]\n",
            "After growing:\n",
            "loss: 1.094636082649231 - accuracy: 0.6144999861717224 - val_loss: 1.364301323890686 - val_accuracy: 0.5235999822616577 - penalty: 1.0000000000000004e-08\n",
            "layer sizes: [3072, 3288, 3403, 2164, 811, 10]\n",
            "Before pruning:\n",
            "loss: 1.0735093355178833 - accuracy: 0.6183800101280212 - val_loss: 1.379501223564148 - val_accuracy: 0.5253999829292297 - penalty: 1.0000000000000004e-08\n",
            "layer sizes: [3072, 3288, 3403, 2164, 811, 10]\n",
            "After pruning:\n",
            "loss: 1.0735034942626953 - accuracy: 0.6182600259780884 - val_loss: 1.3794472217559814 - val_accuracy: 0.5256999731063843 - penalty: 1.0000000000000004e-08\n",
            "layer sizes: [3072, 3201, 3403, 2164, 811, 10]\n",
            "##########################################################\n",
            "Epoch 48/50\n",
            "Before growing:\n",
            "loss: 1.0735034942626953 - accuracy: 0.6182600259780884 - val_loss: 1.3794472217559814 - val_accuracy: 0.5256999731063843 - penalty: 1.0000000000000004e-08\n",
            "layer sizes: [3072, 3201, 3403, 2164, 811, 10]\n",
            "After growing:\n",
            "loss: 1.0735034942626953 - accuracy: 0.6182600259780884 - val_loss: 1.3794472217559814 - val_accuracy: 0.5256999731063843 - penalty: 1.0000000000000005e-09\n",
            "layer sizes: [3072, 3841, 4083, 2596, 973, 10]\n",
            "Before pruning:\n",
            "loss: 1.1313385963439941 - accuracy: 0.6038200259208679 - val_loss: 1.4001394510269165 - val_accuracy: 0.5216000080108643 - penalty: 1.0000000000000005e-09\n",
            "layer sizes: [3072, 3841, 4083, 2596, 973, 10]\n",
            "After pruning:\n",
            "loss: 1.1313385963439941 - accuracy: 0.6038200259208679 - val_loss: 1.4001394510269165 - val_accuracy: 0.5216000080108643 - penalty: 1.0000000000000005e-09\n",
            "layer sizes: [3072, 3841, 4083, 2596, 973, 10]\n",
            "##########################################################\n",
            "Epoch 49/50\n",
            "Before growing:\n",
            "loss: 1.1313385963439941 - accuracy: 0.6038200259208679 - val_loss: 1.4001394510269165 - val_accuracy: 0.5216000080108643 - penalty: 1.0000000000000005e-09\n",
            "layer sizes: [3072, 3841, 4083, 2596, 973, 10]\n",
            "After growing:\n",
            "loss: 1.1313385963439941 - accuracy: 0.6038200259208679 - val_loss: 1.400139570236206 - val_accuracy: 0.5216000080108643 - penalty: 1.0000000000000005e-09\n",
            "layer sizes: [3072, 4609, 4899, 3115, 1167, 10]\n",
            "Before pruning:\n",
            "loss: 1.1265474557876587 - accuracy: 0.5961400270462036 - val_loss: 1.3908588886260986 - val_accuracy: 0.5202000141143799 - penalty: 1.0000000000000005e-09\n",
            "layer sizes: [3072, 4609, 4899, 3115, 1167, 10]\n",
            "After pruning:\n",
            "loss: 1.1265474557876587 - accuracy: 0.5961400270462036 - val_loss: 1.3908588886260986 - val_accuracy: 0.5202000141143799 - penalty: 1.0000000000000005e-09\n",
            "layer sizes: [3072, 4609, 4899, 3115, 1167, 10]\n",
            "##########################################################\n",
            "Epoch 50/50\n",
            "Before growing:\n",
            "loss: 1.1265474557876587 - accuracy: 0.5961400270462036 - val_loss: 1.3908588886260986 - val_accuracy: 0.5202000141143799 - penalty: 1.0000000000000005e-09\n",
            "layer sizes: [3072, 4609, 4899, 3115, 1167, 10]\n",
            "After growing:\n",
            "loss: 1.1265474557876587 - accuracy: 0.5961400270462036 - val_loss: 1.3908588886260986 - val_accuracy: 0.5202000141143799 - penalty: 1.0000000000000005e-09\n",
            "layer sizes: [3072, 5530, 5878, 3738, 1400, 10]\n",
            "Before pruning:\n",
            "loss: 1.2126522064208984 - accuracy: 0.5607200264930725 - val_loss: 1.4103004932403564 - val_accuracy: 0.5037999749183655 - penalty: 1.0000000000000005e-09\n",
            "layer sizes: [3072, 5530, 5878, 3738, 1400, 10]\n",
            "After pruning:\n",
            "loss: 1.2126522064208984 - accuracy: 0.5607200264930725 - val_loss: 1.4103004932403564 - val_accuracy: 0.5037999749183655 - penalty: 1.0000000000000005e-09\n",
            "layer sizes: [3072, 5530, 5878, 3738, 1400, 10]\n",
            "CPU times: user 18min 38s, sys: 14.9 s, total: 18min 53s\n",
            "Wall time: 19min 13s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3Xtj5FYfMkP",
        "outputId": "c9fa85e4-ca5e-45e0-f4f6-4654c380be28"
      },
      "source": [
        "%%time\n",
        "\n",
        "model = SSModel(layer_sizes=[3072, 300, 300, 300, 300, 10], activation='selu', regularization_penalty=0.0001, regularization_method='weighted_l1', kernel_initializer='lecun_normal')\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002)\n",
        "\n",
        "train_model(model, X_train_norm, y_train, optimizer, epochs, self_scaling_epochs, batch_size, min_new_neurons, validation_data=(X_test_norm, y_test), regularization_penalty_multiplier=0.1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "##########################################################\n",
            "Epoch 1/50\n",
            "Before growing:\n",
            "loss: 2.72518253326416 - accuracy: 0.10886000096797943 - val_loss: 2.734980821609497 - val_accuracy: 0.10670000314712524 - penalty: 0.0001\n",
            "layer sizes: [3072, 300, 300, 300, 300, 10]\n",
            "After growing:\n",
            "loss: 2.72518253326416 - accuracy: 0.10886000096797943 - val_loss: 2.734980821609497 - val_accuracy: 0.10670000314712524 - penalty: 0.0001\n",
            "layer sizes: [3072, 360, 360, 360, 360, 10]\n",
            "Before pruning:\n",
            "loss: 1.8462209701538086 - accuracy: 0.3119400143623352 - val_loss: 1.8498504161834717 - val_accuracy: 0.30640000104904175 - penalty: 0.0001\n",
            "layer sizes: [3072, 360, 360, 360, 360, 10]\n",
            "After pruning:\n",
            "loss: 1.8453584909439087 - accuracy: 0.3140600025653839 - val_loss: 1.848866581916809 - val_accuracy: 0.30880001187324524 - penalty: 0.0001\n",
            "layer sizes: [3072, 13, 62, 33, 64, 10]\n",
            "##########################################################\n",
            "Epoch 2/50\n",
            "Before growing:\n",
            "loss: 1.8453584909439087 - accuracy: 0.3140600025653839 - val_loss: 1.848866581916809 - val_accuracy: 0.30880001187324524 - penalty: 0.0001\n",
            "layer sizes: [3072, 13, 62, 33, 64, 10]\n",
            "After growing:\n",
            "loss: 1.8453584909439087 - accuracy: 0.3140600025653839 - val_loss: 1.848866581916809 - val_accuracy: 0.30880001187324524 - penalty: 0.0001\n",
            "layer sizes: [3072, 63, 112, 83, 114, 10]\n",
            "Before pruning:\n",
            "loss: 1.7795809507369995 - accuracy: 0.35482001304626465 - val_loss: 1.7862861156463623 - val_accuracy: 0.35190001130104065 - penalty: 0.0001\n",
            "layer sizes: [3072, 63, 112, 83, 114, 10]\n",
            "After pruning:\n",
            "loss: 1.7795488834381104 - accuracy: 0.3548400104045868 - val_loss: 1.7862470149993896 - val_accuracy: 0.352400004863739 - penalty: 0.0001\n",
            "layer sizes: [3072, 9, 43, 20, 55, 10]\n",
            "##########################################################\n",
            "Epoch 3/50\n",
            "Before growing:\n",
            "loss: 1.7795488834381104 - accuracy: 0.3548400104045868 - val_loss: 1.7862470149993896 - val_accuracy: 0.352400004863739 - penalty: 0.0001\n",
            "layer sizes: [3072, 9, 43, 20, 55, 10]\n",
            "After growing:\n",
            "loss: 1.7795488834381104 - accuracy: 0.3548400104045868 - val_loss: 1.7862470149993896 - val_accuracy: 0.352400004863739 - penalty: 0.0001\n",
            "layer sizes: [3072, 59, 93, 70, 105, 10]\n",
            "Before pruning:\n",
            "loss: 1.7359932661056519 - accuracy: 0.3706200122833252 - val_loss: 1.7434598207473755 - val_accuracy: 0.3635999858379364 - penalty: 0.0001\n",
            "layer sizes: [3072, 59, 93, 70, 105, 10]\n",
            "After pruning:\n",
            "loss: 1.7360093593597412 - accuracy: 0.3706200122833252 - val_loss: 1.7434773445129395 - val_accuracy: 0.3634999990463257 - penalty: 0.0001\n",
            "layer sizes: [3072, 9, 33, 18, 50, 10]\n",
            "##########################################################\n",
            "Epoch 4/50\n",
            "Before growing:\n",
            "loss: 1.7360093593597412 - accuracy: 0.3706200122833252 - val_loss: 1.7434773445129395 - val_accuracy: 0.3634999990463257 - penalty: 0.0001\n",
            "layer sizes: [3072, 9, 33, 18, 50, 10]\n",
            "After growing:\n",
            "loss: 1.7360094785690308 - accuracy: 0.3706200122833252 - val_loss: 1.7434773445129395 - val_accuracy: 0.3634999990463257 - penalty: 0.0001\n",
            "layer sizes: [3072, 59, 83, 68, 100, 10]\n",
            "Before pruning:\n",
            "loss: 1.7196085453033447 - accuracy: 0.3804199993610382 - val_loss: 1.729251503944397 - val_accuracy: 0.3709999918937683 - penalty: 0.0001\n",
            "layer sizes: [3072, 59, 83, 68, 100, 10]\n",
            "After pruning:\n",
            "loss: 1.7196271419525146 - accuracy: 0.3804599940776825 - val_loss: 1.7292747497558594 - val_accuracy: 0.3709999918937683 - penalty: 0.0001\n",
            "layer sizes: [3072, 9, 33, 16, 47, 10]\n",
            "##########################################################\n",
            "Epoch 5/50\n",
            "Before growing:\n",
            "loss: 1.7196271419525146 - accuracy: 0.3804599940776825 - val_loss: 1.7292747497558594 - val_accuracy: 0.3709999918937683 - penalty: 0.0001\n",
            "layer sizes: [3072, 9, 33, 16, 47, 10]\n",
            "After growing:\n",
            "loss: 1.7196273803710938 - accuracy: 0.3804599940776825 - val_loss: 1.7292747497558594 - val_accuracy: 0.3709999918937683 - penalty: 0.0001\n",
            "layer sizes: [3072, 59, 83, 66, 97, 10]\n",
            "Before pruning:\n",
            "loss: 1.7003880739212036 - accuracy: 0.3849799931049347 - val_loss: 1.7113796472549438 - val_accuracy: 0.3831000030040741 - penalty: 0.0001\n",
            "layer sizes: [3072, 59, 83, 66, 97, 10]\n",
            "After pruning:\n",
            "loss: 1.7003179788589478 - accuracy: 0.38514000177383423 - val_loss: 1.7112910747528076 - val_accuracy: 0.382999986410141 - penalty: 0.0001\n",
            "layer sizes: [3072, 8, 33, 16, 47, 10]\n",
            "##########################################################\n",
            "Epoch 6/50\n",
            "Before growing:\n",
            "loss: 1.7003179788589478 - accuracy: 0.38514000177383423 - val_loss: 1.7112910747528076 - val_accuracy: 0.382999986410141 - penalty: 0.0001\n",
            "layer sizes: [3072, 8, 33, 16, 47, 10]\n",
            "After growing:\n",
            "loss: 1.7003179788589478 - accuracy: 0.38514000177383423 - val_loss: 1.7112910747528076 - val_accuracy: 0.382999986410141 - penalty: 0.0001\n",
            "layer sizes: [3072, 58, 83, 66, 97, 10]\n",
            "Before pruning:\n",
            "loss: 1.6788606643676758 - accuracy: 0.3943600058555603 - val_loss: 1.6921656131744385 - val_accuracy: 0.3862999975681305 - penalty: 0.0001\n",
            "layer sizes: [3072, 58, 83, 66, 97, 10]\n",
            "After pruning:\n",
            "loss: 1.6788583993911743 - accuracy: 0.39427998661994934 - val_loss: 1.6921603679656982 - val_accuracy: 0.3862999975681305 - penalty: 0.0001\n",
            "layer sizes: [3072, 8, 29, 16, 45, 10]\n",
            "##########################################################\n",
            "Epoch 7/50\n",
            "Before growing:\n",
            "loss: 1.6788583993911743 - accuracy: 0.39427998661994934 - val_loss: 1.6921603679656982 - val_accuracy: 0.3862999975681305 - penalty: 0.0001\n",
            "layer sizes: [3072, 8, 29, 16, 45, 10]\n",
            "After growing:\n",
            "loss: 1.6788583993911743 - accuracy: 0.39427998661994934 - val_loss: 1.6921603679656982 - val_accuracy: 0.3862999975681305 - penalty: 0.0001\n",
            "layer sizes: [3072, 58, 79, 66, 95, 10]\n",
            "Before pruning:\n",
            "loss: 1.6621378660202026 - accuracy: 0.3998599946498871 - val_loss: 1.6773020029067993 - val_accuracy: 0.3905999958515167 - penalty: 0.0001\n",
            "layer sizes: [3072, 58, 79, 66, 95, 10]\n",
            "After pruning:\n",
            "loss: 1.6621677875518799 - accuracy: 0.3998199999332428 - val_loss: 1.6773256063461304 - val_accuracy: 0.3905999958515167 - penalty: 0.0001\n",
            "layer sizes: [3072, 8, 27, 15, 40, 10]\n",
            "##########################################################\n",
            "Epoch 8/50\n",
            "Before growing:\n",
            "loss: 1.6621677875518799 - accuracy: 0.3998199999332428 - val_loss: 1.6773256063461304 - val_accuracy: 0.3905999958515167 - penalty: 0.0001\n",
            "layer sizes: [3072, 8, 27, 15, 40, 10]\n",
            "After growing:\n",
            "loss: 1.6621676683425903 - accuracy: 0.3998199999332428 - val_loss: 1.6773253679275513 - val_accuracy: 0.3905999958515167 - penalty: 0.0001\n",
            "layer sizes: [3072, 58, 77, 65, 90, 10]\n",
            "Before pruning:\n",
            "loss: 1.647430658340454 - accuracy: 0.40220001339912415 - val_loss: 1.6642810106277466 - val_accuracy: 0.3937000036239624 - penalty: 0.0001\n",
            "layer sizes: [3072, 58, 77, 65, 90, 10]\n",
            "After pruning:\n",
            "loss: 1.6474415063858032 - accuracy: 0.40215998888015747 - val_loss: 1.664294719696045 - val_accuracy: 0.3937000036239624 - penalty: 0.0001\n",
            "layer sizes: [3072, 8, 27, 12, 39, 10]\n",
            "##########################################################\n",
            "Epoch 9/50\n",
            "Before growing:\n",
            "loss: 1.6474415063858032 - accuracy: 0.40215998888015747 - val_loss: 1.664294719696045 - val_accuracy: 0.3937000036239624 - penalty: 0.0001\n",
            "layer sizes: [3072, 8, 27, 12, 39, 10]\n",
            "After growing:\n",
            "loss: 1.6474415063858032 - accuracy: 0.40215998888015747 - val_loss: 1.6642944812774658 - val_accuracy: 0.3937000036239624 - penalty: 0.0001\n",
            "layer sizes: [3072, 58, 77, 62, 89, 10]\n",
            "Before pruning:\n",
            "loss: 1.6389553546905518 - accuracy: 0.40446001291275024 - val_loss: 1.6572824716567993 - val_accuracy: 0.39149999618530273 - penalty: 0.0001\n",
            "layer sizes: [3072, 58, 77, 62, 89, 10]\n",
            "After pruning:\n",
            "loss: 1.6379327774047852 - accuracy: 0.4047999978065491 - val_loss: 1.656313419342041 - val_accuracy: 0.39149999618530273 - penalty: 0.0001\n",
            "layer sizes: [3072, 8, 25, 12, 39, 10]\n",
            "##########################################################\n",
            "Epoch 10/50\n",
            "Before growing:\n",
            "loss: 1.6379327774047852 - accuracy: 0.4047999978065491 - val_loss: 1.656313419342041 - val_accuracy: 0.39149999618530273 - penalty: 0.0001\n",
            "layer sizes: [3072, 8, 25, 12, 39, 10]\n",
            "After growing:\n",
            "loss: 1.6379326581954956 - accuracy: 0.4047999978065491 - val_loss: 1.656313419342041 - val_accuracy: 0.39149999618530273 - penalty: 0.0001\n",
            "layer sizes: [3072, 58, 75, 62, 89, 10]\n",
            "Before pruning:\n",
            "loss: 1.6295086145401 - accuracy: 0.4113599956035614 - val_loss: 1.6471565961837769 - val_accuracy: 0.40290001034736633 - penalty: 0.0001\n",
            "layer sizes: [3072, 58, 75, 62, 89, 10]\n",
            "After pruning:\n",
            "loss: 1.6294755935668945 - accuracy: 0.4115400016307831 - val_loss: 1.6471439599990845 - val_accuracy: 0.40290001034736633 - penalty: 0.0001\n",
            "layer sizes: [3072, 8, 25, 12, 38, 10]\n",
            "##########################################################\n",
            "Epoch 11/50\n",
            "Before growing:\n",
            "loss: 1.6294755935668945 - accuracy: 0.4115400016307831 - val_loss: 1.6471439599990845 - val_accuracy: 0.40290001034736633 - penalty: 0.0001\n",
            "layer sizes: [3072, 8, 25, 12, 38, 10]\n",
            "After growing:\n",
            "loss: 1.6294758319854736 - accuracy: 0.4115400016307831 - val_loss: 1.6471441984176636 - val_accuracy: 0.40290001034736633 - penalty: 0.0001\n",
            "layer sizes: [3072, 58, 75, 62, 88, 10]\n",
            "Before pruning:\n",
            "loss: 1.6239887475967407 - accuracy: 0.4093399941921234 - val_loss: 1.64248788356781 - val_accuracy: 0.4016999900341034 - penalty: 0.0001\n",
            "layer sizes: [3072, 58, 75, 62, 88, 10]\n",
            "After pruning:\n",
            "loss: 1.623927354812622 - accuracy: 0.40926000475883484 - val_loss: 1.6424373388290405 - val_accuracy: 0.40149998664855957 - penalty: 0.0001\n",
            "layer sizes: [3072, 8, 24, 12, 38, 10]\n",
            "##########################################################\n",
            "Epoch 12/50\n",
            "Before growing:\n",
            "loss: 1.623927354812622 - accuracy: 0.40926000475883484 - val_loss: 1.6424373388290405 - val_accuracy: 0.40149998664855957 - penalty: 0.0001\n",
            "layer sizes: [3072, 8, 24, 12, 38, 10]\n",
            "After growing:\n",
            "loss: 1.6239272356033325 - accuracy: 0.40926000475883484 - val_loss: 1.6424373388290405 - val_accuracy: 0.40149998664855957 - penalty: 0.0001\n",
            "layer sizes: [3072, 58, 74, 62, 88, 10]\n",
            "Before pruning:\n",
            "loss: 1.6083757877349854 - accuracy: 0.41971999406814575 - val_loss: 1.6299922466278076 - val_accuracy: 0.40369999408721924 - penalty: 0.0001\n",
            "layer sizes: [3072, 58, 74, 62, 88, 10]\n",
            "After pruning:\n",
            "loss: 1.608512043952942 - accuracy: 0.41958001255989075 - val_loss: 1.630104899406433 - val_accuracy: 0.4041999876499176 - penalty: 0.0001\n",
            "layer sizes: [3072, 12, 24, 12, 38, 10]\n",
            "##########################################################\n",
            "Epoch 13/50\n",
            "Before growing:\n",
            "loss: 1.608512043952942 - accuracy: 0.41958001255989075 - val_loss: 1.630104899406433 - val_accuracy: 0.4041999876499176 - penalty: 0.0001\n",
            "layer sizes: [3072, 12, 24, 12, 38, 10]\n",
            "After growing:\n",
            "loss: 1.608512043952942 - accuracy: 0.41958001255989075 - val_loss: 1.630104899406433 - val_accuracy: 0.4041999876499176 - penalty: 0.0001\n",
            "layer sizes: [3072, 62, 74, 62, 88, 10]\n",
            "Before pruning:\n",
            "loss: 1.610434889793396 - accuracy: 0.4178999960422516 - val_loss: 1.6314300298690796 - val_accuracy: 0.41100001335144043 - penalty: 0.0001\n",
            "layer sizes: [3072, 62, 74, 62, 88, 10]\n",
            "After pruning:\n",
            "loss: 1.6104421615600586 - accuracy: 0.41797998547554016 - val_loss: 1.63144052028656 - val_accuracy: 0.4106000065803528 - penalty: 0.0001\n",
            "layer sizes: [3072, 8, 24, 12, 38, 10]\n",
            "##########################################################\n",
            "Epoch 14/50\n",
            "Before growing:\n",
            "loss: 1.6104421615600586 - accuracy: 0.41797998547554016 - val_loss: 1.63144052028656 - val_accuracy: 0.4106000065803528 - penalty: 0.0001\n",
            "layer sizes: [3072, 8, 24, 12, 38, 10]\n",
            "After growing:\n",
            "loss: 1.6104421615600586 - accuracy: 0.41797998547554016 - val_loss: 1.6314406394958496 - val_accuracy: 0.4106000065803528 - penalty: 1e-05\n",
            "layer sizes: [3072, 58, 74, 62, 88, 10]\n",
            "Before pruning:\n",
            "loss: 1.5841261148452759 - accuracy: 0.4314599931240082 - val_loss: 1.6132171154022217 - val_accuracy: 0.41929998993873596 - penalty: 1e-05\n",
            "layer sizes: [3072, 58, 74, 62, 88, 10]\n",
            "After pruning:\n",
            "loss: 1.584245204925537 - accuracy: 0.4314199984073639 - val_loss: 1.6133265495300293 - val_accuracy: 0.4196999967098236 - penalty: 1e-05\n",
            "layer sizes: [3072, 33, 24, 12, 66, 10]\n",
            "##########################################################\n",
            "Epoch 15/50\n",
            "Before growing:\n",
            "loss: 1.584245204925537 - accuracy: 0.4314199984073639 - val_loss: 1.6133265495300293 - val_accuracy: 0.4196999967098236 - penalty: 1e-05\n",
            "layer sizes: [3072, 33, 24, 12, 66, 10]\n",
            "After growing:\n",
            "loss: 1.584244966506958 - accuracy: 0.4314199984073639 - val_loss: 1.6133263111114502 - val_accuracy: 0.4196999967098236 - penalty: 1e-05\n",
            "layer sizes: [3072, 83, 74, 62, 116, 10]\n",
            "Before pruning:\n",
            "loss: 1.5600205659866333 - accuracy: 0.4397200047969818 - val_loss: 1.5928893089294434 - val_accuracy: 0.4239000082015991 - penalty: 1e-05\n",
            "layer sizes: [3072, 83, 74, 62, 116, 10]\n",
            "After pruning:\n",
            "loss: 1.5603649616241455 - accuracy: 0.43974000215530396 - val_loss: 1.5932536125183105 - val_accuracy: 0.42399999499320984 - penalty: 1e-05\n",
            "layer sizes: [3072, 40, 28, 14, 51, 10]\n",
            "##########################################################\n",
            "Epoch 16/50\n",
            "Before growing:\n",
            "loss: 1.5603649616241455 - accuracy: 0.43974000215530396 - val_loss: 1.5932536125183105 - val_accuracy: 0.42399999499320984 - penalty: 1e-05\n",
            "layer sizes: [3072, 40, 28, 14, 51, 10]\n",
            "After growing:\n",
            "loss: 1.5603649616241455 - accuracy: 0.43974000215530396 - val_loss: 1.5932536125183105 - val_accuracy: 0.42399999499320984 - penalty: 1e-05\n",
            "layer sizes: [3072, 90, 78, 64, 101, 10]\n",
            "Before pruning:\n",
            "loss: 1.5468981266021729 - accuracy: 0.4472399950027466 - val_loss: 1.5824804306030273 - val_accuracy: 0.4334000051021576 - penalty: 1e-05\n",
            "layer sizes: [3072, 90, 78, 64, 101, 10]\n",
            "After pruning:\n",
            "loss: 1.546866536140442 - accuracy: 0.4476799964904785 - val_loss: 1.5826170444488525 - val_accuracy: 0.4334999918937683 - penalty: 1e-05\n",
            "layer sizes: [3072, 45, 29, 12, 52, 10]\n",
            "##########################################################\n",
            "Epoch 17/50\n",
            "Before growing:\n",
            "loss: 1.546866536140442 - accuracy: 0.4476799964904785 - val_loss: 1.5826170444488525 - val_accuracy: 0.4334999918937683 - penalty: 1e-05\n",
            "layer sizes: [3072, 45, 29, 12, 52, 10]\n",
            "After growing:\n",
            "loss: 1.546866536140442 - accuracy: 0.4476799964904785 - val_loss: 1.5826170444488525 - val_accuracy: 0.4334999918937683 - penalty: 1e-05\n",
            "layer sizes: [3072, 95, 79, 62, 102, 10]\n",
            "Before pruning:\n",
            "loss: 1.5230534076690674 - accuracy: 0.45576000213623047 - val_loss: 1.5663713216781616 - val_accuracy: 0.4381999969482422 - penalty: 1e-05\n",
            "layer sizes: [3072, 95, 79, 62, 102, 10]\n",
            "After pruning:\n",
            "loss: 1.5226233005523682 - accuracy: 0.4560199975967407 - val_loss: 1.5659899711608887 - val_accuracy: 0.4390000104904175 - penalty: 1e-05\n",
            "layer sizes: [3072, 38, 28, 12, 48, 10]\n",
            "##########################################################\n",
            "Epoch 18/50\n",
            "Before growing:\n",
            "loss: 1.5226233005523682 - accuracy: 0.4560199975967407 - val_loss: 1.5659899711608887 - val_accuracy: 0.4390000104904175 - penalty: 1e-05\n",
            "layer sizes: [3072, 38, 28, 12, 48, 10]\n",
            "After growing:\n",
            "loss: 1.5226233005523682 - accuracy: 0.4560199975967407 - val_loss: 1.5659898519515991 - val_accuracy: 0.4390000104904175 - penalty: 1e-05\n",
            "layer sizes: [3072, 88, 78, 62, 98, 10]\n",
            "Before pruning:\n",
            "loss: 1.5146815776824951 - accuracy: 0.4582599997520447 - val_loss: 1.5641071796417236 - val_accuracy: 0.44110000133514404 - penalty: 1e-05\n",
            "layer sizes: [3072, 88, 78, 62, 98, 10]\n",
            "After pruning:\n",
            "loss: 1.514621615409851 - accuracy: 0.45844000577926636 - val_loss: 1.564042329788208 - val_accuracy: 0.4406999945640564 - penalty: 1e-05\n",
            "layer sizes: [3072, 43, 34, 12, 43, 10]\n",
            "##########################################################\n",
            "Epoch 19/50\n",
            "Before growing:\n",
            "loss: 1.514621615409851 - accuracy: 0.45844000577926636 - val_loss: 1.564042329788208 - val_accuracy: 0.4406999945640564 - penalty: 1e-05\n",
            "layer sizes: [3072, 43, 34, 12, 43, 10]\n",
            "After growing:\n",
            "loss: 1.514621615409851 - accuracy: 0.45844000577926636 - val_loss: 1.564042329788208 - val_accuracy: 0.4406999945640564 - penalty: 1e-05\n",
            "layer sizes: [3072, 93, 84, 62, 93, 10]\n",
            "Before pruning:\n",
            "loss: 1.5013014078140259 - accuracy: 0.4640600085258484 - val_loss: 1.5546541213989258 - val_accuracy: 0.44600000977516174 - penalty: 1e-05\n",
            "layer sizes: [3072, 93, 84, 62, 93, 10]\n",
            "After pruning:\n",
            "loss: 1.500988245010376 - accuracy: 0.4643799960613251 - val_loss: 1.554347276687622 - val_accuracy: 0.44609999656677246 - penalty: 1e-05\n",
            "layer sizes: [3072, 33, 32, 12, 43, 10]\n",
            "##########################################################\n",
            "Epoch 20/50\n",
            "Before growing:\n",
            "loss: 1.500988245010376 - accuracy: 0.4643799960613251 - val_loss: 1.554347276687622 - val_accuracy: 0.44609999656677246 - penalty: 1e-05\n",
            "layer sizes: [3072, 33, 32, 12, 43, 10]\n",
            "After growing:\n",
            "loss: 1.5009881258010864 - accuracy: 0.4643799960613251 - val_loss: 1.554347276687622 - val_accuracy: 0.44609999656677246 - penalty: 1e-05\n",
            "layer sizes: [3072, 83, 82, 62, 93, 10]\n",
            "Before pruning:\n",
            "loss: 1.4905756711959839 - accuracy: 0.4663800001144409 - val_loss: 1.5536754131317139 - val_accuracy: 0.44359999895095825 - penalty: 1e-05\n",
            "layer sizes: [3072, 83, 82, 62, 93, 10]\n",
            "After pruning:\n",
            "loss: 1.490293025970459 - accuracy: 0.4664199948310852 - val_loss: 1.5534402132034302 - val_accuracy: 0.4442000091075897 - penalty: 1e-05\n",
            "layer sizes: [3072, 57, 31, 12, 65, 10]\n",
            "##########################################################\n",
            "Epoch 21/50\n",
            "Before growing:\n",
            "loss: 1.490293025970459 - accuracy: 0.4664199948310852 - val_loss: 1.5534402132034302 - val_accuracy: 0.4442000091075897 - penalty: 1e-05\n",
            "layer sizes: [3072, 57, 31, 12, 65, 10]\n",
            "After growing:\n",
            "loss: 1.490293025970459 - accuracy: 0.4664199948310852 - val_loss: 1.5534402132034302 - val_accuracy: 0.4442000091075897 - penalty: 1e-05\n",
            "layer sizes: [3072, 107, 81, 62, 115, 10]\n",
            "Before pruning:\n",
            "loss: 1.4797459840774536 - accuracy: 0.47218000888824463 - val_loss: 1.5460437536239624 - val_accuracy: 0.44780001044273376 - penalty: 1e-05\n",
            "layer sizes: [3072, 107, 81, 62, 115, 10]\n",
            "After pruning:\n",
            "loss: 1.4807277917861938 - accuracy: 0.4716799855232239 - val_loss: 1.5468471050262451 - val_accuracy: 0.44690001010894775 - penalty: 1e-05\n",
            "layer sizes: [3072, 30, 30, 12, 47, 10]\n",
            "##########################################################\n",
            "Epoch 22/50\n",
            "Before growing:\n",
            "loss: 1.4807277917861938 - accuracy: 0.4716799855232239 - val_loss: 1.5468471050262451 - val_accuracy: 0.44690001010894775 - penalty: 1e-05\n",
            "layer sizes: [3072, 30, 30, 12, 47, 10]\n",
            "After growing:\n",
            "loss: 1.4807279109954834 - accuracy: 0.4716799855232239 - val_loss: 1.5468471050262451 - val_accuracy: 0.44690001010894775 - penalty: 1e-05\n",
            "layer sizes: [3072, 80, 80, 62, 97, 10]\n",
            "Before pruning:\n",
            "loss: 1.4747068881988525 - accuracy: 0.4749000072479248 - val_loss: 1.5468600988388062 - val_accuracy: 0.45080000162124634 - penalty: 1e-05\n",
            "layer sizes: [3072, 80, 80, 62, 97, 10]\n",
            "After pruning:\n",
            "loss: 1.4751282930374146 - accuracy: 0.47477999329566956 - val_loss: 1.5475765466690063 - val_accuracy: 0.4512999951839447 - penalty: 1e-05\n",
            "layer sizes: [3072, 33, 30, 12, 48, 10]\n",
            "##########################################################\n",
            "Epoch 23/50\n",
            "Before growing:\n",
            "loss: 1.4751282930374146 - accuracy: 0.47477999329566956 - val_loss: 1.5475765466690063 - val_accuracy: 0.4512999951839447 - penalty: 1e-05\n",
            "layer sizes: [3072, 33, 30, 12, 48, 10]\n",
            "After growing:\n",
            "loss: 1.4751282930374146 - accuracy: 0.47477999329566956 - val_loss: 1.547576665878296 - val_accuracy: 0.4512999951839447 - penalty: 1.0000000000000002e-06\n",
            "layer sizes: [3072, 83, 80, 62, 98, 10]\n",
            "Before pruning:\n",
            "loss: 1.4349408149719238 - accuracy: 0.4882600009441376 - val_loss: 1.509688138961792 - val_accuracy: 0.4634000062942505 - penalty: 1.0000000000000002e-06\n",
            "layer sizes: [3072, 83, 80, 62, 98, 10]\n",
            "After pruning:\n",
            "loss: 1.4349403381347656 - accuracy: 0.4882600009441376 - val_loss: 1.5096867084503174 - val_accuracy: 0.4634000062942505 - penalty: 1.0000000000000002e-06\n",
            "layer sizes: [3072, 83, 80, 57, 74, 10]\n",
            "##########################################################\n",
            "Epoch 24/50\n",
            "Before growing:\n",
            "loss: 1.4349403381347656 - accuracy: 0.4882600009441376 - val_loss: 1.5096867084503174 - val_accuracy: 0.4634000062942505 - penalty: 1.0000000000000002e-06\n",
            "layer sizes: [3072, 83, 80, 57, 74, 10]\n",
            "After growing:\n",
            "loss: 1.4349403381347656 - accuracy: 0.4882600009441376 - val_loss: 1.5096867084503174 - val_accuracy: 0.4634000062942505 - penalty: 1.0000000000000002e-06\n",
            "layer sizes: [3072, 133, 130, 107, 124, 10]\n",
            "Before pruning:\n",
            "loss: 1.3959015607833862 - accuracy: 0.5062199831008911 - val_loss: 1.480178952217102 - val_accuracy: 0.47440001368522644 - penalty: 1.0000000000000002e-06\n",
            "layer sizes: [3072, 133, 130, 107, 124, 10]\n",
            "After pruning:\n",
            "loss: 1.3958929777145386 - accuracy: 0.5062800049781799 - val_loss: 1.4801687002182007 - val_accuracy: 0.47429999709129333 - penalty: 1.0000000000000002e-06\n",
            "layer sizes: [3072, 133, 115, 84, 88, 10]\n",
            "##########################################################\n",
            "Epoch 25/50\n",
            "Before growing:\n",
            "loss: 1.3958929777145386 - accuracy: 0.5062800049781799 - val_loss: 1.4801687002182007 - val_accuracy: 0.47429999709129333 - penalty: 1.0000000000000002e-06\n",
            "layer sizes: [3072, 133, 115, 84, 88, 10]\n",
            "After growing:\n",
            "loss: 1.3958930969238281 - accuracy: 0.5062800049781799 - val_loss: 1.4801687002182007 - val_accuracy: 0.47429999709129333 - penalty: 1.0000000000000002e-06\n",
            "layer sizes: [3072, 183, 165, 134, 138, 10]\n",
            "Before pruning:\n",
            "loss: 1.3580067157745361 - accuracy: 0.5175600051879883 - val_loss: 1.4512299299240112 - val_accuracy: 0.48190000653266907 - penalty: 1.0000000000000002e-06\n",
            "layer sizes: [3072, 183, 165, 134, 138, 10]\n",
            "After pruning:\n",
            "loss: 1.3579909801483154 - accuracy: 0.5174999833106995 - val_loss: 1.4512126445770264 - val_accuracy: 0.48179998993873596 - penalty: 1.0000000000000002e-06\n",
            "layer sizes: [3072, 180, 155, 108, 98, 10]\n",
            "##########################################################\n",
            "Epoch 26/50\n",
            "Before growing:\n",
            "loss: 1.3579909801483154 - accuracy: 0.5174999833106995 - val_loss: 1.4512126445770264 - val_accuracy: 0.48179998993873596 - penalty: 1.0000000000000002e-06\n",
            "layer sizes: [3072, 180, 155, 108, 98, 10]\n",
            "After growing:\n",
            "loss: 1.3579909801483154 - accuracy: 0.5174999833106995 - val_loss: 1.4512125253677368 - val_accuracy: 0.48179998993873596 - penalty: 1.0000000000000002e-06\n",
            "layer sizes: [3072, 230, 205, 158, 148, 10]\n",
            "Before pruning:\n",
            "loss: 1.3317155838012695 - accuracy: 0.5283799767494202 - val_loss: 1.4406826496124268 - val_accuracy: 0.48829999566078186 - penalty: 1.0000000000000002e-06\n",
            "layer sizes: [3072, 230, 205, 158, 148, 10]\n",
            "After pruning:\n",
            "loss: 1.3317437171936035 - accuracy: 0.5285199880599976 - val_loss: 1.4407209157943726 - val_accuracy: 0.4884999990463257 - penalty: 1.0000000000000002e-06\n",
            "layer sizes: [3072, 222, 178, 88, 75, 10]\n",
            "##########################################################\n",
            "Epoch 27/50\n",
            "Before growing:\n",
            "loss: 1.3317437171936035 - accuracy: 0.5285199880599976 - val_loss: 1.4407209157943726 - val_accuracy: 0.4884999990463257 - penalty: 1.0000000000000002e-06\n",
            "layer sizes: [3072, 222, 178, 88, 75, 10]\n",
            "After growing:\n",
            "loss: 1.3317437171936035 - accuracy: 0.5285199880599976 - val_loss: 1.4407209157943726 - val_accuracy: 0.4884999990463257 - penalty: 1.0000000000000002e-06\n",
            "layer sizes: [3072, 272, 228, 138, 125, 10]\n",
            "Before pruning:\n",
            "loss: 1.3097078800201416 - accuracy: 0.534500002861023 - val_loss: 1.427473783493042 - val_accuracy: 0.4921000003814697 - penalty: 1.0000000000000002e-06\n",
            "layer sizes: [3072, 272, 228, 138, 125, 10]\n",
            "After pruning:\n",
            "loss: 1.3096956014633179 - accuracy: 0.5346800088882446 - val_loss: 1.4273957014083862 - val_accuracy: 0.49219998717308044 - penalty: 1.0000000000000002e-06\n",
            "layer sizes: [3072, 236, 173, 98, 73, 10]\n",
            "##########################################################\n",
            "Epoch 28/50\n",
            "Before growing:\n",
            "loss: 1.3096956014633179 - accuracy: 0.5346800088882446 - val_loss: 1.4273957014083862 - val_accuracy: 0.49219998717308044 - penalty: 1.0000000000000002e-06\n",
            "layer sizes: [3072, 236, 173, 98, 73, 10]\n",
            "After growing:\n",
            "loss: 1.3096956014633179 - accuracy: 0.5346800088882446 - val_loss: 1.4273957014083862 - val_accuracy: 0.49219998717308044 - penalty: 1.0000000000000002e-06\n",
            "layer sizes: [3072, 286, 223, 148, 123, 10]\n",
            "Before pruning:\n",
            "loss: 1.289367914199829 - accuracy: 0.5420799851417542 - val_loss: 1.420088529586792 - val_accuracy: 0.49959999322891235 - penalty: 1.0000000000000002e-06\n",
            "layer sizes: [3072, 286, 223, 148, 123, 10]\n",
            "After pruning:\n",
            "loss: 1.2895053625106812 - accuracy: 0.5420200228691101 - val_loss: 1.4200977087020874 - val_accuracy: 0.4997999966144562 - penalty: 1.0000000000000002e-06\n",
            "layer sizes: [3072, 243, 200, 97, 77, 10]\n",
            "##########################################################\n",
            "Epoch 29/50\n",
            "Before growing:\n",
            "loss: 1.2895053625106812 - accuracy: 0.5420200228691101 - val_loss: 1.4200977087020874 - val_accuracy: 0.4997999966144562 - penalty: 1.0000000000000002e-06\n",
            "layer sizes: [3072, 243, 200, 97, 77, 10]\n",
            "After growing:\n",
            "loss: 1.2895054817199707 - accuracy: 0.5420200228691101 - val_loss: 1.4200977087020874 - val_accuracy: 0.4997999966144562 - penalty: 1.0000000000000002e-06\n",
            "layer sizes: [3072, 293, 250, 147, 127, 10]\n",
            "Before pruning:\n",
            "loss: 1.2665081024169922 - accuracy: 0.5515000224113464 - val_loss: 1.4087936878204346 - val_accuracy: 0.4977000057697296 - penalty: 1.0000000000000002e-06\n",
            "layer sizes: [3072, 293, 250, 147, 127, 10]\n",
            "After pruning:\n",
            "loss: 1.2665753364562988 - accuracy: 0.5515199899673462 - val_loss: 1.4088596105575562 - val_accuracy: 0.49790000915527344 - penalty: 1.0000000000000002e-06\n",
            "layer sizes: [3072, 241, 210, 105, 108, 10]\n",
            "##########################################################\n",
            "Epoch 30/50\n",
            "Before growing:\n",
            "loss: 1.2665753364562988 - accuracy: 0.5515199899673462 - val_loss: 1.4088596105575562 - val_accuracy: 0.49790000915527344 - penalty: 1.0000000000000002e-06\n",
            "layer sizes: [3072, 241, 210, 105, 108, 10]\n",
            "After growing:\n",
            "loss: 1.2665753364562988 - accuracy: 0.5515199899673462 - val_loss: 1.4088597297668457 - val_accuracy: 0.49790000915527344 - penalty: 1.0000000000000002e-06\n",
            "layer sizes: [3072, 291, 260, 155, 158, 10]\n",
            "Before pruning:\n",
            "loss: 1.258278489112854 - accuracy: 0.5540400147438049 - val_loss: 1.4098691940307617 - val_accuracy: 0.5026000142097473 - penalty: 1.0000000000000002e-06\n",
            "layer sizes: [3072, 291, 260, 155, 158, 10]\n",
            "After pruning:\n",
            "loss: 1.2582627534866333 - accuracy: 0.553879976272583 - val_loss: 1.409778356552124 - val_accuracy: 0.5024999976158142 - penalty: 1.0000000000000002e-06\n",
            "layer sizes: [3072, 255, 207, 99, 67, 10]\n",
            "##########################################################\n",
            "Epoch 31/50\n",
            "Before growing:\n",
            "loss: 1.2582627534866333 - accuracy: 0.553879976272583 - val_loss: 1.409778356552124 - val_accuracy: 0.5024999976158142 - penalty: 1.0000000000000002e-06\n",
            "layer sizes: [3072, 255, 207, 99, 67, 10]\n",
            "After growing:\n",
            "loss: 1.2582628726959229 - accuracy: 0.553879976272583 - val_loss: 1.409778356552124 - val_accuracy: 0.5024999976158142 - penalty: 1.0000000000000002e-07\n",
            "layer sizes: [3072, 306, 257, 149, 117, 10]\n",
            "Before pruning:\n",
            "loss: 1.2499921321868896 - accuracy: 0.5590800046920776 - val_loss: 1.4143530130386353 - val_accuracy: 0.5002999901771545 - penalty: 1.0000000000000002e-07\n",
            "layer sizes: [3072, 306, 257, 149, 117, 10]\n",
            "After pruning:\n",
            "loss: 1.2499921321868896 - accuracy: 0.5590800046920776 - val_loss: 1.4143530130386353 - val_accuracy: 0.5002999901771545 - penalty: 1.0000000000000002e-07\n",
            "layer sizes: [3072, 306, 257, 149, 117, 10]\n",
            "##########################################################\n",
            "Epoch 32/50\n",
            "Before growing:\n",
            "loss: 1.2499921321868896 - accuracy: 0.5590800046920776 - val_loss: 1.4143530130386353 - val_accuracy: 0.5002999901771545 - penalty: 1.0000000000000002e-07\n",
            "layer sizes: [3072, 306, 257, 149, 117, 10]\n",
            "After growing:\n",
            "loss: 1.2499921321868896 - accuracy: 0.5590800046920776 - val_loss: 1.4143530130386353 - val_accuracy: 0.5002999901771545 - penalty: 1.0000000000000002e-07\n",
            "layer sizes: [3072, 367, 308, 199, 167, 10]\n",
            "Before pruning:\n",
            "loss: 1.2217375040054321 - accuracy: 0.56632000207901 - val_loss: 1.402287244796753 - val_accuracy: 0.5054000020027161 - penalty: 1.0000000000000002e-07\n",
            "layer sizes: [3072, 367, 308, 199, 167, 10]\n",
            "After pruning:\n",
            "loss: 1.221737265586853 - accuracy: 0.56632000207901 - val_loss: 1.4022868871688843 - val_accuracy: 0.5054000020027161 - penalty: 1.0000000000000002e-07\n",
            "layer sizes: [3072, 367, 308, 199, 159, 10]\n",
            "##########################################################\n",
            "Epoch 33/50\n",
            "Before growing:\n",
            "loss: 1.221737265586853 - accuracy: 0.56632000207901 - val_loss: 1.4022868871688843 - val_accuracy: 0.5054000020027161 - penalty: 1.0000000000000002e-07\n",
            "layer sizes: [3072, 367, 308, 199, 159, 10]\n",
            "After growing:\n",
            "loss: 1.221737265586853 - accuracy: 0.56632000207901 - val_loss: 1.4022867679595947 - val_accuracy: 0.5054000020027161 - penalty: 1.0000000000000002e-07\n",
            "layer sizes: [3072, 440, 369, 249, 209, 10]\n",
            "Before pruning:\n",
            "loss: 1.2089563608169556 - accuracy: 0.5735599994659424 - val_loss: 1.3980793952941895 - val_accuracy: 0.5091999769210815 - penalty: 1.0000000000000002e-07\n",
            "layer sizes: [3072, 440, 369, 249, 209, 10]\n",
            "After pruning:\n",
            "loss: 1.2089561223983765 - accuracy: 0.5735599994659424 - val_loss: 1.3980792760849 - val_accuracy: 0.5091999769210815 - penalty: 1.0000000000000002e-07\n",
            "layer sizes: [3072, 440, 369, 249, 200, 10]\n",
            "##########################################################\n",
            "Epoch 34/50\n",
            "Before growing:\n",
            "loss: 1.2089561223983765 - accuracy: 0.5735599994659424 - val_loss: 1.3980792760849 - val_accuracy: 0.5091999769210815 - penalty: 1.0000000000000002e-07\n",
            "layer sizes: [3072, 440, 369, 249, 200, 10]\n",
            "After growing:\n",
            "loss: 1.2089561223983765 - accuracy: 0.5735599994659424 - val_loss: 1.3980791568756104 - val_accuracy: 0.5091999769210815 - penalty: 1.0000000000000002e-07\n",
            "layer sizes: [3072, 528, 442, 299, 250, 10]\n",
            "Before pruning:\n",
            "loss: 1.1975187063217163 - accuracy: 0.5762400031089783 - val_loss: 1.398771047592163 - val_accuracy: 0.5094000101089478 - penalty: 1.0000000000000002e-07\n",
            "layer sizes: [3072, 528, 442, 299, 250, 10]\n",
            "After pruning:\n",
            "loss: 1.1975301504135132 - accuracy: 0.5762199759483337 - val_loss: 1.3987796306610107 - val_accuracy: 0.5094000101089478 - penalty: 1.0000000000000002e-07\n",
            "layer sizes: [3072, 526, 442, 299, 199, 10]\n",
            "##########################################################\n",
            "Epoch 35/50\n",
            "Before growing:\n",
            "loss: 1.1975301504135132 - accuracy: 0.5762199759483337 - val_loss: 1.3987796306610107 - val_accuracy: 0.5094000101089478 - penalty: 1.0000000000000002e-07\n",
            "layer sizes: [3072, 526, 442, 299, 199, 10]\n",
            "After growing:\n",
            "loss: 1.1975301504135132 - accuracy: 0.5762199759483337 - val_loss: 1.3987796306610107 - val_accuracy: 0.5094000101089478 - penalty: 1.0000000000000004e-08\n",
            "layer sizes: [3072, 631, 530, 358, 249, 10]\n",
            "Before pruning:\n",
            "loss: 1.1732268333435059 - accuracy: 0.5859799981117249 - val_loss: 1.3816866874694824 - val_accuracy: 0.517300009727478 - penalty: 1.0000000000000004e-08\n",
            "layer sizes: [3072, 631, 530, 358, 249, 10]\n",
            "After pruning:\n",
            "loss: 1.1732268333435059 - accuracy: 0.5859799981117249 - val_loss: 1.3816866874694824 - val_accuracy: 0.517300009727478 - penalty: 1.0000000000000004e-08\n",
            "layer sizes: [3072, 631, 530, 358, 249, 10]\n",
            "##########################################################\n",
            "Epoch 36/50\n",
            "Before growing:\n",
            "loss: 1.1732268333435059 - accuracy: 0.5859799981117249 - val_loss: 1.3816866874694824 - val_accuracy: 0.517300009727478 - penalty: 1.0000000000000004e-08\n",
            "layer sizes: [3072, 631, 530, 358, 249, 10]\n",
            "After growing:\n",
            "loss: 1.1732268333435059 - accuracy: 0.5859799981117249 - val_loss: 1.3816866874694824 - val_accuracy: 0.517300009727478 - penalty: 1.0000000000000004e-08\n",
            "layer sizes: [3072, 757, 636, 429, 299, 10]\n",
            "Before pruning:\n",
            "loss: 1.1441746950149536 - accuracy: 0.5954399704933167 - val_loss: 1.3667200803756714 - val_accuracy: 0.5209000110626221 - penalty: 1.0000000000000004e-08\n",
            "layer sizes: [3072, 757, 636, 429, 299, 10]\n",
            "After pruning:\n",
            "loss: 1.1441746950149536 - accuracy: 0.5954399704933167 - val_loss: 1.3667200803756714 - val_accuracy: 0.5209000110626221 - penalty: 1.0000000000000004e-08\n",
            "layer sizes: [3072, 757, 636, 429, 299, 10]\n",
            "##########################################################\n",
            "Epoch 37/50\n",
            "Before growing:\n",
            "loss: 1.1441746950149536 - accuracy: 0.5954399704933167 - val_loss: 1.3667200803756714 - val_accuracy: 0.5209000110626221 - penalty: 1.0000000000000004e-08\n",
            "layer sizes: [3072, 757, 636, 429, 299, 10]\n",
            "After growing:\n",
            "loss: 1.1441746950149536 - accuracy: 0.5954399704933167 - val_loss: 1.3667200803756714 - val_accuracy: 0.5209000110626221 - penalty: 1.0000000000000004e-08\n",
            "layer sizes: [3072, 908, 763, 514, 358, 10]\n",
            "Before pruning:\n",
            "loss: 1.139864444732666 - accuracy: 0.5959799885749817 - val_loss: 1.3734679222106934 - val_accuracy: 0.5264999866485596 - penalty: 1.0000000000000004e-08\n",
            "layer sizes: [3072, 908, 763, 514, 358, 10]\n",
            "After pruning:\n",
            "loss: 1.139864444732666 - accuracy: 0.5959799885749817 - val_loss: 1.3734679222106934 - val_accuracy: 0.5264999866485596 - penalty: 1.0000000000000004e-08\n",
            "layer sizes: [3072, 908, 763, 514, 358, 10]\n",
            "##########################################################\n",
            "Epoch 38/50\n",
            "Before growing:\n",
            "loss: 1.139864444732666 - accuracy: 0.5959799885749817 - val_loss: 1.3734679222106934 - val_accuracy: 0.5264999866485596 - penalty: 1.0000000000000004e-08\n",
            "layer sizes: [3072, 908, 763, 514, 358, 10]\n",
            "After growing:\n",
            "loss: 1.139864444732666 - accuracy: 0.5959799885749817 - val_loss: 1.3734679222106934 - val_accuracy: 0.5264999866485596 - penalty: 1.0000000000000005e-09\n",
            "layer sizes: [3072, 1089, 915, 616, 429, 10]\n",
            "Before pruning:\n",
            "loss: 1.1308801174163818 - accuracy: 0.5980799794197083 - val_loss: 1.3821003437042236 - val_accuracy: 0.5169000029563904 - penalty: 1.0000000000000005e-09\n",
            "layer sizes: [3072, 1089, 915, 616, 429, 10]\n",
            "After pruning:\n",
            "loss: 1.1308801174163818 - accuracy: 0.5980799794197083 - val_loss: 1.3821003437042236 - val_accuracy: 0.5169000029563904 - penalty: 1.0000000000000005e-09\n",
            "layer sizes: [3072, 1089, 915, 616, 429, 10]\n",
            "##########################################################\n",
            "Epoch 39/50\n",
            "Before growing:\n",
            "loss: 1.1308801174163818 - accuracy: 0.5980799794197083 - val_loss: 1.3821003437042236 - val_accuracy: 0.5169000029563904 - penalty: 1.0000000000000005e-09\n",
            "layer sizes: [3072, 1089, 915, 616, 429, 10]\n",
            "After growing:\n",
            "loss: 1.1308799982070923 - accuracy: 0.5980799794197083 - val_loss: 1.3821003437042236 - val_accuracy: 0.5169000029563904 - penalty: 1.0000000000000005e-09\n",
            "layer sizes: [3072, 1306, 1098, 739, 514, 10]\n",
            "Before pruning:\n",
            "loss: 1.1096429824829102 - accuracy: 0.6060400009155273 - val_loss: 1.372436285018921 - val_accuracy: 0.5259000062942505 - penalty: 1.0000000000000005e-09\n",
            "layer sizes: [3072, 1306, 1098, 739, 514, 10]\n",
            "After pruning:\n",
            "loss: 1.1096429824829102 - accuracy: 0.6060400009155273 - val_loss: 1.372436285018921 - val_accuracy: 0.5259000062942505 - penalty: 1.0000000000000005e-09\n",
            "layer sizes: [3072, 1306, 1098, 739, 514, 10]\n",
            "##########################################################\n",
            "Epoch 40/50\n",
            "Before growing:\n",
            "loss: 1.1096429824829102 - accuracy: 0.6060400009155273 - val_loss: 1.372436285018921 - val_accuracy: 0.5259000062942505 - penalty: 1.0000000000000005e-09\n",
            "layer sizes: [3072, 1306, 1098, 739, 514, 10]\n",
            "After growing:\n",
            "loss: 1.1096429824829102 - accuracy: 0.6060400009155273 - val_loss: 1.372436285018921 - val_accuracy: 0.5259000062942505 - penalty: 1.0000000000000005e-09\n",
            "layer sizes: [3072, 1567, 1317, 886, 616, 10]\n",
            "Before pruning:\n",
            "loss: 1.0777684450149536 - accuracy: 0.610040009021759 - val_loss: 1.349364161491394 - val_accuracy: 0.5254999995231628 - penalty: 1.0000000000000005e-09\n",
            "layer sizes: [3072, 1567, 1317, 886, 616, 10]\n",
            "After pruning:\n",
            "loss: 1.0777684450149536 - accuracy: 0.610040009021759 - val_loss: 1.349364161491394 - val_accuracy: 0.5254999995231628 - penalty: 1.0000000000000005e-09\n",
            "layer sizes: [3072, 1567, 1317, 886, 616, 10]\n",
            "##########################################################\n",
            "Epoch 41/50\n",
            "Before growing:\n",
            "loss: 1.0777684450149536 - accuracy: 0.610040009021759 - val_loss: 1.349364161491394 - val_accuracy: 0.5254999995231628 - penalty: 1.0000000000000005e-09\n",
            "layer sizes: [3072, 1567, 1317, 886, 616, 10]\n",
            "After growing:\n",
            "loss: 1.0777684450149536 - accuracy: 0.610040009021759 - val_loss: 1.3493640422821045 - val_accuracy: 0.5254999995231628 - penalty: 1.0000000000000005e-09\n",
            "layer sizes: [3072, 1880, 1580, 1063, 739, 10]\n",
            "Before pruning:\n",
            "loss: 1.0937949419021606 - accuracy: 0.6132400035858154 - val_loss: 1.399574637413025 - val_accuracy: 0.5235999822616577 - penalty: 1.0000000000000005e-09\n",
            "layer sizes: [3072, 1880, 1580, 1063, 739, 10]\n",
            "After pruning:\n",
            "loss: 1.0937949419021606 - accuracy: 0.6132400035858154 - val_loss: 1.399574637413025 - val_accuracy: 0.5235999822616577 - penalty: 1.0000000000000005e-09\n",
            "layer sizes: [3072, 1880, 1580, 1063, 739, 10]\n",
            "##########################################################\n",
            "Epoch 42/50\n",
            "Before growing:\n",
            "loss: 1.0937949419021606 - accuracy: 0.6132400035858154 - val_loss: 1.399574637413025 - val_accuracy: 0.5235999822616577 - penalty: 1.0000000000000005e-09\n",
            "layer sizes: [3072, 1880, 1580, 1063, 739, 10]\n",
            "After growing:\n",
            "loss: 1.0937949419021606 - accuracy: 0.6132400035858154 - val_loss: 1.3995747566223145 - val_accuracy: 0.5235999822616577 - penalty: 1.0000000000000006e-10\n",
            "layer sizes: [3072, 2256, 1896, 1275, 886, 10]\n",
            "Before pruning:\n",
            "loss: 1.1086931228637695 - accuracy: 0.6081399917602539 - val_loss: 1.431315302848816 - val_accuracy: 0.5241000056266785 - penalty: 1.0000000000000006e-10\n",
            "layer sizes: [3072, 2256, 1896, 1275, 886, 10]\n",
            "After pruning:\n",
            "loss: 1.1086931228637695 - accuracy: 0.6081399917602539 - val_loss: 1.431315302848816 - val_accuracy: 0.5241000056266785 - penalty: 1.0000000000000006e-10\n",
            "layer sizes: [3072, 2256, 1896, 1275, 886, 10]\n",
            "##########################################################\n",
            "Epoch 43/50\n",
            "Before growing:\n",
            "loss: 1.1086931228637695 - accuracy: 0.6081399917602539 - val_loss: 1.431315302848816 - val_accuracy: 0.5241000056266785 - penalty: 1.0000000000000006e-10\n",
            "layer sizes: [3072, 2256, 1896, 1275, 886, 10]\n",
            "After growing:\n",
            "loss: 1.10869300365448 - accuracy: 0.6081399917602539 - val_loss: 1.431315302848816 - val_accuracy: 0.5241000056266785 - penalty: 1.0000000000000006e-10\n",
            "layer sizes: [3072, 2707, 2275, 1530, 1063, 10]\n",
            "Before pruning:\n",
            "loss: 1.0340038537979126 - accuracy: 0.6299600005149841 - val_loss: 1.3525187969207764 - val_accuracy: 0.5354999899864197 - penalty: 1.0000000000000006e-10\n",
            "layer sizes: [3072, 2707, 2275, 1530, 1063, 10]\n",
            "After pruning:\n",
            "loss: 1.0340038537979126 - accuracy: 0.6299600005149841 - val_loss: 1.3525187969207764 - val_accuracy: 0.5354999899864197 - penalty: 1.0000000000000006e-10\n",
            "layer sizes: [3072, 2707, 2275, 1530, 1063, 10]\n",
            "##########################################################\n",
            "Epoch 44/50\n",
            "Before growing:\n",
            "loss: 1.0340038537979126 - accuracy: 0.6299600005149841 - val_loss: 1.3525187969207764 - val_accuracy: 0.5354999899864197 - penalty: 1.0000000000000006e-10\n",
            "layer sizes: [3072, 2707, 2275, 1530, 1063, 10]\n",
            "After growing:\n",
            "loss: 1.0340038537979126 - accuracy: 0.6299600005149841 - val_loss: 1.3525185585021973 - val_accuracy: 0.5354999899864197 - penalty: 1.0000000000000006e-10\n",
            "layer sizes: [3072, 3248, 2730, 1836, 1275, 10]\n",
            "Before pruning:\n",
            "loss: 1.0874229669570923 - accuracy: 0.611519992351532 - val_loss: 1.3770283460617065 - val_accuracy: 0.5254999995231628 - penalty: 1.0000000000000006e-10\n",
            "layer sizes: [3072, 3248, 2730, 1836, 1275, 10]\n",
            "After pruning:\n",
            "loss: 1.0874229669570923 - accuracy: 0.611519992351532 - val_loss: 1.3770283460617065 - val_accuracy: 0.5254999995231628 - penalty: 1.0000000000000006e-10\n",
            "layer sizes: [3072, 3248, 2730, 1836, 1275, 10]\n",
            "##########################################################\n",
            "Epoch 45/50\n",
            "Before growing:\n",
            "loss: 1.0874229669570923 - accuracy: 0.611519992351532 - val_loss: 1.3770283460617065 - val_accuracy: 0.5254999995231628 - penalty: 1.0000000000000006e-10\n",
            "layer sizes: [3072, 3248, 2730, 1836, 1275, 10]\n",
            "After growing:\n",
            "loss: 1.0874228477478027 - accuracy: 0.611519992351532 - val_loss: 1.3770281076431274 - val_accuracy: 0.5254999995231628 - penalty: 1.0000000000000006e-10\n",
            "layer sizes: [3072, 3897, 3276, 2203, 1530, 10]\n",
            "Before pruning:\n",
            "loss: 1.0345796346664429 - accuracy: 0.6231600046157837 - val_loss: 1.3914750814437866 - val_accuracy: 0.5182999968528748 - penalty: 1.0000000000000006e-10\n",
            "layer sizes: [3072, 3897, 3276, 2203, 1530, 10]\n",
            "After pruning:\n",
            "loss: 1.0345796346664429 - accuracy: 0.6231600046157837 - val_loss: 1.3914750814437866 - val_accuracy: 0.5182999968528748 - penalty: 1.0000000000000006e-10\n",
            "layer sizes: [3072, 3897, 3276, 2203, 1530, 10]\n",
            "##########################################################\n",
            "Epoch 46/50\n",
            "Before growing:\n",
            "loss: 1.0345796346664429 - accuracy: 0.6231600046157837 - val_loss: 1.3914750814437866 - val_accuracy: 0.5182999968528748 - penalty: 1.0000000000000006e-10\n",
            "layer sizes: [3072, 3897, 3276, 2203, 1530, 10]\n",
            "After growing:\n",
            "loss: 1.0345795154571533 - accuracy: 0.6231600046157837 - val_loss: 1.3914748430252075 - val_accuracy: 0.5182999968528748 - penalty: 1.0000000000000006e-10\n",
            "layer sizes: [3072, 4676, 3931, 2643, 1836, 10]\n",
            "Before pruning:\n",
            "loss: 1.053978443145752 - accuracy: 0.6288599967956543 - val_loss: 1.3523423671722412 - val_accuracy: 0.5295000076293945 - penalty: 1.0000000000000006e-10\n",
            "layer sizes: [3072, 4676, 3931, 2643, 1836, 10]\n",
            "After pruning:\n",
            "loss: 1.053978443145752 - accuracy: 0.6288599967956543 - val_loss: 1.3523423671722412 - val_accuracy: 0.5295000076293945 - penalty: 1.0000000000000006e-10\n",
            "layer sizes: [3072, 4676, 3931, 2643, 1836, 10]\n",
            "##########################################################\n",
            "Epoch 47/50\n",
            "Before growing:\n",
            "loss: 1.053978443145752 - accuracy: 0.6288599967956543 - val_loss: 1.3523423671722412 - val_accuracy: 0.5295000076293945 - penalty: 1.0000000000000006e-10\n",
            "layer sizes: [3072, 4676, 3931, 2643, 1836, 10]\n",
            "After growing:\n",
            "loss: 1.0539788007736206 - accuracy: 0.6288599967956543 - val_loss: 1.3523426055908203 - val_accuracy: 0.5295000076293945 - penalty: 1.0000000000000006e-10\n",
            "layer sizes: [3072, 5611, 4717, 3171, 2203, 10]\n",
            "Before pruning:\n",
            "loss: 1.297102451324463 - accuracy: 0.5451200008392334 - val_loss: 1.4886653423309326 - val_accuracy: 0.4934000074863434 - penalty: 1.0000000000000006e-10\n",
            "layer sizes: [3072, 5611, 4717, 3171, 2203, 10]\n",
            "After pruning:\n",
            "loss: 1.297102451324463 - accuracy: 0.5451200008392334 - val_loss: 1.4886653423309326 - val_accuracy: 0.4934000074863434 - penalty: 1.0000000000000006e-10\n",
            "layer sizes: [3072, 5611, 4717, 3171, 2203, 10]\n",
            "##########################################################\n",
            "Epoch 48/50\n",
            "Before growing:\n",
            "loss: 1.297102451324463 - accuracy: 0.5451200008392334 - val_loss: 1.4886653423309326 - val_accuracy: 0.4934000074863434 - penalty: 1.0000000000000006e-10\n",
            "layer sizes: [3072, 5611, 4717, 3171, 2203, 10]\n",
            "After growing:\n",
            "loss: 1.2971022129058838 - accuracy: 0.5451200008392334 - val_loss: 1.4886648654937744 - val_accuracy: 0.4934000074863434 - penalty: 1.0000000000000006e-10\n",
            "layer sizes: [3072, 6733, 5660, 3805, 2643, 10]\n",
            "Before pruning:\n",
            "loss: 1.1188875436782837 - accuracy: 0.5988799929618835 - val_loss: 1.4333966970443726 - val_accuracy: 0.510200023651123 - penalty: 1.0000000000000006e-10\n",
            "layer sizes: [3072, 6733, 5660, 3805, 2643, 10]\n",
            "After pruning:\n",
            "loss: 1.1188875436782837 - accuracy: 0.5988799929618835 - val_loss: 1.4333966970443726 - val_accuracy: 0.510200023651123 - penalty: 1.0000000000000006e-10\n",
            "layer sizes: [3072, 6733, 5660, 3805, 2643, 10]\n",
            "##########################################################\n",
            "Epoch 49/50\n",
            "Before growing:\n",
            "loss: 1.1188875436782837 - accuracy: 0.5988799929618835 - val_loss: 1.4333966970443726 - val_accuracy: 0.510200023651123 - penalty: 1.0000000000000006e-10\n",
            "layer sizes: [3072, 6733, 5660, 3805, 2643, 10]\n",
            "After growing:\n",
            "loss: 1.1188876628875732 - accuracy: 0.5988799929618835 - val_loss: 1.4333966970443726 - val_accuracy: 0.510200023651123 - penalty: 1.0000000000000006e-10\n",
            "layer sizes: [3072, 8079, 6792, 4566, 3171, 10]\n",
            "Before pruning:\n",
            "loss: 1.2127140760421753 - accuracy: 0.5672600269317627 - val_loss: 1.4324830770492554 - val_accuracy: 0.5016000270843506 - penalty: 1.0000000000000006e-10\n",
            "layer sizes: [3072, 8079, 6792, 4566, 3171, 10]\n",
            "After pruning:\n",
            "loss: 1.2127140760421753 - accuracy: 0.5672600269317627 - val_loss: 1.4324830770492554 - val_accuracy: 0.5016000270843506 - penalty: 1.0000000000000006e-10\n",
            "layer sizes: [3072, 8079, 6792, 4566, 3171, 10]\n",
            "##########################################################\n",
            "Epoch 50/50\n",
            "Before growing:\n",
            "loss: 1.2127140760421753 - accuracy: 0.5672600269317627 - val_loss: 1.4324830770492554 - val_accuracy: 0.5016000270843506 - penalty: 1.0000000000000006e-10\n",
            "layer sizes: [3072, 8079, 6792, 4566, 3171, 10]\n",
            "After growing:\n",
            "loss: 1.21271550655365 - accuracy: 0.5672600269317627 - val_loss: 1.4324841499328613 - val_accuracy: 0.5016000270843506 - penalty: 1.0000000000000006e-10\n",
            "layer sizes: [3072, 9694, 8150, 5479, 3805, 10]\n",
            "Before pruning:\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zd1PcfL2wwKi"
      },
      "source": [
        "epochs = 50\n",
        "self_scaling_epochs = 50\n",
        "batch_size = 32\n",
        "min_new_neurons = 200"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZ7G4J7twmL9",
        "outputId": "cec88786-f2f1-482f-ae04-1c4b150fd426",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# SCALING FACTOR *= 100\n",
        "\n",
        "%%time\n",
        "\n",
        "model = SSModel(layer_sizes=[3072, 300, 300, 300, 300, 10], activation='selu', regularization_penalty=0.0001, regularization_method='weighted_l1', kernel_initializer='lecun_normal')\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002)\n",
        "\n",
        "train_model(model, X_train_norm, y_train, optimizer, epochs, self_scaling_epochs, batch_size, min_new_neurons, validation_data=(X_test_norm, y_test), regularization_penalty_multiplier=0.1, growth_percentage=0.5)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "##########################################################\n",
            "Epoch 1/50\n",
            "Before growing:\n",
            "loss: 2.5309395790100098 - accuracy: 0.13176000118255615 - val_loss: 2.52573561668396 - val_accuracy: 0.1274999976158142 - penalty: 0.0001\n",
            "hidden layer sizes: [300, 300, 300, 300], total neurons: 1200\n",
            "After growing:\n",
            "loss: 2.5339834690093994 - accuracy: 0.1315000057220459 - val_loss: 2.528728723526001 - val_accuracy: 0.12700000405311584 - penalty: 0.0001\n",
            "hidden layer sizes: [500, 500, 500, 500], total neurons: 2000\n",
            "Before pruning:\n",
            "loss: 1.8360446691513062 - accuracy: 0.3235200047492981 - val_loss: 1.8377223014831543 - val_accuracy: 0.3240000009536743 - penalty: 0.0001\n",
            "hidden layer sizes: [500, 500, 500, 500], total neurons: 2000\n",
            "After pruning:\n",
            "loss: 1.8354604244232178 - accuracy: 0.323419988155365 - val_loss: 1.8373503684997559 - val_accuracy: 0.3246999979019165 - penalty: 0.0001\n",
            "hidden layer sizes: [14, 53, 37, 67], total neurons: 171\n",
            "##########################################################\n",
            "Epoch 2/50\n",
            "Before growing:\n",
            "loss: 1.8354604244232178 - accuracy: 0.323419988155365 - val_loss: 1.8373503684997559 - val_accuracy: 0.3246999979019165 - penalty: 0.0001\n",
            "hidden layer sizes: [14, 53, 37, 67], total neurons: 171\n",
            "After growing:\n",
            "loss: 1.835081696510315 - accuracy: 0.3237999975681305 - val_loss: 1.8369765281677246 - val_accuracy: 0.32409998774528503 - penalty: 0.0001\n",
            "hidden layer sizes: [214, 253, 237, 267], total neurons: 971\n",
            "Before pruning:\n",
            "loss: 1.7601996660232544 - accuracy: 0.3580400049686432 - val_loss: 1.7683172225952148 - val_accuracy: 0.3569999933242798 - penalty: 0.0001\n",
            "hidden layer sizes: [214, 253, 237, 267], total neurons: 971\n",
            "After pruning:\n",
            "loss: 1.760257363319397 - accuracy: 0.35778000950813293 - val_loss: 1.7683500051498413 - val_accuracy: 0.3569999933242798 - penalty: 0.0001\n",
            "hidden layer sizes: [11, 38, 20, 52], total neurons: 121\n",
            "##########################################################\n",
            "Epoch 3/50\n",
            "Before growing:\n",
            "loss: 1.760257363319397 - accuracy: 0.35778000950813293 - val_loss: 1.7683500051498413 - val_accuracy: 0.3569999933242798 - penalty: 0.0001\n",
            "hidden layer sizes: [11, 38, 20, 52], total neurons: 121\n",
            "After growing:\n",
            "loss: 1.7600888013839722 - accuracy: 0.35806000232696533 - val_loss: 1.7682878971099854 - val_accuracy: 0.3578000068664551 - penalty: 0.0001\n",
            "hidden layer sizes: [211, 238, 220, 252], total neurons: 921\n",
            "Before pruning:\n",
            "loss: 1.7236216068267822 - accuracy: 0.3706800043582916 - val_loss: 1.730420708656311 - val_accuracy: 0.36970001459121704 - penalty: 0.0001\n",
            "hidden layer sizes: [211, 238, 220, 252], total neurons: 921\n",
            "After pruning:\n",
            "loss: 1.7236157655715942 - accuracy: 0.37081998586654663 - val_loss: 1.7304025888442993 - val_accuracy: 0.36959999799728394 - penalty: 0.0001\n",
            "hidden layer sizes: [10, 36, 18, 50], total neurons: 114\n",
            "##########################################################\n",
            "Epoch 4/50\n",
            "Before growing:\n",
            "loss: 1.7236157655715942 - accuracy: 0.37081998586654663 - val_loss: 1.7304025888442993 - val_accuracy: 0.36959999799728394 - penalty: 0.0001\n",
            "hidden layer sizes: [10, 36, 18, 50], total neurons: 114\n",
            "After growing:\n",
            "loss: 1.7233302593231201 - accuracy: 0.3711400032043457 - val_loss: 1.7302857637405396 - val_accuracy: 0.37059998512268066 - penalty: 0.0001\n",
            "hidden layer sizes: [210, 236, 218, 250], total neurons: 914\n",
            "Before pruning:\n",
            "loss: 1.6927613019943237 - accuracy: 0.38214001059532166 - val_loss: 1.7047933340072632 - val_accuracy: 0.37610000371932983 - penalty: 0.0001\n",
            "hidden layer sizes: [210, 236, 218, 250], total neurons: 914\n",
            "After pruning:\n",
            "loss: 1.6927499771118164 - accuracy: 0.3821600079536438 - val_loss: 1.7047749757766724 - val_accuracy: 0.37619999051094055 - penalty: 0.0001\n",
            "hidden layer sizes: [10, 32, 16, 46], total neurons: 104\n",
            "##########################################################\n",
            "Epoch 5/50\n",
            "Before growing:\n",
            "loss: 1.6927499771118164 - accuracy: 0.3821600079536438 - val_loss: 1.7047749757766724 - val_accuracy: 0.37619999051094055 - penalty: 0.0001\n",
            "hidden layer sizes: [10, 32, 16, 46], total neurons: 104\n",
            "After growing:\n",
            "loss: 1.6924904584884644 - accuracy: 0.38242000341415405 - val_loss: 1.7045148611068726 - val_accuracy: 0.375900000333786 - penalty: 0.0001\n",
            "hidden layer sizes: [210, 232, 216, 246], total neurons: 904\n",
            "Before pruning:\n",
            "loss: 1.6803730726242065 - accuracy: 0.390639990568161 - val_loss: 1.696803092956543 - val_accuracy: 0.38499999046325684 - penalty: 0.0001\n",
            "hidden layer sizes: [210, 232, 216, 246], total neurons: 904\n",
            "After pruning:\n",
            "loss: 1.6806927919387817 - accuracy: 0.3902199864387512 - val_loss: 1.6971038579940796 - val_accuracy: 0.3856000006198883 - penalty: 0.0001\n",
            "hidden layer sizes: [10, 27, 16, 45], total neurons: 98\n",
            "##########################################################\n",
            "Epoch 6/50\n",
            "Before growing:\n",
            "loss: 1.6806927919387817 - accuracy: 0.3902199864387512 - val_loss: 1.6971038579940796 - val_accuracy: 0.3856000006198883 - penalty: 0.0001\n",
            "hidden layer sizes: [10, 27, 16, 45], total neurons: 98\n",
            "After growing:\n",
            "loss: 1.6804486513137817 - accuracy: 0.38995999097824097 - val_loss: 1.6967917680740356 - val_accuracy: 0.3862999975681305 - penalty: 0.0001\n",
            "hidden layer sizes: [210, 227, 216, 245], total neurons: 898\n",
            "Before pruning:\n",
            "loss: 1.6939568519592285 - accuracy: 0.38420000672340393 - val_loss: 1.7088689804077148 - val_accuracy: 0.37369999289512634 - penalty: 0.0001\n",
            "hidden layer sizes: [210, 227, 216, 245], total neurons: 898\n",
            "After pruning:\n",
            "loss: 1.6933345794677734 - accuracy: 0.38433998823165894 - val_loss: 1.7082769870758057 - val_accuracy: 0.374099999666214 - penalty: 0.0001\n",
            "hidden layer sizes: [11, 25, 16, 44], total neurons: 96\n",
            "##########################################################\n",
            "Epoch 7/50\n",
            "Before growing:\n",
            "loss: 1.6933345794677734 - accuracy: 0.38433998823165894 - val_loss: 1.7082769870758057 - val_accuracy: 0.374099999666214 - penalty: 0.0001\n",
            "hidden layer sizes: [11, 25, 16, 44], total neurons: 96\n",
            "After growing:\n",
            "loss: 1.6934932470321655 - accuracy: 0.3844600021839142 - val_loss: 1.7081867456436157 - val_accuracy: 0.37439998984336853 - penalty: 1e-05\n",
            "hidden layer sizes: [211, 225, 216, 244], total neurons: 896\n",
            "Before pruning:\n",
            "loss: 1.6365512609481812 - accuracy: 0.40573999285697937 - val_loss: 1.6617122888565063 - val_accuracy: 0.3917999863624573 - penalty: 1e-05\n",
            "hidden layer sizes: [211, 225, 216, 244], total neurons: 896\n",
            "After pruning:\n",
            "loss: 1.637028694152832 - accuracy: 0.4060800075531006 - val_loss: 1.6621484756469727 - val_accuracy: 0.3921999931335449 - penalty: 1e-05\n",
            "hidden layer sizes: [26, 28, 16, 52], total neurons: 122\n",
            "##########################################################\n",
            "Epoch 8/50\n",
            "Before growing:\n",
            "loss: 1.637028694152832 - accuracy: 0.4060800075531006 - val_loss: 1.6621484756469727 - val_accuracy: 0.3921999931335449 - penalty: 1e-05\n",
            "hidden layer sizes: [26, 28, 16, 52], total neurons: 122\n",
            "After growing:\n",
            "loss: 1.6368207931518555 - accuracy: 0.40623998641967773 - val_loss: 1.661991000175476 - val_accuracy: 0.39250001311302185 - penalty: 1e-05\n",
            "hidden layer sizes: [226, 228, 216, 252], total neurons: 922\n",
            "Before pruning:\n",
            "loss: 1.5943719148635864 - accuracy: 0.4223000109195709 - val_loss: 1.6260836124420166 - val_accuracy: 0.4108000099658966 - penalty: 1e-05\n",
            "hidden layer sizes: [226, 228, 216, 252], total neurons: 922\n",
            "After pruning:\n",
            "loss: 1.5939043760299683 - accuracy: 0.4224399924278259 - val_loss: 1.6257449388504028 - val_accuracy: 0.4106999933719635 - penalty: 1e-05\n",
            "hidden layer sizes: [34, 27, 16, 82], total neurons: 159\n",
            "##########################################################\n",
            "Epoch 9/50\n",
            "Before growing:\n",
            "loss: 1.5939043760299683 - accuracy: 0.4224399924278259 - val_loss: 1.6257449388504028 - val_accuracy: 0.4106999933719635 - penalty: 1e-05\n",
            "hidden layer sizes: [34, 27, 16, 82], total neurons: 159\n",
            "After growing:\n",
            "loss: 1.5931187868118286 - accuracy: 0.4229600131511688 - val_loss: 1.6251966953277588 - val_accuracy: 0.4104999899864197 - penalty: 1e-05\n",
            "hidden layer sizes: [234, 227, 216, 282], total neurons: 959\n",
            "Before pruning:\n",
            "loss: 1.5710055828094482 - accuracy: 0.43290001153945923 - val_loss: 1.6093732118606567 - val_accuracy: 0.42160001397132874 - penalty: 1e-05\n",
            "hidden layer sizes: [234, 227, 216, 282], total neurons: 959\n",
            "After pruning:\n",
            "loss: 1.5708366632461548 - accuracy: 0.4331600069999695 - val_loss: 1.6092453002929688 - val_accuracy: 0.42170000076293945 - penalty: 1e-05\n",
            "hidden layer sizes: [37, 29, 16, 79], total neurons: 161\n",
            "##########################################################\n",
            "Epoch 10/50\n",
            "Before growing:\n",
            "loss: 1.5708366632461548 - accuracy: 0.4331600069999695 - val_loss: 1.6092453002929688 - val_accuracy: 0.42170000076293945 - penalty: 1e-05\n",
            "hidden layer sizes: [37, 29, 16, 79], total neurons: 161\n",
            "After growing:\n",
            "loss: 1.5711504220962524 - accuracy: 0.4330199956893921 - val_loss: 1.609405517578125 - val_accuracy: 0.42239999771118164 - penalty: 1e-05\n",
            "hidden layer sizes: [237, 229, 216, 279], total neurons: 961\n",
            "Before pruning:\n",
            "loss: 1.5476881265640259 - accuracy: 0.44137999415397644 - val_loss: 1.5901226997375488 - val_accuracy: 0.42579999566078186 - penalty: 1e-05\n",
            "hidden layer sizes: [237, 229, 216, 279], total neurons: 961\n",
            "After pruning:\n",
            "loss: 1.5473060607910156 - accuracy: 0.44152000546455383 - val_loss: 1.5897730588912964 - val_accuracy: 0.42570000886917114 - penalty: 1e-05\n",
            "hidden layer sizes: [53, 27, 16, 43], total neurons: 139\n",
            "##########################################################\n",
            "Epoch 11/50\n",
            "Before growing:\n",
            "loss: 1.5473060607910156 - accuracy: 0.44152000546455383 - val_loss: 1.5897730588912964 - val_accuracy: 0.42570000886917114 - penalty: 1e-05\n",
            "hidden layer sizes: [53, 27, 16, 43], total neurons: 139\n",
            "After growing:\n",
            "loss: 1.5468744039535522 - accuracy: 0.44137999415397644 - val_loss: 1.5894347429275513 - val_accuracy: 0.4271000027656555 - penalty: 1e-05\n",
            "hidden layer sizes: [253, 227, 216, 243], total neurons: 939\n",
            "Before pruning:\n",
            "loss: 1.5317198038101196 - accuracy: 0.44797998666763306 - val_loss: 1.5819065570831299 - val_accuracy: 0.4345000088214874 - penalty: 1e-05\n",
            "hidden layer sizes: [253, 227, 216, 243], total neurons: 939\n",
            "After pruning:\n",
            "loss: 1.5305218696594238 - accuracy: 0.44784000515937805 - val_loss: 1.580676555633545 - val_accuracy: 0.43529999256134033 - penalty: 1e-05\n",
            "hidden layer sizes: [38, 29, 16, 50], total neurons: 133\n",
            "##########################################################\n",
            "Epoch 12/50\n",
            "Before growing:\n",
            "loss: 1.5305218696594238 - accuracy: 0.44784000515937805 - val_loss: 1.580676555633545 - val_accuracy: 0.43529999256134033 - penalty: 1e-05\n",
            "hidden layer sizes: [38, 29, 16, 50], total neurons: 133\n",
            "After growing:\n",
            "loss: 1.5308587551116943 - accuracy: 0.44793999195098877 - val_loss: 1.5807956457138062 - val_accuracy: 0.4345000088214874 - penalty: 1e-05\n",
            "hidden layer sizes: [238, 229, 216, 250], total neurons: 933\n",
            "Before pruning:\n",
            "loss: 1.5131258964538574 - accuracy: 0.45410001277923584 - val_loss: 1.5681812763214111 - val_accuracy: 0.438400000333786 - penalty: 1e-05\n",
            "hidden layer sizes: [238, 229, 216, 250], total neurons: 933\n",
            "After pruning:\n",
            "loss: 1.5135471820831299 - accuracy: 0.45419999957084656 - val_loss: 1.5684810876846313 - val_accuracy: 0.4375999867916107 - penalty: 1e-05\n",
            "hidden layer sizes: [41, 27, 16, 102], total neurons: 186\n",
            "##########################################################\n",
            "Epoch 13/50\n",
            "Before growing:\n",
            "loss: 1.5135471820831299 - accuracy: 0.45419999957084656 - val_loss: 1.5684810876846313 - val_accuracy: 0.4375999867916107 - penalty: 1e-05\n",
            "hidden layer sizes: [41, 27, 16, 102], total neurons: 186\n",
            "After growing:\n",
            "loss: 1.5135114192962646 - accuracy: 0.45447999238967896 - val_loss: 1.5682957172393799 - val_accuracy: 0.43869999051094055 - penalty: 1e-05\n",
            "hidden layer sizes: [241, 227, 216, 302], total neurons: 986\n",
            "Before pruning:\n",
            "loss: 1.4907050132751465 - accuracy: 0.46452000737190247 - val_loss: 1.5494734048843384 - val_accuracy: 0.44609999656677246 - penalty: 1e-05\n",
            "hidden layer sizes: [241, 227, 216, 302], total neurons: 986\n",
            "After pruning:\n",
            "loss: 1.4909206628799438 - accuracy: 0.4640200138092041 - val_loss: 1.5496180057525635 - val_accuracy: 0.44600000977516174 - penalty: 1e-05\n",
            "hidden layer sizes: [38, 29, 16, 66], total neurons: 149\n",
            "##########################################################\n",
            "Epoch 14/50\n",
            "Before growing:\n",
            "loss: 1.4909206628799438 - accuracy: 0.4640200138092041 - val_loss: 1.5496180057525635 - val_accuracy: 0.44600000977516174 - penalty: 1e-05\n",
            "hidden layer sizes: [38, 29, 16, 66], total neurons: 149\n",
            "After growing:\n",
            "loss: 1.491151213645935 - accuracy: 0.46424001455307007 - val_loss: 1.5497844219207764 - val_accuracy: 0.44699999690055847 - penalty: 1e-05\n",
            "hidden layer sizes: [238, 229, 216, 266], total neurons: 949\n",
            "Before pruning:\n",
            "loss: 1.4834425449371338 - accuracy: 0.4670799970626831 - val_loss: 1.5468600988388062 - val_accuracy: 0.44769999384880066 - penalty: 1e-05\n",
            "hidden layer sizes: [238, 229, 216, 266], total neurons: 949\n",
            "After pruning:\n",
            "loss: 1.4833049774169922 - accuracy: 0.4669399857521057 - val_loss: 1.5468738079071045 - val_accuracy: 0.447299987077713 - penalty: 1e-05\n",
            "hidden layer sizes: [36, 27, 16, 48], total neurons: 127\n",
            "##########################################################\n",
            "Epoch 15/50\n",
            "Before growing:\n",
            "loss: 1.4833049774169922 - accuracy: 0.4669399857521057 - val_loss: 1.5468738079071045 - val_accuracy: 0.447299987077713 - penalty: 1e-05\n",
            "hidden layer sizes: [36, 27, 16, 48], total neurons: 127\n",
            "After growing:\n",
            "loss: 1.484002709388733 - accuracy: 0.4670799970626831 - val_loss: 1.5475627183914185 - val_accuracy: 0.4489000141620636 - penalty: 1e-05\n",
            "hidden layer sizes: [236, 227, 216, 248], total neurons: 927\n",
            "Before pruning:\n",
            "loss: 1.4698312282562256 - accuracy: 0.47380000352859497 - val_loss: 1.5342669486999512 - val_accuracy: 0.4480000138282776 - penalty: 1e-05\n",
            "hidden layer sizes: [236, 227, 216, 248], total neurons: 927\n",
            "After pruning:\n",
            "loss: 1.4699479341506958 - accuracy: 0.473580002784729 - val_loss: 1.5342730283737183 - val_accuracy: 0.4481000006198883 - penalty: 1e-05\n",
            "hidden layer sizes: [36, 27, 16, 53], total neurons: 132\n",
            "##########################################################\n",
            "Epoch 16/50\n",
            "Before growing:\n",
            "loss: 1.4699479341506958 - accuracy: 0.473580002784729 - val_loss: 1.5342730283737183 - val_accuracy: 0.4481000006198883 - penalty: 1e-05\n",
            "hidden layer sizes: [36, 27, 16, 53], total neurons: 132\n",
            "After growing:\n",
            "loss: 1.4705250263214111 - accuracy: 0.47350001335144043 - val_loss: 1.5347059965133667 - val_accuracy: 0.44830000400543213 - penalty: 1e-05\n",
            "hidden layer sizes: [236, 227, 216, 253], total neurons: 932\n",
            "Before pruning:\n",
            "loss: 1.4631205797195435 - accuracy: 0.47457998991012573 - val_loss: 1.529753565788269 - val_accuracy: 0.4528000056743622 - penalty: 1e-05\n",
            "hidden layer sizes: [236, 227, 216, 253], total neurons: 932\n",
            "After pruning:\n",
            "loss: 1.4638259410858154 - accuracy: 0.47426000237464905 - val_loss: 1.5303865671157837 - val_accuracy: 0.4519999921321869 - penalty: 1e-05\n",
            "hidden layer sizes: [34, 28, 16, 56], total neurons: 134\n",
            "##########################################################\n",
            "Epoch 17/50\n",
            "Before growing:\n",
            "loss: 1.4638259410858154 - accuracy: 0.47426000237464905 - val_loss: 1.5303865671157837 - val_accuracy: 0.4519999921321869 - penalty: 1e-05\n",
            "hidden layer sizes: [34, 28, 16, 56], total neurons: 134\n",
            "After growing:\n",
            "loss: 1.4632461071014404 - accuracy: 0.47512000799179077 - val_loss: 1.529905915260315 - val_accuracy: 0.453000009059906 - penalty: 1e-05\n",
            "hidden layer sizes: [234, 228, 216, 256], total neurons: 934\n",
            "Before pruning:\n",
            "loss: 1.4562197923660278 - accuracy: 0.47582000494003296 - val_loss: 1.5292160511016846 - val_accuracy: 0.4535999894142151 - penalty: 1e-05\n",
            "hidden layer sizes: [234, 228, 216, 256], total neurons: 934\n",
            "After pruning:\n",
            "loss: 1.456649899482727 - accuracy: 0.47571998834609985 - val_loss: 1.529677152633667 - val_accuracy: 0.45339998602867126 - penalty: 1e-05\n",
            "hidden layer sizes: [34, 27, 16, 50], total neurons: 127\n",
            "##########################################################\n",
            "Epoch 18/50\n",
            "Before growing:\n",
            "loss: 1.456649899482727 - accuracy: 0.47571998834609985 - val_loss: 1.529677152633667 - val_accuracy: 0.45339998602867126 - penalty: 1e-05\n",
            "hidden layer sizes: [34, 27, 16, 50], total neurons: 127\n",
            "After growing:\n",
            "loss: 1.4563056230545044 - accuracy: 0.47543999552726746 - val_loss: 1.529232144355774 - val_accuracy: 0.45210000872612 - penalty: 1e-05\n",
            "hidden layer sizes: [234, 227, 216, 250], total neurons: 927\n",
            "Before pruning:\n",
            "loss: 1.4533663988113403 - accuracy: 0.47898000478744507 - val_loss: 1.5270854234695435 - val_accuracy: 0.45260000228881836 - penalty: 1e-05\n",
            "hidden layer sizes: [234, 227, 216, 250], total neurons: 927\n",
            "After pruning:\n",
            "loss: 1.453608751296997 - accuracy: 0.4796000123023987 - val_loss: 1.527320384979248 - val_accuracy: 0.45320001244544983 - penalty: 1e-05\n",
            "hidden layer sizes: [35, 28, 16, 46], total neurons: 125\n",
            "##########################################################\n",
            "Epoch 19/50\n",
            "Before growing:\n",
            "loss: 1.453608751296997 - accuracy: 0.4796000123023987 - val_loss: 1.527320384979248 - val_accuracy: 0.45320001244544983 - penalty: 1e-05\n",
            "hidden layer sizes: [35, 28, 16, 46], total neurons: 125\n",
            "After growing:\n",
            "loss: 1.4536770582199097 - accuracy: 0.47944000363349915 - val_loss: 1.5273280143737793 - val_accuracy: 0.4526999890804291 - penalty: 1e-05\n",
            "hidden layer sizes: [235, 228, 216, 246], total neurons: 925\n",
            "Before pruning:\n",
            "loss: 1.4390300512313843 - accuracy: 0.48157998919487 - val_loss: 1.516427993774414 - val_accuracy: 0.4530999958515167 - penalty: 1e-05\n",
            "hidden layer sizes: [235, 228, 216, 246], total neurons: 925\n",
            "After pruning:\n",
            "loss: 1.439165472984314 - accuracy: 0.4811199903488159 - val_loss: 1.5165808200836182 - val_accuracy: 0.45320001244544983 - penalty: 1e-05\n",
            "hidden layer sizes: [42, 28, 16, 78], total neurons: 164\n",
            "##########################################################\n",
            "Epoch 20/50\n",
            "Before growing:\n",
            "loss: 1.439165472984314 - accuracy: 0.4811199903488159 - val_loss: 1.5165808200836182 - val_accuracy: 0.45320001244544983 - penalty: 1e-05\n",
            "hidden layer sizes: [42, 28, 16, 78], total neurons: 164\n",
            "After growing:\n",
            "loss: 1.4395004510879517 - accuracy: 0.4821400046348572 - val_loss: 1.5168707370758057 - val_accuracy: 0.4544999897480011 - penalty: 1e-05\n",
            "hidden layer sizes: [242, 228, 216, 278], total neurons: 964\n",
            "Before pruning:\n",
            "loss: 1.437606930732727 - accuracy: 0.483460009098053 - val_loss: 1.5159579515457153 - val_accuracy: 0.45489999651908875 - penalty: 1e-05\n",
            "hidden layer sizes: [242, 228, 216, 278], total neurons: 964\n",
            "After pruning:\n",
            "loss: 1.4375044107437134 - accuracy: 0.4840399920940399 - val_loss: 1.5159393548965454 - val_accuracy: 0.4546999931335449 - penalty: 1e-05\n",
            "hidden layer sizes: [36, 27, 16, 44], total neurons: 123\n",
            "##########################################################\n",
            "Epoch 21/50\n",
            "Before growing:\n",
            "loss: 1.4375044107437134 - accuracy: 0.4840399920940399 - val_loss: 1.5159393548965454 - val_accuracy: 0.4546999931335449 - penalty: 1e-05\n",
            "hidden layer sizes: [36, 27, 16, 44], total neurons: 123\n",
            "After growing:\n",
            "loss: 1.4378831386566162 - accuracy: 0.4840199947357178 - val_loss: 1.5162402391433716 - val_accuracy: 0.4544000029563904 - penalty: 1e-05\n",
            "hidden layer sizes: [236, 227, 216, 244], total neurons: 923\n",
            "Before pruning:\n",
            "loss: 1.42805814743042 - accuracy: 0.48583999276161194 - val_loss: 1.5100579261779785 - val_accuracy: 0.45680001378059387 - penalty: 1e-05\n",
            "hidden layer sizes: [236, 227, 216, 244], total neurons: 923\n",
            "After pruning:\n",
            "loss: 1.4281306266784668 - accuracy: 0.48607999086380005 - val_loss: 1.5101780891418457 - val_accuracy: 0.4569000005722046 - penalty: 1e-05\n",
            "hidden layer sizes: [35, 27, 16, 69], total neurons: 147\n",
            "##########################################################\n",
            "Epoch 22/50\n",
            "Before growing:\n",
            "loss: 1.4281306266784668 - accuracy: 0.48607999086380005 - val_loss: 1.5101780891418457 - val_accuracy: 0.4569000005722046 - penalty: 1e-05\n",
            "hidden layer sizes: [35, 27, 16, 69], total neurons: 147\n",
            "After growing:\n",
            "loss: 1.4279868602752686 - accuracy: 0.48565998673439026 - val_loss: 1.5102763175964355 - val_accuracy: 0.45669999718666077 - penalty: 1e-05\n",
            "hidden layer sizes: [235, 227, 216, 269], total neurons: 947\n",
            "Before pruning:\n",
            "loss: 1.4222708940505981 - accuracy: 0.4899600148200989 - val_loss: 1.5071011781692505 - val_accuracy: 0.4611000120639801 - penalty: 1e-05\n",
            "hidden layer sizes: [235, 227, 216, 269], total neurons: 947\n",
            "After pruning:\n",
            "loss: 1.4225165843963623 - accuracy: 0.48993998765945435 - val_loss: 1.5074352025985718 - val_accuracy: 0.460999995470047 - penalty: 1e-05\n",
            "hidden layer sizes: [34, 27, 16, 57], total neurons: 134\n",
            "##########################################################\n",
            "Epoch 23/50\n",
            "Before growing:\n",
            "loss: 1.4225165843963623 - accuracy: 0.48993998765945435 - val_loss: 1.5074352025985718 - val_accuracy: 0.460999995470047 - penalty: 1e-05\n",
            "hidden layer sizes: [34, 27, 16, 57], total neurons: 134\n",
            "After growing:\n",
            "loss: 1.4231277704238892 - accuracy: 0.48962000012397766 - val_loss: 1.5077824592590332 - val_accuracy: 0.45969998836517334 - penalty: 1e-05\n",
            "hidden layer sizes: [234, 227, 216, 257], total neurons: 934\n",
            "Before pruning:\n",
            "loss: 1.410932183265686 - accuracy: 0.4937399923801422 - val_loss: 1.4987157583236694 - val_accuracy: 0.46070000529289246 - penalty: 1e-05\n",
            "hidden layer sizes: [234, 227, 216, 257], total neurons: 934\n",
            "After pruning:\n",
            "loss: 1.4112309217453003 - accuracy: 0.4939599931240082 - val_loss: 1.4987608194351196 - val_accuracy: 0.46059998869895935 - penalty: 1e-05\n",
            "hidden layer sizes: [39, 27, 16, 72], total neurons: 154\n",
            "##########################################################\n",
            "Epoch 24/50\n",
            "Before growing:\n",
            "loss: 1.4112309217453003 - accuracy: 0.4939599931240082 - val_loss: 1.4987608194351196 - val_accuracy: 0.46059998869895935 - penalty: 1e-05\n",
            "hidden layer sizes: [39, 27, 16, 72], total neurons: 154\n",
            "After growing:\n",
            "loss: 1.411382794380188 - accuracy: 0.49371999502182007 - val_loss: 1.4988657236099243 - val_accuracy: 0.4611999988555908 - penalty: 1e-05\n",
            "hidden layer sizes: [239, 227, 216, 272], total neurons: 954\n",
            "Before pruning:\n",
            "loss: 1.4061042070388794 - accuracy: 0.49647998809814453 - val_loss: 1.496606707572937 - val_accuracy: 0.4627000093460083 - penalty: 1e-05\n",
            "hidden layer sizes: [239, 227, 216, 272], total neurons: 954\n",
            "After pruning:\n",
            "loss: 1.4058271646499634 - accuracy: 0.49671998620033264 - val_loss: 1.496367335319519 - val_accuracy: 0.46299999952316284 - penalty: 1e-05\n",
            "hidden layer sizes: [39, 27, 16, 68], total neurons: 150\n",
            "##########################################################\n",
            "Epoch 25/50\n",
            "Before growing:\n",
            "loss: 1.4058271646499634 - accuracy: 0.49671998620033264 - val_loss: 1.496367335319519 - val_accuracy: 0.46299999952316284 - penalty: 1e-05\n",
            "hidden layer sizes: [39, 27, 16, 68], total neurons: 150\n",
            "After growing:\n",
            "loss: 1.4062503576278687 - accuracy: 0.4969800114631653 - val_loss: 1.4965481758117676 - val_accuracy: 0.4629000127315521 - penalty: 1e-05\n",
            "hidden layer sizes: [239, 227, 216, 268], total neurons: 950\n",
            "Before pruning:\n",
            "loss: 1.4045077562332153 - accuracy: 0.4975399971008301 - val_loss: 1.4997261762619019 - val_accuracy: 0.4634999930858612 - penalty: 1e-05\n",
            "hidden layer sizes: [239, 227, 216, 268], total neurons: 950\n",
            "After pruning:\n",
            "loss: 1.4045673608779907 - accuracy: 0.4976400136947632 - val_loss: 1.4997915029525757 - val_accuracy: 0.4636000096797943 - penalty: 1e-05\n",
            "hidden layer sizes: [31, 29, 16, 94], total neurons: 170\n",
            "##########################################################\n",
            "Epoch 26/50\n",
            "Before growing:\n",
            "loss: 1.4045673608779907 - accuracy: 0.4976400136947632 - val_loss: 1.4997915029525757 - val_accuracy: 0.4636000096797943 - penalty: 1e-05\n",
            "hidden layer sizes: [31, 29, 16, 94], total neurons: 170\n",
            "After growing:\n",
            "loss: 1.4044944047927856 - accuracy: 0.4971800148487091 - val_loss: 1.5000109672546387 - val_accuracy: 0.4634000062942505 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [231, 229, 216, 294], total neurons: 970\n",
            "Before pruning:\n",
            "loss: 1.3825727701187134 - accuracy: 0.5059599876403809 - val_loss: 1.4836400747299194 - val_accuracy: 0.4681999981403351 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [231, 229, 216, 294], total neurons: 970\n",
            "After pruning:\n",
            "loss: 1.3825488090515137 - accuracy: 0.5058799982070923 - val_loss: 1.483622670173645 - val_accuracy: 0.4684999883174896 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [222, 225, 182, 126], total neurons: 755\n",
            "##########################################################\n",
            "Epoch 27/50\n",
            "Before growing:\n",
            "loss: 1.3825488090515137 - accuracy: 0.5058799982070923 - val_loss: 1.483622670173645 - val_accuracy: 0.4684999883174896 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [222, 225, 182, 126], total neurons: 755\n",
            "After growing:\n",
            "loss: 1.3826087713241577 - accuracy: 0.5063999891281128 - val_loss: 1.48359215259552 - val_accuracy: 0.46779999136924744 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [422, 425, 382, 326], total neurons: 1555\n",
            "Before pruning:\n",
            "loss: 1.3572832345962524 - accuracy: 0.5167199969291687 - val_loss: 1.4709091186523438 - val_accuracy: 0.4731000065803528 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [422, 425, 382, 326], total neurons: 1555\n",
            "After pruning:\n",
            "loss: 1.356432318687439 - accuracy: 0.5170000195503235 - val_loss: 1.4698795080184937 - val_accuracy: 0.4742000102996826 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [273, 344, 166, 194], total neurons: 977\n",
            "##########################################################\n",
            "Epoch 28/50\n",
            "Before growing:\n",
            "loss: 1.356432318687439 - accuracy: 0.5170000195503235 - val_loss: 1.4698795080184937 - val_accuracy: 0.4742000102996826 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [273, 344, 166, 194], total neurons: 977\n",
            "After growing:\n",
            "loss: 1.3569093942642212 - accuracy: 0.5169000029563904 - val_loss: 1.470180869102478 - val_accuracy: 0.47429999709129333 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [473, 544, 366, 394], total neurons: 1777\n",
            "Before pruning:\n",
            "loss: 1.3387421369552612 - accuracy: 0.523859977722168 - val_loss: 1.46088707447052 - val_accuracy: 0.4796999990940094 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [473, 544, 366, 394], total neurons: 1777\n",
            "After pruning:\n",
            "loss: 1.3376331329345703 - accuracy: 0.5240399837493896 - val_loss: 1.4593638181686401 - val_accuracy: 0.4799000024795532 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [275, 389, 167, 229], total neurons: 1060\n",
            "##########################################################\n",
            "Epoch 29/50\n",
            "Before growing:\n",
            "loss: 1.3376331329345703 - accuracy: 0.5240399837493896 - val_loss: 1.4593638181686401 - val_accuracy: 0.4799000024795532 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [275, 389, 167, 229], total neurons: 1060\n",
            "After growing:\n",
            "loss: 1.3372719287872314 - accuracy: 0.5241400003433228 - val_loss: 1.4589123725891113 - val_accuracy: 0.47999998927116394 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [475, 589, 367, 429], total neurons: 1860\n",
            "Before pruning:\n",
            "loss: 1.3113149404525757 - accuracy: 0.5337799787521362 - val_loss: 1.4389877319335938 - val_accuracy: 0.4875999987125397 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [475, 589, 367, 429], total neurons: 1860\n",
            "After pruning:\n",
            "loss: 1.311712384223938 - accuracy: 0.5334600210189819 - val_loss: 1.4389593601226807 - val_accuracy: 0.4878999888896942 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [253, 367, 166, 119], total neurons: 905\n",
            "##########################################################\n",
            "Epoch 30/50\n",
            "Before growing:\n",
            "loss: 1.311712384223938 - accuracy: 0.5334600210189819 - val_loss: 1.4389593601226807 - val_accuracy: 0.4878999888896942 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [253, 367, 166, 119], total neurons: 905\n",
            "After growing:\n",
            "loss: 1.312248706817627 - accuracy: 0.5336999893188477 - val_loss: 1.4392751455307007 - val_accuracy: 0.4878999888896942 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [453, 567, 366, 319], total neurons: 1705\n",
            "Before pruning:\n",
            "loss: 1.3041037321090698 - accuracy: 0.5356600284576416 - val_loss: 1.4408270120620728 - val_accuracy: 0.4885999858379364 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [453, 567, 366, 319], total neurons: 1705\n",
            "After pruning:\n",
            "loss: 1.3036348819732666 - accuracy: 0.536080002784729 - val_loss: 1.4406101703643799 - val_accuracy: 0.4887999892234802 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [283, 396, 203, 156], total neurons: 1038\n",
            "##########################################################\n",
            "Epoch 31/50\n",
            "Before growing:\n",
            "loss: 1.3036348819732666 - accuracy: 0.536080002784729 - val_loss: 1.4406101703643799 - val_accuracy: 0.4887999892234802 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [283, 396, 203, 156], total neurons: 1038\n",
            "After growing:\n",
            "loss: 1.3036038875579834 - accuracy: 0.5358399748802185 - val_loss: 1.4405763149261475 - val_accuracy: 0.4884999990463257 - penalty: 1.0000000000000002e-07\n",
            "hidden layer sizes: [483, 596, 403, 356], total neurons: 1838\n",
            "Before pruning:\n",
            "loss: 1.2860997915267944 - accuracy: 0.5430799722671509 - val_loss: 1.43025803565979 - val_accuracy: 0.4894999861717224 - penalty: 1.0000000000000002e-07\n",
            "hidden layer sizes: [483, 596, 403, 356], total neurons: 1838\n",
            "After pruning:\n",
            "loss: 1.2860989570617676 - accuracy: 0.5431399941444397 - val_loss: 1.4302546977996826 - val_accuracy: 0.4894999861717224 - penalty: 1.0000000000000002e-07\n",
            "hidden layer sizes: [483, 596, 403, 257], total neurons: 1739\n",
            "##########################################################\n",
            "Epoch 32/50\n",
            "Before growing:\n",
            "loss: 1.2860989570617676 - accuracy: 0.5431399941444397 - val_loss: 1.4302546977996826 - val_accuracy: 0.4894999861717224 - penalty: 1.0000000000000002e-07\n",
            "hidden layer sizes: [483, 596, 403, 257], total neurons: 1739\n",
            "After growing:\n",
            "loss: 1.2860525846481323 - accuracy: 0.543179988861084 - val_loss: 1.4302825927734375 - val_accuracy: 0.4893999993801117 - penalty: 1.0000000000000002e-07\n",
            "hidden layer sizes: [724, 894, 604, 457], total neurons: 2679\n",
            "Before pruning:\n",
            "loss: 1.2624415159225464 - accuracy: 0.5520399808883667 - val_loss: 1.416395902633667 - val_accuracy: 0.4948999881744385 - penalty: 1.0000000000000002e-07\n",
            "hidden layer sizes: [724, 894, 604, 457], total neurons: 2679\n",
            "After pruning:\n",
            "loss: 1.2624369859695435 - accuracy: 0.5519999861717224 - val_loss: 1.4163918495178223 - val_accuracy: 0.49480000138282776 - penalty: 1.0000000000000002e-07\n",
            "hidden layer sizes: [724, 886, 604, 313], total neurons: 2527\n",
            "##########################################################\n",
            "Epoch 33/50\n",
            "Before growing:\n",
            "loss: 1.2624369859695435 - accuracy: 0.5519999861717224 - val_loss: 1.4163918495178223 - val_accuracy: 0.49480000138282776 - penalty: 1.0000000000000002e-07\n",
            "hidden layer sizes: [724, 886, 604, 313], total neurons: 2527\n",
            "After growing:\n",
            "loss: 1.2625669240951538 - accuracy: 0.551800012588501 - val_loss: 1.4164706468582153 - val_accuracy: 0.4950999915599823 - penalty: 1.0000000000000002e-07\n",
            "hidden layer sizes: [1086, 1329, 906, 513], total neurons: 3834\n",
            "Before pruning:\n",
            "loss: 1.2563683986663818 - accuracy: 0.5551000237464905 - val_loss: 1.4165204763412476 - val_accuracy: 0.49410000443458557 - penalty: 1.0000000000000002e-07\n",
            "hidden layer sizes: [1086, 1329, 906, 513], total neurons: 3834\n",
            "After pruning:\n",
            "loss: 1.2561980485916138 - accuracy: 0.5552800297737122 - val_loss: 1.416375994682312 - val_accuracy: 0.49399998784065247 - penalty: 1.0000000000000002e-07\n",
            "hidden layer sizes: [1049, 1267, 899, 306], total neurons: 3521\n",
            "##########################################################\n",
            "Epoch 34/50\n",
            "Before growing:\n",
            "loss: 1.2561980485916138 - accuracy: 0.5552800297737122 - val_loss: 1.416375994682312 - val_accuracy: 0.49399998784065247 - penalty: 1.0000000000000002e-07\n",
            "hidden layer sizes: [1049, 1267, 899, 306], total neurons: 3521\n",
            "After growing:\n",
            "loss: 1.2560359239578247 - accuracy: 0.5553399920463562 - val_loss: 1.4161254167556763 - val_accuracy: 0.4934999942779541 - penalty: 1.0000000000000002e-07\n",
            "hidden layer sizes: [1573, 1900, 1348, 506], total neurons: 5327\n",
            "Before pruning:\n",
            "loss: 1.2151209115982056 - accuracy: 0.5662400126457214 - val_loss: 1.3976863622665405 - val_accuracy: 0.5040000081062317 - penalty: 1.0000000000000002e-07\n",
            "hidden layer sizes: [1573, 1900, 1348, 506], total neurons: 5327\n",
            "After pruning:\n",
            "loss: 1.2145299911499023 - accuracy: 0.5665199756622314 - val_loss: 1.3972920179367065 - val_accuracy: 0.5033000111579895 - penalty: 1.0000000000000002e-07\n",
            "hidden layer sizes: [1185, 1796, 1310, 459], total neurons: 4750\n",
            "##########################################################\n",
            "Epoch 35/50\n",
            "Before growing:\n",
            "loss: 1.2145299911499023 - accuracy: 0.5665199756622314 - val_loss: 1.3972920179367065 - val_accuracy: 0.5033000111579895 - penalty: 1.0000000000000002e-07\n",
            "hidden layer sizes: [1185, 1796, 1310, 459], total neurons: 4750\n",
            "After growing:\n",
            "loss: 1.214615821838379 - accuracy: 0.5667200088500977 - val_loss: 1.397464632987976 - val_accuracy: 0.5041000247001648 - penalty: 1.0000000000000002e-07\n",
            "hidden layer sizes: [1777, 2694, 1965, 688], total neurons: 7124\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function IteratorResourceDeleter.__del__ at 0x7f344e38ac20>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\", line 546, in __del__\n",
            "    handle=self._handle, deleter=self._deleter)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 1264, in delete_iterator\n",
            "    _ctx, \"DeleteIterator\", name, handle, deleter)\n",
            "KeyboardInterrupt: \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Before pruning:\n",
            "loss: 1.2014533281326294 - accuracy: 0.572380006313324 - val_loss: 1.3904670476913452 - val_accuracy: 0.5128999948501587 - penalty: 1.0000000000000002e-07\n",
            "hidden layer sizes: [1777, 2694, 1965, 688], total neurons: 7124\n",
            "After pruning:\n",
            "loss: 1.1997673511505127 - accuracy: 0.5733399987220764 - val_loss: 1.3894532918930054 - val_accuracy: 0.5133000016212463 - penalty: 1.0000000000000002e-07\n",
            "hidden layer sizes: [1397, 2434, 1834, 641], total neurons: 6306\n",
            "##########################################################\n",
            "Epoch 36/50\n",
            "Before growing:\n",
            "loss: 1.1997673511505127 - accuracy: 0.5733399987220764 - val_loss: 1.3894532918930054 - val_accuracy: 0.5133000016212463 - penalty: 1.0000000000000002e-07\n",
            "hidden layer sizes: [1397, 2434, 1834, 641], total neurons: 6306\n",
            "After growing:\n",
            "loss: 1.1999669075012207 - accuracy: 0.5731599926948547 - val_loss: 1.3896238803863525 - val_accuracy: 0.5135999917984009 - penalty: 1.0000000000000002e-07\n",
            "hidden layer sizes: [2095, 3651, 2751, 961], total neurons: 9458\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-cdcdce9c3c6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\nmodel = SSModel(layer_sizes=[3072, 300, 300, 300, 300, 10], activation='selu', regularization_penalty=0.0001, regularization_method='weighted_l1', kernel_initializer='lecun_normal')\\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.0002)\\n\\ntrain_model(model, X_train_norm, y_train, optimizer, epochs, self_scaling_epochs, batch_size, min_new_neurons, validation_data=(X_test_norm, y_test), regularization_penalty_multiplier=0.1, growth_percentage=0.5)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-53>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-1aef28cd5fb8>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, x, y, optimizer, epochs, self_scaling_epochs, batch_size, min_new_neurons, validation_data, pruning_threshold, regularization_penalty_multiplier, growth_percentage, print_neurons)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_categorical_crossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mloss_value\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m     if any(isinstance(x, (\n\u001b[1;32m    984\u001b[0m         tf.Tensor, np.ndarray, float, int)) for x in input_list):\n\u001b[0;32m--> 985\u001b[0;31m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_convert_numpy_or_python_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m       \u001b[0minput_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_convert_numpy_or_python_types\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   3297\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_convert_numpy_or_python_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3298\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3299\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3300\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2_with_dispatch\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1429\u001b[0m   \"\"\"\n\u001b[1;32m   1430\u001b[0m   return convert_to_tensor_v2(\n\u001b[0;32m-> 1431\u001b[0;31m       value, dtype=dtype, dtype_hint=dtype_hint, name=name)\n\u001b[0m\u001b[1;32m   1432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmqOHXLl4C3T",
        "outputId": "61b7642d-d7a2-44f0-874f-a7f364f7978b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# SCALING FACTOR *= 1000\n",
        "\n",
        "%%time\n",
        "\n",
        "model = SSModel(layer_sizes=[3072, 300, 300, 300, 300, 10], activation='selu', regularization_penalty=0.0001, regularization_method='weighted_l1', kernel_initializer='lecun_normal')\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002)\n",
        "\n",
        "train_model(model, X_train_norm, y_train, optimizer, epochs, self_scaling_epochs, batch_size, min_new_neurons, validation_data=(X_test_norm, y_test), regularization_penalty_multiplier=0.1, growth_percentage=0.5)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "##########################################################\n",
            "Epoch 1/50\n",
            "Before growing:\n",
            "loss: 2.5994713306427 - accuracy: 0.10595999658107758 - val_loss: 2.5933938026428223 - val_accuracy: 0.10350000113248825 - penalty: 0.0001\n",
            "hidden layer sizes: [300, 300, 300, 300], total neurons: 1200\n",
            "After growing:\n",
            "loss: 3.0187888145446777 - accuracy: 0.1050800010561943 - val_loss: 3.009610652923584 - val_accuracy: 0.10440000146627426 - penalty: 0.0001\n",
            "hidden layer sizes: [500, 500, 500, 500], total neurons: 2000\n",
            "Before pruning:\n",
            "loss: 1.8526203632354736 - accuracy: 0.3181599974632263 - val_loss: 1.8550060987472534 - val_accuracy: 0.3163999915122986 - penalty: 0.0001\n",
            "hidden layer sizes: [500, 500, 500, 500], total neurons: 2000\n",
            "After pruning:\n",
            "loss: 1.8521884679794312 - accuracy: 0.31863999366760254 - val_loss: 1.8549309968948364 - val_accuracy: 0.31700000166893005 - penalty: 0.0001\n",
            "hidden layer sizes: [16, 60, 35, 70], total neurons: 181\n",
            "##########################################################\n",
            "Epoch 2/50\n",
            "Before growing:\n",
            "loss: 1.8521884679794312 - accuracy: 0.31863999366760254 - val_loss: 1.8549309968948364 - val_accuracy: 0.31700000166893005 - penalty: 0.0001\n",
            "hidden layer sizes: [16, 60, 35, 70], total neurons: 181\n",
            "After growing:\n",
            "loss: 2.523418664932251 - accuracy: 0.15484000742435455 - val_loss: 2.561310052871704 - val_accuracy: 0.1467999964952469 - penalty: 0.0001\n",
            "hidden layer sizes: [216, 260, 235, 270], total neurons: 981\n",
            "Before pruning:\n",
            "loss: 1.7733983993530273 - accuracy: 0.3491800129413605 - val_loss: 1.7782135009765625 - val_accuracy: 0.3463999927043915 - penalty: 0.0001\n",
            "hidden layer sizes: [216, 260, 235, 270], total neurons: 981\n",
            "After pruning:\n",
            "loss: 1.773439645767212 - accuracy: 0.3484799861907959 - val_loss: 1.7783808708190918 - val_accuracy: 0.34610000252723694 - penalty: 0.0001\n",
            "hidden layer sizes: [9, 50, 25, 62], total neurons: 146\n",
            "##########################################################\n",
            "Epoch 3/50\n",
            "Before growing:\n",
            "loss: 1.773439645767212 - accuracy: 0.3484799861907959 - val_loss: 1.7783808708190918 - val_accuracy: 0.34610000252723694 - penalty: 0.0001\n",
            "hidden layer sizes: [9, 50, 25, 62], total neurons: 146\n",
            "After growing:\n",
            "loss: 2.496006965637207 - accuracy: 0.17827999591827393 - val_loss: 2.489091157913208 - val_accuracy: 0.18039999902248383 - penalty: 0.0001\n",
            "hidden layer sizes: [209, 250, 225, 262], total neurons: 946\n",
            "Before pruning:\n",
            "loss: 1.733241081237793 - accuracy: 0.37053999304771423 - val_loss: 1.7372428178787231 - val_accuracy: 0.37070000171661377 - penalty: 0.0001\n",
            "hidden layer sizes: [209, 250, 225, 262], total neurons: 946\n",
            "After pruning:\n",
            "loss: 1.7331159114837646 - accuracy: 0.37018001079559326 - val_loss: 1.7370167970657349 - val_accuracy: 0.37059998512268066 - penalty: 0.0001\n",
            "hidden layer sizes: [10, 46, 21, 55], total neurons: 132\n",
            "##########################################################\n",
            "Epoch 4/50\n",
            "Before growing:\n",
            "loss: 1.7331159114837646 - accuracy: 0.37018001079559326 - val_loss: 1.7370167970657349 - val_accuracy: 0.37059998512268066 - penalty: 0.0001\n",
            "hidden layer sizes: [10, 46, 21, 55], total neurons: 132\n",
            "After growing:\n",
            "loss: 2.3256847858428955 - accuracy: 0.24410000443458557 - val_loss: 2.320723533630371 - val_accuracy: 0.24950000643730164 - penalty: 0.0001\n",
            "hidden layer sizes: [210, 246, 221, 255], total neurons: 932\n",
            "Before pruning:\n",
            "loss: 1.7120361328125 - accuracy: 0.37389999628067017 - val_loss: 1.7214058637619019 - val_accuracy: 0.375900000333786 - penalty: 0.0001\n",
            "hidden layer sizes: [210, 246, 221, 255], total neurons: 932\n",
            "After pruning:\n",
            "loss: 1.7128568887710571 - accuracy: 0.37261998653411865 - val_loss: 1.7224503755569458 - val_accuracy: 0.3734999895095825 - penalty: 0.0001\n",
            "hidden layer sizes: [9, 55, 32, 47], total neurons: 143\n",
            "##########################################################\n",
            "Epoch 5/50\n",
            "Before growing:\n",
            "loss: 1.7128568887710571 - accuracy: 0.37261998653411865 - val_loss: 1.7224503755569458 - val_accuracy: 0.3734999895095825 - penalty: 0.0001\n",
            "hidden layer sizes: [9, 55, 32, 47], total neurons: 143\n",
            "After growing:\n",
            "loss: 2.475759744644165 - accuracy: 0.21649999916553497 - val_loss: 2.480156660079956 - val_accuracy: 0.2160000056028366 - penalty: 0.0001\n",
            "hidden layer sizes: [209, 255, 232, 247], total neurons: 943\n",
            "Before pruning:\n",
            "loss: 1.6928995847702026 - accuracy: 0.3834199905395508 - val_loss: 1.7063153982162476 - val_accuracy: 0.3763999938964844 - penalty: 0.0001\n",
            "hidden layer sizes: [209, 255, 232, 247], total neurons: 943\n",
            "After pruning:\n",
            "loss: 1.6924508810043335 - accuracy: 0.38398000597953796 - val_loss: 1.7059053182601929 - val_accuracy: 0.3783000111579895 - penalty: 0.0001\n",
            "hidden layer sizes: [15, 47, 19, 62], total neurons: 143\n",
            "##########################################################\n",
            "Epoch 6/50\n",
            "Before growing:\n",
            "loss: 1.6924508810043335 - accuracy: 0.38398000597953796 - val_loss: 1.7059053182601929 - val_accuracy: 0.3783000111579895 - penalty: 0.0001\n",
            "hidden layer sizes: [15, 47, 19, 62], total neurons: 143\n",
            "After growing:\n",
            "loss: 2.7089786529541016 - accuracy: 0.1910800039768219 - val_loss: 2.6957690715789795 - val_accuracy: 0.19020000100135803 - penalty: 0.0001\n",
            "hidden layer sizes: [215, 247, 219, 262], total neurons: 943\n",
            "Before pruning:\n",
            "loss: 1.6819208860397339 - accuracy: 0.3867200016975403 - val_loss: 1.7020505666732788 - val_accuracy: 0.37959998846054077 - penalty: 0.0001\n",
            "hidden layer sizes: [215, 247, 219, 262], total neurons: 943\n",
            "After pruning:\n",
            "loss: 1.6798151731491089 - accuracy: 0.38769999146461487 - val_loss: 1.699649453163147 - val_accuracy: 0.38119998574256897 - penalty: 0.0001\n",
            "hidden layer sizes: [11, 48, 27, 46], total neurons: 132\n",
            "##########################################################\n",
            "Epoch 7/50\n",
            "Before growing:\n",
            "loss: 1.6798151731491089 - accuracy: 0.38769999146461487 - val_loss: 1.699649453163147 - val_accuracy: 0.38119998574256897 - penalty: 0.0001\n",
            "hidden layer sizes: [11, 48, 27, 46], total neurons: 132\n",
            "After growing:\n",
            "loss: 2.42111873626709 - accuracy: 0.2063400000333786 - val_loss: 2.4228878021240234 - val_accuracy: 0.20999999344348907 - penalty: 0.0001\n",
            "hidden layer sizes: [211, 248, 227, 246], total neurons: 932\n",
            "Before pruning:\n",
            "loss: 1.6644401550292969 - accuracy: 0.39386001229286194 - val_loss: 1.6836152076721191 - val_accuracy: 0.3880000114440918 - penalty: 0.0001\n",
            "hidden layer sizes: [211, 248, 227, 246], total neurons: 932\n",
            "After pruning:\n",
            "loss: 1.6638599634170532 - accuracy: 0.3939400017261505 - val_loss: 1.6831945180892944 - val_accuracy: 0.3880999982357025 - penalty: 0.0001\n",
            "hidden layer sizes: [15, 45, 24, 61], total neurons: 145\n",
            "##########################################################\n",
            "Epoch 8/50\n",
            "Before growing:\n",
            "loss: 1.6638599634170532 - accuracy: 0.3939400017261505 - val_loss: 1.6831945180892944 - val_accuracy: 0.3880999982357025 - penalty: 0.0001\n",
            "hidden layer sizes: [15, 45, 24, 61], total neurons: 145\n",
            "After growing:\n",
            "loss: 2.5932443141937256 - accuracy: 0.2028599977493286 - val_loss: 2.5916764736175537 - val_accuracy: 0.20630000531673431 - penalty: 0.0001\n",
            "hidden layer sizes: [215, 245, 224, 261], total neurons: 945\n",
            "Before pruning:\n",
            "loss: 1.6544471979141235 - accuracy: 0.39649999141693115 - val_loss: 1.677518606185913 - val_accuracy: 0.38449999690055847 - penalty: 0.0001\n",
            "hidden layer sizes: [215, 245, 224, 261], total neurons: 945\n",
            "After pruning:\n",
            "loss: 1.6549134254455566 - accuracy: 0.39629998803138733 - val_loss: 1.6774945259094238 - val_accuracy: 0.38589999079704285 - penalty: 0.0001\n",
            "hidden layer sizes: [11, 64, 26, 50], total neurons: 151\n",
            "##########################################################\n",
            "Epoch 9/50\n",
            "Before growing:\n",
            "loss: 1.6549134254455566 - accuracy: 0.39629998803138733 - val_loss: 1.6774945259094238 - val_accuracy: 0.38589999079704285 - penalty: 0.0001\n",
            "hidden layer sizes: [11, 64, 26, 50], total neurons: 151\n",
            "After growing:\n",
            "loss: 2.4059741497039795 - accuracy: 0.24070000648498535 - val_loss: 2.429429531097412 - val_accuracy: 0.23469999432563782 - penalty: 0.0001\n",
            "hidden layer sizes: [211, 264, 226, 250], total neurons: 951\n",
            "Before pruning:\n",
            "loss: 1.6477280855178833 - accuracy: 0.40004000067710876 - val_loss: 1.6685386896133423 - val_accuracy: 0.391400009393692 - penalty: 0.0001\n",
            "hidden layer sizes: [211, 264, 226, 250], total neurons: 951\n",
            "After pruning:\n",
            "loss: 1.6475673913955688 - accuracy: 0.40049999952316284 - val_loss: 1.6681820154190063 - val_accuracy: 0.3903999924659729 - penalty: 0.0001\n",
            "hidden layer sizes: [16, 46, 29, 56], total neurons: 147\n",
            "##########################################################\n",
            "Epoch 10/50\n",
            "Before growing:\n",
            "loss: 1.6475673913955688 - accuracy: 0.40049999952316284 - val_loss: 1.6681820154190063 - val_accuracy: 0.3903999924659729 - penalty: 0.0001\n",
            "hidden layer sizes: [16, 46, 29, 56], total neurons: 147\n",
            "After growing:\n",
            "loss: 2.374021053314209 - accuracy: 0.22553999722003937 - val_loss: 2.394353151321411 - val_accuracy: 0.22630000114440918 - penalty: 0.0001\n",
            "hidden layer sizes: [216, 246, 229, 256], total neurons: 947\n",
            "Before pruning:\n",
            "loss: 1.6403700113296509 - accuracy: 0.4029200077056885 - val_loss: 1.664381980895996 - val_accuracy: 0.392300009727478 - penalty: 0.0001\n",
            "hidden layer sizes: [216, 246, 229, 256], total neurons: 947\n",
            "After pruning:\n",
            "loss: 1.639377474784851 - accuracy: 0.4032000005245209 - val_loss: 1.663282036781311 - val_accuracy: 0.3917999863624573 - penalty: 0.0001\n",
            "hidden layer sizes: [16, 53, 18, 65], total neurons: 152\n",
            "##########################################################\n",
            "Epoch 11/50\n",
            "Before growing:\n",
            "loss: 1.639377474784851 - accuracy: 0.4032000005245209 - val_loss: 1.663282036781311 - val_accuracy: 0.3917999863624573 - penalty: 0.0001\n",
            "hidden layer sizes: [16, 53, 18, 65], total neurons: 152\n",
            "After growing:\n",
            "loss: 2.505998134613037 - accuracy: 0.23824000358581543 - val_loss: 2.534132719039917 - val_accuracy: 0.2386000007390976 - penalty: 0.0001\n",
            "hidden layer sizes: [216, 253, 218, 265], total neurons: 952\n",
            "Before pruning:\n",
            "loss: 1.6380343437194824 - accuracy: 0.4037399888038635 - val_loss: 1.6643660068511963 - val_accuracy: 0.39430001378059387 - penalty: 0.0001\n",
            "hidden layer sizes: [216, 253, 218, 265], total neurons: 952\n",
            "After pruning:\n",
            "loss: 1.637719988822937 - accuracy: 0.40483999252319336 - val_loss: 1.6636414527893066 - val_accuracy: 0.39579999446868896 - penalty: 0.0001\n",
            "hidden layer sizes: [9, 41, 34, 49], total neurons: 133\n",
            "##########################################################\n",
            "Epoch 12/50\n",
            "Before growing:\n",
            "loss: 1.637719988822937 - accuracy: 0.40483999252319336 - val_loss: 1.6636414527893066 - val_accuracy: 0.39579999446868896 - penalty: 0.0001\n",
            "hidden layer sizes: [9, 41, 34, 49], total neurons: 133\n",
            "After growing:\n",
            "loss: 2.445460557937622 - accuracy: 0.193900004029274 - val_loss: 2.460886001586914 - val_accuracy: 0.19120000302791595 - penalty: 1e-05\n",
            "hidden layer sizes: [209, 241, 234, 249], total neurons: 933\n",
            "Before pruning:\n",
            "loss: 1.508185625076294 - accuracy: 0.45603999495506287 - val_loss: 1.5403335094451904 - val_accuracy: 0.445499986410141 - penalty: 1e-05\n",
            "hidden layer sizes: [209, 241, 234, 249], total neurons: 933\n",
            "After pruning:\n",
            "loss: 1.5083149671554565 - accuracy: 0.45579999685287476 - val_loss: 1.539666771888733 - val_accuracy: 0.4456999897956848 - penalty: 1e-05\n",
            "hidden layer sizes: [137, 241, 234, 249], total neurons: 861\n",
            "##########################################################\n",
            "Epoch 13/50\n",
            "Before growing:\n",
            "loss: 1.5083149671554565 - accuracy: 0.45579999685287476 - val_loss: 1.539666771888733 - val_accuracy: 0.4456999897956848 - penalty: 1e-05\n",
            "hidden layer sizes: [137, 241, 234, 249], total neurons: 861\n",
            "After growing:\n",
            "loss: 2.006809949874878 - accuracy: 0.32120001316070557 - val_loss: 2.032956123352051 - val_accuracy: 0.32120001316070557 - penalty: 1e-05\n",
            "hidden layer sizes: [337, 441, 434, 449], total neurons: 1661\n",
            "Before pruning:\n",
            "loss: 1.4947881698608398 - accuracy: 0.46592000126838684 - val_loss: 1.5360958576202393 - val_accuracy: 0.453900009393692 - penalty: 1e-05\n",
            "hidden layer sizes: [337, 441, 434, 449], total neurons: 1661\n",
            "After pruning:\n",
            "loss: 1.4951353073120117 - accuracy: 0.4659999907016754 - val_loss: 1.5354957580566406 - val_accuracy: 0.45329999923706055 - penalty: 1e-05\n",
            "hidden layer sizes: [90, 388, 269, 327], total neurons: 1074\n",
            "##########################################################\n",
            "Epoch 14/50\n",
            "Before growing:\n",
            "loss: 1.4951353073120117 - accuracy: 0.4659999907016754 - val_loss: 1.5354957580566406 - val_accuracy: 0.45329999923706055 - penalty: 1e-05\n",
            "hidden layer sizes: [90, 388, 269, 327], total neurons: 1074\n",
            "After growing:\n",
            "loss: 2.0703563690185547 - accuracy: 0.27663999795913696 - val_loss: 2.0807676315307617 - val_accuracy: 0.27410000562667847 - penalty: 1e-05\n",
            "hidden layer sizes: [290, 588, 469, 527], total neurons: 1874\n",
            "Before pruning:\n",
            "loss: 1.4733712673187256 - accuracy: 0.4763000011444092 - val_loss: 1.5185153484344482 - val_accuracy: 0.4575999975204468 - penalty: 1e-05\n",
            "hidden layer sizes: [290, 588, 469, 527], total neurons: 1874\n",
            "After pruning:\n",
            "loss: 1.4694916009902954 - accuracy: 0.4769200086593628 - val_loss: 1.5138448476791382 - val_accuracy: 0.46050000190734863 - penalty: 1e-05\n",
            "hidden layer sizes: [96, 338, 196, 271], total neurons: 901\n",
            "##########################################################\n",
            "Epoch 15/50\n",
            "Before growing:\n",
            "loss: 1.4694916009902954 - accuracy: 0.4769200086593628 - val_loss: 1.5138448476791382 - val_accuracy: 0.46050000190734863 - penalty: 1e-05\n",
            "hidden layer sizes: [96, 338, 196, 271], total neurons: 901\n",
            "After growing:\n",
            "loss: 1.972825288772583 - accuracy: 0.32109999656677246 - val_loss: 2.007343292236328 - val_accuracy: 0.3077999949455261 - penalty: 1e-05\n",
            "hidden layer sizes: [296, 538, 396, 471], total neurons: 1701\n",
            "Before pruning:\n",
            "loss: 1.4594881534576416 - accuracy: 0.47530001401901245 - val_loss: 1.507764220237732 - val_accuracy: 0.4611999988555908 - penalty: 1e-05\n",
            "hidden layer sizes: [296, 538, 396, 471], total neurons: 1701\n",
            "After pruning:\n",
            "loss: 1.4578683376312256 - accuracy: 0.4768199920654297 - val_loss: 1.5067572593688965 - val_accuracy: 0.46209999918937683 - penalty: 1e-05\n",
            "hidden layer sizes: [69, 325, 229, 310], total neurons: 933\n",
            "##########################################################\n",
            "Epoch 16/50\n",
            "Before growing:\n",
            "loss: 1.4578683376312256 - accuracy: 0.4768199920654297 - val_loss: 1.5067572593688965 - val_accuracy: 0.46209999918937683 - penalty: 1e-05\n",
            "hidden layer sizes: [69, 325, 229, 310], total neurons: 933\n",
            "After growing:\n",
            "loss: 1.8891197443008423 - accuracy: 0.34035998582839966 - val_loss: 1.9207476377487183 - val_accuracy: 0.3294999897480011 - penalty: 1e-05\n",
            "hidden layer sizes: [269, 525, 429, 510], total neurons: 1733\n",
            "Before pruning:\n",
            "loss: 1.4320814609527588 - accuracy: 0.49017998576164246 - val_loss: 1.4884546995162964 - val_accuracy: 0.46799999475479126 - penalty: 1e-05\n",
            "hidden layer sizes: [269, 525, 429, 510], total neurons: 1733\n",
            "After pruning:\n",
            "loss: 1.4292253255844116 - accuracy: 0.4920400083065033 - val_loss: 1.4867637157440186 - val_accuracy: 0.46779999136924744 - penalty: 1e-05\n",
            "hidden layer sizes: [95, 354, 220, 315], total neurons: 984\n",
            "##########################################################\n",
            "Epoch 17/50\n",
            "Before growing:\n",
            "loss: 1.4292253255844116 - accuracy: 0.4920400083065033 - val_loss: 1.4867637157440186 - val_accuracy: 0.46779999136924744 - penalty: 1e-05\n",
            "hidden layer sizes: [95, 354, 220, 315], total neurons: 984\n",
            "After growing:\n",
            "loss: 1.950314998626709 - accuracy: 0.3100599944591522 - val_loss: 2.0165441036224365 - val_accuracy: 0.2912999987602234 - penalty: 1e-05\n",
            "hidden layer sizes: [295, 554, 420, 515], total neurons: 1784\n",
            "Before pruning:\n",
            "loss: 1.4179832935333252 - accuracy: 0.49140000343322754 - val_loss: 1.4819016456604004 - val_accuracy: 0.46799999475479126 - penalty: 1e-05\n",
            "hidden layer sizes: [295, 554, 420, 515], total neurons: 1784\n",
            "After pruning:\n",
            "loss: 1.4118236303329468 - accuracy: 0.49417999386787415 - val_loss: 1.4767733812332153 - val_accuracy: 0.4699000120162964 - penalty: 1e-05\n",
            "hidden layer sizes: [87, 320, 236, 351], total neurons: 994\n",
            "##########################################################\n",
            "Epoch 18/50\n",
            "Before growing:\n",
            "loss: 1.4118236303329468 - accuracy: 0.49417999386787415 - val_loss: 1.4767733812332153 - val_accuracy: 0.4699000120162964 - penalty: 1e-05\n",
            "hidden layer sizes: [87, 320, 236, 351], total neurons: 994\n",
            "After growing:\n",
            "loss: 1.887563705444336 - accuracy: 0.3492799997329712 - val_loss: 1.9377548694610596 - val_accuracy: 0.3425000011920929 - penalty: 1e-05\n",
            "hidden layer sizes: [287, 520, 436, 551], total neurons: 1794\n",
            "Before pruning:\n",
            "loss: 1.3970867395401 - accuracy: 0.5016400218009949 - val_loss: 1.4636856317520142 - val_accuracy: 0.4765999913215637 - penalty: 1e-05\n",
            "hidden layer sizes: [287, 520, 436, 551], total neurons: 1794\n",
            "After pruning:\n",
            "loss: 1.3981423377990723 - accuracy: 0.5011600255966187 - val_loss: 1.4651130437850952 - val_accuracy: 0.4772000014781952 - penalty: 1e-05\n",
            "hidden layer sizes: [82, 347, 219, 306], total neurons: 954\n",
            "##########################################################\n",
            "Epoch 19/50\n",
            "Before growing:\n",
            "loss: 1.3981423377990723 - accuracy: 0.5011600255966187 - val_loss: 1.4651130437850952 - val_accuracy: 0.4772000014781952 - penalty: 1e-05\n",
            "hidden layer sizes: [82, 347, 219, 306], total neurons: 954\n",
            "After growing:\n",
            "loss: 1.787264108657837 - accuracy: 0.37400001287460327 - val_loss: 1.8368706703186035 - val_accuracy: 0.3596000075340271 - penalty: 1e-05\n",
            "hidden layer sizes: [282, 547, 419, 506], total neurons: 1754\n",
            "Before pruning:\n",
            "loss: 1.3906923532485962 - accuracy: 0.5036399960517883 - val_loss: 1.4650641679763794 - val_accuracy: 0.47530001401901245 - penalty: 1e-05\n",
            "hidden layer sizes: [282, 547, 419, 506], total neurons: 1754\n",
            "After pruning:\n",
            "loss: 1.3878552913665771 - accuracy: 0.5052000284194946 - val_loss: 1.4619574546813965 - val_accuracy: 0.47609999775886536 - penalty: 1e-05\n",
            "hidden layer sizes: [96, 332, 234, 367], total neurons: 1029\n",
            "##########################################################\n",
            "Epoch 20/50\n",
            "Before growing:\n",
            "loss: 1.3878552913665771 - accuracy: 0.5052000284194946 - val_loss: 1.4619574546813965 - val_accuracy: 0.47609999775886536 - penalty: 1e-05\n",
            "hidden layer sizes: [96, 332, 234, 367], total neurons: 1029\n",
            "After growing:\n",
            "loss: 1.7540806531906128 - accuracy: 0.37571999430656433 - val_loss: 1.8089721202850342 - val_accuracy: 0.36480000615119934 - penalty: 1e-05\n",
            "hidden layer sizes: [296, 532, 434, 567], total neurons: 1829\n",
            "Before pruning:\n",
            "loss: 1.3845900297164917 - accuracy: 0.508080005645752 - val_loss: 1.4616756439208984 - val_accuracy: 0.4731000065803528 - penalty: 1e-05\n",
            "hidden layer sizes: [296, 532, 434, 567], total neurons: 1829\n",
            "After pruning:\n",
            "loss: 1.3827762603759766 - accuracy: 0.5087000131607056 - val_loss: 1.4594824314117432 - val_accuracy: 0.47699999809265137 - penalty: 1e-05\n",
            "hidden layer sizes: [87, 344, 213, 328], total neurons: 972\n",
            "##########################################################\n",
            "Epoch 21/50\n",
            "Before growing:\n",
            "loss: 1.3827762603759766 - accuracy: 0.5087000131607056 - val_loss: 1.4594824314117432 - val_accuracy: 0.47699999809265137 - penalty: 1e-05\n",
            "hidden layer sizes: [87, 344, 213, 328], total neurons: 972\n",
            "After growing:\n",
            "loss: 1.7521638870239258 - accuracy: 0.3862000107765198 - val_loss: 1.804316759109497 - val_accuracy: 0.37380000948905945 - penalty: 1e-05\n",
            "hidden layer sizes: [287, 544, 413, 528], total neurons: 1772\n",
            "Before pruning:\n",
            "loss: 1.3756071329116821 - accuracy: 0.5107200145721436 - val_loss: 1.4587956666946411 - val_accuracy: 0.47780001163482666 - penalty: 1e-05\n",
            "hidden layer sizes: [287, 544, 413, 528], total neurons: 1772\n",
            "After pruning:\n",
            "loss: 1.375014066696167 - accuracy: 0.5107600092887878 - val_loss: 1.4583853483200073 - val_accuracy: 0.4772000014781952 - penalty: 1e-05\n",
            "hidden layer sizes: [77, 302, 197, 298], total neurons: 874\n",
            "##########################################################\n",
            "Epoch 22/50\n",
            "Before growing:\n",
            "loss: 1.375014066696167 - accuracy: 0.5107600092887878 - val_loss: 1.4583853483200073 - val_accuracy: 0.4772000014781952 - penalty: 1e-05\n",
            "hidden layer sizes: [77, 302, 197, 298], total neurons: 874\n",
            "After growing:\n",
            "loss: 1.7297587394714355 - accuracy: 0.3930799961090088 - val_loss: 1.8140265941619873 - val_accuracy: 0.37049999833106995 - penalty: 1e-05\n",
            "hidden layer sizes: [277, 502, 397, 498], total neurons: 1674\n",
            "Before pruning:\n",
            "loss: 1.3687057495117188 - accuracy: 0.5113800168037415 - val_loss: 1.4524052143096924 - val_accuracy: 0.47929999232292175 - penalty: 1e-05\n",
            "hidden layer sizes: [277, 502, 397, 498], total neurons: 1674\n",
            "After pruning:\n",
            "loss: 1.3676115274429321 - accuracy: 0.512179970741272 - val_loss: 1.451059103012085 - val_accuracy: 0.4778999984264374 - penalty: 1e-05\n",
            "hidden layer sizes: [92, 356, 212, 336], total neurons: 996\n",
            "##########################################################\n",
            "Epoch 23/50\n",
            "Before growing:\n",
            "loss: 1.3676115274429321 - accuracy: 0.512179970741272 - val_loss: 1.451059103012085 - val_accuracy: 0.4778999984264374 - penalty: 1e-05\n",
            "hidden layer sizes: [92, 356, 212, 336], total neurons: 996\n",
            "After growing:\n",
            "loss: 1.7848190069198608 - accuracy: 0.3830600082874298 - val_loss: 1.8562449216842651 - val_accuracy: 0.36820000410079956 - penalty: 1e-05\n",
            "hidden layer sizes: [292, 556, 412, 536], total neurons: 1796\n",
            "Before pruning:\n",
            "loss: 1.366147518157959 - accuracy: 0.514460027217865 - val_loss: 1.4547953605651855 - val_accuracy: 0.4805000126361847 - penalty: 1e-05\n",
            "hidden layer sizes: [292, 556, 412, 536], total neurons: 1796\n",
            "After pruning:\n",
            "loss: 1.3635343313217163 - accuracy: 0.5169199705123901 - val_loss: 1.4525150060653687 - val_accuracy: 0.48159998655319214 - penalty: 1e-05\n",
            "hidden layer sizes: [74, 321, 213, 324], total neurons: 932\n",
            "##########################################################\n",
            "Epoch 24/50\n",
            "Before growing:\n",
            "loss: 1.3635343313217163 - accuracy: 0.5169199705123901 - val_loss: 1.4525150060653687 - val_accuracy: 0.48159998655319214 - penalty: 1e-05\n",
            "hidden layer sizes: [74, 321, 213, 324], total neurons: 932\n",
            "After growing:\n",
            "loss: 1.7257356643676758 - accuracy: 0.3976599872112274 - val_loss: 1.811404824256897 - val_accuracy: 0.37630000710487366 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [274, 521, 413, 524], total neurons: 1732\n",
            "Before pruning:\n",
            "loss: 1.351546287536621 - accuracy: 0.5158600211143494 - val_loss: 1.4588420391082764 - val_accuracy: 0.4805999994277954 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [274, 521, 413, 524], total neurons: 1732\n",
            "After pruning:\n",
            "loss: 1.351546287536621 - accuracy: 0.5158600211143494 - val_loss: 1.4588420391082764 - val_accuracy: 0.4805999994277954 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [274, 521, 413, 524], total neurons: 1732\n",
            "##########################################################\n",
            "Epoch 25/50\n",
            "Before growing:\n",
            "loss: 1.351546287536621 - accuracy: 0.5158600211143494 - val_loss: 1.4588420391082764 - val_accuracy: 0.4805999994277954 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [274, 521, 413, 524], total neurons: 1732\n",
            "After growing:\n",
            "loss: 1.6281810998916626 - accuracy: 0.44815999269485474 - val_loss: 1.7233176231384277 - val_accuracy: 0.4244000017642975 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [474, 781, 619, 786], total neurons: 2660\n",
            "Before pruning:\n",
            "loss: 1.388663411140442 - accuracy: 0.502020001411438 - val_loss: 1.4880850315093994 - val_accuracy: 0.47040000557899475 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [474, 781, 619, 786], total neurons: 2660\n",
            "After pruning:\n",
            "loss: 1.3887498378753662 - accuracy: 0.5019000172615051 - val_loss: 1.4881724119186401 - val_accuracy: 0.47029998898506165 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [470, 781, 619, 785], total neurons: 2655\n",
            "##########################################################\n",
            "Epoch 26/50\n",
            "Before growing:\n",
            "loss: 1.3887498378753662 - accuracy: 0.5019000172615051 - val_loss: 1.4881724119186401 - val_accuracy: 0.47029998898506165 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [470, 781, 619, 785], total neurons: 2655\n",
            "After growing:\n",
            "loss: 1.6074388027191162 - accuracy: 0.4338400065898895 - val_loss: 1.7131576538085938 - val_accuracy: 0.40310001373291016 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [705, 1171, 928, 1177], total neurons: 3981\n",
            "Before pruning:\n",
            "loss: 1.3558043241500854 - accuracy: 0.517520010471344 - val_loss: 1.4653375148773193 - val_accuracy: 0.48179998993873596 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [705, 1171, 928, 1177], total neurons: 3981\n",
            "After pruning:\n",
            "loss: 1.3544940948486328 - accuracy: 0.5179399847984314 - val_loss: 1.464605689048767 - val_accuracy: 0.4810999929904938 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [500, 1151, 914, 1160], total neurons: 3725\n",
            "##########################################################\n",
            "Epoch 27/50\n",
            "Before growing:\n",
            "loss: 1.3544940948486328 - accuracy: 0.5179399847984314 - val_loss: 1.464605689048767 - val_accuracy: 0.4810999929904938 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [500, 1151, 914, 1160], total neurons: 3725\n",
            "After growing:\n",
            "loss: 1.550826072692871 - accuracy: 0.4574599862098694 - val_loss: 1.6369423866271973 - val_accuracy: 0.4341999888420105 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [750, 1726, 1371, 1740], total neurons: 5587\n",
            "Before pruning:\n",
            "loss: 1.3372008800506592 - accuracy: 0.5253400206565857 - val_loss: 1.4552704095840454 - val_accuracy: 0.48339998722076416 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [750, 1726, 1371, 1740], total neurons: 5587\n",
            "After pruning:\n",
            "loss: 1.3356964588165283 - accuracy: 0.5262799859046936 - val_loss: 1.4544419050216675 - val_accuracy: 0.48410001397132874 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [544, 1427, 1084, 1600], total neurons: 4655\n",
            "##########################################################\n",
            "Epoch 28/50\n",
            "Before growing:\n",
            "loss: 1.3356964588165283 - accuracy: 0.5262799859046936 - val_loss: 1.4544419050216675 - val_accuracy: 0.48410001397132874 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [544, 1427, 1084, 1600], total neurons: 4655\n",
            "After growing:\n",
            "loss: 1.5371763706207275 - accuracy: 0.4491400122642517 - val_loss: 1.6466996669769287 - val_accuracy: 0.41920000314712524 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [816, 2140, 1626, 2400], total neurons: 6982\n",
            "Before pruning:\n",
            "loss: 1.3275400400161743 - accuracy: 0.5236200094223022 - val_loss: 1.4473172426223755 - val_accuracy: 0.4885999858379364 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [816, 2140, 1626, 2400], total neurons: 6982\n",
            "After pruning:\n",
            "loss: 1.3177975416183472 - accuracy: 0.5277400016784668 - val_loss: 1.4388024806976318 - val_accuracy: 0.4921000003814697 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [437, 1357, 964, 1642], total neurons: 4400\n",
            "##########################################################\n",
            "Epoch 29/50\n",
            "Before growing:\n",
            "loss: 1.3177975416183472 - accuracy: 0.5277400016784668 - val_loss: 1.4388024806976318 - val_accuracy: 0.4921000003814697 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [437, 1357, 964, 1642], total neurons: 4400\n",
            "After growing:\n",
            "loss: 1.4660531282424927 - accuracy: 0.47617998719215393 - val_loss: 1.5846673250198364 - val_accuracy: 0.448199987411499 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [655, 2035, 1446, 2463], total neurons: 6599\n",
            "Before pruning:\n",
            "loss: 1.2949954271316528 - accuracy: 0.5393199920654297 - val_loss: 1.4265252351760864 - val_accuracy: 0.49970000982284546 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [655, 2035, 1446, 2463], total neurons: 6599\n",
            "After pruning:\n",
            "loss: 1.2911776304244995 - accuracy: 0.5414800047874451 - val_loss: 1.4221720695495605 - val_accuracy: 0.501800000667572 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [515, 1315, 1030, 1168], total neurons: 4028\n",
            "##########################################################\n",
            "Epoch 30/50\n",
            "Before growing:\n",
            "loss: 1.2911776304244995 - accuracy: 0.5414800047874451 - val_loss: 1.4221720695495605 - val_accuracy: 0.501800000667572 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [515, 1315, 1030, 1168], total neurons: 4028\n",
            "After growing:\n",
            "loss: 1.4630621671676636 - accuracy: 0.48410001397132874 - val_loss: 1.568608283996582 - val_accuracy: 0.450300008058548 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [772, 1972, 1545, 1752], total neurons: 6041\n",
            "Before pruning:\n",
            "loss: 1.2883399724960327 - accuracy: 0.5395200252532959 - val_loss: 1.4334158897399902 - val_accuracy: 0.4912000000476837 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [772, 1972, 1545, 1752], total neurons: 6041\n",
            "After pruning:\n",
            "loss: 1.2845970392227173 - accuracy: 0.5412200093269348 - val_loss: 1.429729700088501 - val_accuracy: 0.49309998750686646 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [425, 1338, 1024, 1224], total neurons: 4011\n",
            "##########################################################\n",
            "Epoch 31/50\n",
            "Before growing:\n",
            "loss: 1.2845970392227173 - accuracy: 0.5412200093269348 - val_loss: 1.429729700088501 - val_accuracy: 0.49309998750686646 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [425, 1338, 1024, 1224], total neurons: 4011\n",
            "After growing:\n",
            "loss: 1.3936529159545898 - accuracy: 0.5065799951553345 - val_loss: 1.5259302854537964 - val_accuracy: 0.4627000093460083 - penalty: 1.0000000000000002e-07\n",
            "hidden layer sizes: [637, 2007, 1536, 1836], total neurons: 6016\n",
            "Before pruning:\n",
            "loss: 1.3369837999343872 - accuracy: 0.5199000239372253 - val_loss: 1.4716570377349854 - val_accuracy: 0.4844000041484833 - penalty: 1.0000000000000002e-07\n",
            "hidden layer sizes: [637, 2007, 1536, 1836], total neurons: 6016\n",
            "After pruning:\n",
            "loss: 1.3369837999343872 - accuracy: 0.5199000239372253 - val_loss: 1.4716570377349854 - val_accuracy: 0.4844000041484833 - penalty: 1.0000000000000002e-07\n",
            "hidden layer sizes: [637, 2007, 1536, 1835], total neurons: 6015\n",
            "##########################################################\n",
            "Epoch 32/50\n",
            "Before growing:\n",
            "loss: 1.3369837999343872 - accuracy: 0.5199000239372253 - val_loss: 1.4716570377349854 - val_accuracy: 0.4844000041484833 - penalty: 1.0000000000000002e-07\n",
            "hidden layer sizes: [637, 2007, 1536, 1835], total neurons: 6015\n",
            "After growing:\n",
            "loss: 1.545770287513733 - accuracy: 0.45851999521255493 - val_loss: 1.6941707134246826 - val_accuracy: 0.4205000102519989 - penalty: 1.0000000000000002e-07\n",
            "hidden layer sizes: [955, 3010, 2304, 2752], total neurons: 9021\n",
            "Before pruning:\n",
            "loss: 1.3213719129562378 - accuracy: 0.5331400036811829 - val_loss: 1.465940237045288 - val_accuracy: 0.4925999939441681 - penalty: 1.0000000000000002e-07\n",
            "hidden layer sizes: [955, 3010, 2304, 2752], total neurons: 9021\n",
            "After pruning:\n",
            "loss: 1.321372389793396 - accuracy: 0.5331400036811829 - val_loss: 1.465940237045288 - val_accuracy: 0.4925999939441681 - penalty: 1.0000000000000002e-07\n",
            "hidden layer sizes: [955, 3010, 2303, 2752], total neurons: 9020\n",
            "##########################################################\n",
            "Epoch 33/50\n",
            "Before growing:\n",
            "loss: 1.321372389793396 - accuracy: 0.5331400036811829 - val_loss: 1.465940237045288 - val_accuracy: 0.4925999939441681 - penalty: 1.0000000000000002e-07\n",
            "hidden layer sizes: [955, 3010, 2303, 2752], total neurons: 9020\n",
            "After growing:\n",
            "loss: 1.4747949838638306 - accuracy: 0.4896799921989441 - val_loss: 1.611956000328064 - val_accuracy: 0.45899999141693115 - penalty: 1.0000000000000002e-07\n",
            "hidden layer sizes: [1432, 4515, 3454, 4128], total neurons: 13529\n",
            "Before pruning:\n",
            "loss: 1.434297800064087 - accuracy: 0.5119600296020508 - val_loss: 1.5855767726898193 - val_accuracy: 0.4740000069141388 - penalty: 1.0000000000000002e-07\n",
            "hidden layer sizes: [1432, 4515, 3454, 4128], total neurons: 13529\n",
            "After pruning:\n",
            "loss: 1.4332003593444824 - accuracy: 0.5123999714851379 - val_loss: 1.5844188928604126 - val_accuracy: 0.47380000352859497 - penalty: 1.0000000000000002e-07\n",
            "hidden layer sizes: [1423, 4187, 3196, 4128], total neurons: 12934\n",
            "##########################################################\n",
            "Epoch 34/50\n",
            "Before growing:\n",
            "loss: 1.4332003593444824 - accuracy: 0.5123999714851379 - val_loss: 1.5844188928604126 - val_accuracy: 0.47380000352859497 - penalty: 1.0000000000000002e-07\n",
            "hidden layer sizes: [1423, 4187, 3196, 4128], total neurons: 12934\n",
            "After growing:\n",
            "loss: 1.7625025510787964 - accuracy: 0.4183799922466278 - val_loss: 1.9260019063949585 - val_accuracy: 0.39010000228881836 - penalty: 1.0000000000000002e-07\n",
            "hidden layer sizes: [2134, 6280, 4794, 6192], total neurons: 19400\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-47001d43738a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\nmodel = SSModel(layer_sizes=[3072, 300, 300, 300, 300, 10], activation='selu', regularization_penalty=0.0001, regularization_method='weighted_l1', kernel_initializer='lecun_normal')\\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.0002)\\n\\ntrain_model(model, X_train_norm, y_train, optimizer, epochs, self_scaling_epochs, batch_size, min_new_neurons, validation_data=(X_test_norm, y_test), regularization_penalty_multiplier=0.1, growth_percentage=0.5)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-53>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-67746070c246>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, x, y, optimizer, epochs, self_scaling_epochs, batch_size, min_new_neurons, validation_data, pruning_threshold, regularization_penalty_multiplier, growth_percentage, print_neurons)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself_scaling_epochs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, name, experimental_aggregate_gradients)\u001b[0m\n\u001b[1;32m    653\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mexperimental_aggregate_gradients\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform_unaggregated_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_aggregate_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m       \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36m_aggregate_gradients\u001b[0;34m(self, grads_and_vars)\u001b[0m\n\u001b[1;32m    472\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_aggregator\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m     \"\"\"\n\u001b[0;32m--> 474\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_aggregator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_transform_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/utils.py\u001b[0m in \u001b[0;36mall_reduce_sum_gradients\u001b[0;34m(grads_and_vars)\u001b[0m\n\u001b[1;32m     34\u001b[0m       \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpair\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiltered_grads_and_vars\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m       reduced = tf.distribute.get_strategy().extended._replica_ctx_all_reduce(  # pylint: disable=protected-access\n\u001b[0;32m---> 36\u001b[0;31m           tf.distribute.ReduceOp.SUM, grads)\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m       \u001b[0;31m# TODO(b/183257003): Remove this branch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_replica_ctx_all_reduce\u001b[0;34m(self, reduce_op, value, options)\u001b[0m\n\u001b[1;32m   2456\u001b[0m                                   options)\n\u001b[1;32m   2457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2458\u001b[0;31m     \u001b[0mreduced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplica_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerge_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2459\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduced\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mmerge_call\u001b[0;34m(self, merge_fn, args, kwargs)\u001b[0m\n\u001b[1;32m   3062\u001b[0m     merge_fn = autograph.tf_convert(\n\u001b[1;32m   3063\u001b[0m         merge_fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[0;32m-> 3064\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merge_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerge_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3066\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_merge_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_merge_call\u001b[0;34m(self, merge_fn, args, kwargs)\u001b[0m\n\u001b[1;32m   3069\u001b[0m         distribution_strategy_context._CrossReplicaThreadMode(self._strategy))  # pylint: disable=protected-access\n\u001b[1;32m   3070\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3071\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmerge_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_strategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3072\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3073\u001b[0m       \u001b[0m_pop_per_thread_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    595\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUNSPECIFIED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mismethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mmerge_fn\u001b[0;34m(_, flat_value)\u001b[0m\n\u001b[1;32m   2454\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmerge_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2455\u001b[0m       return self.batch_reduce_to(reduce_op, [(v, v) for v in flat_value],\n\u001b[0;32m-> 2456\u001b[0;31m                                   options)\n\u001b[0m\u001b[1;32m   2457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2458\u001b[0m     \u001b[0mreduced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplica_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerge_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mbatch_reduce_to\u001b[0;34m(self, reduce_op, value_destination_pairs, options)\u001b[0m\n\u001b[1;32m   2416\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2417\u001b[0m       \u001b[0mreduce_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReduceOp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduce_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2418\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_reduce_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_destination_pairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2420\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_batch_reduce_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_destination_pairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_batch_reduce_to\u001b[0;34m(self, reduce_op, value_destination_pairs, options)\u001b[0m\n\u001b[1;32m   2421\u001b[0m     return [\n\u001b[1;32m   2422\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestinations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2423\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalue_destination_pairs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2424\u001b[0m     ]\n\u001b[1;32m   2425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2421\u001b[0m     return [\n\u001b[1;32m   2422\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestinations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2423\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalue_destination_pairs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2424\u001b[0m     ]\n\u001b[1;32m   2425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzQgJRlh89M2",
        "outputId": "9c022b1b-e422-4245-e28c-cbd2c8c0d2cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# SCALING FACTOR *= 1000, SCALED ON SINGLE AXIS\n",
        "\n",
        "%%time\n",
        "\n",
        "model = SSModel(layer_sizes=[3072, 300, 300, 300, 300, 10], activation='selu', regularization_penalty=0.0001, regularization_method='weighted_l1', kernel_initializer='lecun_normal')\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002)\n",
        "\n",
        "train_model(model, X_train_norm, y_train, optimizer, epochs, self_scaling_epochs, batch_size, min_new_neurons, validation_data=(X_test_norm, y_test), regularization_penalty_multiplier=0.1, growth_percentage=0.5)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "##########################################################\n",
            "Epoch 1/50\n",
            "Before growing:\n",
            "loss: 2.597078561782837 - accuracy: 0.1146399974822998 - val_loss: 2.581150770187378 - val_accuracy: 0.11819999665021896 - penalty: 0.0001\n",
            "hidden layer sizes: [300, 300, 300, 300], total neurons: 1200\n",
            "After growing:\n",
            "loss: 3.046143054962158 - accuracy: 0.10559999942779541 - val_loss: 3.03760027885437 - val_accuracy: 0.10509999841451645 - penalty: 0.0001\n",
            "hidden layer sizes: [500, 500, 500, 500], total neurons: 2000\n",
            "Before pruning:\n",
            "loss: 1.852358102798462 - accuracy: 0.31672000885009766 - val_loss: 1.853013277053833 - val_accuracy: 0.314300000667572 - penalty: 0.0001\n",
            "hidden layer sizes: [500, 500, 500, 500], total neurons: 2000\n",
            "After pruning:\n",
            "loss: 1.8524550199508667 - accuracy: 0.3180199861526489 - val_loss: 1.8531001806259155 - val_accuracy: 0.31679999828338623 - penalty: 0.0001\n",
            "hidden layer sizes: [18, 58, 36, 73], total neurons: 185\n",
            "##########################################################\n",
            "Epoch 2/50\n",
            "Before growing:\n",
            "loss: 1.8524550199508667 - accuracy: 0.3180199861526489 - val_loss: 1.8531001806259155 - val_accuracy: 0.31679999828338623 - penalty: 0.0001\n",
            "hidden layer sizes: [18, 58, 36, 73], total neurons: 185\n",
            "After growing:\n",
            "loss: 2.4386813640594482 - accuracy: 0.17047999799251556 - val_loss: 2.4411420822143555 - val_accuracy: 0.16349999606609344 - penalty: 0.0001\n",
            "hidden layer sizes: [218, 258, 236, 273], total neurons: 985\n",
            "Before pruning:\n",
            "loss: 1.791007161140442 - accuracy: 0.3342599868774414 - val_loss: 1.7938687801361084 - val_accuracy: 0.33730000257492065 - penalty: 0.0001\n",
            "hidden layer sizes: [218, 258, 236, 273], total neurons: 985\n",
            "After pruning:\n",
            "loss: 1.7889823913574219 - accuracy: 0.3355199992656708 - val_loss: 1.7916431427001953 - val_accuracy: 0.3391000032424927 - penalty: 0.0001\n",
            "hidden layer sizes: [16, 47, 20, 60], total neurons: 143\n",
            "##########################################################\n",
            "Epoch 3/50\n",
            "Before growing:\n",
            "loss: 1.7889823913574219 - accuracy: 0.3355199992656708 - val_loss: 1.7916431427001953 - val_accuracy: 0.3391000032424927 - penalty: 0.0001\n",
            "hidden layer sizes: [16, 47, 20, 60], total neurons: 143\n",
            "After growing:\n",
            "loss: 2.476468801498413 - accuracy: 0.18265999853610992 - val_loss: 2.4699718952178955 - val_accuracy: 0.18140000104904175 - penalty: 0.0001\n",
            "hidden layer sizes: [216, 247, 220, 260], total neurons: 943\n",
            "Before pruning:\n",
            "loss: 1.7472655773162842 - accuracy: 0.35506001114845276 - val_loss: 1.7521661520004272 - val_accuracy: 0.350600004196167 - penalty: 0.0001\n",
            "hidden layer sizes: [216, 247, 220, 260], total neurons: 943\n",
            "After pruning:\n",
            "loss: 1.7481287717819214 - accuracy: 0.3546000123023987 - val_loss: 1.7535666227340698 - val_accuracy: 0.35109999775886536 - penalty: 0.0001\n",
            "hidden layer sizes: [13, 47, 21, 55], total neurons: 136\n",
            "##########################################################\n",
            "Epoch 4/50\n",
            "Before growing:\n",
            "loss: 1.7481287717819214 - accuracy: 0.3546000123023987 - val_loss: 1.7535666227340698 - val_accuracy: 0.35109999775886536 - penalty: 0.0001\n",
            "hidden layer sizes: [13, 47, 21, 55], total neurons: 136\n",
            "After growing:\n",
            "loss: 2.4718570709228516 - accuracy: 0.18016000092029572 - val_loss: 2.477510452270508 - val_accuracy: 0.18039999902248383 - penalty: 0.0001\n",
            "hidden layer sizes: [213, 247, 221, 255], total neurons: 936\n",
            "Before pruning:\n",
            "loss: 1.7378919124603271 - accuracy: 0.35826000571250916 - val_loss: 1.746766448020935 - val_accuracy: 0.35580000281333923 - penalty: 0.0001\n",
            "hidden layer sizes: [213, 247, 221, 255], total neurons: 936\n",
            "After pruning:\n",
            "loss: 1.7378703355789185 - accuracy: 0.3591200113296509 - val_loss: 1.7463421821594238 - val_accuracy: 0.35510000586509705 - penalty: 0.0001\n",
            "hidden layer sizes: [10, 52, 18, 54], total neurons: 134\n",
            "##########################################################\n",
            "Epoch 5/50\n",
            "Before growing:\n",
            "loss: 1.7378703355789185 - accuracy: 0.3591200113296509 - val_loss: 1.7463421821594238 - val_accuracy: 0.35510000586509705 - penalty: 0.0001\n",
            "hidden layer sizes: [10, 52, 18, 54], total neurons: 134\n",
            "After growing:\n",
            "loss: 2.8197834491729736 - accuracy: 0.13091999292373657 - val_loss: 2.8037655353546143 - val_accuracy: 0.13289999961853027 - penalty: 0.0001\n",
            "hidden layer sizes: [210, 252, 218, 254], total neurons: 934\n",
            "Before pruning:\n",
            "loss: 1.7143259048461914 - accuracy: 0.36517998576164246 - val_loss: 1.7256245613098145 - val_accuracy: 0.36039999127388 - penalty: 0.0001\n",
            "hidden layer sizes: [210, 252, 218, 254], total neurons: 934\n",
            "After pruning:\n",
            "loss: 1.714871883392334 - accuracy: 0.36531999707221985 - val_loss: 1.7259488105773926 - val_accuracy: 0.3594000041484833 - penalty: 0.0001\n",
            "hidden layer sizes: [9, 36, 24, 53], total neurons: 122\n",
            "##########################################################\n",
            "Epoch 6/50\n",
            "Before growing:\n",
            "loss: 1.714871883392334 - accuracy: 0.36531999707221985 - val_loss: 1.7259488105773926 - val_accuracy: 0.3594000041484833 - penalty: 0.0001\n",
            "hidden layer sizes: [9, 36, 24, 53], total neurons: 122\n",
            "After growing:\n",
            "loss: 2.44398832321167 - accuracy: 0.20367999374866486 - val_loss: 2.4309775829315186 - val_accuracy: 0.20509999990463257 - penalty: 0.0001\n",
            "hidden layer sizes: [209, 236, 224, 253], total neurons: 922\n",
            "Before pruning:\n",
            "loss: 1.7062021493911743 - accuracy: 0.36974000930786133 - val_loss: 1.7198964357376099 - val_accuracy: 0.3659999966621399 - penalty: 0.0001\n",
            "hidden layer sizes: [209, 236, 224, 253], total neurons: 922\n",
            "After pruning:\n",
            "loss: 1.7066056728363037 - accuracy: 0.3693599998950958 - val_loss: 1.7205477952957153 - val_accuracy: 0.36640000343322754 - penalty: 0.0001\n",
            "hidden layer sizes: [13, 44, 24, 56], total neurons: 137\n",
            "##########################################################\n",
            "Epoch 7/50\n",
            "Before growing:\n",
            "loss: 1.7066056728363037 - accuracy: 0.3693599998950958 - val_loss: 1.7205477952957153 - val_accuracy: 0.36640000343322754 - penalty: 0.0001\n",
            "hidden layer sizes: [13, 44, 24, 56], total neurons: 137\n",
            "After growing:\n",
            "loss: 2.317166566848755 - accuracy: 0.21505999565124512 - val_loss: 2.3368847370147705 - val_accuracy: 0.2087000012397766 - penalty: 0.0001\n",
            "hidden layer sizes: [213, 244, 224, 256], total neurons: 937\n",
            "Before pruning:\n",
            "loss: 1.693251371383667 - accuracy: 0.37163999676704407 - val_loss: 1.706816554069519 - val_accuracy: 0.367900013923645 - penalty: 0.0001\n",
            "hidden layer sizes: [213, 244, 224, 256], total neurons: 937\n",
            "After pruning:\n",
            "loss: 1.6937962770462036 - accuracy: 0.3717600107192993 - val_loss: 1.7076667547225952 - val_accuracy: 0.36800000071525574 - penalty: 0.0001\n",
            "hidden layer sizes: [11, 49, 28, 52], total neurons: 140\n",
            "##########################################################\n",
            "Epoch 8/50\n",
            "Before growing:\n",
            "loss: 1.6937962770462036 - accuracy: 0.3717600107192993 - val_loss: 1.7076667547225952 - val_accuracy: 0.36800000071525574 - penalty: 0.0001\n",
            "hidden layer sizes: [11, 49, 28, 52], total neurons: 140\n",
            "After growing:\n",
            "loss: 2.3213818073272705 - accuracy: 0.21996000409126282 - val_loss: 2.3195488452911377 - val_accuracy: 0.22349999845027924 - penalty: 0.0001\n",
            "hidden layer sizes: [211, 249, 228, 252], total neurons: 940\n",
            "Before pruning:\n",
            "loss: 1.676041841506958 - accuracy: 0.38058000802993774 - val_loss: 1.6940985918045044 - val_accuracy: 0.373199999332428 - penalty: 0.0001\n",
            "hidden layer sizes: [211, 249, 228, 252], total neurons: 940\n",
            "After pruning:\n",
            "loss: 1.6727162599563599 - accuracy: 0.3806999921798706 - val_loss: 1.690474033355713 - val_accuracy: 0.37619999051094055 - penalty: 0.0001\n",
            "hidden layer sizes: [17, 42, 26, 52], total neurons: 137\n",
            "##########################################################\n",
            "Epoch 9/50\n",
            "Before growing:\n",
            "loss: 1.6727162599563599 - accuracy: 0.3806999921798706 - val_loss: 1.690474033355713 - val_accuracy: 0.37619999051094055 - penalty: 0.0001\n",
            "hidden layer sizes: [17, 42, 26, 52], total neurons: 137\n",
            "After growing:\n",
            "loss: 2.1926462650299072 - accuracy: 0.2566399872303009 - val_loss: 2.2133185863494873 - val_accuracy: 0.24539999663829803 - penalty: 0.0001\n",
            "hidden layer sizes: [217, 242, 226, 252], total neurons: 937\n",
            "Before pruning:\n",
            "loss: 1.6598511934280396 - accuracy: 0.3903999924659729 - val_loss: 1.6787301301956177 - val_accuracy: 0.37940001487731934 - penalty: 0.0001\n",
            "hidden layer sizes: [217, 242, 226, 252], total neurons: 937\n",
            "After pruning:\n",
            "loss: 1.6607040166854858 - accuracy: 0.3908799886703491 - val_loss: 1.6795464754104614 - val_accuracy: 0.37959998846054077 - penalty: 0.0001\n",
            "hidden layer sizes: [13, 52, 29, 73], total neurons: 167\n",
            "##########################################################\n",
            "Epoch 10/50\n",
            "Before growing:\n",
            "loss: 1.6607040166854858 - accuracy: 0.3908799886703491 - val_loss: 1.6795464754104614 - val_accuracy: 0.37959998846054077 - penalty: 0.0001\n",
            "hidden layer sizes: [13, 52, 29, 73], total neurons: 167\n",
            "After growing:\n",
            "loss: 2.3312833309173584 - accuracy: 0.24459999799728394 - val_loss: 2.343878984451294 - val_accuracy: 0.23810000717639923 - penalty: 0.0001\n",
            "hidden layer sizes: [213, 252, 229, 273], total neurons: 967\n",
            "Before pruning:\n",
            "loss: 1.6447498798370361 - accuracy: 0.40018001198768616 - val_loss: 1.6652278900146484 - val_accuracy: 0.3905999958515167 - penalty: 0.0001\n",
            "hidden layer sizes: [213, 252, 229, 273], total neurons: 967\n",
            "After pruning:\n",
            "loss: 1.6448556184768677 - accuracy: 0.4005599915981293 - val_loss: 1.6654138565063477 - val_accuracy: 0.3912000060081482 - penalty: 0.0001\n",
            "hidden layer sizes: [14, 50, 28, 60], total neurons: 152\n",
            "##########################################################\n",
            "Epoch 11/50\n",
            "Before growing:\n",
            "loss: 1.6448556184768677 - accuracy: 0.4005599915981293 - val_loss: 1.6654138565063477 - val_accuracy: 0.3912000060081482 - penalty: 0.0001\n",
            "hidden layer sizes: [14, 50, 28, 60], total neurons: 152\n",
            "After growing:\n",
            "loss: 2.4896671772003174 - accuracy: 0.20802000164985657 - val_loss: 2.4854702949523926 - val_accuracy: 0.2168000042438507 - penalty: 0.0001\n",
            "hidden layer sizes: [214, 250, 228, 260], total neurons: 952\n",
            "Before pruning:\n",
            "loss: 1.6432870626449585 - accuracy: 0.4000599980354309 - val_loss: 1.6627259254455566 - val_accuracy: 0.38420000672340393 - penalty: 0.0001\n",
            "hidden layer sizes: [214, 250, 228, 260], total neurons: 952\n",
            "After pruning:\n",
            "loss: 1.6446771621704102 - accuracy: 0.39917999505996704 - val_loss: 1.6636216640472412 - val_accuracy: 0.38499999046325684 - penalty: 0.0001\n",
            "hidden layer sizes: [12, 38, 27, 59], total neurons: 136\n",
            "##########################################################\n",
            "Epoch 12/50\n",
            "Before growing:\n",
            "loss: 1.6446771621704102 - accuracy: 0.39917999505996704 - val_loss: 1.6636216640472412 - val_accuracy: 0.38499999046325684 - penalty: 0.0001\n",
            "hidden layer sizes: [12, 38, 27, 59], total neurons: 136\n",
            "After growing:\n",
            "loss: 2.5064749717712402 - accuracy: 0.2348800003528595 - val_loss: 2.516207695007324 - val_accuracy: 0.23409999907016754 - penalty: 0.0001\n",
            "hidden layer sizes: [212, 238, 227, 259], total neurons: 936\n",
            "Before pruning:\n",
            "loss: 1.634229063987732 - accuracy: 0.4050399959087372 - val_loss: 1.656270146369934 - val_accuracy: 0.39079999923706055 - penalty: 0.0001\n",
            "hidden layer sizes: [212, 238, 227, 259], total neurons: 936\n",
            "After pruning:\n",
            "loss: 1.6339207887649536 - accuracy: 0.4043999910354614 - val_loss: 1.6555075645446777 - val_accuracy: 0.3901999890804291 - penalty: 0.0001\n",
            "hidden layer sizes: [21, 43, 27, 52], total neurons: 143\n",
            "##########################################################\n",
            "Epoch 13/50\n",
            "Before growing:\n",
            "loss: 1.6339207887649536 - accuracy: 0.4043999910354614 - val_loss: 1.6555075645446777 - val_accuracy: 0.3901999890804291 - penalty: 0.0001\n",
            "hidden layer sizes: [21, 43, 27, 52], total neurons: 143\n",
            "After growing:\n",
            "loss: 2.5418124198913574 - accuracy: 0.22020000219345093 - val_loss: 2.5235860347747803 - val_accuracy: 0.22269999980926514 - penalty: 0.0001\n",
            "hidden layer sizes: [221, 243, 227, 252], total neurons: 943\n",
            "Before pruning:\n",
            "loss: 1.6250474452972412 - accuracy: 0.4086199998855591 - val_loss: 1.651383638381958 - val_accuracy: 0.3937000036239624 - penalty: 0.0001\n",
            "hidden layer sizes: [221, 243, 227, 252], total neurons: 943\n",
            "After pruning:\n",
            "loss: 1.624853253364563 - accuracy: 0.4077000021934509 - val_loss: 1.6506404876708984 - val_accuracy: 0.3937000036239624 - penalty: 0.0001\n",
            "hidden layer sizes: [15, 38, 31, 80], total neurons: 164\n",
            "##########################################################\n",
            "Epoch 14/50\n",
            "Before growing:\n",
            "loss: 1.624853253364563 - accuracy: 0.4077000021934509 - val_loss: 1.6506404876708984 - val_accuracy: 0.3937000036239624 - penalty: 0.0001\n",
            "hidden layer sizes: [15, 38, 31, 80], total neurons: 164\n",
            "After growing:\n",
            "loss: 2.3435616493225098 - accuracy: 0.24772000312805176 - val_loss: 2.3581857681274414 - val_accuracy: 0.24289999902248383 - penalty: 0.0001\n",
            "hidden layer sizes: [215, 238, 231, 280], total neurons: 964\n",
            "Before pruning:\n",
            "loss: 1.6296133995056152 - accuracy: 0.40450000762939453 - val_loss: 1.6551238298416138 - val_accuracy: 0.3928000032901764 - penalty: 0.0001\n",
            "hidden layer sizes: [215, 238, 231, 280], total neurons: 964\n",
            "After pruning:\n",
            "loss: 1.6286252737045288 - accuracy: 0.40544000267982483 - val_loss: 1.6540746688842773 - val_accuracy: 0.3939000070095062 - penalty: 0.0001\n",
            "hidden layer sizes: [16, 51, 24, 63], total neurons: 154\n",
            "##########################################################\n",
            "Epoch 15/50\n",
            "Before growing:\n",
            "loss: 1.6286252737045288 - accuracy: 0.40544000267982483 - val_loss: 1.6540746688842773 - val_accuracy: 0.3939000070095062 - penalty: 0.0001\n",
            "hidden layer sizes: [16, 51, 24, 63], total neurons: 154\n",
            "After growing:\n",
            "loss: 2.302490472793579 - accuracy: 0.26583999395370483 - val_loss: 2.3237383365631104 - val_accuracy: 0.26109999418258667 - penalty: 1e-05\n",
            "hidden layer sizes: [216, 251, 224, 263], total neurons: 954\n",
            "Before pruning:\n",
            "loss: 1.5374010801315308 - accuracy: 0.44839999079704285 - val_loss: 1.5723209381103516 - val_accuracy: 0.43630000948905945 - penalty: 1e-05\n",
            "hidden layer sizes: [216, 251, 224, 263], total neurons: 954\n",
            "After pruning:\n",
            "loss: 1.5352246761322021 - accuracy: 0.44944000244140625 - val_loss: 1.5700069665908813 - val_accuracy: 0.43790000677108765 - penalty: 1e-05\n",
            "hidden layer sizes: [127, 251, 224, 263], total neurons: 865\n",
            "##########################################################\n",
            "Epoch 16/50\n",
            "Before growing:\n",
            "loss: 1.5352246761322021 - accuracy: 0.44944000244140625 - val_loss: 1.5700069665908813 - val_accuracy: 0.43790000677108765 - penalty: 1e-05\n",
            "hidden layer sizes: [127, 251, 224, 263], total neurons: 865\n",
            "After growing:\n",
            "loss: 2.0363216400146484 - accuracy: 0.2991800010204315 - val_loss: 2.054994821548462 - val_accuracy: 0.2985000014305115 - penalty: 1e-05\n",
            "hidden layer sizes: [327, 451, 424, 463], total neurons: 1665\n",
            "Before pruning:\n",
            "loss: 1.5067096948623657 - accuracy: 0.4563399851322174 - val_loss: 1.548185110092163 - val_accuracy: 0.4433000087738037 - penalty: 1e-05\n",
            "hidden layer sizes: [327, 451, 424, 463], total neurons: 1665\n",
            "After pruning:\n",
            "loss: 1.5029351711273193 - accuracy: 0.4573200047016144 - val_loss: 1.5443906784057617 - val_accuracy: 0.4429999887943268 - penalty: 1e-05\n",
            "hidden layer sizes: [100, 375, 305, 376], total neurons: 1156\n",
            "##########################################################\n",
            "Epoch 17/50\n",
            "Before growing:\n",
            "loss: 1.5029351711273193 - accuracy: 0.4573200047016144 - val_loss: 1.5443906784057617 - val_accuracy: 0.4429999887943268 - penalty: 1e-05\n",
            "hidden layer sizes: [100, 375, 305, 376], total neurons: 1156\n",
            "After growing:\n",
            "loss: 1.9602781534194946 - accuracy: 0.31161999702453613 - val_loss: 1.9803720712661743 - val_accuracy: 0.30640000104904175 - penalty: 1e-05\n",
            "hidden layer sizes: [300, 575, 505, 576], total neurons: 1956\n",
            "Before pruning:\n",
            "loss: 1.4850502014160156 - accuracy: 0.46612000465393066 - val_loss: 1.5286608934402466 - val_accuracy: 0.4551999866962433 - penalty: 1e-05\n",
            "hidden layer sizes: [300, 575, 505, 576], total neurons: 1956\n",
            "After pruning:\n",
            "loss: 1.481213927268982 - accuracy: 0.4678199887275696 - val_loss: 1.5257415771484375 - val_accuracy: 0.45739999413490295 - penalty: 1e-05\n",
            "hidden layer sizes: [97, 304, 219, 288], total neurons: 908\n",
            "##########################################################\n",
            "Epoch 18/50\n",
            "Before growing:\n",
            "loss: 1.481213927268982 - accuracy: 0.4678199887275696 - val_loss: 1.5257415771484375 - val_accuracy: 0.45739999413490295 - penalty: 1e-05\n",
            "hidden layer sizes: [97, 304, 219, 288], total neurons: 908\n",
            "After growing:\n",
            "loss: 2.002941608428955 - accuracy: 0.31481999158859253 - val_loss: 2.0510506629943848 - val_accuracy: 0.3091000020503998 - penalty: 1e-05\n",
            "hidden layer sizes: [297, 504, 419, 488], total neurons: 1708\n",
            "Before pruning:\n",
            "loss: 1.4543977975845337 - accuracy: 0.47585999965667725 - val_loss: 1.5049841403961182 - val_accuracy: 0.4652999937534332 - penalty: 1e-05\n",
            "hidden layer sizes: [297, 504, 419, 488], total neurons: 1708\n",
            "After pruning:\n",
            "loss: 1.4546356201171875 - accuracy: 0.476859986782074 - val_loss: 1.5053716897964478 - val_accuracy: 0.46389999985694885 - penalty: 1e-05\n",
            "hidden layer sizes: [67, 353, 265, 337], total neurons: 1022\n",
            "##########################################################\n",
            "Epoch 19/50\n",
            "Before growing:\n",
            "loss: 1.4546356201171875 - accuracy: 0.476859986782074 - val_loss: 1.5053716897964478 - val_accuracy: 0.46389999985694885 - penalty: 1e-05\n",
            "hidden layer sizes: [67, 353, 265, 337], total neurons: 1022\n",
            "After growing:\n",
            "loss: 2.068791627883911 - accuracy: 0.29778000712394714 - val_loss: 2.1080684661865234 - val_accuracy: 0.2856999933719635 - penalty: 1e-05\n",
            "hidden layer sizes: [267, 553, 465, 537], total neurons: 1822\n",
            "Before pruning:\n",
            "loss: 1.4550197124481201 - accuracy: 0.47488000988960266 - val_loss: 1.5118821859359741 - val_accuracy: 0.46230000257492065 - penalty: 1e-05\n",
            "hidden layer sizes: [267, 553, 465, 537], total neurons: 1822\n",
            "After pruning:\n",
            "loss: 1.4483137130737305 - accuracy: 0.47718000411987305 - val_loss: 1.5041851997375488 - val_accuracy: 0.4641000032424927 - penalty: 1e-05\n",
            "hidden layer sizes: [108, 334, 246, 273], total neurons: 961\n",
            "##########################################################\n",
            "Epoch 20/50\n",
            "Before growing:\n",
            "loss: 1.4483137130737305 - accuracy: 0.47718000411987305 - val_loss: 1.5041851997375488 - val_accuracy: 0.4641000032424927 - penalty: 1e-05\n",
            "hidden layer sizes: [108, 334, 246, 273], total neurons: 961\n",
            "After growing:\n",
            "loss: 1.9928758144378662 - accuracy: 0.3108600080013275 - val_loss: 2.020134687423706 - val_accuracy: 0.3043000102043152 - penalty: 1e-05\n",
            "hidden layer sizes: [308, 534, 446, 473], total neurons: 1761\n",
            "Before pruning:\n",
            "loss: 1.4436895847320557 - accuracy: 0.4800800085067749 - val_loss: 1.5001342296600342 - val_accuracy: 0.4681999981403351 - penalty: 1e-05\n",
            "hidden layer sizes: [308, 534, 446, 473], total neurons: 1761\n",
            "After pruning:\n",
            "loss: 1.4405834674835205 - accuracy: 0.4825200140476227 - val_loss: 1.4966230392456055 - val_accuracy: 0.4699000120162964 - penalty: 1e-05\n",
            "hidden layer sizes: [80, 320, 201, 348], total neurons: 949\n",
            "##########################################################\n",
            "Epoch 21/50\n",
            "Before growing:\n",
            "loss: 1.4405834674835205 - accuracy: 0.4825200140476227 - val_loss: 1.4966230392456055 - val_accuracy: 0.4699000120162964 - penalty: 1e-05\n",
            "hidden layer sizes: [80, 320, 201, 348], total neurons: 949\n",
            "After growing:\n",
            "loss: 1.8771799802780151 - accuracy: 0.3351399898529053 - val_loss: 1.9242647886276245 - val_accuracy: 0.32850000262260437 - penalty: 1e-05\n",
            "hidden layer sizes: [280, 520, 401, 548], total neurons: 1749\n",
            "Before pruning:\n",
            "loss: 1.4191136360168457 - accuracy: 0.48962000012397766 - val_loss: 1.482852578163147 - val_accuracy: 0.47049999237060547 - penalty: 1e-05\n",
            "hidden layer sizes: [280, 520, 401, 548], total neurons: 1749\n",
            "After pruning:\n",
            "loss: 1.4176132678985596 - accuracy: 0.491239994764328 - val_loss: 1.482194185256958 - val_accuracy: 0.47189998626708984 - penalty: 1e-05\n",
            "hidden layer sizes: [72, 294, 263, 331], total neurons: 960\n",
            "##########################################################\n",
            "Epoch 22/50\n",
            "Before growing:\n",
            "loss: 1.4176132678985596 - accuracy: 0.491239994764328 - val_loss: 1.482194185256958 - val_accuracy: 0.47189998626708984 - penalty: 1e-05\n",
            "hidden layer sizes: [72, 294, 263, 331], total neurons: 960\n",
            "After growing:\n",
            "loss: 1.8836950063705444 - accuracy: 0.34463998675346375 - val_loss: 1.9385912418365479 - val_accuracy: 0.3393000066280365 - penalty: 1e-05\n",
            "hidden layer sizes: [272, 494, 463, 531], total neurons: 1760\n",
            "Before pruning:\n",
            "loss: 1.4174869060516357 - accuracy: 0.49125999212265015 - val_loss: 1.4828228950500488 - val_accuracy: 0.4713999927043915 - penalty: 1e-05\n",
            "hidden layer sizes: [272, 494, 463, 531], total neurons: 1760\n",
            "After pruning:\n",
            "loss: 1.4133520126342773 - accuracy: 0.4934200048446655 - val_loss: 1.4796507358551025 - val_accuracy: 0.4729999899864197 - penalty: 1e-05\n",
            "hidden layer sizes: [97, 354, 205, 273], total neurons: 929\n",
            "##########################################################\n",
            "Epoch 23/50\n",
            "Before growing:\n",
            "loss: 1.4133520126342773 - accuracy: 0.4934200048446655 - val_loss: 1.4796507358551025 - val_accuracy: 0.4729999899864197 - penalty: 1e-05\n",
            "hidden layer sizes: [97, 354, 205, 273], total neurons: 929\n",
            "After growing:\n",
            "loss: 1.7659317255020142 - accuracy: 0.3826799988746643 - val_loss: 1.82048499584198 - val_accuracy: 0.3693000078201294 - penalty: 1e-05\n",
            "hidden layer sizes: [297, 554, 405, 473], total neurons: 1729\n",
            "Before pruning:\n",
            "loss: 1.4032303094863892 - accuracy: 0.49498000741004944 - val_loss: 1.474476933479309 - val_accuracy: 0.4731000065803528 - penalty: 1e-05\n",
            "hidden layer sizes: [297, 554, 405, 473], total neurons: 1729\n",
            "After pruning:\n",
            "loss: 1.4003208875656128 - accuracy: 0.4962199926376343 - val_loss: 1.4718984365463257 - val_accuracy: 0.47620001435279846 - penalty: 1e-05\n",
            "hidden layer sizes: [84, 220, 202, 344], total neurons: 850\n",
            "##########################################################\n",
            "Epoch 24/50\n",
            "Before growing:\n",
            "loss: 1.4003208875656128 - accuracy: 0.4962199926376343 - val_loss: 1.4718984365463257 - val_accuracy: 0.47620001435279846 - penalty: 1e-05\n",
            "hidden layer sizes: [84, 220, 202, 344], total neurons: 850\n",
            "After growing:\n",
            "loss: 1.8180370330810547 - accuracy: 0.36250001192092896 - val_loss: 1.8829941749572754 - val_accuracy: 0.3481999933719635 - penalty: 1e-05\n",
            "hidden layer sizes: [284, 420, 402, 544], total neurons: 1650\n",
            "Before pruning:\n",
            "loss: 1.400847315788269 - accuracy: 0.4978399872779846 - val_loss: 1.4733036756515503 - val_accuracy: 0.47450000047683716 - penalty: 1e-05\n",
            "hidden layer sizes: [284, 420, 402, 544], total neurons: 1650\n",
            "After pruning:\n",
            "loss: 1.397093415260315 - accuracy: 0.49911999702453613 - val_loss: 1.4701614379882812 - val_accuracy: 0.4749999940395355 - penalty: 1e-05\n",
            "hidden layer sizes: [74, 341, 290, 388], total neurons: 1093\n",
            "##########################################################\n",
            "Epoch 25/50\n",
            "Before growing:\n",
            "loss: 1.397093415260315 - accuracy: 0.49911999702453613 - val_loss: 1.4701614379882812 - val_accuracy: 0.4749999940395355 - penalty: 1e-05\n",
            "hidden layer sizes: [74, 341, 290, 388], total neurons: 1093\n",
            "After growing:\n",
            "loss: 1.747615933418274 - accuracy: 0.3774400055408478 - val_loss: 1.7922810316085815 - val_accuracy: 0.36480000615119934 - penalty: 1e-05\n",
            "hidden layer sizes: [274, 541, 490, 588], total neurons: 1893\n",
            "Before pruning:\n",
            "loss: 1.3856768608093262 - accuracy: 0.5025200247764587 - val_loss: 1.465087652206421 - val_accuracy: 0.47929999232292175 - penalty: 1e-05\n",
            "hidden layer sizes: [274, 541, 490, 588], total neurons: 1893\n",
            "After pruning:\n",
            "loss: 1.385857343673706 - accuracy: 0.502839982509613 - val_loss: 1.46504545211792 - val_accuracy: 0.48019999265670776 - penalty: 1e-05\n",
            "hidden layer sizes: [92, 324, 278, 366], total neurons: 1060\n",
            "##########################################################\n",
            "Epoch 26/50\n",
            "Before growing:\n",
            "loss: 1.385857343673706 - accuracy: 0.502839982509613 - val_loss: 1.46504545211792 - val_accuracy: 0.48019999265670776 - penalty: 1e-05\n",
            "hidden layer sizes: [92, 324, 278, 366], total neurons: 1060\n",
            "After growing:\n",
            "loss: 1.6966723203659058 - accuracy: 0.3981199860572815 - val_loss: 1.7750492095947266 - val_accuracy: 0.3781000077724457 - penalty: 1e-05\n",
            "hidden layer sizes: [292, 524, 478, 566], total neurons: 1860\n",
            "Before pruning:\n",
            "loss: 1.3793200254440308 - accuracy: 0.5074800252914429 - val_loss: 1.4625104665756226 - val_accuracy: 0.4790000021457672 - penalty: 1e-05\n",
            "hidden layer sizes: [292, 524, 478, 566], total neurons: 1860\n",
            "After pruning:\n",
            "loss: 1.375525951385498 - accuracy: 0.5085600018501282 - val_loss: 1.4594520330429077 - val_accuracy: 0.4797999858856201 - penalty: 1e-05\n",
            "hidden layer sizes: [85, 297, 165, 381], total neurons: 928\n",
            "##########################################################\n",
            "Epoch 27/50\n",
            "Before growing:\n",
            "loss: 1.375525951385498 - accuracy: 0.5085600018501282 - val_loss: 1.4594520330429077 - val_accuracy: 0.4797999858856201 - penalty: 1e-05\n",
            "hidden layer sizes: [85, 297, 165, 381], total neurons: 928\n",
            "After growing:\n",
            "loss: 1.7780194282531738 - accuracy: 0.37143999338150024 - val_loss: 1.853383183479309 - val_accuracy: 0.35429999232292175 - penalty: 1e-05\n",
            "hidden layer sizes: [285, 497, 365, 581], total neurons: 1728\n",
            "Before pruning:\n",
            "loss: 1.3782144784927368 - accuracy: 0.5058199763298035 - val_loss: 1.465255856513977 - val_accuracy: 0.47620001435279846 - penalty: 1e-05\n",
            "hidden layer sizes: [285, 497, 365, 581], total neurons: 1728\n",
            "After pruning:\n",
            "loss: 1.3707175254821777 - accuracy: 0.508080005645752 - val_loss: 1.458540439605713 - val_accuracy: 0.4790000021457672 - penalty: 1e-05\n",
            "hidden layer sizes: [93, 319, 246, 275], total neurons: 933\n",
            "##########################################################\n",
            "Epoch 28/50\n",
            "Before growing:\n",
            "loss: 1.3707175254821777 - accuracy: 0.508080005645752 - val_loss: 1.458540439605713 - val_accuracy: 0.4790000021457672 - penalty: 1e-05\n",
            "hidden layer sizes: [93, 319, 246, 275], total neurons: 933\n",
            "After growing:\n",
            "loss: 1.7311348915100098 - accuracy: 0.3928599953651428 - val_loss: 1.7996220588684082 - val_accuracy: 0.37630000710487366 - penalty: 1e-05\n",
            "hidden layer sizes: [293, 519, 446, 475], total neurons: 1733\n",
            "Before pruning:\n",
            "loss: 1.3660906553268433 - accuracy: 0.5108000040054321 - val_loss: 1.4543730020523071 - val_accuracy: 0.478300005197525 - penalty: 1e-05\n",
            "hidden layer sizes: [293, 519, 446, 475], total neurons: 1733\n",
            "After pruning:\n",
            "loss: 1.3635109663009644 - accuracy: 0.5113999843597412 - val_loss: 1.4533452987670898 - val_accuracy: 0.48089998960494995 - penalty: 1e-05\n",
            "hidden layer sizes: [85, 336, 211, 396], total neurons: 1028\n",
            "##########################################################\n",
            "Epoch 29/50\n",
            "Before growing:\n",
            "loss: 1.3635109663009644 - accuracy: 0.5113999843597412 - val_loss: 1.4533452987670898 - val_accuracy: 0.48089998960494995 - penalty: 1e-05\n",
            "hidden layer sizes: [85, 336, 211, 396], total neurons: 1028\n",
            "After growing:\n",
            "loss: 1.765115737915039 - accuracy: 0.3806400001049042 - val_loss: 1.8279038667678833 - val_accuracy: 0.35830000042915344 - penalty: 1e-05\n",
            "hidden layer sizes: [285, 536, 411, 596], total neurons: 1828\n",
            "Before pruning:\n",
            "loss: 1.3603309392929077 - accuracy: 0.5149400234222412 - val_loss: 1.4549981355667114 - val_accuracy: 0.48179998993873596 - penalty: 1e-05\n",
            "hidden layer sizes: [285, 536, 411, 596], total neurons: 1828\n",
            "After pruning:\n",
            "loss: 1.3585962057113647 - accuracy: 0.515500009059906 - val_loss: 1.45374596118927 - val_accuracy: 0.48339998722076416 - penalty: 1e-05\n",
            "hidden layer sizes: [79, 282, 245, 388], total neurons: 994\n",
            "##########################################################\n",
            "Epoch 30/50\n",
            "Before growing:\n",
            "loss: 1.3585962057113647 - accuracy: 0.515500009059906 - val_loss: 1.45374596118927 - val_accuracy: 0.48339998722076416 - penalty: 1e-05\n",
            "hidden layer sizes: [79, 282, 245, 388], total neurons: 994\n",
            "After growing:\n",
            "loss: 1.7446887493133545 - accuracy: 0.38929998874664307 - val_loss: 1.8201987743377686 - val_accuracy: 0.37389999628067017 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [279, 482, 445, 588], total neurons: 1794\n",
            "Before pruning:\n",
            "loss: 1.3581897020339966 - accuracy: 0.5147600173950195 - val_loss: 1.470320463180542 - val_accuracy: 0.48080000281333923 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [279, 482, 445, 588], total neurons: 1794\n",
            "After pruning:\n",
            "loss: 1.3581897020339966 - accuracy: 0.5147600173950195 - val_loss: 1.470320463180542 - val_accuracy: 0.48080000281333923 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [279, 482, 445, 588], total neurons: 1794\n",
            "##########################################################\n",
            "Epoch 31/50\n",
            "Before growing:\n",
            "loss: 1.3581897020339966 - accuracy: 0.5147600173950195 - val_loss: 1.470320463180542 - val_accuracy: 0.48080000281333923 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [279, 482, 445, 588], total neurons: 1794\n",
            "After growing:\n",
            "loss: 1.5493786334991455 - accuracy: 0.4477599859237671 - val_loss: 1.6537816524505615 - val_accuracy: 0.4221000075340271 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [479, 723, 667, 882], total neurons: 2751\n",
            "Before pruning:\n",
            "loss: 1.3561733961105347 - accuracy: 0.5140600204467773 - val_loss: 1.4546561241149902 - val_accuracy: 0.48179998993873596 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [479, 723, 667, 882], total neurons: 2751\n",
            "After pruning:\n",
            "loss: 1.3561506271362305 - accuracy: 0.5140200257301331 - val_loss: 1.4546685218811035 - val_accuracy: 0.48190000653266907 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [469, 723, 667, 876], total neurons: 2735\n",
            "##########################################################\n",
            "Epoch 32/50\n",
            "Before growing:\n",
            "loss: 1.3561506271362305 - accuracy: 0.5140200257301331 - val_loss: 1.4546685218811035 - val_accuracy: 0.48190000653266907 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [469, 723, 667, 876], total neurons: 2735\n",
            "After growing:\n",
            "loss: 1.5936827659606934 - accuracy: 0.41554000973701477 - val_loss: 1.6895560026168823 - val_accuracy: 0.38749998807907104 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [703, 1084, 1000, 1314], total neurons: 4101\n",
            "Before pruning:\n",
            "loss: 1.3488993644714355 - accuracy: 0.5170199871063232 - val_loss: 1.4638744592666626 - val_accuracy: 0.4799000024795532 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [703, 1084, 1000, 1314], total neurons: 4101\n",
            "After pruning:\n",
            "loss: 1.3460419178009033 - accuracy: 0.518559992313385 - val_loss: 1.4610326290130615 - val_accuracy: 0.48069998621940613 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [537, 1079, 976, 1224], total neurons: 3816\n",
            "##########################################################\n",
            "Epoch 33/50\n",
            "Before growing:\n",
            "loss: 1.3460419178009033 - accuracy: 0.518559992313385 - val_loss: 1.4610326290130615 - val_accuracy: 0.48069998621940613 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [537, 1079, 976, 1224], total neurons: 3816\n",
            "After growing:\n",
            "loss: 1.548696517944336 - accuracy: 0.4392400085926056 - val_loss: 1.6589423418045044 - val_accuracy: 0.41350001096725464 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [805, 1618, 1464, 1836], total neurons: 5723\n",
            "Before pruning:\n",
            "loss: 1.3306351900100708 - accuracy: 0.5224599838256836 - val_loss: 1.4453586339950562 - val_accuracy: 0.4812000095844269 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [805, 1618, 1464, 1836], total neurons: 5723\n",
            "After pruning:\n",
            "loss: 1.3262184858322144 - accuracy: 0.5246999859809875 - val_loss: 1.44218111038208 - val_accuracy: 0.4830000102519989 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [458, 1347, 1140, 1550], total neurons: 4495\n",
            "##########################################################\n",
            "Epoch 34/50\n",
            "Before growing:\n",
            "loss: 1.3262184858322144 - accuracy: 0.5246999859809875 - val_loss: 1.44218111038208 - val_accuracy: 0.4830000102519989 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [458, 1347, 1140, 1550], total neurons: 4495\n",
            "After growing:\n",
            "loss: 1.603203535079956 - accuracy: 0.42201998829841614 - val_loss: 1.7143101692199707 - val_accuracy: 0.388700008392334 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [687, 2020, 1710, 2325], total neurons: 6742\n",
            "Before pruning:\n",
            "loss: 1.3227990865707397 - accuracy: 0.5262200236320496 - val_loss: 1.4477957487106323 - val_accuracy: 0.4878999888896942 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [687, 2020, 1710, 2325], total neurons: 6742\n",
            "After pruning:\n",
            "loss: 1.316597819328308 - accuracy: 0.5287799835205078 - val_loss: 1.4414681196212769 - val_accuracy: 0.48890000581741333 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [481, 1253, 1165, 1354], total neurons: 4253\n",
            "##########################################################\n",
            "Epoch 35/50\n",
            "Before growing:\n",
            "loss: 1.316597819328308 - accuracy: 0.5287799835205078 - val_loss: 1.4414681196212769 - val_accuracy: 0.48890000581741333 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [481, 1253, 1165, 1354], total neurons: 4253\n",
            "After growing:\n",
            "loss: 1.5045092105865479 - accuracy: 0.4713999927043915 - val_loss: 1.623977780342102 - val_accuracy: 0.4334999918937683 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [721, 1879, 1747, 2031], total neurons: 6378\n",
            "Before pruning:\n",
            "loss: 1.296687364578247 - accuracy: 0.5393000245094299 - val_loss: 1.4285922050476074 - val_accuracy: 0.49000000953674316 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [721, 1879, 1747, 2031], total neurons: 6378\n",
            "After pruning:\n",
            "loss: 1.2936670780181885 - accuracy: 0.5405200123786926 - val_loss: 1.4261678457260132 - val_accuracy: 0.49059998989105225 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [489, 1257, 1049, 1044], total neurons: 3839\n",
            "##########################################################\n",
            "Epoch 36/50\n",
            "Before growing:\n",
            "loss: 1.2936670780181885 - accuracy: 0.5405200123786926 - val_loss: 1.4261678457260132 - val_accuracy: 0.49059998989105225 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [489, 1257, 1049, 1044], total neurons: 3839\n",
            "After growing:\n",
            "loss: 1.4503593444824219 - accuracy: 0.48712000250816345 - val_loss: 1.5744574069976807 - val_accuracy: 0.44589999318122864 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [733, 1885, 1573, 1566], total neurons: 5757\n",
            "Before pruning:\n",
            "loss: 1.2990467548370361 - accuracy: 0.5363600254058838 - val_loss: 1.4387582540512085 - val_accuracy: 0.4894999861717224 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [733, 1885, 1573, 1566], total neurons: 5757\n",
            "After pruning:\n",
            "loss: 1.2949939966201782 - accuracy: 0.5374199748039246 - val_loss: 1.4353556632995605 - val_accuracy: 0.491100013256073 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [505, 1122, 907, 1067], total neurons: 3601\n",
            "##########################################################\n",
            "Epoch 37/50\n",
            "Before growing:\n",
            "loss: 1.2949939966201782 - accuracy: 0.5374199748039246 - val_loss: 1.4353556632995605 - val_accuracy: 0.491100013256073 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [505, 1122, 907, 1067], total neurons: 3601\n",
            "After growing:\n",
            "loss: 1.4459915161132812 - accuracy: 0.4825200140476227 - val_loss: 1.5734984874725342 - val_accuracy: 0.45010000467300415 - penalty: 1.0000000000000002e-07\n",
            "hidden layer sizes: [757, 1683, 1360, 1600], total neurons: 5400\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-47001d43738a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\nmodel = SSModel(layer_sizes=[3072, 300, 300, 300, 300, 10], activation='selu', regularization_penalty=0.0001, regularization_method='weighted_l1', kernel_initializer='lecun_normal')\\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.0002)\\n\\ntrain_model(model, X_train_norm, y_train, optimizer, epochs, self_scaling_epochs, batch_size, min_new_neurons, validation_data=(X_test_norm, y_test), regularization_penalty_multiplier=0.1, growth_percentage=0.5)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-53>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-67746070c246>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, x, y, optimizer, epochs, self_scaling_epochs, batch_size, min_new_neurons, validation_data, pruning_threshold, regularization_penalty_multiplier, growth_percentage, print_neurons)\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mloss_value\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1088\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1090\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    157\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_grad.py\u001b[0m in \u001b[0;36m_MulGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m   (sx, rx, must_reduce_x), (sy, ry, must_reduce_y) = (\n\u001b[0;32m-> 1377\u001b[0;31m       SmartBroadcastGradientArgs(x, y, grad))\n\u001b[0m\u001b[1;32m   1378\u001b[0m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1379\u001b[0m   \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_grad.py\u001b[0m in \u001b[0;36mSmartBroadcastGradientArgs\u001b[0;34m(x, y, grad)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0msx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0msy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0mrx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_gradient_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mbroadcast_gradient_args\u001b[0;34m(s0, s1, name)\u001b[0m\n\u001b[1;32m    734\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0;32m--> 736\u001b[0;31m         _ctx, \"BroadcastGradientArgs\", name, s0, s1)\n\u001b[0m\u001b[1;32m    737\u001b[0m       \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_BroadcastGradientArgsOutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2XaL-KSFhW5",
        "outputId": "32b2fdc4-3b28-4ba7-d142-bd2d64e0a601",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# SCALED ON SINGLE AXIS\n",
        "\n",
        "%%time\n",
        "\n",
        "model = SSModel(layer_sizes=[3072, 300, 300, 300, 300, 10], activation='selu', regularization_penalty=0.0001, regularization_method='weighted_l1', kernel_initializer='lecun_normal')\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002)\n",
        "\n",
        "train_model(model, X_train_norm, y_train, optimizer, epochs, self_scaling_epochs, batch_size, min_new_neurons, validation_data=(X_test_norm, y_test), regularization_penalty_multiplier=0.1, growth_percentage=0.5)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "##########################################################\n",
            "Epoch 1/50\n",
            "Before growing:\n",
            "loss: 2.5947258472442627 - accuracy: 0.11299999803304672 - val_loss: 2.617108106613159 - val_accuracy: 0.10949999839067459 - penalty: 0.0001\n",
            "hidden layer sizes: [300, 300, 300, 300], total neurons: 1200\n",
            "After growing:\n",
            "loss: 2.5946528911590576 - accuracy: 0.11292000114917755 - val_loss: 2.6170480251312256 - val_accuracy: 0.1096000000834465 - penalty: 0.0001\n",
            "hidden layer sizes: [500, 500, 500, 500], total neurons: 2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-47001d43738a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\nmodel = SSModel(layer_sizes=[3072, 300, 300, 300, 300, 10], activation='selu', regularization_penalty=0.0001, regularization_method='weighted_l1', kernel_initializer='lecun_normal')\\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.0002)\\n\\ntrain_model(model, X_train_norm, y_train, optimizer, epochs, self_scaling_epochs, batch_size, min_new_neurons, validation_data=(X_test_norm, y_test), regularization_penalty_multiplier=0.1, growth_percentage=0.5)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-53>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-299a9dc05640>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, x, y, optimizer, epochs, self_scaling_epochs, batch_size, min_new_neurons, validation_data, pruning_threshold, regularization_penalty_multiplier, growth_percentage, print_neurons)\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mloss_value\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1088\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1090\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    157\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vkppPGYCDS4",
        "outputId": "55157e1d-131b-4c9a-cd69-dd56e9f3472f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# SCALING FACTOR *= 100, SCALED ON SINGLE AXIS\n",
        "\n",
        "%%time\n",
        "\n",
        "model = SSModel(layer_sizes=[3072, 300, 300, 300, 300, 10], activation='selu', regularization_penalty=0.0001, regularization_method='weighted_l1', kernel_initializer='lecun_normal')\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002)\n",
        "\n",
        "train_model(model, X_train_norm, y_train, optimizer, epochs, self_scaling_epochs, batch_size, min_new_neurons, validation_data=(X_test_norm, y_test), regularization_penalty_multiplier=0.1, growth_percentage=0.5)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "##########################################################\n",
            "Epoch 1/50\n",
            "Before growing:\n",
            "loss: 2.6810338497161865 - accuracy: 0.09408000111579895 - val_loss: 2.670012950897217 - val_accuracy: 0.09539999812841415 - penalty: 0.0001\n",
            "hidden layer sizes: [300, 300, 300, 300], total neurons: 1200\n",
            "After growing:\n",
            "loss: 2.709690570831299 - accuracy: 0.09126000106334686 - val_loss: 2.6982622146606445 - val_accuracy: 0.09449999779462814 - penalty: 0.0001\n",
            "hidden layer sizes: [500, 500, 500, 500], total neurons: 2000\n",
            "Before pruning:\n",
            "loss: 1.8405990600585938 - accuracy: 0.33528000116348267 - val_loss: 1.843217134475708 - val_accuracy: 0.3328000009059906 - penalty: 0.0001\n",
            "hidden layer sizes: [500, 500, 500, 500], total neurons: 2000\n",
            "After pruning:\n",
            "loss: 1.8411425352096558 - accuracy: 0.3349199891090393 - val_loss: 1.8437937498092651 - val_accuracy: 0.33180001378059387 - penalty: 0.0001\n",
            "hidden layer sizes: [13, 61, 37, 69], total neurons: 180\n",
            "##########################################################\n",
            "Epoch 2/50\n",
            "Before growing:\n",
            "loss: 1.8411425352096558 - accuracy: 0.3349199891090393 - val_loss: 1.8437937498092651 - val_accuracy: 0.33180001378059387 - penalty: 0.0001\n",
            "hidden layer sizes: [13, 61, 37, 69], total neurons: 180\n",
            "After growing:\n",
            "loss: 1.8481428623199463 - accuracy: 0.33215999603271484 - val_loss: 1.851670265197754 - val_accuracy: 0.3255999982357025 - penalty: 0.0001\n",
            "hidden layer sizes: [213, 261, 237, 269], total neurons: 980\n",
            "Before pruning:\n",
            "loss: 1.778849720954895 - accuracy: 0.35106000304222107 - val_loss: 1.7866289615631104 - val_accuracy: 0.3409999907016754 - penalty: 0.0001\n",
            "hidden layer sizes: [213, 261, 237, 269], total neurons: 980\n",
            "After pruning:\n",
            "loss: 1.7789490222930908 - accuracy: 0.35054001212120056 - val_loss: 1.7867051362991333 - val_accuracy: 0.34060001373291016 - penalty: 0.0001\n",
            "hidden layer sizes: [9, 43, 20, 59], total neurons: 131\n",
            "##########################################################\n",
            "Epoch 3/50\n",
            "Before growing:\n",
            "loss: 1.7789490222930908 - accuracy: 0.35054001212120056 - val_loss: 1.7867051362991333 - val_accuracy: 0.34060001373291016 - penalty: 0.0001\n",
            "hidden layer sizes: [9, 43, 20, 59], total neurons: 131\n",
            "After growing:\n",
            "loss: 1.7789605855941772 - accuracy: 0.3497599959373474 - val_loss: 1.7875847816467285 - val_accuracy: 0.3416999876499176 - penalty: 0.0001\n",
            "hidden layer sizes: [209, 243, 220, 259], total neurons: 931\n",
            "Before pruning:\n",
            "loss: 1.756174087524414 - accuracy: 0.35708001255989075 - val_loss: 1.7667160034179688 - val_accuracy: 0.35100001096725464 - penalty: 0.0001\n",
            "hidden layer sizes: [209, 243, 220, 259], total neurons: 931\n",
            "After pruning:\n",
            "loss: 1.7563546895980835 - accuracy: 0.3568600118160248 - val_loss: 1.766763687133789 - val_accuracy: 0.35109999775886536 - penalty: 0.0001\n",
            "hidden layer sizes: [8, 39, 19, 51], total neurons: 117\n",
            "##########################################################\n",
            "Epoch 4/50\n",
            "Before growing:\n",
            "loss: 1.7563546895980835 - accuracy: 0.3568600118160248 - val_loss: 1.766763687133789 - val_accuracy: 0.35109999775886536 - penalty: 0.0001\n",
            "hidden layer sizes: [8, 39, 19, 51], total neurons: 117\n",
            "After growing:\n",
            "loss: 1.7601367235183716 - accuracy: 0.35798001289367676 - val_loss: 1.7679617404937744 - val_accuracy: 0.3515999913215637 - penalty: 0.0001\n",
            "hidden layer sizes: [208, 239, 219, 251], total neurons: 917\n",
            "Before pruning:\n",
            "loss: 1.7404489517211914 - accuracy: 0.3641200065612793 - val_loss: 1.7538594007492065 - val_accuracy: 0.35519999265670776 - penalty: 0.0001\n",
            "hidden layer sizes: [208, 239, 219, 251], total neurons: 917\n",
            "After pruning:\n",
            "loss: 1.7404615879058838 - accuracy: 0.3644599914550781 - val_loss: 1.7538892030715942 - val_accuracy: 0.3553999960422516 - penalty: 0.0001\n",
            "hidden layer sizes: [9, 37, 18, 52], total neurons: 116\n",
            "##########################################################\n",
            "Epoch 5/50\n",
            "Before growing:\n",
            "loss: 1.7404615879058838 - accuracy: 0.3644599914550781 - val_loss: 1.7538892030715942 - val_accuracy: 0.3553999960422516 - penalty: 0.0001\n",
            "hidden layer sizes: [9, 37, 18, 52], total neurons: 116\n",
            "After growing:\n",
            "loss: 1.745206594467163 - accuracy: 0.3609200119972229 - val_loss: 1.7604478597640991 - val_accuracy: 0.349700003862381 - penalty: 0.0001\n",
            "hidden layer sizes: [209, 237, 218, 252], total neurons: 916\n",
            "Before pruning:\n",
            "loss: 1.73752760887146 - accuracy: 0.3671799898147583 - val_loss: 1.7511625289916992 - val_accuracy: 0.3573000133037567 - penalty: 0.0001\n",
            "hidden layer sizes: [209, 237, 218, 252], total neurons: 916\n",
            "After pruning:\n",
            "loss: 1.7368375062942505 - accuracy: 0.3671799898147583 - val_loss: 1.7504127025604248 - val_accuracy: 0.3578999936580658 - penalty: 0.0001\n",
            "hidden layer sizes: [12, 33, 17, 53], total neurons: 115\n",
            "##########################################################\n",
            "Epoch 6/50\n",
            "Before growing:\n",
            "loss: 1.7368375062942505 - accuracy: 0.3671799898147583 - val_loss: 1.7504127025604248 - val_accuracy: 0.3578999936580658 - penalty: 0.0001\n",
            "hidden layer sizes: [12, 33, 17, 53], total neurons: 115\n",
            "After growing:\n",
            "loss: 1.7388023138046265 - accuracy: 0.36517998576164246 - val_loss: 1.752503514289856 - val_accuracy: 0.353300005197525 - penalty: 0.0001\n",
            "hidden layer sizes: [212, 233, 217, 253], total neurons: 915\n",
            "Before pruning:\n",
            "loss: 1.7208796739578247 - accuracy: 0.37342000007629395 - val_loss: 1.7394636869430542 - val_accuracy: 0.3630000054836273 - penalty: 0.0001\n",
            "hidden layer sizes: [212, 233, 217, 253], total neurons: 915\n",
            "After pruning:\n",
            "loss: 1.7213692665100098 - accuracy: 0.3731600046157837 - val_loss: 1.7400277853012085 - val_accuracy: 0.3628000020980835 - penalty: 0.0001\n",
            "hidden layer sizes: [9, 33, 16, 50], total neurons: 108\n",
            "##########################################################\n",
            "Epoch 7/50\n",
            "Before growing:\n",
            "loss: 1.7213692665100098 - accuracy: 0.3731600046157837 - val_loss: 1.7400277853012085 - val_accuracy: 0.3628000020980835 - penalty: 0.0001\n",
            "hidden layer sizes: [9, 33, 16, 50], total neurons: 108\n",
            "After growing:\n",
            "loss: 1.7321830987930298 - accuracy: 0.37101998925209045 - val_loss: 1.7482664585113525 - val_accuracy: 0.3601999878883362 - penalty: 0.0001\n",
            "hidden layer sizes: [209, 233, 216, 250], total neurons: 908\n",
            "Before pruning:\n",
            "loss: 1.7123692035675049 - accuracy: 0.3779999911785126 - val_loss: 1.729917049407959 - val_accuracy: 0.36730000376701355 - penalty: 0.0001\n",
            "hidden layer sizes: [209, 233, 216, 250], total neurons: 908\n",
            "After pruning:\n",
            "loss: 1.7124179601669312 - accuracy: 0.3778199851512909 - val_loss: 1.7297968864440918 - val_accuracy: 0.3671000003814697 - penalty: 0.0001\n",
            "hidden layer sizes: [9, 32, 15, 49], total neurons: 105\n",
            "##########################################################\n",
            "Epoch 8/50\n",
            "Before growing:\n",
            "loss: 1.7124179601669312 - accuracy: 0.3778199851512909 - val_loss: 1.7297968864440918 - val_accuracy: 0.3671000003814697 - penalty: 0.0001\n",
            "hidden layer sizes: [9, 32, 15, 49], total neurons: 105\n",
            "After growing:\n",
            "loss: 1.7170287370681763 - accuracy: 0.37571999430656433 - val_loss: 1.7340788841247559 - val_accuracy: 0.36660000681877136 - penalty: 0.0001\n",
            "hidden layer sizes: [209, 232, 215, 249], total neurons: 905\n",
            "Before pruning:\n",
            "loss: 1.7066264152526855 - accuracy: 0.38019999861717224 - val_loss: 1.7267152070999146 - val_accuracy: 0.3693000078201294 - penalty: 0.0001\n",
            "hidden layer sizes: [209, 232, 215, 249], total neurons: 905\n",
            "After pruning:\n",
            "loss: 1.7063062191009521 - accuracy: 0.380840003490448 - val_loss: 1.7261898517608643 - val_accuracy: 0.3700000047683716 - penalty: 0.0001\n",
            "hidden layer sizes: [12, 30, 15, 65], total neurons: 122\n",
            "##########################################################\n",
            "Epoch 9/50\n",
            "Before growing:\n",
            "loss: 1.7063062191009521 - accuracy: 0.380840003490448 - val_loss: 1.7261898517608643 - val_accuracy: 0.3700000047683716 - penalty: 0.0001\n",
            "hidden layer sizes: [12, 30, 15, 65], total neurons: 122\n",
            "After growing:\n",
            "loss: 1.7127615213394165 - accuracy: 0.38082000613212585 - val_loss: 1.7311304807662964 - val_accuracy: 0.3691999912261963 - penalty: 0.0001\n",
            "hidden layer sizes: [212, 230, 215, 265], total neurons: 922\n",
            "Before pruning:\n",
            "loss: 1.7122008800506592 - accuracy: 0.37602001428604126 - val_loss: 1.730615258216858 - val_accuracy: 0.36250001192092896 - penalty: 0.0001\n",
            "hidden layer sizes: [212, 230, 215, 265], total neurons: 922\n",
            "After pruning:\n",
            "loss: 1.7117823362350464 - accuracy: 0.3762199878692627 - val_loss: 1.730204463005066 - val_accuracy: 0.3637999892234802 - penalty: 0.0001\n",
            "hidden layer sizes: [9, 28, 14, 43], total neurons: 94\n",
            "##########################################################\n",
            "Epoch 10/50\n",
            "Before growing:\n",
            "loss: 1.7117823362350464 - accuracy: 0.3762199878692627 - val_loss: 1.730204463005066 - val_accuracy: 0.3637999892234802 - penalty: 0.0001\n",
            "hidden layer sizes: [9, 28, 14, 43], total neurons: 94\n",
            "After growing:\n",
            "loss: 1.715315341949463 - accuracy: 0.3746599853038788 - val_loss: 1.7342969179153442 - val_accuracy: 0.36320000886917114 - penalty: 1e-05\n",
            "hidden layer sizes: [209, 228, 214, 243], total neurons: 894\n",
            "Before pruning:\n",
            "loss: 1.5955415964126587 - accuracy: 0.4260599911212921 - val_loss: 1.619667410850525 - val_accuracy: 0.41600000858306885 - penalty: 1e-05\n",
            "hidden layer sizes: [209, 228, 214, 243], total neurons: 894\n",
            "After pruning:\n",
            "loss: 1.5953738689422607 - accuracy: 0.4263800084590912 - val_loss: 1.6194899082183838 - val_accuracy: 0.415800005197525 - penalty: 1e-05\n",
            "hidden layer sizes: [114, 217, 178, 209], total neurons: 718\n",
            "##########################################################\n",
            "Epoch 11/50\n",
            "Before growing:\n",
            "loss: 1.5953738689422607 - accuracy: 0.4263800084590912 - val_loss: 1.6194899082183838 - val_accuracy: 0.415800005197525 - penalty: 1e-05\n",
            "hidden layer sizes: [114, 217, 178, 209], total neurons: 718\n",
            "After growing:\n",
            "loss: 1.5980284214019775 - accuracy: 0.42434000968933105 - val_loss: 1.6235921382904053 - val_accuracy: 0.4171000123023987 - penalty: 1e-05\n",
            "hidden layer sizes: [314, 417, 378, 409], total neurons: 1518\n",
            "Before pruning:\n",
            "loss: 1.5438838005065918 - accuracy: 0.44839999079704285 - val_loss: 1.5742524862289429 - val_accuracy: 0.43209999799728394 - penalty: 1e-05\n",
            "hidden layer sizes: [314, 417, 378, 409], total neurons: 1518\n",
            "After pruning:\n",
            "loss: 1.5422165393829346 - accuracy: 0.4489800035953522 - val_loss: 1.5721698999404907 - val_accuracy: 0.43389999866485596 - penalty: 1e-05\n",
            "hidden layer sizes: [85, 233, 148, 217], total neurons: 683\n",
            "##########################################################\n",
            "Epoch 12/50\n",
            "Before growing:\n",
            "loss: 1.5422165393829346 - accuracy: 0.4489800035953522 - val_loss: 1.5721698999404907 - val_accuracy: 0.43389999866485596 - penalty: 1e-05\n",
            "hidden layer sizes: [85, 233, 148, 217], total neurons: 683\n",
            "After growing:\n",
            "loss: 1.548777461051941 - accuracy: 0.4453999996185303 - val_loss: 1.5772340297698975 - val_accuracy: 0.43140000104904175 - penalty: 1e-05\n",
            "hidden layer sizes: [285, 433, 348, 417], total neurons: 1483\n",
            "Before pruning:\n",
            "loss: 1.508135199546814 - accuracy: 0.4591200053691864 - val_loss: 1.5424963235855103 - val_accuracy: 0.4438000023365021 - penalty: 1e-05\n",
            "hidden layer sizes: [285, 433, 348, 417], total neurons: 1483\n",
            "After pruning:\n",
            "loss: 1.505683422088623 - accuracy: 0.46059998869895935 - val_loss: 1.5405468940734863 - val_accuracy: 0.44670000672340393 - penalty: 1e-05\n",
            "hidden layer sizes: [99, 180, 105, 213], total neurons: 597\n",
            "##########################################################\n",
            "Epoch 13/50\n",
            "Before growing:\n",
            "loss: 1.505683422088623 - accuracy: 0.46059998869895935 - val_loss: 1.5405468940734863 - val_accuracy: 0.44670000672340393 - penalty: 1e-05\n",
            "hidden layer sizes: [99, 180, 105, 213], total neurons: 597\n",
            "After growing:\n",
            "loss: 1.516302227973938 - accuracy: 0.4577600061893463 - val_loss: 1.5507922172546387 - val_accuracy: 0.4447999894618988 - penalty: 1e-05\n",
            "hidden layer sizes: [299, 380, 305, 413], total neurons: 1397\n",
            "Before pruning:\n",
            "loss: 1.4842865467071533 - accuracy: 0.4680199921131134 - val_loss: 1.524842619895935 - val_accuracy: 0.45170000195503235 - penalty: 1e-05\n",
            "hidden layer sizes: [299, 380, 305, 413], total neurons: 1397\n",
            "After pruning:\n",
            "loss: 1.4800434112548828 - accuracy: 0.4694800078868866 - val_loss: 1.5208170413970947 - val_accuracy: 0.4519999921321869 - penalty: 1e-05\n",
            "hidden layer sizes: [78, 175, 133, 186], total neurons: 572\n",
            "##########################################################\n",
            "Epoch 14/50\n",
            "Before growing:\n",
            "loss: 1.4800434112548828 - accuracy: 0.4694800078868866 - val_loss: 1.5208170413970947 - val_accuracy: 0.4519999921321869 - penalty: 1e-05\n",
            "hidden layer sizes: [78, 175, 133, 186], total neurons: 572\n",
            "After growing:\n",
            "loss: 1.488943099975586 - accuracy: 0.465719997882843 - val_loss: 1.5291448831558228 - val_accuracy: 0.4478999972343445 - penalty: 1e-05\n",
            "hidden layer sizes: [278, 375, 333, 386], total neurons: 1372\n",
            "Before pruning:\n",
            "loss: 1.4529143571853638 - accuracy: 0.47947999835014343 - val_loss: 1.5028156042099 - val_accuracy: 0.46059998869895935 - penalty: 1e-05\n",
            "hidden layer sizes: [278, 375, 333, 386], total neurons: 1372\n",
            "After pruning:\n",
            "loss: 1.451733112335205 - accuracy: 0.47999998927116394 - val_loss: 1.5018956661224365 - val_accuracy: 0.4611000120639801 - penalty: 1e-05\n",
            "hidden layer sizes: [70, 175, 92, 143], total neurons: 480\n",
            "##########################################################\n",
            "Epoch 15/50\n",
            "Before growing:\n",
            "loss: 1.451733112335205 - accuracy: 0.47999998927116394 - val_loss: 1.5018956661224365 - val_accuracy: 0.4611000120639801 - penalty: 1e-05\n",
            "hidden layer sizes: [70, 175, 92, 143], total neurons: 480\n",
            "After growing:\n",
            "loss: 1.4567869901657104 - accuracy: 0.47760000824928284 - val_loss: 1.5067476034164429 - val_accuracy: 0.4603999853134155 - penalty: 1e-05\n",
            "hidden layer sizes: [270, 375, 292, 343], total neurons: 1280\n",
            "Before pruning:\n",
            "loss: 1.434983253479004 - accuracy: 0.48510000109672546 - val_loss: 1.487504005432129 - val_accuracy: 0.46619999408721924 - penalty: 1e-05\n",
            "hidden layer sizes: [270, 375, 292, 343], total neurons: 1280\n",
            "After pruning:\n",
            "loss: 1.4333882331848145 - accuracy: 0.4860199987888336 - val_loss: 1.4852380752563477 - val_accuracy: 0.47049999237060547 - penalty: 1e-05\n",
            "hidden layer sizes: [68, 193, 112, 229], total neurons: 602\n",
            "##########################################################\n",
            "Epoch 16/50\n",
            "Before growing:\n",
            "loss: 1.4333882331848145 - accuracy: 0.4860199987888336 - val_loss: 1.4852380752563477 - val_accuracy: 0.47049999237060547 - penalty: 1e-05\n",
            "hidden layer sizes: [68, 193, 112, 229], total neurons: 602\n",
            "After growing:\n",
            "loss: 1.4400116205215454 - accuracy: 0.48486000299453735 - val_loss: 1.4904593229293823 - val_accuracy: 0.46779999136924744 - penalty: 1e-05\n",
            "hidden layer sizes: [268, 393, 312, 429], total neurons: 1402\n",
            "Before pruning:\n",
            "loss: 1.421118140220642 - accuracy: 0.49046000838279724 - val_loss: 1.4766662120819092 - val_accuracy: 0.47530001401901245 - penalty: 1e-05\n",
            "hidden layer sizes: [268, 393, 312, 429], total neurons: 1402\n",
            "After pruning:\n",
            "loss: 1.420775055885315 - accuracy: 0.4899199903011322 - val_loss: 1.476354956626892 - val_accuracy: 0.4749999940395355 - penalty: 1e-05\n",
            "hidden layer sizes: [89, 143, 117, 249], total neurons: 598\n",
            "##########################################################\n",
            "Epoch 17/50\n",
            "Before growing:\n",
            "loss: 1.420775055885315 - accuracy: 0.4899199903011322 - val_loss: 1.476354956626892 - val_accuracy: 0.4749999940395355 - penalty: 1e-05\n",
            "hidden layer sizes: [89, 143, 117, 249], total neurons: 598\n",
            "After growing:\n",
            "loss: 1.4250037670135498 - accuracy: 0.489300012588501 - val_loss: 1.477723479270935 - val_accuracy: 0.4742000102996826 - penalty: 1e-05\n",
            "hidden layer sizes: [289, 343, 317, 449], total neurons: 1398\n",
            "Before pruning:\n",
            "loss: 1.4132232666015625 - accuracy: 0.49333998560905457 - val_loss: 1.4695650339126587 - val_accuracy: 0.4749999940395355 - penalty: 1e-05\n",
            "hidden layer sizes: [289, 343, 317, 449], total neurons: 1398\n",
            "After pruning:\n",
            "loss: 1.410954475402832 - accuracy: 0.49445998668670654 - val_loss: 1.4679442644119263 - val_accuracy: 0.4763999879360199 - penalty: 1e-05\n",
            "hidden layer sizes: [76, 211, 136, 239], total neurons: 662\n",
            "##########################################################\n",
            "Epoch 18/50\n",
            "Before growing:\n",
            "loss: 1.410954475402832 - accuracy: 0.49445998668670654 - val_loss: 1.4679442644119263 - val_accuracy: 0.4763999879360199 - penalty: 1e-05\n",
            "hidden layer sizes: [76, 211, 136, 239], total neurons: 662\n",
            "After growing:\n",
            "loss: 1.419423222541809 - accuracy: 0.49211999773979187 - val_loss: 1.476287841796875 - val_accuracy: 0.47530001401901245 - penalty: 1e-05\n",
            "hidden layer sizes: [276, 411, 336, 439], total neurons: 1462\n",
            "Before pruning:\n",
            "loss: 1.4132354259490967 - accuracy: 0.49344000220298767 - val_loss: 1.476568579673767 - val_accuracy: 0.4747999906539917 - penalty: 1e-05\n",
            "hidden layer sizes: [276, 411, 336, 439], total neurons: 1462\n",
            "After pruning:\n",
            "loss: 1.4123003482818604 - accuracy: 0.49421998858451843 - val_loss: 1.475861668586731 - val_accuracy: 0.47369998693466187 - penalty: 1e-05\n",
            "hidden layer sizes: [67, 186, 91, 180], total neurons: 524\n",
            "##########################################################\n",
            "Epoch 19/50\n",
            "Before growing:\n",
            "loss: 1.4123003482818604 - accuracy: 0.49421998858451843 - val_loss: 1.475861668586731 - val_accuracy: 0.47369998693466187 - penalty: 1e-05\n",
            "hidden layer sizes: [67, 186, 91, 180], total neurons: 524\n",
            "After growing:\n",
            "loss: 1.4195747375488281 - accuracy: 0.49182000756263733 - val_loss: 1.4864153861999512 - val_accuracy: 0.4690000116825104 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [267, 386, 291, 380], total neurons: 1324\n",
            "Before pruning:\n",
            "loss: 1.3903963565826416 - accuracy: 0.5055999755859375 - val_loss: 1.4573895931243896 - val_accuracy: 0.48080000281333923 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [267, 386, 291, 380], total neurons: 1324\n",
            "After pruning:\n",
            "loss: 1.3903956413269043 - accuracy: 0.5056399703025818 - val_loss: 1.4573872089385986 - val_accuracy: 0.48080000281333923 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [267, 386, 291, 375], total neurons: 1319\n",
            "##########################################################\n",
            "Epoch 20/50\n",
            "Before growing:\n",
            "loss: 1.3903956413269043 - accuracy: 0.5056399703025818 - val_loss: 1.4573872089385986 - val_accuracy: 0.48080000281333923 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [267, 386, 291, 375], total neurons: 1319\n",
            "After growing:\n",
            "loss: 1.3912065029144287 - accuracy: 0.5060799717903137 - val_loss: 1.4594838619232178 - val_accuracy: 0.4814999997615814 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [467, 586, 491, 575], total neurons: 2119\n",
            "Before pruning:\n",
            "loss: 1.3777471780776978 - accuracy: 0.5076599717140198 - val_loss: 1.4553037881851196 - val_accuracy: 0.4805000126361847 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [467, 586, 491, 575], total neurons: 2119\n",
            "After pruning:\n",
            "loss: 1.3776335716247559 - accuracy: 0.5077199935913086 - val_loss: 1.4551563262939453 - val_accuracy: 0.48089998960494995 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [466, 586, 491, 450], total neurons: 1993\n",
            "##########################################################\n",
            "Epoch 21/50\n",
            "Before growing:\n",
            "loss: 1.3776335716247559 - accuracy: 0.5077199935913086 - val_loss: 1.4551563262939453 - val_accuracy: 0.48089998960494995 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [466, 586, 491, 450], total neurons: 1993\n",
            "After growing:\n",
            "loss: 1.380392074584961 - accuracy: 0.5064200162887573 - val_loss: 1.4571049213409424 - val_accuracy: 0.4805000126361847 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [699, 879, 736, 675], total neurons: 2989\n",
            "Before pruning:\n",
            "loss: 1.3613216876983643 - accuracy: 0.5140200257301331 - val_loss: 1.4452916383743286 - val_accuracy: 0.4880000054836273 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [699, 879, 736, 675], total neurons: 2989\n",
            "After pruning:\n",
            "loss: 1.36063551902771 - accuracy: 0.5143200159072876 - val_loss: 1.444501519203186 - val_accuracy: 0.4878000020980835 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [537, 850, 726, 447], total neurons: 2560\n",
            "##########################################################\n",
            "Epoch 22/50\n",
            "Before growing:\n",
            "loss: 1.36063551902771 - accuracy: 0.5143200159072876 - val_loss: 1.444501519203186 - val_accuracy: 0.4878000020980835 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [537, 850, 726, 447], total neurons: 2560\n",
            "After growing:\n",
            "loss: 1.3631314039230347 - accuracy: 0.5118200182914734 - val_loss: 1.4466066360473633 - val_accuracy: 0.4869999885559082 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [805, 1275, 1089, 670], total neurons: 3839\n",
            "Before pruning:\n",
            "loss: 1.3443583250045776 - accuracy: 0.5201200246810913 - val_loss: 1.4411067962646484 - val_accuracy: 0.4943000078201294 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [805, 1275, 1089, 670], total neurons: 3839\n",
            "After pruning:\n",
            "loss: 1.341298222541809 - accuracy: 0.5210999846458435 - val_loss: 1.4380236864089966 - val_accuracy: 0.4936000108718872 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [514, 1092, 947, 445], total neurons: 2998\n",
            "##########################################################\n",
            "Epoch 23/50\n",
            "Before growing:\n",
            "loss: 1.341298222541809 - accuracy: 0.5210999846458435 - val_loss: 1.4380236864089966 - val_accuracy: 0.4936000108718872 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [514, 1092, 947, 445], total neurons: 2998\n",
            "After growing:\n",
            "loss: 1.3434357643127441 - accuracy: 0.5198799967765808 - val_loss: 1.4402679204940796 - val_accuracy: 0.4957999885082245 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [771, 1638, 1420, 667], total neurons: 4496\n",
            "Before pruning:\n",
            "loss: 1.3159335851669312 - accuracy: 0.5297999978065491 - val_loss: 1.4233672618865967 - val_accuracy: 0.49630001187324524 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [771, 1638, 1420, 667], total neurons: 4496\n",
            "After pruning:\n",
            "loss: 1.3141701221466064 - accuracy: 0.530460000038147 - val_loss: 1.4219518899917603 - val_accuracy: 0.49810001254081726 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [486, 1133, 1028, 430], total neurons: 3077\n",
            "##########################################################\n",
            "Epoch 24/50\n",
            "Before growing:\n",
            "loss: 1.3141701221466064 - accuracy: 0.530460000038147 - val_loss: 1.4219518899917603 - val_accuracy: 0.49810001254081726 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [486, 1133, 1028, 430], total neurons: 3077\n",
            "After growing:\n",
            "loss: 1.3173542022705078 - accuracy: 0.5305399894714355 - val_loss: 1.4239747524261475 - val_accuracy: 0.4959999918937683 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [729, 1699, 1542, 645], total neurons: 4615\n",
            "Before pruning:\n",
            "loss: 1.3009761571884155 - accuracy: 0.5364800095558167 - val_loss: 1.4140146970748901 - val_accuracy: 0.49869999289512634 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [729, 1699, 1542, 645], total neurons: 4615\n",
            "After pruning:\n",
            "loss: 1.2994194030761719 - accuracy: 0.5367000102996826 - val_loss: 1.412539005279541 - val_accuracy: 0.4997999966144562 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [432, 984, 916, 427], total neurons: 2759\n",
            "##########################################################\n",
            "Epoch 25/50\n",
            "Before growing:\n",
            "loss: 1.2994194030761719 - accuracy: 0.5367000102996826 - val_loss: 1.412539005279541 - val_accuracy: 0.4997999966144562 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [432, 984, 916, 427], total neurons: 2759\n",
            "After growing:\n",
            "loss: 1.3008754253387451 - accuracy: 0.5359399914741516 - val_loss: 1.4134141206741333 - val_accuracy: 0.4993000030517578 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [648, 1476, 1374, 640], total neurons: 4138\n",
            "Before pruning:\n",
            "loss: 1.2874760627746582 - accuracy: 0.5396599769592285 - val_loss: 1.4152814149856567 - val_accuracy: 0.5009999871253967 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [648, 1476, 1374, 640], total neurons: 4138\n",
            "After pruning:\n",
            "loss: 1.2857444286346436 - accuracy: 0.5406200289726257 - val_loss: 1.4132248163223267 - val_accuracy: 0.5033000111579895 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [465, 1050, 888, 369], total neurons: 2772\n",
            "##########################################################\n",
            "Epoch 26/50\n",
            "Before growing:\n",
            "loss: 1.2857444286346436 - accuracy: 0.5406200289726257 - val_loss: 1.4132248163223267 - val_accuracy: 0.5033000111579895 - penalty: 1.0000000000000002e-06\n",
            "hidden layer sizes: [465, 1050, 888, 369], total neurons: 2772\n",
            "After growing:\n",
            "loss: 1.2869131565093994 - accuracy: 0.5394200086593628 - val_loss: 1.4135876893997192 - val_accuracy: 0.5013999938964844 - penalty: 1.0000000000000002e-07\n",
            "hidden layer sizes: [697, 1575, 1332, 569], total neurons: 4173\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-47001d43738a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\nmodel = SSModel(layer_sizes=[3072, 300, 300, 300, 300, 10], activation='selu', regularization_penalty=0.0001, regularization_method='weighted_l1', kernel_initializer='lecun_normal')\\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.0002)\\n\\ntrain_model(model, X_train_norm, y_train, optimizer, epochs, self_scaling_epochs, batch_size, min_new_neurons, validation_data=(X_test_norm, y_test), regularization_penalty_multiplier=0.1, growth_percentage=0.5)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-53>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-1aef28cd5fb8>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, x, y, optimizer, epochs, self_scaling_epochs, batch_size, min_new_neurons, validation_data, pruning_threshold, regularization_penalty_multiplier, growth_percentage, print_neurons)\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mloss_value\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1068\u001b[0m     \u001b[0mflat_sources\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_handle_or_self\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mflat_sources\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1070\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbackprop_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIsTrainable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1071\u001b[0m         logging.vlog(\n\u001b[1;32m   1072\u001b[0m             \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWARN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"The dtype of the source tensor must be \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop_util.py\u001b[0m in \u001b[0;36mIsTrainable\u001b[0;34m(tensor_or_dtype)\u001b[0m\n\u001b[1;32m     59\u001b[0m   return dtype.base_dtype in (dtypes.float16, dtypes.float32, dtypes.float64,\n\u001b[1;32m     60\u001b[0m                               \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomplex64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomplex128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                               dtypes.resource, dtypes.variant, dtypes.bfloat16)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py\u001b[0m in \u001b[0;36m__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_type_enum\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_type_enum\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__ne__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5YPYKgWoHRJ"
      },
      "source": [
        "## Glass dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwLrPng0jJ_r"
      },
      "source": [
        "**TODO test training while increasing the size of the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByLRkOAPoGcc"
      },
      "source": [
        "epochs = 60\n",
        "self_scaling_epochs = 40\n",
        "batch_size = 32\n",
        "min_new_neurons = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "n1E3YM25oGh3",
        "outputId": "b67d6631-9f98-42b9-fa40-c3756412992e"
      },
      "source": [
        "%%time\n",
        "\n",
        "model = SSModel(layer_sizes=[10, 100, 100, 100, 100, 8], activation='selu', regularization_penalty=0.001, \n",
        "                regularization_method='weighted_l1', kernel_initializer='lecun_normal')\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.002)\n",
        "\n",
        "train_model(model, X_train_norm, y_train, optimizer, epochs, self_scaling_epochs, batch_size, \n",
        "            min_new_neurons, validation_data=(X_test_norm, y_test), pruning_threshold=0.01, print_neurons=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "##########################################################\n",
            "Epoch 1/60\n",
            "Before growing:\n",
            "loss: 2.4186086654663086 - accuracy: 0.12751677632331848 - val_loss: 2.2934510707855225 - val_accuracy: 0.13846154510974884 - penalty: 0.001\n",
            "hidden layer sizes: [100, 100, 100, 100], total neurons: 400\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "After growing:\n",
            "loss: 2.6235435009002686 - accuracy: 0.20134228467941284 - val_loss: 2.5807840824127197 - val_accuracy: 0.26153847575187683 - penalty: 0.001\n",
            "hidden layer sizes: [150, 150, 150, 150], total neurons: 600\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "Before pruning:\n",
            "loss: 0.8358197808265686 - accuracy: 0.7583892345428467 - val_loss: 0.885572075843811 - val_accuracy: 0.692307710647583 - penalty: 0.001\n",
            "hidden layer sizes: [150, 150, 150, 150], total neurons: 600\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "After pruning:\n",
            "loss: 0.8358197808265686 - accuracy: 0.7583892345428467 - val_loss: 0.885572075843811 - val_accuracy: 0.692307710647583 - penalty: 0.001\n",
            "hidden layer sizes: [150, 150, 150, 150], total neurons: 600\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "##########################################################\n",
            "Epoch 2/60\n",
            "Before growing:\n",
            "loss: 0.8358197808265686 - accuracy: 0.7583892345428467 - val_loss: 0.885572075843811 - val_accuracy: 0.692307710647583 - penalty: 0.001\n",
            "hidden layer sizes: [150, 150, 150, 150], total neurons: 600\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "After growing:\n",
            "loss: 1.1441763639450073 - accuracy: 0.6107382774353027 - val_loss: 1.1953297853469849 - val_accuracy: 0.5692307949066162 - penalty: 0.001\n",
            "hidden layer sizes: [200, 200, 200, 200], total neurons: 800\n",
            "11111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "11111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "11111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "11111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "Before pruning:\n",
            "loss: 0.6350050568580627 - accuracy: 0.8053691387176514 - val_loss: 0.7342689037322998 - val_accuracy: 0.7692307829856873 - penalty: 0.001\n",
            "hidden layer sizes: [200, 200, 200, 200], total neurons: 800\n",
            "11111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "11111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "11111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "11111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "After pruning:\n",
            "loss: 0.6350050568580627 - accuracy: 0.8053691387176514 - val_loss: 0.7342689037322998 - val_accuracy: 0.7692307829856873 - penalty: 0.001\n",
            "hidden layer sizes: [200, 200, 200, 200], total neurons: 800\n",
            "11111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "11111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "11111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "11111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "##########################################################\n",
            "Epoch 3/60\n",
            "Before growing:\n",
            "loss: 0.6350050568580627 - accuracy: 0.8053691387176514 - val_loss: 0.7342689037322998 - val_accuracy: 0.7692307829856873 - penalty: 0.001\n",
            "hidden layer sizes: [200, 200, 200, 200], total neurons: 800\n",
            "11111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "11111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "11111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "11111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "After growing:\n",
            "loss: 0.977018415927887 - accuracy: 0.6711409687995911 - val_loss: 1.415647268295288 - val_accuracy: 0.6153846383094788 - penalty: 0.001\n",
            "hidden layer sizes: [250, 250, 250, 250], total neurons: 1000\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "Before pruning:\n",
            "loss: 0.5382381081581116 - accuracy: 0.8456375598907471 - val_loss: 0.7194244861602783 - val_accuracy: 0.7846153974533081 - penalty: 0.001\n",
            "hidden layer sizes: [250, 250, 250, 250], total neurons: 1000\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "After pruning:\n",
            "loss: 0.5382381081581116 - accuracy: 0.8456375598907471 - val_loss: 0.7194244861602783 - val_accuracy: 0.7846153974533081 - penalty: 0.001\n",
            "hidden layer sizes: [250, 250, 250, 250], total neurons: 1000\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "##########################################################\n",
            "Epoch 4/60\n",
            "Before growing:\n",
            "loss: 0.5382381081581116 - accuracy: 0.8456375598907471 - val_loss: 0.7194244861602783 - val_accuracy: 0.7846153974533081 - penalty: 0.001\n",
            "hidden layer sizes: [250, 250, 250, 250], total neurons: 1000\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "After growing:\n",
            "loss: 0.8627102971076965 - accuracy: 0.7785235047340393 - val_loss: 0.9150302410125732 - val_accuracy: 0.7692307829856873 - penalty: 0.001\n",
            "hidden layer sizes: [300, 300, 300, 300], total neurons: 1200\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "Before pruning:\n",
            "loss: 0.4575343728065491 - accuracy: 0.899328887462616 - val_loss: 0.6736072301864624 - val_accuracy: 0.7384615540504456 - penalty: 0.001\n",
            "hidden layer sizes: [300, 300, 300, 300], total neurons: 1200\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "After pruning:\n",
            "loss: 0.4575343728065491 - accuracy: 0.899328887462616 - val_loss: 0.6736072301864624 - val_accuracy: 0.7384615540504456 - penalty: 0.001\n",
            "hidden layer sizes: [300, 300, 300, 300], total neurons: 1200\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "##########################################################\n",
            "Epoch 5/60\n",
            "Before growing:\n",
            "loss: 0.4575343728065491 - accuracy: 0.899328887462616 - val_loss: 0.6736072301864624 - val_accuracy: 0.7384615540504456 - penalty: 0.001\n",
            "hidden layer sizes: [300, 300, 300, 300], total neurons: 1200\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "After growing:\n",
            "loss: 0.8444935083389282 - accuracy: 0.7785235047340393 - val_loss: 1.0429041385650635 - val_accuracy: 0.6615384817123413 - penalty: 0.001\n",
            "hidden layer sizes: [360, 360, 360, 360], total neurons: 1440\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "Before pruning:\n",
            "loss: 0.4172387421131134 - accuracy: 0.8791946172714233 - val_loss: 0.5986689925193787 - val_accuracy: 0.8461538553237915 - penalty: 0.001\n",
            "hidden layer sizes: [360, 360, 360, 360], total neurons: 1440\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "After pruning:\n",
            "loss: 0.4172387421131134 - accuracy: 0.8791946172714233 - val_loss: 0.5986689925193787 - val_accuracy: 0.8461538553237915 - penalty: 0.001\n",
            "hidden layer sizes: [360, 360, 360, 360], total neurons: 1440\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "##########################################################\n",
            "Epoch 6/60\n",
            "Before growing:\n",
            "loss: 0.4172387421131134 - accuracy: 0.8791946172714233 - val_loss: 0.5986689925193787 - val_accuracy: 0.8461538553237915 - penalty: 0.001\n",
            "hidden layer sizes: [360, 360, 360, 360], total neurons: 1440\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "After growing:\n",
            "loss: 0.7381306886672974 - accuracy: 0.7852349281311035 - val_loss: 0.8912801742553711 - val_accuracy: 0.7384615540504456 - penalty: 0.001\n",
            "hidden layer sizes: [432, 432, 432, 432], total neurons: 1728\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "Before pruning:\n",
            "loss: 0.42187008261680603 - accuracy: 0.8724831938743591 - val_loss: 0.5672515630722046 - val_accuracy: 0.8153846263885498 - penalty: 0.001\n",
            "hidden layer sizes: [432, 432, 432, 432], total neurons: 1728\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111211111111111111111111111111111121111111121111111111111112111111112111111111121111111111111121111211111111112121111122111221111221212211111121121211111212112121121\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111121121111111111121111111111111111111122121222112111111121221112111121112212111121121122222212221121111111\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111112111111111111111111111111111111111111111111111111111111111111121111111111111111111111111111111111111111111111112111111111212121211122211111111112212221212122112122111212211111111211121211\n",
            "After pruning:\n",
            "loss: 0.42187008261680603 - accuracy: 0.8724831938743591 - val_loss: 0.5672515630722046 - val_accuracy: 0.8153846263885498 - penalty: 0.001\n",
            "hidden layer sizes: [432, 432, 432, 432], total neurons: 1728\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111211111111111111111111111111111121111111121111111111111112111111112111111111121111111111111121111211111111112121111122111221111221212211111121121211111212112121121\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111121121111111111121111111111111111111122121222112111111121221112111121112212111121121122222212221121111111\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111112111111111111111111111111111111111111111111111111111111111111121111111111111111111111111111111111111111111111112111111111212121211122211111111112212221212122112122111212211111111211121211\n",
            "##########################################################\n",
            "Epoch 7/60\n",
            "Before growing:\n",
            "loss: 0.42187008261680603 - accuracy: 0.8724831938743591 - val_loss: 0.5672515630722046 - val_accuracy: 0.8153846263885498 - penalty: 0.001\n",
            "hidden layer sizes: [432, 432, 432, 432], total neurons: 1728\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111211111111111111111111111111111121111111121111111111111112111111112111111111121111111111111121111211111111112121111122111221111221212211111121121211111212112121121\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111121121111111111121111111111111111111122121222112111111121221112111121112212111121121122222212221121111111\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111112111111111111111111111111111111111111111111111111111111111111121111111111111111111111111111111111111111111111112111111111212121211122211111111112212221212122112122111212211111111211121211\n",
            "After growing:\n",
            "loss: 0.6194885969161987 - accuracy: 0.8120805621147156 - val_loss: 0.8461720943450928 - val_accuracy: 0.7384615540504456 - penalty: 0.001\n",
            "hidden layer sizes: [518, 518, 518, 518], total neurons: 2072\n",
            "11111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "11111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111121111111111111111111111111111112111111112111111111111111211111111211111111112111111111111112111121111111111212111112211122111122121221111112112121111121211212112122222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "11111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111112112111111111112111111111111111111112212122211211111112122111211112111221211112112112222221222112111111122222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "11111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111211111111111111111111111111111111111111111111111111111111111112111111111111111111111111111111111111111111111111211111111121212121112221111111111221222121212211212211121221111111121112121122222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "Before pruning:\n",
            "loss: 0.36807429790496826 - accuracy: 0.8791946172714233 - val_loss: 0.5460952520370483 - val_accuracy: 0.892307698726654 - penalty: 0.001\n",
            "hidden layer sizes: [518, 518, 518, 518], total neurons: 2072\n",
            "11111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "11111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111112111121211112111111111112111121211111211212111122222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "11111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111121122121112112111112111211111112111111121111112112222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "11111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111121111111111111121111111111111111111121111122111212111111111121111121111112111211112222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "After pruning:\n",
            "loss: 0.36807429790496826 - accuracy: 0.8791946172714233 - val_loss: 0.5460952520370483 - val_accuracy: 0.892307698726654 - penalty: 0.001\n",
            "hidden layer sizes: [518, 518, 518, 518], total neurons: 2072\n",
            "11111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "11111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111112111121211112111111111112111121211111211212111122222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "11111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111121122121112112111112111211111112111111121111112112222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "11111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111121111111111111121111111111111111111121111122111212111111111121111121111112111211112222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "##########################################################\n",
            "Epoch 8/60\n",
            "Before growing:\n",
            "loss: 0.36807429790496826 - accuracy: 0.8791946172714233 - val_loss: 0.5460952520370483 - val_accuracy: 0.892307698726654 - penalty: 0.001\n",
            "hidden layer sizes: [518, 518, 518, 518], total neurons: 2072\n",
            "11111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "11111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111112111121211112111111111112111121211111211212111122222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "11111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111121122121112112111112111211111112111111121111112112222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "11111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111121111111111111121111111111111111111121111122111212111111111121111121111112111211112222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "After growing:\n",
            "loss: 0.5734967589378357 - accuracy: 0.7785235047340393 - val_loss: 0.7484609484672546 - val_accuracy: 0.692307710647583 - penalty: 0.001\n",
            "hidden layer sizes: [621, 621, 621, 621], total neurons: 2484\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111121111212111121111111111121111212111112112121111222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111211221211121121111121112111111121111111211111121122222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111211111111111111211111111111111111111211111221112121111111111211111211111121112111122222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "Before pruning:\n",
            "loss: 0.30944469571113586 - accuracy: 0.9530201554298401 - val_loss: 0.5048856139183044 - val_accuracy: 0.892307698726654 - penalty: 0.001\n",
            "hidden layer sizes: [621, 621, 621, 621], total neurons: 2484\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111211111121121212111121211111111212111121111112222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111121111212112112211111111121121111221111111221111212222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111211111111112211122111111111221211211111111221211121211111112222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "After pruning:\n",
            "loss: 0.30944469571113586 - accuracy: 0.9530201554298401 - val_loss: 0.5048856139183044 - val_accuracy: 0.892307698726654 - penalty: 0.001\n",
            "hidden layer sizes: [621, 621, 621, 621], total neurons: 2484\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111211111121121212111121211111111212111121111112222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111121111212112112211111111121121111221111111221111212222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111211111111112211122111111111221211211111111221211121211111112222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "##########################################################\n",
            "Epoch 9/60\n",
            "Before growing:\n",
            "loss: 0.30944469571113586 - accuracy: 0.9530201554298401 - val_loss: 0.5048856139183044 - val_accuracy: 0.892307698726654 - penalty: 0.001\n",
            "hidden layer sizes: [621, 621, 621, 621], total neurons: 2484\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111211111121121212111121211111111212111121111112222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111121111212112112211111111121121111221111111221111212222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111211111111112211122111111111221211211111111221211121211111112222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "After growing:\n",
            "loss: 0.5416628122329712 - accuracy: 0.8590604066848755 - val_loss: 0.8331934809684753 - val_accuracy: 0.7384615540504456 - penalty: 0.001\n",
            "hidden layer sizes: [745, 745, 745, 745], total neurons: 2980\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111112111111211212121111212111111112121111211111122222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111211112121121122111111111211211112211111112211112122222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111112111111111122111221111111112212112111111112212111212111111122222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "Before pruning:\n",
            "loss: 0.3596351444721222 - accuracy: 0.8926174640655518 - val_loss: 0.5275224447250366 - val_accuracy: 0.8153846263885498 - penalty: 0.001\n",
            "hidden layer sizes: [745, 745, 745, 745], total neurons: 2980\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111211111111211111111111111122111111211111121111112222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111121111111111111111211111112111111111111111111111112222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111112111111111111111111111222112212112112222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "After pruning:\n",
            "loss: 0.3596351444721222 - accuracy: 0.8926174640655518 - val_loss: 0.5275224447250366 - val_accuracy: 0.8153846263885498 - penalty: 0.001\n",
            "hidden layer sizes: [745, 745, 745, 745], total neurons: 2980\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111211111111211111111111111122111111211111121111112222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111121111111111111111211111112111111111111111111111112222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111112111111111111111111111222112212112112222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "##########################################################\n",
            "Epoch 10/60\n",
            "Before growing:\n",
            "loss: 0.3596351444721222 - accuracy: 0.8926174640655518 - val_loss: 0.5275224447250366 - val_accuracy: 0.8153846263885498 - penalty: 0.001\n",
            "hidden layer sizes: [745, 745, 745, 745], total neurons: 2980\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111211111111211111111111111122111111211111121111112222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111121111111111111111211111112111111111111111111111112222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111112111111111111111111111222112212112112222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "After growing:\n",
            "loss: 0.7632456421852112 - accuracy: 0.7248322367668152 - val_loss: 0.8803303241729736 - val_accuracy: 0.6615384817123413 - penalty: 0.001\n",
            "hidden layer sizes: [894, 894, 894, 894], total neurons: 3576\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111121111111121111111111111112211111121111112111111222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111112111111111111111121111111211111111111111111111111222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111211111111111111111111122211221211211222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "Before pruning:\n",
            "loss: 0.3568958640098572 - accuracy: 0.8926174640655518 - val_loss: 0.5533744096755981 - val_accuracy: 0.8307692408561707 - penalty: 0.001\n",
            "hidden layer sizes: [894, 894, 894, 894], total neurons: 3576\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111122222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111121111111111111111111111111122222222222122222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111122222221222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "After pruning:\n",
            "loss: 0.3568958640098572 - accuracy: 0.8926174640655518 - val_loss: 0.5533744096755981 - val_accuracy: 0.8307692408561707 - penalty: 0.001\n",
            "hidden layer sizes: [894, 894, 894, 894], total neurons: 3576\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111122222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111121111111111111111111111111122222222222122222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111122222221222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "##########################################################\n",
            "Epoch 11/60\n",
            "Before growing:\n",
            "loss: 0.3568958640098572 - accuracy: 0.8926174640655518 - val_loss: 0.5533744096755981 - val_accuracy: 0.8307692408561707 - penalty: 0.001\n",
            "hidden layer sizes: [894, 894, 894, 894], total neurons: 3576\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111122222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111121111111111111111111111111122222222222122222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111122222221222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "After growing:\n",
            "loss: 0.6938759684562683 - accuracy: 0.7718120813369751 - val_loss: 0.8858904242515564 - val_accuracy: 0.7230769395828247 - penalty: 0.001\n",
            "hidden layer sizes: [1072, 1072, 1072, 1072], total neurons: 4288\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111211111111111111111111111111222222222221222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111222222212222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "Before pruning:\n",
            "loss: 0.3545073866844177 - accuracy: 0.9463087320327759 - val_loss: 0.5724414587020874 - val_accuracy: 0.8307692408561707 - penalty: 0.001\n",
            "hidden layer sizes: [1072, 1072, 1072, 1072], total neurons: 4288\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "1111111111111111111111111111111111111111111111111111111111111111121111111111111111111111111111111111222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111211111111111111111111111111222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "1111111111111111111111111111111111112111111111111111111111111111111111111111111111111111111111111111222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "After pruning:\n",
            "loss: 0.3545073866844177 - accuracy: 0.9463087320327759 - val_loss: 0.5724414587020874 - val_accuracy: 0.8307692408561707 - penalty: 0.001\n",
            "hidden layer sizes: [1072, 1072, 1072, 1072], total neurons: 4288\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "1111111111111111111111111111111111111111111111111111111111111111121111111111111111111111111111111111222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111211111111111111111111111111222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "1111111111111111111111111111111111112111111111111111111111111111111111111111111111111111111111111111222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "##########################################################\n",
            "Epoch 12/60\n",
            "Before growing:\n",
            "loss: 0.3545073866844177 - accuracy: 0.9463087320327759 - val_loss: 0.5724414587020874 - val_accuracy: 0.8307692408561707 - penalty: 0.001\n",
            "hidden layer sizes: [1072, 1072, 1072, 1072], total neurons: 4288\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "1111111111111111111111111111111111111111111111111111111111111111121111111111111111111111111111111111222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111211111111111111111111111111222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "1111111111111111111111111111111111112111111111111111111111111111111111111111111111111111111111111111222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "After growing:\n",
            "loss: 0.6075555682182312 - accuracy: 0.8053691387176514 - val_loss: 0.9168964624404907 - val_accuracy: 0.6307692527770996 - penalty: 0.001\n",
            "hidden layer sizes: [1286, 1286, 1286, 1286], total neurons: 5144\n",
            "11111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "11111111111111111111111111111111111111111111111111111111111111111211111111111111111111111111111111112222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "11111111111111111111111111111111111111111111111111111111111111111111111112111111111111111111111111112222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "11111111111111111111111111111111111121111111111111111111111111111111111111111111111111111111111111112222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "Before pruning:\n",
            "loss: 0.4730416536331177 - accuracy: 0.7986577153205872 - val_loss: 0.6598789095878601 - val_accuracy: 0.7538461685180664 - penalty: 0.001\n",
            "hidden layer sizes: [1286, 1286, 1286, 1286], total neurons: 5144\n",
            "11111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "11111111111111111111111211111111111111121111121111121111211111111211111111111121111111111211111111112222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "11111111111111111111111111111111111111121111111111111111111111111111111112111111211111111121211111112222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "11111111111111111111111111111111111121112111111121111111211121111111111211111111121111111111111211112222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "After pruning:\n",
            "loss: 0.4730416536331177 - accuracy: 0.7986577153205872 - val_loss: 0.6598789095878601 - val_accuracy: 0.7538461685180664 - penalty: 0.001\n",
            "hidden layer sizes: [1286, 1286, 1286, 1286], total neurons: 5144\n",
            "11111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "11111111111111111111111211111111111111121111121111121111211111111211111111111121111111111211111111112222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "11111111111111111111111111111111111111121111111111111111111111111111111112111111211111111121211111112222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "11111111111111111111111111111111111121112111111121111111211121111111111211111111121111111111111211112222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "##########################################################\n",
            "Epoch 13/60\n",
            "Before growing:\n",
            "loss: 0.4730416536331177 - accuracy: 0.7986577153205872 - val_loss: 0.6598789095878601 - val_accuracy: 0.7538461685180664 - penalty: 0.001\n",
            "hidden layer sizes: [1286, 1286, 1286, 1286], total neurons: 5144\n",
            "11111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "11111111111111111111111211111111111111121111121111121111211111111211111111111121111111111211111111112222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "11111111111111111111111111111111111111121111111111111111111111111111111112111111211111111121211111112222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "11111111111111111111111111111111111121112111111121111111211121111111111211111111121111111111111211112222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "After growing:\n",
            "loss: 0.45957738161087036 - accuracy: 0.8322147727012634 - val_loss: 0.7604051232337952 - val_accuracy: 0.7384615540504456 - penalty: 0.001\n",
            "hidden layer sizes: [1543, 1543, 1543, 1543], total neurons: 6172\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "1111111111111111111111121111111111111112111112111112111121111111121111111111112111111111121111111111222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "1111111111111111111111111111111111111112111111111111111111111111111111111211111121111111112121111111222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "1111111111111111111111111111111111112111211111112111111121112111111111121111111112111111111111121111222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "Before pruning:\n",
            "loss: 0.4031318128108978 - accuracy: 0.8791946172714233 - val_loss: 0.6099794507026672 - val_accuracy: 0.800000011920929 - penalty: 0.001\n",
            "hidden layer sizes: [1543, 1543, 1543, 1543], total neurons: 6172\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "1111111111111111111111121121211111122112221122112122212121122221121212121111212112121112121111112212222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "1111111111111111111111111111112111111112111111121112111121122112112211221222222121221122212122121121222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "1111111111111111111111111111111111212211211111212121111122112111111212221111112212111111111122221111222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "After pruning:\n",
            "loss: 0.4031318128108978 - accuracy: 0.8791946172714233 - val_loss: 0.6099794507026672 - val_accuracy: 0.800000011920929 - penalty: 0.001\n",
            "hidden layer sizes: [1543, 1543, 1543, 1543], total neurons: 6172\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "1111111111111111111111121121211111122112221122112122212121122221121212121111212112121112121111112212222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "1111111111111111111111111111112111111112111111121112111121122112112211221222222121221122212122121121222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "1111111111111111111111111111111111212211211111212121111122112111111212221111112212111111111122221111222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "##########################################################\n",
            "Epoch 14/60\n",
            "Before growing:\n",
            "loss: 0.4031318128108978 - accuracy: 0.8791946172714233 - val_loss: 0.6099794507026672 - val_accuracy: 0.800000011920929 - penalty: 0.001\n",
            "hidden layer sizes: [1543, 1543, 1543, 1543], total neurons: 6172\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "1111111111111111111111121121211111122112221122112122212121122221121212121111212112121112121111112212222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "1111111111111111111111111111112111111112111111121112111121122112112211221222222121221122212122121121222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "1111111111111111111111111111111111212211211111212121111122112111111212221111112212111111111122221111222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "After growing:\n",
            "loss: 0.535520076751709 - accuracy: 0.791946291923523 - val_loss: 0.6616605520248413 - val_accuracy: 0.7538461685180664 - penalty: 0.001\n",
            "hidden layer sizes: [1851, 1851, 1851, 1851], total neurons: 7404\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "111111111111111111111112112121111112211222112211212221212112222112121212111121211212111212111111221222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "111111111111111111111111111111211111111211111112111211112112211211221122122222212122112221212212112122222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "111111111111111111111111111111111121221121111121212111112211211111121222111111221211111111112222111122222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "Before pruning:\n",
            "loss: 0.5421507358551025 - accuracy: 0.8590604066848755 - val_loss: 0.7577247023582458 - val_accuracy: 0.8307692408561707 - penalty: 0.001\n",
            "hidden layer sizes: [1851, 1851, 1851, 1851], total neurons: 7404\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "111111111111111112121112112222122122211222122222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "111111111111111112112111111121222121221221122222222221212222222222222222222222222222222222222222212222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "111111111111111111112111222112121121222221222221222212222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "After pruning:\n",
            "loss: 0.5421507358551025 - accuracy: 0.8590604066848755 - val_loss: 0.7577247023582458 - val_accuracy: 0.8307692408561707 - penalty: 0.001\n",
            "hidden layer sizes: [1851, 1851, 1851, 1851], total neurons: 7404\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "111111111111111112121112112222122122211222122222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "111111111111111112112111111121222121221221122222222221212222222222222222222222222222222222222222212222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "111111111111111111112111222112121121222221222221222212222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "##########################################################\n",
            "Epoch 15/60\n",
            "Before growing:\n",
            "loss: 0.5421507358551025 - accuracy: 0.8590604066848755 - val_loss: 0.7577247023582458 - val_accuracy: 0.8307692408561707 - penalty: 0.001\n",
            "hidden layer sizes: [1851, 1851, 1851, 1851], total neurons: 7404\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "111111111111111112121112112222122122211222122222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "111111111111111112112111111121222121221221122222222221212222222222222222222222222222222222222222212222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "111111111111111111112111222112121121222221222221222212222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "After growing:\n",
            "loss: 0.598572313785553 - accuracy: 0.8389261960983276 - val_loss: 0.8387911915779114 - val_accuracy: 0.7538461685180664 - penalty: 0.001\n",
            "hidden layer sizes: [2221, 2221, 2221, 2221], total neurons: 8884\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "1111111111111111121211121122221221222112221222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "1111111111111111121121111111212221212212211222222222212122222222222222222222222222222222222222222122222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "1111111111111111111121112221121211212222212222212222122222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "Before pruning:\n",
            "loss: 0.5019102096557617 - accuracy: 0.7986577153205872 - val_loss: 0.7800610661506653 - val_accuracy: 0.6615384817123413 - penalty: 0.001\n",
            "hidden layer sizes: [2221, 2221, 2221, 2221], total neurons: 8884\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "1111111221211212122212222222221222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "1111111111111121121121121121222222212222222222222222222222222222222222222222222222222222222222222122222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "1111111111111122111122112221222221222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "After pruning:\n",
            "loss: 0.5019102096557617 - accuracy: 0.7986577153205872 - val_loss: 0.7800610661506653 - val_accuracy: 0.6615384817123413 - penalty: 0.001\n",
            "hidden layer sizes: [2221, 2221, 2221, 2221], total neurons: 8884\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "1111111221211212122212222222221222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "1111111111111121121121121121222222212222222222222222222222222222222222222222222222222222222222222122222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "1111111111111122111122112221222221222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "##########################################################\n",
            "Epoch 16/60\n",
            "Before growing:\n",
            "loss: 0.5019102096557617 - accuracy: 0.7986577153205872 - val_loss: 0.7800610661506653 - val_accuracy: 0.6615384817123413 - penalty: 0.001\n",
            "hidden layer sizes: [2221, 2221, 2221, 2221], total neurons: 8884\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "1111111221211212122212222222221222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "1111111111111121121121121121222222212222222222222222222222222222222222222222222222222222222222222122222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "1111111111111122111122112221222221222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "After growing:\n",
            "loss: 0.7579154968261719 - accuracy: 0.7248322367668152 - val_loss: 1.074813961982727 - val_accuracy: 0.6461538672447205 - penalty: 0.001\n",
            "hidden layer sizes: [2665, 2665, 2665, 2665], total neurons: 10660\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "1111111221211212122212222222221222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "1111111111111121121121121121222222212222222222222222222222222222222222222222222222222222222222222122222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "1111111111111122111122112221222221222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "Before pruning:\n",
            "loss: 0.5111637711524963 - accuracy: 0.8389261960983276 - val_loss: 0.707014262676239 - val_accuracy: 0.7846153974533081 - penalty: 0.001\n",
            "hidden layer sizes: [2665, 2665, 2665, 2665], total neurons: 10660\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "1111111221221212222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "1111111111111121121221122121222222212222222222222222222222222222222222222222222222222222222222222122222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "1111111111111122111122112222222221222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "After pruning:\n",
            "loss: 0.5111637711524963 - accuracy: 0.8389261960983276 - val_loss: 0.707014262676239 - val_accuracy: 0.7846153974533081 - penalty: 0.001\n",
            "hidden layer sizes: [2665, 2665, 2665, 2665], total neurons: 10660\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "1111111221221212222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "1111111111111121121221122121222222212222222222222222222222222222222222222222222222222222222222222122222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "1111111111111122111122112222222221222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "##########################################################\n",
            "Epoch 17/60\n",
            "Before growing:\n",
            "loss: 0.5111637711524963 - accuracy: 0.8389261960983276 - val_loss: 0.707014262676239 - val_accuracy: 0.7846153974533081 - penalty: 0.001\n",
            "hidden layer sizes: [2665, 2665, 2665, 2665], total neurons: 10660\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "1111111221221212222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "1111111111111121121221122121222222212222222222222222222222222222222222222222222222222222222222222122222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "1111111111111122111122112222222221222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "After growing:\n",
            "loss: 0.818561851978302 - accuracy: 0.7114093899726868 - val_loss: 1.0060163736343384 - val_accuracy: 0.692307710647583 - penalty: 0.001\n",
            "hidden layer sizes: [3198, 3198, 3198, 3198], total neurons: 12792\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "111111122122121222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "111111111111112112122112212122222221222222222222222222222222222222222222222222222222222222222222212222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "111111111111112211112211222222222122222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "Before pruning:\n",
            "loss: 0.8255738019943237 - accuracy: 0.7248322367668152 - val_loss: 0.9047858119010925 - val_accuracy: 0.7076923251152039 - penalty: 0.001\n",
            "hidden layer sizes: [3198, 3198, 3198, 3198], total neurons: 12792\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "111111122122221222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "111111112112112112122112222122222221222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "111111111111212211112211222222222122222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "After pruning:\n",
            "loss: 0.8255738019943237 - accuracy: 0.7248322367668152 - val_loss: 0.9047858119010925 - val_accuracy: 0.7076923251152039 - penalty: 0.001\n",
            "hidden layer sizes: [3198, 3198, 3198, 3198], total neurons: 12792\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "111111122122221222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "111111112112112112122112222122222221222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "111111111111212211112211222222222122222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "##########################################################\n",
            "Epoch 18/60\n",
            "Before growing:\n",
            "loss: 0.8255738019943237 - accuracy: 0.7248322367668152 - val_loss: 0.9047858119010925 - val_accuracy: 0.7076923251152039 - penalty: 0.001\n",
            "hidden layer sizes: [3198, 3198, 3198, 3198], total neurons: 12792\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "111111122122221222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "111111112112112112122112222122222221222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "111111111111212211112211222222222122222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "After growing:\n",
            "loss: 0.8242607116699219 - accuracy: 0.7583892345428467 - val_loss: 0.9866958856582642 - val_accuracy: 0.7538461685180664 - penalty: 0.001\n",
            "hidden layer sizes: [3837, 3837, 3837, 3837], total neurons: 15348\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "111111122122221222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "111111112112112112122112222122222221222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "111111111111212211112211222222222122222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "Before pruning:\n",
            "loss: 0.6420342326164246 - accuracy: 0.791946291923523 - val_loss: 0.7172613143920898 - val_accuracy: 0.8153846263885498 - penalty: 0.001\n",
            "hidden layer sizes: [3837, 3837, 3837, 3837], total neurons: 15348\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111211111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "111121122222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "111111112112112112122112222122222221222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "111111111111212211112212222222222122222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "After pruning:\n",
            "loss: 0.6420342326164246 - accuracy: 0.791946291923523 - val_loss: 0.7172613143920898 - val_accuracy: 0.8153846263885498 - penalty: 0.001\n",
            "hidden layer sizes: [3837, 3837, 3837, 3837], total neurons: 15348\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111211111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "111121122222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "111111112112112112122112222122222221222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "111111111111212211112212222222222122222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "##########################################################\n",
            "Epoch 19/60\n",
            "Before growing:\n",
            "loss: 0.6420342326164246 - accuracy: 0.791946291923523 - val_loss: 0.7172613143920898 - val_accuracy: 0.8153846263885498 - penalty: 0.001\n",
            "hidden layer sizes: [3837, 3837, 3837, 3837], total neurons: 15348\n",
            "111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111211111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "111121122222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "111111112112112112122112222122222221222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "111111111111212211112212222222222122222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "After growing:\n",
            "loss: 0.8454020619392395 - accuracy: 0.7315436005592346 - val_loss: 0.8297144770622253 - val_accuracy: 0.7846153974533081 - penalty: 0.001\n",
            "hidden layer sizes: [4604, 4604, 4604, 4604], total neurons: 18416\n",
            "11111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111121111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "11112112222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "11111111211211211212211222212222222122222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "11111111111121221111221222222222212222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "Before pruning:\n",
            "loss: 0.6457246541976929 - accuracy: 0.8255033493041992 - val_loss: 0.8156687021255493 - val_accuracy: 0.8153846263885498 - penalty: 0.001\n",
            "hidden layer sizes: [4604, 4604, 4604, 4604], total neurons: 18416\n",
            "11111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111121111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "11112112222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "11111111211211211212211222222222222122222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "11111111111121221121221222222222212222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "After pruning:\n",
            "loss: 0.6457246541976929 - accuracy: 0.8255033493041992 - val_loss: 0.8156687021255493 - val_accuracy: 0.8153846263885498 - penalty: 0.001\n",
            "hidden layer sizes: [4604, 4604, 4604, 4604], total neurons: 18416\n",
            "11111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111121111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "11112112222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "11111111211211211212211222222222222122222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "11111111111121221121221222222222212222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "##########################################################\n",
            "Epoch 20/60\n",
            "Before growing:\n",
            "loss: 0.6457246541976929 - accuracy: 0.8255033493041992 - val_loss: 0.8156687021255493 - val_accuracy: 0.8153846263885498 - penalty: 0.001\n",
            "hidden layer sizes: [4604, 4604, 4604, 4604], total neurons: 18416\n",
            "11111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111121111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "11112112222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "11111111211211211212211222222222222122222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "11111111111121221121221222222222212222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "After growing:\n",
            "loss: 0.7580564022064209 - accuracy: 0.6979866027832031 - val_loss: 0.8812553286552429 - val_accuracy: 0.6000000238418579 - penalty: 0.001\n",
            "hidden layer sizes: [5524, 5524, 5524, 5524], total neurons: 22096\n",
            "1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111112111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
            "1111211222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "1111111121121121121221122222222222212222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n",
            "1111111111112122112122122222222221222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ResourceExhaustedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-149-15327f82716e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\nmodel = SSModel(layer_sizes=[10, 100, 100, 100, 100, 8], activation='selu', regularization_penalty=0.001, \\n                regularization_method='weighted_l1', kernel_initializer='lecun_normal')\\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.002)\\n\\ntrain_model(model, X_train_norm, y_train, optimizer, epochs, self_scaling_epochs, batch_size, \\n            min_new_neurons, validation_data=(X_test_norm, y_test), pruning_threshold=0.01, print_neurons=True)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-53>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-99-da6d44c360d4>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, x, y, optimizer, epochs, self_scaling_epochs, batch_size, min_new_neurons, validation_data, pruning_threshold, regularization_penalty_multiplier, print_neurons)\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0mloss_value\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1078\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1079\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1080\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    157\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_grad.py\u001b[0m in \u001b[0;36m_SumGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m           \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_0_shape\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;31m# The shape and reduction indices are statically known, so we use a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mtile\u001b[0;34m(input, multiples, name)\u001b[0m\n\u001b[1;32m  11506\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11507\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 11508\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  11509\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11510\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6895\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6896\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6897\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6898\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[5525,5524] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Tile]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPE4AGtXg3Nw"
      },
      "source": [
        "## Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ue6f5dEe7rG"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(10, activation='selu', kernel_initializer='lecun_normal'),\n",
        "    tf.keras.layers.Dense(66, activation='selu', kernel_initializer='lecun_normal'),\n",
        "    tf.keras.layers.Dense(13, activation='selu', kernel_initializer='lecun_normal'),\n",
        "    tf.keras.layers.Dense(10, activation='selu', kernel_initializer='lecun_normal'),\n",
        "    tf.keras.layers.Dense(27, activation='selu', kernel_initializer='lecun_normal'),\n",
        "    tf.keras.layers.Dense(8, activation='softmax', kernel_initializer='lecun_normal'),\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJ5w_e-65EnL"
      },
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.002), metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjZ7lqpF5E2Z",
        "outputId": "1fa37125-aafa-4e98-d20f-4a4bbacc4b54"
      },
      "source": [
        "%%time\n",
        "\n",
        "model.fit(X_train_norm, y_train, epochs=60, validation_data=(X_test_norm, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "5/5 [==============================] - 1s 35ms/step - loss: 2.2734 - accuracy: 0.1342 - val_loss: 1.5871 - val_accuracy: 0.4462\n",
            "Epoch 2/60\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 1.5645 - accuracy: 0.5235 - val_loss: 1.4996 - val_accuracy: 0.5846\n",
            "Epoch 3/60\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 1.3287 - accuracy: 0.6644 - val_loss: 1.2981 - val_accuracy: 0.6154\n",
            "Epoch 4/60\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 1.0804 - accuracy: 0.7248 - val_loss: 1.0719 - val_accuracy: 0.6615\n",
            "Epoch 5/60\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.8965 - accuracy: 0.7584 - val_loss: 0.9357 - val_accuracy: 0.6923\n",
            "Epoch 6/60\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 0.7831 - accuracy: 0.7651 - val_loss: 0.8577 - val_accuracy: 0.7385\n",
            "Epoch 7/60\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.7072 - accuracy: 0.7852 - val_loss: 0.7962 - val_accuracy: 0.7538\n",
            "Epoch 8/60\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.6371 - accuracy: 0.7987 - val_loss: 0.7362 - val_accuracy: 0.8000\n",
            "Epoch 9/60\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.5732 - accuracy: 0.8121 - val_loss: 0.6947 - val_accuracy: 0.7692\n",
            "Epoch 10/60\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.5205 - accuracy: 0.8121 - val_loss: 0.6647 - val_accuracy: 0.7692\n",
            "Epoch 11/60\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.4738 - accuracy: 0.8456 - val_loss: 0.6302 - val_accuracy: 0.7846\n",
            "Epoch 12/60\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.4321 - accuracy: 0.8591 - val_loss: 0.6126 - val_accuracy: 0.7846\n",
            "Epoch 13/60\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.3955 - accuracy: 0.8725 - val_loss: 0.5946 - val_accuracy: 0.8308\n",
            "Epoch 14/60\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.3589 - accuracy: 0.9060 - val_loss: 0.5787 - val_accuracy: 0.8462\n",
            "Epoch 15/60\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.3273 - accuracy: 0.9262 - val_loss: 0.5506 - val_accuracy: 0.8462\n",
            "Epoch 16/60\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.2985 - accuracy: 0.9396 - val_loss: 0.5287 - val_accuracy: 0.8462\n",
            "Epoch 17/60\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.2738 - accuracy: 0.9530 - val_loss: 0.5097 - val_accuracy: 0.8615\n",
            "Epoch 18/60\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.2428 - accuracy: 0.9530 - val_loss: 0.5016 - val_accuracy: 0.8615\n",
            "Epoch 19/60\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.2219 - accuracy: 0.9664 - val_loss: 0.4873 - val_accuracy: 0.8769\n",
            "Epoch 20/60\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.2019 - accuracy: 0.9732 - val_loss: 0.4721 - val_accuracy: 0.8769\n",
            "Epoch 21/60\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.1804 - accuracy: 0.9732 - val_loss: 0.4580 - val_accuracy: 0.8769\n",
            "Epoch 22/60\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.1627 - accuracy: 0.9799 - val_loss: 0.4289 - val_accuracy: 0.8769\n",
            "Epoch 23/60\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.1490 - accuracy: 0.9933 - val_loss: 0.4191 - val_accuracy: 0.8769\n",
            "Epoch 24/60\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.1330 - accuracy: 0.9933 - val_loss: 0.4167 - val_accuracy: 0.8769\n",
            "Epoch 25/60\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.1215 - accuracy: 1.0000 - val_loss: 0.4152 - val_accuracy: 0.8769\n",
            "Epoch 26/60\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.1100 - accuracy: 1.0000 - val_loss: 0.4124 - val_accuracy: 0.8769\n",
            "Epoch 27/60\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.0996 - accuracy: 1.0000 - val_loss: 0.4091 - val_accuracy: 0.8769\n",
            "Epoch 28/60\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.0888 - accuracy: 1.0000 - val_loss: 0.3991 - val_accuracy: 0.8769\n",
            "Epoch 29/60\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.0818 - accuracy: 1.0000 - val_loss: 0.3935 - val_accuracy: 0.8769\n",
            "Epoch 30/60\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.0748 - accuracy: 1.0000 - val_loss: 0.3922 - val_accuracy: 0.8769\n",
            "Epoch 31/60\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.0682 - accuracy: 1.0000 - val_loss: 0.3990 - val_accuracy: 0.8769\n",
            "Epoch 32/60\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.0632 - accuracy: 1.0000 - val_loss: 0.4031 - val_accuracy: 0.8615\n",
            "Epoch 33/60\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.0573 - accuracy: 1.0000 - val_loss: 0.4029 - val_accuracy: 0.8769\n",
            "Epoch 34/60\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.0528 - accuracy: 1.0000 - val_loss: 0.3995 - val_accuracy: 0.8769\n",
            "Epoch 35/60\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.0486 - accuracy: 1.0000 - val_loss: 0.4009 - val_accuracy: 0.8769\n",
            "Epoch 36/60\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.0452 - accuracy: 1.0000 - val_loss: 0.4019 - val_accuracy: 0.8769\n",
            "Epoch 37/60\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.0423 - accuracy: 1.0000 - val_loss: 0.4074 - val_accuracy: 0.8769\n",
            "Epoch 38/60\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.0389 - accuracy: 1.0000 - val_loss: 0.4104 - val_accuracy: 0.8615\n",
            "Epoch 39/60\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.0365 - accuracy: 1.0000 - val_loss: 0.4077 - val_accuracy: 0.8769\n",
            "Epoch 40/60\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.0335 - accuracy: 1.0000 - val_loss: 0.4050 - val_accuracy: 0.8923\n",
            "Epoch 41/60\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.0313 - accuracy: 1.0000 - val_loss: 0.4057 - val_accuracy: 0.8923\n",
            "Epoch 42/60\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.0294 - accuracy: 1.0000 - val_loss: 0.4092 - val_accuracy: 0.8769\n",
            "Epoch 43/60\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.0275 - accuracy: 1.0000 - val_loss: 0.4143 - val_accuracy: 0.8462\n",
            "Epoch 44/60\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.0259 - accuracy: 1.0000 - val_loss: 0.4158 - val_accuracy: 0.8462\n",
            "Epoch 45/60\n",
            "5/5 [==============================] - 0s 11ms/step - loss: 0.0242 - accuracy: 1.0000 - val_loss: 0.4160 - val_accuracy: 0.8615\n",
            "Epoch 46/60\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.0228 - accuracy: 1.0000 - val_loss: 0.4185 - val_accuracy: 0.8615\n",
            "Epoch 47/60\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.0215 - accuracy: 1.0000 - val_loss: 0.4186 - val_accuracy: 0.8615\n",
            "Epoch 48/60\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.0202 - accuracy: 1.0000 - val_loss: 0.4240 - val_accuracy: 0.8462\n",
            "Epoch 49/60\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.0191 - accuracy: 1.0000 - val_loss: 0.4263 - val_accuracy: 0.8615\n",
            "Epoch 50/60\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.0182 - accuracy: 1.0000 - val_loss: 0.4235 - val_accuracy: 0.8615\n",
            "Epoch 51/60\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.0172 - accuracy: 1.0000 - val_loss: 0.4263 - val_accuracy: 0.8769\n",
            "Epoch 52/60\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.0164 - accuracy: 1.0000 - val_loss: 0.4312 - val_accuracy: 0.8615\n",
            "Epoch 53/60\n",
            "5/5 [==============================] - 0s 10ms/step - loss: 0.0155 - accuracy: 1.0000 - val_loss: 0.4315 - val_accuracy: 0.8615\n",
            "Epoch 54/60\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.0149 - accuracy: 1.0000 - val_loss: 0.4340 - val_accuracy: 0.8769\n",
            "Epoch 55/60\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.0141 - accuracy: 1.0000 - val_loss: 0.4347 - val_accuracy: 0.8615\n",
            "Epoch 56/60\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.0134 - accuracy: 1.0000 - val_loss: 0.4366 - val_accuracy: 0.8615\n",
            "Epoch 57/60\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.0128 - accuracy: 1.0000 - val_loss: 0.4378 - val_accuracy: 0.8615\n",
            "Epoch 58/60\n",
            "5/5 [==============================] - 0s 11ms/step - loss: 0.0123 - accuracy: 1.0000 - val_loss: 0.4400 - val_accuracy: 0.8615\n",
            "Epoch 59/60\n",
            "5/5 [==============================] - 0s 8ms/step - loss: 0.0117 - accuracy: 1.0000 - val_loss: 0.4391 - val_accuracy: 0.8615\n",
            "Epoch 60/60\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.0113 - accuracy: 1.0000 - val_loss: 0.4391 - val_accuracy: 0.8615\n",
            "CPU times: user 2.55 s, sys: 205 ms, total: 2.75 s\n",
            "Wall time: 2.95 s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8fc2392d50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLkoFgNf5Q4s"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}